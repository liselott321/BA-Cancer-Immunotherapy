{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tidytcells in /home/ubuntu/anaconda3/lib/python3.12/site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install tidytcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision of mhc and V/J values (gene or allele)\n",
    "precision = 'allele'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is not thread safe\n",
    "def create_folders_if_not_exists(folders):\n",
    "  for path in folders:\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = '../../data'\n",
    "pipeline_data_plain = f'{pipeline_data}/plain_datasets'\n",
    "pipeline_data_cleaned = f'{pipeline_data}/cleaned_datasets'\n",
    "pipeline_data_concatenated = f'{pipeline_data}/concatenated_datasets'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "pipeline_data_temp_bucket = f'{pipeline_data}/temp'\n",
    "\n",
    "pipeline_folders = [pipeline_data, pipeline_data_plain, pipeline_data_cleaned, pipeline_data_concatenated, pipeline_data_splitted, pipeline_data_temp_bucket]\n",
    "\n",
    "create_folders_if_not_exists(pipeline_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "IEDB_data_plain = f'{pipeline_data_plain}/IEDB'\n",
    "IEDB_data_cleaned = f'{pipeline_data_cleaned}/IEDB'\n",
    "IEDB_data_fitted = f'{pipeline_data_temp_bucket}/IEDB'\n",
    "\n",
    "IEDB_folders = [IEDB_data_plain, IEDB_data_cleaned, IEDB_data_fitted]\n",
    "create_folders_if_not_exists(IEDB_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook IEDB fit data\n",
    "path_prefix_plain = IEDB_data_plain\n",
    "path_prefix_fitted = IEDB_data_fitted\n",
    "mhc_I_input_beta = f\"{path_prefix_plain}/MHCI_IEDB_beta_export.csv\"\n",
    "mhc_I_output_beta = f\"{path_prefix_fitted}/IEDB_beta_fitted.csv\"\n",
    "mhc_I_input_paired = f\"{path_prefix_plain}/MHCI_IEDB_paired_export.csv\"\n",
    "mhc_I_output_paired = f\"{path_prefix_fitted}/IEDB_paired_fitted.csv\"\n",
    "\n",
    "# fit IEDB data\n",
    "%run data_scripts/IEDB/IEDB_fitted_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook IEDB clean data\n",
    "path_prefix_fitted = IEDB_data_fitted\n",
    "path_prefix_cleaned =  IEDB_data_cleaned\n",
    "fitted_file_beta = \"IEDB_beta_fitted.csv\"\n",
    "fitted_file_paired = \"IEDB_paired_fitted.csv\"\n",
    "cleaned_file_beta = \"IEDB_cleaned_data_beta.csv\"\n",
    "cleaned_file_paired = \"IEDB_cleaned_data_paired.csv\"\n",
    "\n",
    "# clean IEDB data\n",
    "%run data_scripts/IEDB/IEDB_clean_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEDB_cleaned_beta_output = f'{IEDB_data_cleaned}/{cleaned_file_beta}'\n",
    "IEDB_cleaned_paired_output = f'{IEDB_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "McPas_data_plain = f'{pipeline_data_plain}/McPas'\n",
    "McPas_data_cleaned = f'{pipeline_data_cleaned}/McPas'\n",
    "McPas_data_fitted = f'{pipeline_data_temp_bucket}/McPas'\n",
    "\n",
    "McPas_folders = [McPas_data_plain, McPas_data_cleaned, McPas_data_fitted]\n",
    "create_folders_if_not_exists(McPas_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook McPAS fit data\n",
    "input_file = f'{McPas_data_plain}/McPAS-TCR.csv'\n",
    "path_prefix_fitted = McPas_data_fitted\n",
    "fitted_file = 'McPAS_fitted.tsv'\n",
    "\n",
    "# fit McPAS data\n",
    "%run data_scripts/McPas-TCR/fit_data_mcpastcr_both.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 10078 entries\n",
      "whole dataframe has 13701 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 10078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52141/2652240660.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  mcpastcr_cleaned_both_df = mcpastcr_cleaned_both_df[~mask]\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook McPAS clean data\n",
    "fitted_input_file = f'{McPas_data_fitted}/{fitted_file}'\n",
    "path_prefix_cleaned = McPas_data_cleaned\n",
    "cleaned_file_paired = 'McPAS_cleaned_data_paired.tsv'\n",
    "cleaned_file_beta = 'McPAS_cleaned_data_beta.tsv'\n",
    "\n",
    "# clean McPAS data\n",
    "%run data_scripts/McPas-TCR/clean_data_mcpastcr_both.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "McPAS_cleaned_beta_output = f'{McPas_data_cleaned}/{cleaned_file_beta}'\n",
    "McPAS_cleaned_paired_output = f'{McPas_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VDJdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "VDJdb_data_plain = f'{pipeline_data_plain}/VDJdb'\n",
    "VDJdb_data_cleaned = f'{pipeline_data_cleaned}/VDJdb'\n",
    "VDJdb_data_fitted = f'{pipeline_data_temp_bucket}/VDJdb'\n",
    "\n",
    "VDJdb_folders = [VDJdb_data_plain, VDJdb_data_cleaned, VDJdb_data_fitted]\n",
    "create_folders_if_not_exists(VDJdb_folders)\n",
    "\n",
    "fitted_beta_file = 'VDJdb_beta_fitted.tsv'\n",
    "fitted_paired_file = 'VDJdb_paired_fitted.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data paired\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_paired_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_paired_file\n",
    "\n",
    "# fit paired VDJdb data\n",
    "%run data_scripts/VDJdb/fit_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data beta\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_beta_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_beta_file\n",
    "\n",
    "# fit beta VDJdb data\n",
    "%run data_scripts/VDJdb/fit_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 27414 entries\n",
      "whole dataframe has 28119 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 27414\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data paired\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_paired_file}'\n",
    "cleaned_file_paired = 'VDJdb_cleaned_data_paired.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'\n",
    "\n",
    "# clean paired VDJdb data\n",
    "%run data_scripts/VDJdb/clean_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 46507 entries\n",
      "whole dataframe has 49042 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 46507\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data beta\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_beta_file}'\n",
    "cleaned_file_beta = 'VDJdb_cleaned_data_beta.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "\n",
    "# clean beta VDJdb data\n",
    "%run data_scripts/VDJdb/clean_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDJdb_cleaned_beta_output = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "VDJdb_cleaned_paired_output = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pMTnet (beta data only!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kombinierte Datei gespeichert unter: ../../data/plain_datasets/pMTnet/beta_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Dateipfade\n",
    "train_path = \"../../data/plain_datasets/pMTnet/training_data.csv\"\n",
    "test_path = \"../../data/plain_datasets/pMTnet/testing_data.csv\"\n",
    "output_path = \"../../data/plain_datasets/pMTnet/beta_data.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_combined = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "df_combined.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Kombinierte Datei gespeichert unter: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "pMTnet_data_plain = f'{pipeline_data_plain}/pMTnet'\n",
    "pMTnet_data_cleaned = f'{pipeline_data_cleaned}/pMTnet'\n",
    "pMTnet_data_fitted = f'{pipeline_data_temp_bucket}/pMTnet'\n",
    "\n",
    "pMTnet_folders = [pMTnet_data_plain, pMTnet_data_cleaned, pMTnet_data_fitted]\n",
    "create_folders_if_not_exists(pMTnet_folders)\n",
    "fitted_combined_file = 'pMTnet_fitted_data_beta.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefittete Datei gespeichert unter: ../../data/temp/pMTnet/pMTnet_fitted_data_beta.tsv\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook pMTnet fit data combined\n",
    "input_file = f'{pMTnet_data_plain}/beta_data.csv'\n",
    "path_prefix_fitted = pMTnet_data_fitted\n",
    "fitted_file = fitted_combined_file\n",
    "\n",
    "# fit combined pMTnet data\n",
    "%run data_scripts/pMTnet/fit_data_pMTnet_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bereinigte Datei gespeichert unter: ../../data/cleaned_datasets/pMTnet/pMTnet_cleaned_data_beta.tsv\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook pMTnet clean data combined\n",
    "input_file = f'{pMTnet_data_fitted}/{fitted_combined_file}'\n",
    "cleaned_file_combined = 'pMTnet_cleaned_data_beta.tsv'\n",
    "output_file = f'{pMTnet_data_cleaned}/{cleaned_file_combined}'\n",
    "\n",
    "# clean combined pMTnet data\n",
    "%run data_scripts/pMTnet/clean_data_pMTnet_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pMTnet_cleaned_beta_output = f'{pMTnet_data_cleaned}/{cleaned_file_combined}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImmuneCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vorerst auslassen, weil riesiger Datensatz, aber nicht schön beschrieben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# prepare directories\n",
    "ImmCode_data_plain = f'{pipeline_data_plain}/ImmCode'\n",
    "ImmCode_data_cleaned = f'{pipeline_data_cleaned}/ImmCode'\n",
    "ImmCode_data_fitted = f'{pipeline_data_temp_bucket}/ImmCode'\n",
    "\n",
    "ImmCode_folders = [ImmCode_data_plain, ImmCode_data_cleaned, ImmCode_data_fitted]\n",
    "create_folders_if_not_exists(ImmCode_folders)\n",
    "fitted_combined_file = 'ImmCode_fitted_data_beta.tsv'\n",
    "fitted_paired_file = 'ImmCode_paired_fitted.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# prepare parameters for notebook ImmCode fit data combined\n",
    "input_file = f'{ImmCode_data_plain}/beta_data.csv'\n",
    "path_prefix_fitted = ImmCode_data_fitted\n",
    "fitted_file = fitted_combined_file\n",
    "\n",
    "# fit combined ImmCode data\n",
    "%run data_scripts/ImmCode/fit_data_ImmCode_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# prepare parameters for notebook ImmCode clean data combined\n",
    "input_file = f'{ImmCode_data_fitted}/{fitted_combined_file}'\n",
    "cleaned_file_combined = 'ImmCode_cleaned_data_beta.tsv'\n",
    "output_file = f'{ImmCode_data_cleaned}/{cleaned_file_combined}'\n",
    "\n",
    "# clean combined ImmCode data\n",
    "%run data_scripts/ImmCode/clean_data_ImmCode_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# prepare parameters for notebook ImmCode fit data paired\n",
    "input_file = f'{ImmCode_data_plain}/ImmCode_paired_only.tsv'\n",
    "path_prefix_fitted = ImmCode_data_fitted\n",
    "fitted_file = fitted_paired_file\n",
    "\n",
    "# fit paired ImmCode data\n",
    "%run data_scripts/ImmCode/fit_data_ImmCode_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# prepare parameters for notebook ImmCode clean data paired\n",
    "input_file = f'{ImmCode_data_fitted}/{fitted_paired_file}'\n",
    "cleaned_file_paired = 'ImmCode_cleaned_data_paired.tsv'\n",
    "output_file = f'{ImmCode_data_cleaned}/{cleaned_file_paired}'\n",
    "\n",
    "# clean paired ImmCode data\n",
    "%run data_scripts/ImmCode/clean_data_ImmCode_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ImmCode_cleaned_beta_output = f'{ImmCode_data_cleaned}/{cleaned_file_beta}'\n",
    "ImmCode_cleaned_paired_output = f'{ImmCode_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data check per database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "# beta input files\n",
    "vdjdb_beta_read_path = VDJdb_cleaned_beta_output\n",
    "mcpastcr_beta_read_path = McPAS_cleaned_beta_output\n",
    "iedb_beta_read_path = IEDB_cleaned_beta_output\n",
    "pmtnet_beta_read_path = pMTnet_cleaned_beta_output\n",
    "#immcode_beta_read_path = ImmCode_cleaned_beta_output\n",
    "# paired input files\n",
    "vdjdb_paired_read_path = VDJdb_cleaned_paired_output\n",
    "mcpastcr_paired_read_path = McPAS_cleaned_paired_output\n",
    "iedb_paired_read_path = IEDB_cleaned_paired_output\n",
    "#immcode_paired_read_path = ImmCode_cleaned_paired_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VDJdb_beta geladen mit 46507 Einträgen.\n",
      "McPAS_beta geladen mit 9458 Einträgen.\n",
      "IEDB_beta geladen mit 175662 Einträgen.\n",
      "pMTnet_beta geladen mit 32663 Einträgen.\n",
      "VDJdb_paired geladen mit 27414 Einträgen.\n",
      "McPAS_paired geladen mit 1904 Einträgen.\n",
      "IEDB_paired geladen mit 25020 Einträgen.\n",
      "\n",
      "**Wenn VDJdb_beta als Testset verwendet wird:**\n",
      "  - TPP3-Paare im Testset: 176\n",
      "  - Gesamt Test-Paare: 46507\n",
      "\n",
      "**Wenn McPAS_beta als Testset verwendet wird:**\n",
      "  - TPP3-Paare im Testset: 27\n",
      "  - Gesamt Test-Paare: 9458\n",
      "\n",
      "**Wenn IEDB_beta als Testset verwendet wird:**\n",
      "  - TPP3-Paare im Testset: 73837\n",
      "  - Gesamt Test-Paare: 175662\n",
      "\n",
      "**Wenn pMTnet_beta als Testset verwendet wird:**\n",
      "  - TPP3-Paare im Testset: 372\n",
      "  - Gesamt Test-Paare: 32663\n",
      "\n",
      "**Wenn VDJdb_paired als Testset verwendet wird:**\n",
      "  - TPP3-Paare im Testset: 0\n",
      "  - Gesamt Test-Paare: 27414\n",
      "\n",
      "**Wenn McPAS_paired als Testset verwendet wird:**\n",
      "  - TPP3-Paare im Testset: 0\n",
      "  - Gesamt Test-Paare: 1904\n",
      "\n",
      "**Wenn IEDB_paired als Testset verwendet wird:**\n",
      "  - TPP3-Paare im Testset: 0\n",
      "  - Gesamt Test-Paare: 25020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52141/301140575.py:33: DtypeWarning: Columns (1,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(train_file, sep='\\t')\n",
      "/tmp/ipykernel_52141/301140575.py:35: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_file, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**TPP Analysis for VDJdb with Train + Validation**\n",
      "  - TPP3-Paare im Testset: 0\n",
      "  - Gesamt Test-Paare: 46507\n",
      "Anzahl der ursprünglichen TPP3-Paare, die jetzt in negativen Daten des Train/Validation-Sets vorkommen: 0\n",
      "Keine der ursprünglichen TPP3-Paare wurden in den negativen Daten gefunden.\n",
      "Anzahl der ursprünglichen TPP3-Paare, die jetzt in den positiven Train/Validation-Daten vorkommen: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52141/3652232444.py:6: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  concatenated_beta_df = pd.read_csv(concatenated_beta_file, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**TPP Analysis for VDJdb with Concatenated Beta Data**\n",
      "  - TPP3-Paare im Testset: 1338\n",
      "  - Gesamt Test-Paare: 46507\n",
      "VDJdb TPP3-Paare, deren TCR jetzt in negativen Daten aus Train/Validation vorkommt: 0\n",
      "VDJdb TPP3-Paare, deren Epitope jetzt in negativen Daten aus Train/Validation vorkommt: 0\n",
      "TPP3-Paare von cleaned data, die noch im finalen Testset vorhanden sind: 1421\n",
      "\n",
      "✅ TPP3-Paare von cleaned data, die noch im finalen Testset vorhanden sind: 1421\n",
      "🔄 Aktuelle TPP-Klassen dieser Paare im finalen Testset:\n",
      "task\n",
      "TPP1    1421\n",
      "Name: count, dtype: int64\n",
      "⚠️ Anzahl TPP3-Paare, deren TCRs in den negativen Train/Validation-Daten vorkommen: 0\n",
      "⚠️ Anzahl TPP3-Paare, deren Epitope in den negativen Train/Validation-Daten vorkommen: 0\n",
      "✅ Anzahl TPP3-Paare, deren TCRs in den positiven Train/Validation-Daten vorkommen: 1338\n",
      "✅ Anzahl TPP3-Paare, deren Epitope in den positiven Train/Validation-Daten vorkommen: 1338\n"
     ]
    }
   ],
   "source": [
    "%run datacheck_for_testfile.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Concatenation\n",
    "The concatenation includes further cleaning and advanced removal of duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of beta_df: 217783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"H-2Kb\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-H-2KB\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA class I\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-HLACLASSI\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV9-02\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV9-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV13-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV21-03\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV21-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-05\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV2-5\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV1-05\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV1-5\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV13-06\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-03\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV2-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-7*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV1-4*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV8-4*05\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-6*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV19*05\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV19*04\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV26-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV26-1*03\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRBV26*03\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV6-2*02\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV21-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV21-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-6\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRB17-1\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRB17-1\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB06\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB6\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB12\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB12\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBB27\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBB27\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV12-3/4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV12-3/4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"05-07*01\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TR5-7*01\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ38-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ1-7\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-6\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ39*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ45*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ42*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ31*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ33*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ43*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ36*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ9*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ52*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ8*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ37*02\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ3-1*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ54*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ53*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ17-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ19-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ20-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ10-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Negative\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRNEGATIVE\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Donor 13\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRDON/OR13\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ3-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize Donor13: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CALQDXNTGEXFF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CSARTGDRTEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSILGWSEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSLRTRTDTQYX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 9620 entries removed.\n",
      "removed all duplicates (CDR3, Epitope) from distinct values (most_important_columns, keep=False). 26543 entries removed.\n",
      "beta removed entries df length: 26543\n",
      "\n",
      "\n",
      "Number of groups formed: 7582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52141/3398904298.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  duplicates_to_add = pd.concat([duplicates_to_add, group[group['is_duplicated'] == False]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26206 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 185115 entries, 9957 entries have been removed.\n",
      "for beta dataset :\n",
      "size difference is: 9957\n",
      "  175158 information score cleaned: 4.8346521426369335\n",
      "  185115 information score dropout: 4.809572427950193\n",
      "final_beta_df length = 175158\n",
      "length of paired_df: 26924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"H-2Kb\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-H-2KB\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA class I\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-HLACLASSI\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV21-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV1-4\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV2-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV251\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRDAV1*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV24*0\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRAV24*00\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV13/1*02 (F)\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV13/DV1*02\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV1-4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV1-4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV2-2\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV2-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV9*02\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRAV9-1*02\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRA21-01\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRA21-1\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV19-*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV12-4\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV12-4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV12-4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV9-02\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV9-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"CATSESSGQTYEQYF\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRCAT-E--GQTYEQYF\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAJ53*02\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Donor 13\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRDON/OR13\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Negative\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRNEGATIVE\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-6\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ38-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ17-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ19-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ20-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ10-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize TRAJ37: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize XVXNREDKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CVVNNNXDMRF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CVVNGMDSSYKLXF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVERGL##GSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVEDRGEKHXSCL: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVS#SNFGNEKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAASGXGTYKYIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CALPP#YNFNKFYF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAXSGGGSYQLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CI##GGADGLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize XEXXGGXXRXXX##: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVSVLG#SGGSNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CTSVQQAL#GGSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRQLGTGAVVTIN*#F: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVED#ISSGSARQLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VL*#GGGADGLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQDLGY##GGTSYGKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CTPG#SQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAASSWS*#GLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQAQ#YGGSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAIELQAR##: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAG#YGNKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CATLCPQ#NKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQAG#SNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CISP*SGSSNTGKLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIXXL*NDIGENMFLI: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRVA#SGNTPLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRV#NSNSGYALNF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VL*VLPG#ALNF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVND#NTDKLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAGLGGT#GGSNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAPQTRRLATTVS*#W: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSGPS#TDSWGKLQF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSAAX#AGNKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSA#KAAGNKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAXRGGSEKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVN?PPFGNEKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize Donor13: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSILGWSEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CSARTGDRTEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSLRTRTDTQYX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 915 entries removed.\n",
      "removed all duplicates from distinct values (cultivated columns, keep=False). 1961 entries removed.\n",
      "paired removed entries df length: 1961\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52141/2624603129.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  duplicates_to_add = pd.concat([duplicates_to_add, group[group['is_duplicated'] == False]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1945 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 26881 entries, 931 entries have been removed.\n",
      "for paired dataset:\n",
      "size difference is: 931\n",
      "  25950 information score cleaned: 5.406319845857418\n",
      "  26881 information score dropout: 5.47635876641494\n",
      "final_paired_df length: 25950\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "# beta input files\n",
    "#vdjdb_beta_read_path = VDJdb_cleaned_beta_output >> rausgenommen, weil für seaprates testfile nutzen\n",
    "mcpastcr_beta_read_path = McPAS_cleaned_beta_output\n",
    "iedb_beta_read_path = IEDB_cleaned_beta_output \n",
    "pmtnet_beta_read_path = pMTnet_cleaned_beta_output\n",
    "# paired input files\n",
    "#vdjdb_paired_read_path = VDJdb_cleaned_paired_output >> rausgenommen, weil für seaprates testfile nutzen\n",
    "mcpastcr_paired_read_path = McPAS_cleaned_paired_output\n",
    "iedb_paired_read_path = IEDB_cleaned_paired_output \n",
    "# output files\n",
    "output_file_beta = 'beta_concatenated.tsv'\n",
    "output_file_paired = 'paired_concatenated.tsv'\n",
    "\n",
    "create_folders_if_not_exists([custom_dataset_path])\n",
    "\n",
    "%run data_scripts/concatDatasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "# output files\n",
    "output_file_beta = 'beta_concatenated.tsv'\n",
    "output_file_paired = 'paired_concatenated.tsv'\n",
    "\n",
    "concatenated_paired = f'{custom_dataset_path}/{output_file_paired}'\n",
    "concatenated_beta = f'{custom_dataset_path}/{output_file_beta}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "The split creates 3 datasets. Train, Validation and Test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct tcr's: 23068 from 25950\n",
      "unique tcr's: 21070 from 25950\n",
      "unique epitopes: 565 from 25950\n",
      "train data has 4880 entries\n",
      "validation data has 21070 entries\n",
      "validation data has 0 TPP1 tasks (unseen tcr & seen epitopes).\n",
      "validation data has 17391 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validation data has 3679 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/validation ratio is 0.18805394990366087/0.8119460500963391\n",
      "15881 entries will be shifted from test to train so the train/validation ratio can be 0.8/0.2\n",
      "755 entries will be shifted from test to train so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "755 entries need to be shifted from train to test so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "755 entries from train will be moved to test (TPP1)\n",
      "df_train size before: 21516\n",
      "number of tpp1 before: 0\n",
      "number of tpp2 before: 755\n",
      "df_train size after: 20761\n",
      "number of tpp1 after: 755\n",
      "number of tpp2 after: 755\n",
      "train data has 20761 entries\n",
      "validation data has 5189 entries\n",
      "validation data has 755 TPP1 tasks (seen tcr & seen epitopes).\n",
      "validation data has 755 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validation data has 3679 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/validation ratio is 0.800038535645472/0.19996146435452794\n",
      "validation data has 5189 entries\n",
      "train data has 20761 entries\n",
      "validation data has 755 TPP1 tasks (seen tcr & seen epitopes).\n",
      "validation data has 755 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validation data has 3679 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the validation ratio is 0.800038535645472/0.19996146435452794\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for split of paired dataset\n",
    "input_file = concatenated_paired\n",
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "validation_file_name = 'validation.tsv'\n",
    "train_file_name = 'train.tsv'\n",
    "aimed_validation_ratio = 0.2 # this means 20% of the concatenated dataset will be for validation\n",
    "\n",
    "create_folders_if_not_exists([paired_output_folder])\n",
    "\n",
    "# do the split\n",
    "%run data_scripts/data_preparation/split_paired_trainval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct tcr's: 144650 from 175158\n",
      "unique tcr's: 131596 from 175158\n",
      "unique epitopes: 597 from 175158\n",
      "train data has 43562 entries\n",
      "validation data has 131596 entries\n",
      "validation data has 0 TPP1 tasks (unseen tcr & seen epitopes).\n",
      "validation data has 129170 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validation data has 2426 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/validation ratio is 0.24870117265554526/0.7512988273444547\n",
      "96565 entries will be shifted from validation to train so the train/validation ratio can be 0.8/0.2\n",
      "16302 entries will be shifted from validation to train so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "16303 entries need to be shifted from train to validation so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "train data has 140126 entries\n",
      "validation data has 35032 entries\n",
      "validation data has 16303 TPP1 tasks (seen tcr & seen epitopes).\n",
      "validation data has 16303 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validation data has 2426 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/validation ratio is 0.7999977163475263/0.20000228365247377\n",
      "validation data has 35032 entries\n",
      "train data has 140126 entries\n",
      "validation data has 16303 TPP1 tasks (seen tcr & seen epitopes).\n",
      "validation data has 16303 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validation data has 2426 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the validation ratio is 0.7999977163475263/0.20000228365247377\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for split of beta dataset\n",
    "input_file = concatenated_beta\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "aimed_validation_ratio = 0.2 # this means 20% of the concatenated dataset will be for validation\n",
    "\n",
    "create_folders_if_not_exists([beta_output_folder])\n",
    "\n",
    "# do the split\n",
    "%run data_scripts/data_preparation/split_beta_trainval.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus:                 barcode   donor  \\\n",
      "0   AAACCTGAGACAAAGG-4  donor1   \n",
      "1  AAACCTGAGACTGTAA-34  donor1   \n",
      "2   AAACCTGAGAGCCCAA-5  donor1   \n",
      "3  AAACCTGAGAGCTGCA-24  donor1   \n",
      "4   AAACCTGAGAGGGATA-8  donor1   \n",
      "\n",
      "                                  cell_clono_cdr3_aa  \\\n",
      "0  TRA:CAASVSIWTGTASKLTF;TRA:CAAWDMEYGNKLVF;TRB:C...   \n",
      "1                                    TRB:CASDTPVGQFF   \n",
      "2                 TRA:CASYTDKLIF;TRB:CASSGGSISTDTQYF   \n",
      "3                                 TRB:CASSGGQSSYEQYF   \n",
      "4          TRA:CAASGYGNTGRRALTF;TRB:CASSQDPAGGYNEQFF   \n",
      "\n",
      "                                  cell_clono_cdr3_nt     CD3  CD19  CD45RA  \\\n",
      "0  TRA:TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAA...  2125.0   0.0   912.0   \n",
      "1              TRB:TGTGCCAGCGATACCCCGGTTGGGCAGTTCTTC  1023.0   0.0  2028.0   \n",
      "2  TRA:TGTGCTTCCTACACCGACAAGCTCATCTTT;TRB:TGCGCCA...  1598.0   3.0  3454.0   \n",
      "3     TRB:TGCGCCAGCAGTGGCGGACAGAGCTCCTACGAGCAGTACTTC   298.0   1.0   880.0   \n",
      "4  TRA:TGTGCAGCAAGCGGGTATGGAAACACGGGCAGGAGAGCACTT...  1036.0   0.0  2457.0   \n",
      "\n",
      "   CD4    CD8a  CD14  ...  B0702_RPHERNGFTVL_pp65_CMV_binder  \\\n",
      "0  1.0  2223.0   4.0  ...                              False   \n",
      "1  2.0  3485.0   1.0  ...                              False   \n",
      "2  4.0  3383.0   1.0  ...                              False   \n",
      "3  1.0  2389.0   1.0  ...                              False   \n",
      "4  2.0  3427.0   3.0  ...                              False   \n",
      "\n",
      "   B0801_RAKFKQLL_BZLF1_EBV_binder  B0801_ELRRKMMYM_IE-1_CMV_binder  \\\n",
      "0                            False                            False   \n",
      "1                            False                            False   \n",
      "2                            False                            False   \n",
      "3                            False                            False   \n",
      "4                            False                            False   \n",
      "\n",
      "   B0801_FLRGRAYGL_EBNA-3A_EBV_binder  A0101_SLEGGGLGY_NC_binder  \\\n",
      "0                               False                      False   \n",
      "1                               False                      False   \n",
      "2                               False                      False   \n",
      "3                               False                      False   \n",
      "4                               False                      False   \n",
      "\n",
      "   A0101_STEGGGLAY_NC_binder  A0201_ALIAPVHAV_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   A2402_AYSSAGASI_NC_binder  B0702_GPAESAAGL_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   NR(B0801)_AAKGRGAAL_NC_binder  \n",
      "0                          False  \n",
      "1                          False  \n",
      "2                          False  \n",
      "3                          False  \n",
      "4                          False  \n",
      "\n",
      "[5 rows x 118 columns]\n",
      "Meta:                barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_1             True   \n",
      "1  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_2             True   \n",
      "2  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_3             True   \n",
      "3  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_4            False   \n",
      "4  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_5            False   \n",
      "\n",
      "   length chain     v_gene d_gene   j_gene c_gene  full_length productive  \\\n",
      "0     722   TRB   TRBV10-3  TRBD2  TRBJ2-1  TRBC2         True       True   \n",
      "1     605   TRA  TRAV29DV5    NaN   TRAJ44   TRAC         True       True   \n",
      "2     738   TRA    TRAV8-6    NaN   TRAJ47   TRAC         True       True   \n",
      "3     468   TRB        NaN    NaN  TRBJ2-3  TRBC2        False        NaN   \n",
      "4     488   TRB        NaN    NaN  TRBJ2-6  TRBC2        False        NaN   \n",
      "\n",
      "                cdr3                                            cdr3_nt  \\\n",
      "0  CAISDPGLAGGGGEQFF  TGTGCCATCAGTGACCCCGGACTAGCGGGAGGCGGGGGGGAGCAGT...   \n",
      "1  CAASVSIWTGTASKLTF  TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAACTCA...   \n",
      "2     CAAWDMEYGNKLVF         TGTGCCGCCTGGGACATGGAATATGGAAACAAGCTGGTCTTT   \n",
      "3                NaN                                                NaN   \n",
      "4                NaN                                                NaN   \n",
      "\n",
      "   reads  umis raw_clonotype_id         raw_consensus_id  \n",
      "0  32237    18      clonotype19  clonotype19_consensus_1  \n",
      "1   6088     3      clonotype19  clonotype19_consensus_2  \n",
      "2   5358     3      clonotype19  clonotype19_consensus_3  \n",
      "3   2517     1      clonotype19                      NaN  \n",
      "4   2468     1      clonotype19                      NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231714/3850524540.py:9: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n"
     ]
    }
   ],
   "source": [
    "#Daten einlesen\n",
    "\n",
    "combined_donors_path = f'{pipeline_data_plain}/10x/combined_donors_consensus_annotations.csv'\n",
    "all_donors_consensus = pd.read_csv(combined_donors_path, sep=',')\n",
    "\n",
    "print(\"Consensus: \", all_donors_consensus.head())\n",
    "\n",
    "all_donors_meta_path = f'{pipeline_data_plain}/10x/meta.csv'\n",
    "all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n",
    "\n",
    "print(\"Meta: \", all_donors_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Start:  0\n",
      "Batch Start:  1000\n",
      "Batch Start:  2000\n",
      "Batch Start:  3000\n",
      "Batch Start:  4000\n",
      "Batch Start:  5000\n",
      "Batch Start:  6000\n",
      "Batch Start:  7000\n",
      "Batch Start:  8000\n",
      "Batch Start:  9000\n",
      "Batch Start:  10000\n",
      "Batch Start:  11000\n",
      "Batch Start:  12000\n",
      "Batch Start:  13000\n",
      "Batch Start:  14000\n",
      "Batch Start:  15000\n",
      "Batch Start:  16000\n",
      "Batch Start:  17000\n",
      "Batch Start:  18000\n",
      "Batch Start:  19000\n",
      "Batch Start:  20000\n",
      "Batch Start:  21000\n",
      "Batch Start:  22000\n",
      "Batch Start:  23000\n",
      "Batch Start:  24000\n",
      "Batch Start:  25000\n",
      "Batch Start:  26000\n",
      "Batch Start:  27000\n",
      "Batch Start:  28000\n",
      "Batch Start:  29000\n",
      "Batch Start:  30000\n",
      "Batch Start:  31000\n",
      "Batch Start:  32000\n",
      "Batch Start:  33000\n",
      "Batch Start:  34000\n",
      "Batch Start:  35000\n",
      "Batch Start:  36000\n",
      "Batch Start:  37000\n",
      "Batch Start:  38000\n",
      "Batch Start:  39000\n",
      "Batch Start:  40000\n",
      "Batch Start:  41000\n",
      "Batch Start:  42000\n",
      "Batch Start:  43000\n",
      "Batch Start:  44000\n",
      "Batch Start:  45000\n",
      "Batch Start:  46000\n",
      "Batch Start:  47000\n",
      "Batch Start:  48000\n",
      "Batch Start:  49000\n",
      "Batch Start:  50000\n",
      "Batch Start:  51000\n",
      "Batch Start:  52000\n",
      "Batch Start:  53000\n",
      "Batch Start:  54000\n",
      "Batch Start:  55000\n",
      "Batch Start:  56000\n",
      "Batch Start:  57000\n",
      "Batch Start:  58000\n",
      "Batch Start:  59000\n",
      "Batch Start:  60000\n",
      "Batch Start:  61000\n",
      "Batch Start:  62000\n",
      "Batch Start:  63000\n",
      "Batch Start:  64000\n",
      "Batch Start:  65000\n",
      "Batch Start:  66000\n",
      "Batch Start:  67000\n",
      "Batch Start:  68000\n",
      "Batch Start:  69000\n",
      "Batch Start:  70000\n",
      "Batch Start:  71000\n",
      "Batch Start:  72000\n",
      "Batch Start:  73000\n",
      "Batch Start:  74000\n",
      "Batch Start:  75000\n",
      "Batch Start:  76000\n",
      "Batch Start:  77000\n",
      "Batch Start:  78000\n",
      "Batch Start:  79000\n",
      "Batch Start:  80000\n",
      "Batch Start:  81000\n",
      "Batch Start:  82000\n",
      "Batch Start:  83000\n",
      "Batch Start:  84000\n",
      "Batch Start:  85000\n",
      "Batch Start:  86000\n",
      "Batch Start:  87000\n",
      "Batch Start:  88000\n",
      "Batch Start:  89000\n",
      "Batch Start:  90000\n",
      "Batch Start:  91000\n",
      "Batch Start:  92000\n",
      "Batch Start:  93000\n",
      "Batch Start:  94000\n",
      "Batch Start:  95000\n",
      "Batch Start:  96000\n",
      "Batch Start:  97000\n",
      "Batch Start:  98000\n",
      "Batch Start:  99000\n",
      "Batch Start:  100000\n",
      "Batch Start:  101000\n",
      "Batch Start:  102000\n",
      "Batch Start:  103000\n",
      "Batch Start:  104000\n",
      "Batch Start:  105000\n",
      "Batch Start:  106000\n",
      "Batch Start:  107000\n",
      "Batch Start:  108000\n",
      "Batch Start:  109000\n",
      "Batch Start:  110000\n",
      "Batch Start:  111000\n",
      "Batch Start:  112000\n",
      "Batch Start:  113000\n",
      "Batch Start:  114000\n",
      "Batch Start:  115000\n",
      "Batch Start:  116000\n",
      "Batch Start:  117000\n",
      "Batch Start:  118000\n",
      "Batch Start:  119000\n",
      "Batch Start:  120000\n",
      "Batch Start:  121000\n",
      "Batch Start:  122000\n",
      "Batch Start:  123000\n",
      "Batch Start:  124000\n",
      "Batch Start:  125000\n",
      "Batch Start:  126000\n",
      "Batch Start:  127000\n",
      "Batch Start:  128000\n",
      "Batch Start:  129000\n",
      "Batch Start:  130000\n",
      "Batch Start:  131000\n",
      "Batch Start:  132000\n",
      "Batch Start:  133000\n",
      "Batch Start:  134000\n",
      "Batch Start:  135000\n",
      "Batch Start:  136000\n",
      "Batch Start:  137000\n",
      "Batch Start:  138000\n",
      "Batch Start:  139000\n",
      "Batch Start:  140000\n",
      "Batch Start:  141000\n",
      "Batch Start:  142000\n",
      "Batch Start:  143000\n",
      "Batch Start:  144000\n",
      "Batch Start:  145000\n",
      "Batch Start:  146000\n",
      "Batch Start:  147000\n",
      "Batch Start:  148000\n",
      "Batch Start:  149000\n",
      "Batch Start:  150000\n",
      "Batch Start:  151000\n",
      "Batch Start:  152000\n",
      "Batch Start:  153000\n",
      "Batch Start:  154000\n",
      "Batch Start:  155000\n",
      "Batch Start:  156000\n",
      "Batch Start:  157000\n",
      "Batch Start:  158000\n",
      "Batch Start:  159000\n",
      "Batch Start:  160000\n",
      "Batch Start:  161000\n",
      "Batch Start:  162000\n",
      "Batch Start:  163000\n",
      "Batch Start:  164000\n",
      "Batch Start:  165000\n",
      "Batch Start:  166000\n",
      "Batch Start:  167000\n",
      "Batch Start:  168000\n",
      "Batch Start:  169000\n",
      "Batch Start:  170000\n",
      "Batch Start:  171000\n",
      "Batch Start:  172000\n",
      "Batch Start:  173000\n",
      "Batch Start:  174000\n",
      "Batch Start:  175000\n",
      "Batch Start:  176000\n",
      "Batch Start:  177000\n",
      "Batch Start:  178000\n",
      "Batch Start:  179000\n",
      "Batch Start:  180000\n",
      "Batch Start:  181000\n",
      "Batch Start:  182000\n",
      "Batch Start:  183000\n",
      "Batch Start:  184000\n",
      "Batch Start:  185000\n",
      "Batch Start:  186000\n",
      "Batch Start:  187000\n",
      "Batch Start:  188000\n",
      "Batch Start:  189000\n",
      "             TCR_name      TRBV     TRBJ           TRB_CDR3   TRBC  \\\n",
      "0  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "1  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "2  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "3  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "4  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "\n",
      "      Epitope          MHC Binding task  \n",
      "0   VTEHDTLLY  HLA-A*01:01       0  nan  \n",
      "1   KTWGQYWQV  HLA-A*02:01       0  nan  \n",
      "2  ELAGIGILTV  HLA-A*02:01       0  nan  \n",
      "3  CLLWSFQTSA  HLA-A*02:01       0  nan  \n",
      "4   IMDQVPFSV  HLA-A*02:01       0  nan  \n"
     ]
    }
   ],
   "source": [
    "#Dieser Code für ganzen Datensatz laufen lassen\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Festlegen der Batch-Größe für die Verarbeitung\n",
    "batch_size = 1000  # Passe diese Zahl je nach Speicherressourcen an\n",
    "\n",
    "# Identifizieren von Epitope-Spalten, aber ohne \"NR(B0801)_AAKGRGAAL_NC_binder\"\n",
    "epitope_columns = [col for col in all_donors_consensus.columns if '_binder' in col and col != \"NR(B0801)_AAKGRGAAL_NC_binder\"]\n",
    "\n",
    "# Liste für alle Batch-Ergebnisse\n",
    "all_batches = []\n",
    "\n",
    "# Verarbeite `all_donors_consensus` in Batches\n",
    "for batch_start in range(0, len(all_donors_consensus), batch_size):\n",
    "    print(\"Batch Start: \", batch_start)\n",
    "    # Batch definieren\n",
    "    batch = all_donors_consensus.iloc[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Filtern auf Zeilen, die 'TRB:' enthalten\n",
    "    batch_trb = batch[batch['cell_clono_cdr3_aa'].str.contains(\"TRB:\", na=False)]\n",
    "\n",
    "    # Liste, um Zeilen für diesen Batch zu speichern\n",
    "    expanded_rows = []\n",
    "    \n",
    "    # Iteriere durch jede Zeile im Batch\n",
    "    for _, row in batch_trb.iterrows():\n",
    "        for col in epitope_columns:\n",
    "            # Extrahiere MHC und Epitope\n",
    "            match = re.match(r'([A-Z0-9]+)_([A-Z]+)_.*_binder', col)\n",
    "            if match:\n",
    "                mhc_raw, epitope = match.groups()\n",
    "                mhc_formatted = f'HLA-{mhc_raw[0]}*{mhc_raw[1:3]}:{mhc_raw[3:]}'\n",
    "\n",
    "                # Füge `Epitope` und `MHC` zur Zeile hinzu\n",
    "                new_row = row.copy()\n",
    "                new_row['Epitope'] = epitope\n",
    "                new_row['MHC'] = mhc_formatted\n",
    "\n",
    "                # Füge neue Zeile zur Batch-Liste hinzu\n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    # Erstelle einen DataFrame aus dem Batch\n",
    "    batch_df = pd.DataFrame(expanded_rows)\n",
    "    all_batches.append(batch_df)  # Speichere den Batch in der Liste\n",
    "\n",
    "# Kombiniere alle Batch-Ergebnisse zu einem DataFrame\n",
    "expanded_df = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "# Nur die TRB-Chain-Einträge in `all_donors_meta` beibehalten\n",
    "all_donors_meta_trb = all_donors_meta[all_donors_meta['chain'] == 'TRB']\n",
    "\n",
    "# Zusammenführen der beiden DataFrames basierend auf der 'barcode' Spalte\n",
    "merged_df = pd.merge(all_donors_meta_trb, expanded_df[['barcode', 'Epitope', 'MHC']], on='barcode', how='inner')\n",
    "\n",
    "# Spalten umbenennen und Format anpassen\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'barcode': 'TCR_name',\n",
    "    'v_gene': 'TRBV',\n",
    "    'j_gene': 'TRBJ',\n",
    "    'c_gene': 'TRBC',\n",
    "    'cdr3': 'TRB_CDR3'\n",
    "})\n",
    "\n",
    "# Fehlende Spalten auffüllen\n",
    "desired_columns = ['TCR_name', 'TRBV', 'TRBJ', 'TRB_CDR3', 'TRBC', 'Epitope', 'MHC', 'Binding', 'task']\n",
    "for col in desired_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = 'nan' if col == 'task' else '0'\n",
    "\n",
    "# Nur die gewünschten Spalten beibehalten und Zeilen mit `None` in `TRB_CDR3` entfernen\n",
    "final_df = merged_df[desired_columns]\n",
    "final_df = final_df[final_df['TRB_CDR3'] != 'None']\n",
    "\n",
    "final_df = final_df[final_df['TRB_CDR3'].notna() & (final_df['TRB_CDR3'] != '')]\n",
    "\n",
    "# Ausgabe des ersten Teils des Ergebnisses zur Überprüfung\n",
    "print(final_df.head())\n",
    "\n",
    "# Speichern des kombinierten DataFrames\n",
    "output_path = f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv'\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52141/1827301443.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alle Datensätze wurden erfolgreich gespeichert.\n",
      "Train: 280252 Einträge\n",
      "- Unique TCRs: 177220\n",
      "- Unique Epitope: 987\n",
      "- Positive (binding == 1): 140126\n",
      "- Negative (binding == 0): 140126\n",
      "\n",
      "Validation: 210192 Einträge\n",
      "- Unique TCRs: 71316\n",
      "- Unique Epitope: 1456\n",
      "- Positive (binding == 1): 35032\n",
      "- Negative (binding == 0): 175160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Daten laden\n",
    "beta = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv', sep=',')\n",
    "\n",
    "# Schritt 1: Aufteilung in Train und Validation (keine Test-Daten mehr)\n",
    "train_split, validation_split = train_test_split(beta, test_size=0.2, random_state=42)  # 20% für Validation\n",
    "\n",
    "# Positive Samples laden\n",
    "train_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n",
    "validation_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/validation_prenegsamples.tsv', sep='\\t')\n",
    "\n",
    "# Anzahl positiver Samples\n",
    "num_train_pos = len(train_preneg)\n",
    "num_validation_pos = len(validation_preneg)\n",
    "\n",
    "# Zielgrößen für negative Samples\n",
    "train_neg_needed = num_train_pos\n",
    "validation_neg_needed = num_validation_pos * 5\n",
    "\n",
    "# Filter VDJdb TCRs und Epitope aus den Negativdaten\n",
    "vdjdb_tcrs = set(vdjdb_df['TRB_CDR3'])\n",
    "vdjdb_epitopes = set(vdjdb_df['Epitope'])\n",
    "\n",
    "def filter_negatives(df):\n",
    "    return df[\n",
    "        ~df['TRB_CDR3'].isin(vdjdb_tcrs) &\n",
    "        ~df['Epitope'].isin(vdjdb_epitopes)\n",
    "    ]\n",
    "\n",
    "# Funktion zur Sicherstellung, dass alle unique Epitope erhalten bleiben\n",
    "def ensure_unique_epitopes(df, target_count):\n",
    "    unique_epitopes = df['Epitope'].unique()\n",
    "    guaranteed_samples = []\n",
    "    \n",
    "    # Stelle sicher, dass jedes Epitope mindestens einmal vorkommt\n",
    "    for epitope in unique_epitopes:\n",
    "        epitope_group = df[df['Epitope'] == epitope]\n",
    "        if len(epitope_group) > 0:\n",
    "            guaranteed_samples.append(epitope_group.sample(1, random_state=42))\n",
    "    \n",
    "    # Kombiniere garantierte Samples\n",
    "    guaranteed_df = pd.concat(guaranteed_samples, ignore_index=True)\n",
    "    \n",
    "    # Berechne verbleibende Anzahl an Samples\n",
    "    remaining_count = target_count - len(guaranteed_df)\n",
    "    if remaining_count > 0:\n",
    "        remaining_samples = df.sample(remaining_count, random_state=42, replace=True)\n",
    "        return pd.concat([guaranteed_df, remaining_samples], ignore_index=True)\n",
    "    return guaranteed_df\n",
    "\n",
    "# Filter negative Daten, die nicht in VDJdb vorkommen\n",
    "train_filtered_negatives = filter_negatives(train_split)\n",
    "validation_filtered_negatives = filter_negatives(validation_split)\n",
    "\n",
    "# Balancierung der Splits\n",
    "train_balanced_negatives = ensure_unique_epitopes(train_filtered_negatives, train_neg_needed)\n",
    "validation_balanced_negatives = ensure_unique_epitopes(validation_filtered_negatives, validation_neg_needed)\n",
    "\n",
    "# Positive und negative Samples kombinieren\n",
    "train_combined = pd.concat([train_preneg, train_balanced_negatives], ignore_index=True)\n",
    "validation_combined = pd.concat([validation_preneg, validation_balanced_negatives], ignore_index=True)\n",
    "\n",
    "# Speichern der kombinierten Datensätze\n",
    "output_dir = f'{pipeline_data_splitted}/{precision}/beta/'\n",
    "train_combined.to_csv(output_dir + \"train.tsv\", sep='\\t', index=False)\n",
    "validation_combined.to_csv(output_dir + \"validation.tsv\", sep='\\t', index=False)\n",
    "\n",
    "# Berechne Unique Werte\n",
    "unique_tcr_train = train_combined['TRB_CDR3'].nunique()\n",
    "unique_epitope_train = train_combined['Epitope'].nunique()\n",
    "unique_tcr_validation = validation_combined['TRB_CDR3'].nunique()\n",
    "unique_epitope_validation = validation_combined['Epitope'].nunique()\n",
    "\n",
    "train_binding_counts = train_combined['Binding'].value_counts()\n",
    "validation_binding_counts = validation_combined['Binding'].value_counts()\n",
    "\n",
    "# Finale Ausgabe\n",
    "print(\"\\nAlle Datensätze wurden erfolgreich gespeichert.\")\n",
    "print(f\"Train: {len(train_combined)} Einträge\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_train}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_train}\")\n",
    "print(f\"- Positive (binding == 1): {train_binding_counts.get(1, 0)}\")\n",
    "print(f\"- Negative (binding == 0): {train_binding_counts.get(0, 0)}\")\n",
    "\n",
    "print(f\"\\nValidation: {len(validation_combined)} Einträge\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_validation}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_validation}\")\n",
    "print(f\"- Positive (binding == 1): {validation_binding_counts.get(1, 0)}\")\n",
    "print(f\"- Negative (binding == 0): {validation_binding_counts.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Start: 0\n",
      "Batch 0 gespeichert.\n",
      "Batch Start: 1000\n",
      "Batch 1000 gespeichert.\n",
      "Batch Start: 2000\n",
      "Batch 2000 gespeichert.\n",
      "Batch Start: 3000\n",
      "Batch 3000 gespeichert.\n",
      "Batch Start: 4000\n",
      "Batch 4000 gespeichert.\n",
      "Batch Start: 5000\n",
      "Batch 5000 gespeichert.\n",
      "Batch Start: 6000\n",
      "Batch 6000 gespeichert.\n",
      "Batch Start: 7000\n",
      "Batch 7000 gespeichert.\n",
      "Batch Start: 8000\n",
      "Batch 8000 gespeichert.\n",
      "Batch Start: 9000\n",
      "Batch 9000 gespeichert.\n",
      "Batch Start: 10000\n",
      "Batch 10000 gespeichert.\n",
      "Batch Start: 11000\n",
      "Batch 11000 gespeichert.\n",
      "Batch Start: 12000\n",
      "Batch 12000 gespeichert.\n",
      "Batch Start: 13000\n",
      "Batch 13000 gespeichert.\n",
      "Batch Start: 14000\n",
      "Batch 14000 gespeichert.\n",
      "Batch Start: 15000\n",
      "Batch 15000 gespeichert.\n",
      "Batch Start: 16000\n",
      "Batch 16000 gespeichert.\n",
      "Batch Start: 17000\n",
      "Batch 17000 gespeichert.\n",
      "Batch Start: 18000\n",
      "Batch 18000 gespeichert.\n",
      "Batch Start: 19000\n",
      "Batch 19000 gespeichert.\n",
      "Batch Start: 20000\n",
      "Batch 20000 gespeichert.\n",
      "Batch Start: 21000\n",
      "Batch 21000 gespeichert.\n",
      "Batch Start: 22000\n",
      "Batch 22000 gespeichert.\n",
      "Batch Start: 23000\n",
      "Batch 23000 gespeichert.\n",
      "Batch Start: 24000\n",
      "Batch 24000 gespeichert.\n",
      "Batch Start: 25000\n",
      "Batch 25000 gespeichert.\n",
      "Batch Start: 26000\n",
      "Batch 26000 gespeichert.\n",
      "Batch Start: 27000\n",
      "Batch 27000 gespeichert.\n",
      "Batch Start: 28000\n",
      "Batch 28000 gespeichert.\n",
      "Batch Start: 29000\n",
      "Batch 29000 gespeichert.\n",
      "Batch Start: 30000\n",
      "Batch 30000 gespeichert.\n",
      "Batch Start: 31000\n",
      "Batch 31000 gespeichert.\n",
      "Batch Start: 32000\n",
      "Batch 32000 gespeichert.\n",
      "Batch Start: 33000\n",
      "Batch 33000 gespeichert.\n",
      "Batch Start: 34000\n",
      "Batch 34000 gespeichert.\n",
      "Batch Start: 35000\n",
      "Batch 35000 gespeichert.\n",
      "Batch Start: 36000\n",
      "Batch 36000 gespeichert.\n",
      "Batch Start: 37000\n",
      "Batch 37000 gespeichert.\n",
      "Batch Start: 38000\n",
      "Batch 38000 gespeichert.\n",
      "Batch Start: 39000\n",
      "Batch 39000 gespeichert.\n",
      "Batch Start: 40000\n",
      "Batch 40000 gespeichert.\n",
      "Batch Start: 41000\n",
      "Batch 41000 gespeichert.\n",
      "Batch Start: 42000\n",
      "Batch 42000 gespeichert.\n",
      "Batch Start: 43000\n",
      "Batch 43000 gespeichert.\n",
      "Batch Start: 44000\n",
      "Batch 44000 gespeichert.\n",
      "Batch Start: 45000\n",
      "Batch 45000 gespeichert.\n",
      "Batch Start: 46000\n",
      "Batch 46000 gespeichert.\n",
      "Batch Start: 47000\n",
      "Batch 47000 gespeichert.\n",
      "Batch Start: 48000\n",
      "Batch 48000 gespeichert.\n",
      "Batch Start: 49000\n",
      "Batch 49000 gespeichert.\n",
      "Batch Start: 50000\n",
      "Batch 50000 gespeichert.\n",
      "Batch Start: 51000\n",
      "Batch 51000 gespeichert.\n",
      "Batch Start: 52000\n",
      "Batch 52000 gespeichert.\n",
      "Batch Start: 53000\n",
      "Batch 53000 gespeichert.\n",
      "Batch Start: 54000\n",
      "Batch 54000 gespeichert.\n",
      "Batch Start: 55000\n",
      "Batch 55000 gespeichert.\n",
      "Batch Start: 56000\n",
      "Batch 56000 gespeichert.\n",
      "Batch Start: 57000\n",
      "Batch 57000 gespeichert.\n",
      "Batch Start: 58000\n",
      "Batch 58000 gespeichert.\n",
      "Batch Start: 59000\n",
      "Batch 59000 gespeichert.\n",
      "Batch Start: 60000\n",
      "Batch 60000 gespeichert.\n",
      "Batch Start: 61000\n",
      "Batch 61000 gespeichert.\n",
      "Batch Start: 62000\n",
      "Batch 62000 gespeichert.\n",
      "Batch Start: 63000\n",
      "Batch 63000 gespeichert.\n",
      "Batch Start: 64000\n",
      "Batch 64000 gespeichert.\n",
      "Batch Start: 65000\n",
      "Batch 65000 gespeichert.\n",
      "Batch Start: 66000\n",
      "Batch 66000 gespeichert.\n",
      "Batch Start: 67000\n",
      "Batch 67000 gespeichert.\n",
      "Batch Start: 68000\n",
      "Batch 68000 gespeichert.\n",
      "Batch Start: 69000\n",
      "Batch 69000 gespeichert.\n",
      "Batch Start: 70000\n",
      "Batch 70000 gespeichert.\n",
      "Batch Start: 71000\n",
      "Batch 71000 gespeichert.\n",
      "Batch Start: 72000\n",
      "Batch 72000 gespeichert.\n",
      "Batch Start: 73000\n",
      "Batch 73000 gespeichert.\n",
      "Batch Start: 74000\n",
      "Batch 74000 gespeichert.\n",
      "Batch Start: 75000\n",
      "Batch 75000 gespeichert.\n",
      "Batch Start: 76000\n",
      "Batch 76000 gespeichert.\n",
      "Batch Start: 77000\n",
      "Batch 77000 gespeichert.\n",
      "Batch Start: 78000\n",
      "Batch 78000 gespeichert.\n",
      "Batch Start: 79000\n",
      "Batch 79000 gespeichert.\n",
      "Batch Start: 80000\n",
      "Batch 80000 gespeichert.\n",
      "Batch Start: 81000\n",
      "Batch 81000 gespeichert.\n",
      "Batch Start: 82000\n",
      "Batch 82000 gespeichert.\n",
      "Batch Start: 83000\n",
      "Batch 83000 gespeichert.\n",
      "Batch Start: 84000\n",
      "Batch 84000 gespeichert.\n",
      "Batch Start: 85000\n",
      "Batch 85000 gespeichert.\n",
      "Batch Start: 86000\n",
      "Batch 86000 gespeichert.\n",
      "Batch Start: 87000\n",
      "Batch 87000 gespeichert.\n",
      "Batch Start: 88000\n",
      "Batch 88000 gespeichert.\n",
      "Batch Start: 89000\n",
      "Batch 89000 gespeichert.\n",
      "Batch Start: 90000\n",
      "Batch 90000 gespeichert.\n",
      "Batch Start: 91000\n",
      "Batch 91000 gespeichert.\n",
      "Batch Start: 92000\n",
      "Batch 92000 gespeichert.\n",
      "Batch Start: 93000\n",
      "Batch 93000 gespeichert.\n",
      "Batch Start: 94000\n",
      "Batch 94000 gespeichert.\n",
      "Batch Start: 95000\n",
      "Batch 95000 gespeichert.\n",
      "Batch Start: 96000\n",
      "Batch 96000 gespeichert.\n",
      "Batch Start: 97000\n",
      "Batch 97000 gespeichert.\n",
      "Batch Start: 98000\n",
      "Batch 98000 gespeichert.\n",
      "Batch Start: 99000\n",
      "Batch 99000 gespeichert.\n",
      "Batch Start: 100000\n",
      "Batch 100000 gespeichert.\n",
      "Batch Start: 101000\n",
      "Batch 101000 gespeichert.\n",
      "Batch Start: 102000\n",
      "Batch 102000 gespeichert.\n",
      "Batch Start: 103000\n",
      "Batch 103000 gespeichert.\n",
      "Batch Start: 104000\n",
      "Batch 104000 gespeichert.\n",
      "Batch Start: 105000\n",
      "Batch 105000 gespeichert.\n",
      "Batch Start: 106000\n",
      "Batch 106000 gespeichert.\n",
      "Batch Start: 107000\n",
      "Batch 107000 gespeichert.\n",
      "Batch Start: 108000\n",
      "Batch 108000 gespeichert.\n",
      "Batch Start: 109000\n",
      "Batch 109000 gespeichert.\n",
      "Batch Start: 110000\n",
      "Batch 110000 gespeichert.\n",
      "Batch Start: 111000\n",
      "Batch 111000 gespeichert.\n",
      "Batch Start: 112000\n",
      "Batch 112000 gespeichert.\n",
      "Batch Start: 113000\n",
      "Batch 113000 gespeichert.\n",
      "Batch Start: 114000\n",
      "Batch 114000 gespeichert.\n",
      "Batch Start: 115000\n",
      "Batch 115000 gespeichert.\n",
      "Batch Start: 116000\n",
      "Batch 116000 gespeichert.\n",
      "Batch Start: 117000\n",
      "Batch 117000 gespeichert.\n",
      "Batch Start: 118000\n",
      "Batch 118000 gespeichert.\n",
      "Batch Start: 119000\n",
      "Batch 119000 gespeichert.\n",
      "Batch Start: 120000\n",
      "Batch 120000 gespeichert.\n",
      "Batch Start: 121000\n",
      "Batch 121000 gespeichert.\n",
      "Batch Start: 122000\n",
      "Batch 122000 gespeichert.\n",
      "Batch Start: 123000\n",
      "Batch 123000 gespeichert.\n",
      "Batch Start: 124000\n",
      "Batch 124000 gespeichert.\n",
      "Batch Start: 125000\n",
      "Batch 125000 gespeichert.\n",
      "Batch Start: 126000\n",
      "Batch 126000 gespeichert.\n",
      "Batch Start: 127000\n",
      "Batch 127000 gespeichert.\n",
      "Batch Start: 128000\n",
      "Batch 128000 gespeichert.\n",
      "Batch Start: 129000\n",
      "Batch 129000 gespeichert.\n",
      "Batch Start: 130000\n",
      "Batch 130000 gespeichert.\n",
      "Batch Start: 131000\n",
      "Batch 131000 gespeichert.\n",
      "Batch Start: 132000\n",
      "Batch 132000 gespeichert.\n",
      "Batch Start: 133000\n",
      "Batch 133000 gespeichert.\n",
      "Batch Start: 134000\n",
      "Batch 134000 gespeichert.\n",
      "Batch Start: 135000\n",
      "Batch 135000 gespeichert.\n",
      "Batch Start: 136000\n",
      "Batch 136000 gespeichert.\n",
      "Batch Start: 137000\n",
      "Batch 137000 gespeichert.\n",
      "Batch Start: 138000\n",
      "Batch 138000 gespeichert.\n",
      "Batch Start: 139000\n",
      "Batch 139000 gespeichert.\n",
      "Batch Start: 140000\n",
      "Batch 140000 gespeichert.\n",
      "Batch Start: 141000\n",
      "Batch 141000 gespeichert.\n",
      "Batch Start: 142000\n",
      "Batch 142000 gespeichert.\n",
      "Batch Start: 143000\n",
      "Batch 143000 gespeichert.\n",
      "Batch Start: 144000\n",
      "Batch 144000 gespeichert.\n",
      "Batch Start: 145000\n",
      "Batch 145000 gespeichert.\n",
      "Batch Start: 146000\n",
      "Batch 146000 gespeichert.\n",
      "Batch Start: 147000\n",
      "Batch 147000 gespeichert.\n",
      "Batch Start: 148000\n",
      "Batch 148000 gespeichert.\n",
      "Batch Start: 149000\n",
      "Batch 149000 gespeichert.\n",
      "Batch Start: 150000\n",
      "Batch 150000 gespeichert.\n",
      "Batch Start: 151000\n",
      "Batch 151000 gespeichert.\n",
      "Batch Start: 152000\n",
      "Batch 152000 gespeichert.\n",
      "Batch Start: 153000\n",
      "Batch 153000 gespeichert.\n",
      "Batch Start: 154000\n",
      "Batch 154000 gespeichert.\n",
      "Batch Start: 155000\n",
      "Batch 155000 gespeichert.\n",
      "Batch Start: 156000\n",
      "Batch 156000 gespeichert.\n",
      "Batch Start: 157000\n",
      "Batch 157000 gespeichert.\n",
      "Batch Start: 158000\n",
      "Batch 158000 gespeichert.\n",
      "Batch Start: 159000\n",
      "Batch 159000 gespeichert.\n",
      "Batch Start: 160000\n",
      "Batch 160000 gespeichert.\n",
      "Batch Start: 161000\n",
      "Batch 161000 gespeichert.\n",
      "Batch Start: 162000\n",
      "Batch 162000 gespeichert.\n",
      "Batch Start: 163000\n",
      "Batch 163000 gespeichert.\n",
      "Batch Start: 164000\n",
      "Batch 164000 gespeichert.\n",
      "Batch Start: 165000\n",
      "Batch 165000 gespeichert.\n",
      "Batch Start: 166000\n",
      "Batch 166000 gespeichert.\n",
      "Batch Start: 167000\n",
      "Batch 167000 gespeichert.\n",
      "Batch Start: 168000\n",
      "Batch 168000 gespeichert.\n",
      "Batch Start: 169000\n",
      "Batch 169000 gespeichert.\n",
      "Batch Start: 170000\n",
      "Batch 170000 gespeichert.\n",
      "Batch Start: 171000\n",
      "Batch 171000 gespeichert.\n",
      "Batch Start: 172000\n",
      "Batch 172000 gespeichert.\n",
      "Batch Start: 173000\n",
      "Batch 173000 gespeichert.\n",
      "Batch Start: 174000\n",
      "Batch 174000 gespeichert.\n",
      "Batch Start: 175000\n",
      "Batch 175000 gespeichert.\n",
      "Batch Start: 176000\n",
      "Batch 176000 gespeichert.\n",
      "Batch Start: 177000\n",
      "Batch 177000 gespeichert.\n",
      "Batch Start: 178000\n",
      "Batch 178000 gespeichert.\n",
      "Batch Start: 179000\n",
      "Batch 179000 gespeichert.\n",
      "Batch Start: 180000\n",
      "Batch 180000 gespeichert.\n",
      "Batch Start: 181000\n",
      "Batch 181000 gespeichert.\n",
      "Batch Start: 182000\n",
      "Batch 182000 gespeichert.\n",
      "Batch Start: 183000\n",
      "Batch 183000 gespeichert.\n",
      "Batch Start: 184000\n",
      "Batch 184000 gespeichert.\n",
      "Batch Start: 185000\n",
      "Batch 185000 gespeichert.\n",
      "Batch Start: 186000\n",
      "Batch 186000 gespeichert.\n",
      "Batch Start: 187000\n",
      "Batch 187000 gespeichert.\n",
      "Batch Start: 188000\n",
      "Batch 188000 gespeichert.\n",
      "Batch Start: 189000\n",
      "Batch 189000 gespeichert.\n",
      "             TCR_name       TRAV    TRAJ           TRA_CDR3      TRBV  \\\n",
      "0  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "1  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "2  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "3  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "4  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "\n",
      "      TRBJ           TRB_CDR3  TRAC   TRBC     Epitope          MHC Binding  \\\n",
      "0  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   VTEHDTLLY  HLA-A*01:01       0   \n",
      "1  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   KTWGQYWQV  HLA-A*02:01       0   \n",
      "2  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2  ELAGIGILTV  HLA-A*02:01       0   \n",
      "3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2  CLLWSFQTSA  HLA-A*02:01       0   \n",
      "4  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   IMDQVPFSV  HLA-A*02:01       0   \n",
      "\n",
      "  task  \n",
      "0  nan  \n",
      "1  nan  \n",
      "2  nan  \n",
      "3  nan  \n",
      "4  nan  \n",
      "Datei erfolgreich gespeichert!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Annahme: all_donors_consensus und all_donors_meta sind bereits geladen und gefiltert\n",
    "\n",
    "# Festlegen der Batch-Größe für die Verarbeitung\n",
    "batch_size = 1000\n",
    "\n",
    "# Identifizieren von Epitope-Spalten, aber ohne \"NR(B0801)_AAKGRGAAL_NC_binder\"\n",
    "epitope_columns = [col for col in all_donors_consensus.columns if '_binder' in col and col != \"NR(B0801)_AAKGRGAAL_NC_binder\"]\n",
    "\n",
    "# Ausgabe-Datei für Batch-Ergebnisse\n",
    "output_batch_file = f'{pipeline_data_plain}/10x/expanded_batches.csv'\n",
    "\n",
    "# Stelle sicher, dass die Ausgabedatei leer ist\n",
    "with open(output_batch_file, 'w') as f:\n",
    "    f.write('')  # Leere Datei erstellen\n",
    "\n",
    "# Verarbeite `all_donors_consensus` in Batches\n",
    "for batch_start in range(0, len(all_donors_consensus), batch_size):\n",
    "    print(f\"Batch Start: {batch_start}\")\n",
    "    # Definiere Batch\n",
    "    batch = all_donors_consensus.iloc[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Filtern auf Zeilen, die sowohl 'TRA:' als auch 'TRB:' in 'cell_clono_cdr3_aa' enthalten\n",
    "    batch_paired = batch[\n",
    "        batch['cell_clono_cdr3_aa'].str.contains(\"TRA:\", na=False) &\n",
    "        batch['cell_clono_cdr3_aa'].str.contains(\"TRB:\", na=False)\n",
    "    ]\n",
    "\n",
    "    # Liste, um Zeilen für diesen Batch zu speichern\n",
    "    expanded_rows = []\n",
    "    \n",
    "    # Iteriere durch jede Zeile im Batch\n",
    "    for _, row in batch_paired.iterrows():\n",
    "        for col in epitope_columns:\n",
    "            # Extrahiere MHC und Epitope\n",
    "            match = re.match(r'([A-Z0-9]+)_([A-Z]+)_.*_binder', col)\n",
    "            if match:\n",
    "                mhc_raw, epitope = match.groups()\n",
    "                mhc_formatted = f'HLA-{mhc_raw[0]}*{mhc_raw[1:3]}:{mhc_raw[3:]}'\n",
    "\n",
    "                # Füge `Epitope` und `MHC` zur Zeile hinzu\n",
    "                new_row = row.copy()\n",
    "                new_row['Epitope'] = epitope\n",
    "                new_row['MHC'] = mhc_formatted\n",
    "\n",
    "                # Neue Zeile zur Batch-Liste hinzufügen\n",
    "                expanded_rows.append(new_row)\n",
    "\n",
    "    # Erstelle einen DataFrame aus dem Batch\n",
    "    batch_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    # Füge den Batch direkt in die Datei ein\n",
    "    batch_df.to_csv(output_batch_file, mode='a', index=False, header=not batch_start, sep=',')\n",
    "    print(f\"Batch {batch_start} gespeichert.\")\n",
    "\n",
    "# Laden der gespeicherten Batch-Ergebnisse\n",
    "expanded_df = pd.read_csv(output_batch_file)\n",
    "\n",
    "# Nur die Paired-Einträge in `all_donors_meta` beibehalten\n",
    "# Filtern auf Barcodes, die sowohl eine TRA- als auch eine TRB-Kette haben\n",
    "paired_barcodes = all_donors_meta.groupby('barcode').filter(\n",
    "    lambda x: set(x['chain']) == {'TRA', 'TRB'}\n",
    ")['barcode'].unique()\n",
    "all_donors_meta_paired = all_donors_meta[all_donors_meta['barcode'].isin(paired_barcodes)]\n",
    "\n",
    "# Split `all_donors_meta_paired` nach `chain` in separate DataFrames für TRA und TRB\n",
    "alpha_chain = all_donors_meta_paired[all_donors_meta_paired['chain'] == 'TRA'].rename(\n",
    "    columns={'v_gene': 'TRAV', 'j_gene': 'TRAJ', 'cdr3': 'TRA_CDR3', 'c_gene': 'TRAC'}\n",
    ")\n",
    "beta_chain = all_donors_meta_paired[all_donors_meta_paired['chain'] == 'TRB'].rename(\n",
    "    columns={'v_gene': 'TRBV', 'j_gene': 'TRBJ', 'cdr3': 'TRB_CDR3', 'c_gene': 'TRBC'}\n",
    ")\n",
    "\n",
    "# Zusammenführen von alpha_chain und beta_chain anhand der gemeinsamen 'barcode'-Spalte\n",
    "paired_meta = pd.merge(alpha_chain, beta_chain, on='barcode', suffixes=('_alpha', '_beta'))\n",
    "\n",
    "# Zusammenführen von `paired_meta` mit `expanded_df` anhand der 'barcode'-Spalte\n",
    "merged_df = pd.merge(paired_meta, expanded_df[['barcode', 'Epitope', 'MHC']], on='barcode', how='inner')\n",
    "\n",
    "# Spalten umbenennen und Format anpassen\n",
    "merged_df = merged_df.rename(columns={'barcode': 'TCR_name'})\n",
    "\n",
    "# Fehlende Spalten auffüllen\n",
    "desired_columns = [\n",
    "    'TCR_name', 'TRAV', 'TRAJ', 'TRA_CDR3', 'TRBV', 'TRBJ', 'TRB_CDR3', 'TRAC', 'TRBC', \n",
    "    'Epitope', 'MHC', 'Binding', 'task'\n",
    "]\n",
    "for col in desired_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = 'nan' if col == 'task' else '0'\n",
    "\n",
    "# Nur die gewünschten Spalten beibehalten und Zeilen mit `None` in `TRB_CDR3` entfernen\n",
    "final_df = merged_df[desired_columns]\n",
    "final_df = final_df[final_df['TRB_CDR3'] != 'None']\n",
    "\n",
    "final_df = final_df[\n",
    "    final_df['TRB_CDR3'].notna() & (final_df['TRB_CDR3'] != '') &\n",
    "    final_df['TRA_CDR3'].notna() & (final_df['TRA_CDR3'] != '')\n",
    "]\n",
    "\n",
    "# Ausgabe des ersten Teils des Ergebnisses zur Überprüfung\n",
    "print(final_df.head())\n",
    "\n",
    "# Optional: Speichern des kombinierten DataFrames\n",
    "output_path = f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_paired_only_expanded-all.csv'\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(\"Datei erfolgreich gespeichert!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alle Datensätze wurden erfolgreich gespeichert.\n",
      "Train: 41522 Einträge\n",
      "-Unique TCRs: 22801\n",
      "-Unique Epitope: 520\n",
      "Validation: 31134 Einträge\n",
      "-Unique TCRs: 15392\n",
      "-Unique Epitope: 1031\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Daten laden\n",
    "paired_df = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_paired_only_expanded-all.csv', sep=',')\n",
    "\n",
    "# Schritt 1: Aufteilung in Train und Validation (keine Test-Daten mehr)\n",
    "train_split, validation_split = train_test_split(paired_df, test_size=0.2, random_state=42)  # 20% für Validation\n",
    "\n",
    "# Positive Samples laden >> files zuerst umbenennen\n",
    "train_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/paired/train_prenegsamples.tsv', sep='\\t')\n",
    "validation_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/paired/validation_prenegsamples.tsv', sep='\\t')\n",
    "\n",
    "# Anzahl positiver Samples\n",
    "num_train_pos = len(train_preneg)\n",
    "num_validation_pos = len(validation_preneg)\n",
    "\n",
    "# Zielgrößen für negative Samples\n",
    "train_neg_needed = num_train_pos\n",
    "validation_neg_needed = num_validation_pos * 5\n",
    "\n",
    "# Funktion zur Sicherstellung, dass alle unique Epitope erhalten bleiben\n",
    "def ensure_unique_epitopes(df, target_count):\n",
    "    unique_epitopes = df['Epitope'].unique()\n",
    "    guaranteed_samples = []\n",
    "    \n",
    "    # Stelle sicher, dass jedes Epitope mindestens einmal vorkommt\n",
    "    for epitope in unique_epitopes:\n",
    "        epitope_group = df[df['Epitope'] == epitope]\n",
    "        if len(epitope_group) > 0:\n",
    "            guaranteed_samples.append(epitope_group.sample(1, random_state=42))\n",
    "    \n",
    "    # Kombiniere garantierte Samples\n",
    "    guaranteed_df = pd.concat(guaranteed_samples, ignore_index=True)\n",
    "    \n",
    "    # Berechne verbleibende Anzahl an Samples\n",
    "    remaining_count = target_count - len(guaranteed_df)\n",
    "    if remaining_count > 0:\n",
    "        remaining_samples = df.sample(remaining_count, random_state=42, replace=True)\n",
    "        return pd.concat([guaranteed_df, remaining_samples], ignore_index=True)\n",
    "    return guaranteed_df\n",
    "\n",
    "# Balancierung der Splits\n",
    "train_balanced_negatives = ensure_unique_epitopes(train_split, train_neg_needed)\n",
    "validation_balanced_negatives = ensure_unique_epitopes(validation_split, validation_neg_needed)\n",
    "\n",
    "# Positive und negative Samples kombinieren\n",
    "train_combined = pd.concat([train_preneg, train_balanced_negatives], ignore_index=True)\n",
    "validation_combined = pd.concat([validation_preneg, validation_balanced_negatives], ignore_index=True)\n",
    "\n",
    "# Speichern der kombinierten Datensätze\n",
    "output_dir = f'{pipeline_data_splitted}/{precision}/paired/'\n",
    "train_combined.to_csv(output_dir + \"train.tsv\", sep='\\t', index=False)\n",
    "validation_combined.to_csv(output_dir + \"validation.tsv\", sep='\\t', index=False)\n",
    "\n",
    "# Berechne Unique Werte\n",
    "unique_tcr_train = train_combined['TRB_CDR3'].nunique()\n",
    "unique_epitope_train = train_combined['Epitope'].nunique()\n",
    "unique_tcr_validation = validation_combined['TRB_CDR3'].nunique()\n",
    "unique_epitope_validation = validation_combined['Epitope'].nunique()\n",
    "\n",
    "# Finale Ausgabe\n",
    "print(\"\\nAlle Datensätze wurden erfolgreich gespeichert.\")\n",
    "print(f\"Train: {len(train_combined)} Einträge\")\n",
    "print(f\"-Unique TCRs: {unique_tcr_train}\")\n",
    "print(f\"-Unique Epitope: {unique_epitope_train}\")\n",
    "print(f\"Validation: {len(validation_combined)} Einträge\")\n",
    "print(f\"-Unique TCRs: {unique_tcr_validation}\")\n",
    "print(f\"-Unique Epitope: {unique_epitope_validation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Classification \n",
    "The classification in the split notebook correct for positive only data. After adding negative data, some classifications might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "validation_file_name = 'validation.tsv'\n",
    "train_file_name = 'train.tsv'\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 41522 entries\n",
      "validate data has 31134 entries\n",
      "validate data has 15339 TPP1 tasks (seen tcr & seen epitopes).\n",
      "validate data has 12190 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validate data has 3605 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "validate data has 0 TPP4 tasks (seen tcr & unseen epitope).\n"
     ]
    }
   ],
   "source": [
    "# do the classification for paired data\n",
    "paired = True\n",
    "train_data_path = f'{paired_output_folder}/{train_file_name}'\n",
    "validation_data_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "\n",
    "%run data_scripts/data_preparation/classification_notest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allele\n",
      "train data has 41522 entries\n",
      "validate data has 31134 entries\n",
      "validate data has 21747 TPP1 tasks (old value: 15339) (seen tcr & seen epitopes).\n",
      "validate data has 5782 TPP2 tasks (old value: 12190) (unseen tcr & seen epitopes).\n",
      "validate data has 3063 TPP3 tasks (old value: 3605) (unseen tcr & unseen epitope).\n",
      "validate data has 542 TPP4 tasks (old value: 0) (seen tcr & unseen epitope).\n",
      "the train/validate ratio is 0.5714875578066505/0.4285124421933495\n",
      "../../data/splitted_datasets/allele/paired/validate_reclassified_paired_specific.tsv\n",
      "/home/ubuntu/arina/BA-Cancer-Immunotherapy\n",
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250310_170637-cle1dpsb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cle1dpsb' target=\"_blank\">earnest-donkey-21</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cle1dpsb' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cle1dpsb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/allele/paired)... Done. 0.3s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b5a1a35f9649b483bd290cb8899912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-donkey-21</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cle1dpsb' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cle1dpsb</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250310_170637-cle1dpsb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extended classification for paired data\n",
    "paired = True\n",
    "train_path = f'{paired_output_folder}/{train_file_name}'\n",
    "validation_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "output_path = f'{paired_output_folder}/validate_reclassified_paired_specific.tsv'\n",
    "paired_data_path = paired_output_folder\n",
    "alpha_cdr3_name = 'TRA_CDR3'\n",
    "beta_cdr3_name = 'TRB_CDR3'\n",
    "epitope_name = 'Epitope'\n",
    "task_name = 'task'\n",
    "\n",
    "%run data_scripts/data_preparation/paired_reclassification_notest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52141/1243711732.py:1: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(train_data_path, sep=\"\\t\")\n",
      "/tmp/ipykernel_52141/1243711732.py:2: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_validation = pd.read_csv(validation_data_path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 280252 entries\n",
      "validate data has 210192 entries\n",
      "validate data has 163334 TPP1 tasks (seen tcr & seen epitopes).\n",
      "validate data has 44432 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validate data has 2412 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "validate data has 14 TPP4 tasks (seen tcr & unseen epitope).\n"
     ]
    }
   ],
   "source": [
    "# do the classification for beta data\n",
    "paired = False\n",
    "train_data_path = f'{beta_output_folder}/{train_file_name}'\n",
    "validation_data_path = f'{beta_output_folder}/{validation_file_name}'\n",
    "\n",
    "%run data_scripts/data_preparation/classification_notest.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells the classification is checked. If the output says \"Classification is correct\", everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 41522 entries\n",
      "validate data has 31134 entries\n",
      "validate data has 15339 TPP1 tasks (seen tcr & seen epitopes).\n",
      "validate data has 12190 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validate data has 3605 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "validate data has 0 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/validate ratio is 0.5714875578066505/0.4285124421933495\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    31134\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification paired\n",
    "paired = True\n",
    "splitted_data_path = paired_output_folder\n",
    "\n",
    "%run data_scripts/data_preparation/check_task_classification_paired_notest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52141/3803557983.py:15: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(f\"{splitted_data_path}/{train_file_name}\", sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 280252 entries\n",
      "validate data has 210192 entries\n",
      "validate data has 163334 TPP1 tasks (seen tcr & seen epitopes).\n",
      "validate data has 44432 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "validate data has 2412 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "validate data has 14 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/validate ratio is 0.5714250760535351/0.42857492394646485\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    210192\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification beta\n",
    "paired = False\n",
    "splitted_data_path = beta_output_folder\n",
    "\n",
    "%run data_scripts/data_preparation/check_task_classification_beta_notest.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'validation_prenegsamples.tsv', 'test.tsv', 'train.tsv', 'test_reclassified_paired_specific.tsv', 'validation.tsv', 'train_prenegsamples.tsv', 'validate_reclassified_paired_specific.tsv', 'test_prenegsamples.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(path_to_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250310_170705-t7qelcf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/t7qelcf4' target=\"_blank\">likely-grass-22</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/t7qelcf4' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/t7qelcf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/allele/paired)... Done. 0.2s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-grass-22</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/t7qelcf4' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/t7qelcf4</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250310_170705-t7qelcf4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# upload paired data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "dataset_name = f'paired_{precision}'\n",
    "#main_project_name = os.getenv(\"MAIN_PROJECT_NAME\")\n",
    "main_project_name = f\"dataset-{precision}\"\n",
    "\n",
    "%run data_scripts/upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250310_170711-u8gyl9sv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/u8gyl9sv' target=\"_blank\">dutiful-violet-23</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/u8gyl9sv' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/u8gyl9sv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/allele/beta)... Done. 1.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8d013391124c509f5cee09fadf34d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.197 MB of 46.627 MB uploaded\\r'), FloatProgress(value=0.025671545897824336, max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-violet-23</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/u8gyl9sv' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/u8gyl9sv</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250310_170711-u8gyl9sv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload beta data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "dataset_name = f'beta_{precision}'\n",
    "\n",
    "%run data_scripts/upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings >> ProtBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Sollte True zurückgeben\n",
    "print(torch.version.cuda)  # Sollte die richtige CUDA-Version anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Loading: Rostlab/prot_bert\n",
      "Model is on device: cuda:0\n",
      "Processing Batch:  0 64\n",
      "Processing Batch:  64 128\n",
      "Processing Batch:  128 192\n",
      "Processing Batch:  192 256\n",
      "Processing Batch:  256 320\n",
      "Processing Batch:  320 384\n",
      "Processing Batch:  384 448\n",
      "Processing Batch:  448 512\n",
      "Processing Batch:  512 576\n",
      "Processing Batch:  576 640\n",
      "Processing Batch:  640 704\n",
      "Processing Batch:  704 768\n",
      "Processing Batch:  768 832\n",
      "Processing Batch:  832 896\n",
      "Processing Batch:  896 960\n",
      "Processing Batch:  960 1024\n",
      "Processing Batch:  1024 1088\n",
      "Processing Batch:  1088 1152\n",
      "Processing Batch:  1152 1216\n",
      "Processing Batch:  1216 1280\n",
      "Processing Batch:  1280 1344\n",
      "Processing Batch:  1344 1408\n",
      "Processing Batch:  1408 1472\n",
      "Processing Batch:  1472 1536\n",
      "Processing Batch:  1536 1600\n",
      "Processing Batch:  1600 1664\n",
      "Processing Batch:  1664 1728\n",
      "Processing Batch:  1728 1792\n",
      "Processing Batch:  1792 1856\n",
      "Processing Batch:  1856 1920\n",
      "Processing Batch:  1920 1984\n",
      "Processing Batch:  1984 2048\n",
      "Processing Batch:  2048 2112\n",
      "Processing Batch:  2112 2176\n",
      "Processing Batch:  2176 2240\n",
      "Processing Batch:  2240 2304\n",
      "Processing Batch:  2304 2368\n",
      "Processing Batch:  2368 2432\n",
      "Processing Batch:  2432 2496\n",
      "Processing Batch:  2496 2560\n",
      "Processing Batch:  2560 2624\n",
      "Processing Batch:  2624 2688\n",
      "Processing Batch:  2688 2752\n",
      "Processing Batch:  2752 2816\n",
      "Processing Batch:  2816 2880\n",
      "Processing Batch:  2880 2944\n",
      "Processing Batch:  2944 3008\n",
      "Processing Batch:  3008 3072\n",
      "Processing Batch:  3072 3136\n",
      "Processing Batch:  3136 3200\n",
      "Processing Batch:  3200 3264\n",
      "Processing Batch:  3264 3328\n",
      "Processing Batch:  3328 3392\n",
      "Processing Batch:  3392 3456\n",
      "Processing Batch:  3456 3520\n",
      "Processing Batch:  3520 3584\n",
      "Processing Batch:  3584 3648\n",
      "Processing Batch:  3648 3712\n",
      "Processing Batch:  3712 3776\n",
      "Processing Batch:  3776 3840\n",
      "Processing Batch:  3840 3904\n",
      "Processing Batch:  3904 3968\n",
      "Processing Batch:  3968 4032\n",
      "Processing Batch:  4032 4096\n",
      "Processing Batch:  4096 4160\n",
      "Processing Batch:  4160 4224\n",
      "Processing Batch:  4224 4288\n",
      "Processing Batch:  4288 4352\n",
      "Processing Batch:  4352 4416\n",
      "Processing Batch:  4416 4480\n",
      "Processing Batch:  4480 4544\n",
      "Processing Batch:  4544 4608\n",
      "Processing Batch:  4608 4672\n",
      "Processing Batch:  4672 4736\n",
      "Processing Batch:  4736 4800\n",
      "Processing Batch:  4800 4864\n",
      "Processing Batch:  4864 4928\n",
      "Processing Batch:  4928 4992\n",
      "Processing Batch:  4992 5056\n",
      "Processing Batch:  5056 5120\n",
      "Processing Batch:  5120 5184\n",
      "Processing Batch:  5184 5248\n",
      "Processing Batch:  5248 5312\n",
      "Processing Batch:  5312 5376\n",
      "Processing Batch:  5376 5440\n",
      "Processing Batch:  5440 5504\n",
      "Processing Batch:  5504 5568\n",
      "Processing Batch:  5568 5632\n",
      "Processing Batch:  5632 5696\n",
      "Processing Batch:  5696 5760\n",
      "Processing Batch:  5760 5824\n",
      "Processing Batch:  5824 5888\n",
      "Processing Batch:  5888 5952\n",
      "Processing Batch:  5952 6016\n",
      "Processing Batch:  6016 6080\n",
      "Processing Batch:  6080 6144\n",
      "Processing Batch:  6144 6208\n",
      "Processing Batch:  6208 6272\n",
      "Processing Batch:  6272 6336\n",
      "Processing Batch:  6336 6400\n",
      "Processing Batch:  6400 6464\n",
      "Processing Batch:  6464 6528\n",
      "Processing Batch:  6528 6592\n",
      "Processing Batch:  6592 6656\n",
      "Processing Batch:  6656 6720\n",
      "Processing Batch:  6720 6784\n",
      "Processing Batch:  6784 6848\n",
      "Processing Batch:  6848 6912\n",
      "Processing Batch:  6912 6976\n",
      "Processing Batch:  6976 7040\n",
      "Processing Batch:  7040 7104\n",
      "Processing Batch:  7104 7168\n",
      "Processing Batch:  7168 7232\n",
      "Processing Batch:  7232 7296\n",
      "Processing Batch:  7296 7360\n",
      "Processing Batch:  7360 7424\n",
      "Processing Batch:  7424 7488\n",
      "Processing Batch:  7488 7552\n",
      "Processing Batch:  7552 7616\n",
      "Processing Batch:  7616 7680\n",
      "Processing Batch:  7680 7744\n",
      "Processing Batch:  7744 7808\n",
      "Processing Batch:  7808 7872\n",
      "Processing Batch:  7872 7936\n",
      "Processing Batch:  7936 8000\n",
      "Processing Batch:  8000 8064\n",
      "Processing Batch:  8064 8128\n",
      "Processing Batch:  8128 8192\n",
      "Processing Batch:  8192 8256\n",
      "Processing Batch:  8256 8320\n",
      "Processing Batch:  8320 8384\n",
      "Processing Batch:  8384 8448\n",
      "Processing Batch:  8448 8512\n",
      "Processing Batch:  8512 8576\n",
      "Processing Batch:  8576 8640\n",
      "Processing Batch:  8640 8704\n",
      "Processing Batch:  8704 8768\n",
      "Processing Batch:  8768 8832\n",
      "Processing Batch:  8832 8896\n",
      "Processing Batch:  8896 8960\n",
      "Processing Batch:  8960 9024\n",
      "Processing Batch:  9024 9088\n",
      "Processing Batch:  9088 9152\n",
      "Processing Batch:  9152 9216\n",
      "Processing Batch:  9216 9280\n",
      "Processing Batch:  9280 9344\n",
      "Processing Batch:  9344 9408\n",
      "Processing Batch:  9408 9472\n",
      "Processing Batch:  9472 9536\n",
      "Processing Batch:  9536 9600\n",
      "Processing Batch:  9600 9664\n",
      "Processing Batch:  9664 9728\n",
      "Processing Batch:  9728 9792\n",
      "Processing Batch:  9792 9856\n",
      "Processing Batch:  9856 9920\n",
      "Processing Batch:  9920 9984\n",
      "Processing Batch:  9984 10048\n",
      "Processing Batch:  10048 10112\n",
      "Processing Batch:  10112 10176\n",
      "Processing Batch:  10176 10240\n",
      "Processing Batch:  10240 10304\n",
      "Processing Batch:  10304 10368\n",
      "Processing Batch:  10368 10432\n",
      "Processing Batch:  10432 10496\n",
      "Processing Batch:  10496 10560\n",
      "Processing Batch:  10560 10624\n",
      "Processing Batch:  10624 10688\n",
      "Processing Batch:  10688 10752\n",
      "Processing Batch:  10752 10816\n",
      "Processing Batch:  10816 10880\n",
      "Processing Batch:  10880 10944\n",
      "Processing Batch:  10944 11008\n",
      "Processing Batch:  11008 11072\n",
      "Processing Batch:  11072 11136\n",
      "Processing Batch:  11136 11200\n",
      "Processing Batch:  11200 11264\n",
      "Processing Batch:  11264 11328\n",
      "Processing Batch:  11328 11392\n",
      "Processing Batch:  11392 11456\n",
      "Processing Batch:  11456 11520\n",
      "Processing Batch:  11520 11584\n",
      "Processing Batch:  11584 11648\n",
      "Processing Batch:  11648 11712\n",
      "Processing Batch:  11712 11776\n",
      "Processing Batch:  11776 11840\n",
      "Processing Batch:  11840 11904\n",
      "Processing Batch:  11904 11968\n",
      "Processing Batch:  11968 12032\n",
      "Processing Batch:  12032 12096\n",
      "Processing Batch:  12096 12160\n",
      "Processing Batch:  12160 12224\n",
      "Processing Batch:  12224 12288\n",
      "Processing Batch:  12288 12352\n",
      "Processing Batch:  12352 12416\n",
      "Processing Batch:  12416 12480\n",
      "Processing Batch:  12480 12544\n",
      "Processing Batch:  12544 12608\n",
      "Processing Batch:  12608 12672\n",
      "Processing Batch:  12672 12736\n",
      "Processing Batch:  12736 12800\n",
      "Processing Batch:  12800 12864\n",
      "Processing Batch:  12864 12928\n",
      "Processing Batch:  12928 12992\n",
      "Processing Batch:  12992 13056\n",
      "Processing Batch:  13056 13120\n",
      "Processing Batch:  13120 13184\n",
      "Processing Batch:  13184 13248\n",
      "Processing Batch:  13248 13312\n",
      "Processing Batch:  13312 13376\n",
      "Processing Batch:  13376 13440\n",
      "Processing Batch:  13440 13504\n",
      "Processing Batch:  13504 13568\n",
      "Processing Batch:  13568 13632\n",
      "Processing Batch:  13632 13696\n",
      "Processing Batch:  13696 13760\n",
      "Processing Batch:  13760 13824\n",
      "Processing Batch:  13824 13888\n",
      "Processing Batch:  13888 13952\n",
      "Processing Batch:  13952 14016\n",
      "Processing Batch:  14016 14080\n",
      "Processing Batch:  14080 14144\n",
      "Processing Batch:  14144 14208\n",
      "Processing Batch:  14208 14272\n",
      "Processing Batch:  14272 14336\n",
      "Processing Batch:  14336 14400\n",
      "Processing Batch:  14400 14464\n",
      "Processing Batch:  14464 14528\n",
      "Processing Batch:  14528 14592\n",
      "Processing Batch:  14592 14656\n",
      "Processing Batch:  14656 14720\n",
      "Processing Batch:  14720 14784\n",
      "Processing Batch:  14784 14848\n",
      "Processing Batch:  14848 14912\n",
      "Processing Batch:  14912 14976\n",
      "Processing Batch:  14976 15040\n",
      "Processing Batch:  15040 15104\n",
      "Processing Batch:  15104 15168\n",
      "Processing Batch:  15168 15232\n",
      "Processing Batch:  15232 15296\n",
      "Processing Batch:  15296 15360\n",
      "Processing Batch:  15360 15424\n",
      "Processing Batch:  15424 15488\n",
      "Processing Batch:  15488 15552\n",
      "Processing Batch:  15552 15616\n",
      "Processing Batch:  15616 15680\n",
      "Processing Batch:  15680 15744\n",
      "Processing Batch:  15744 15808\n",
      "Processing Batch:  15808 15872\n",
      "Processing Batch:  15872 15936\n",
      "Processing Batch:  15936 16000\n",
      "Processing Batch:  16000 16064\n",
      "Processing Batch:  16064 16128\n",
      "Processing Batch:  16128 16192\n",
      "Processing Batch:  16192 16256\n",
      "Processing Batch:  16256 16320\n",
      "Processing Batch:  16320 16384\n",
      "Processing Batch:  16384 16448\n",
      "Processing Batch:  16448 16512\n",
      "Processing Batch:  16512 16576\n",
      "Processing Batch:  16576 16640\n",
      "Processing Batch:  16640 16704\n",
      "Processing Batch:  16704 16768\n",
      "Processing Batch:  16768 16832\n",
      "Processing Batch:  16832 16896\n",
      "Processing Batch:  16896 16960\n",
      "Processing Batch:  16960 17024\n",
      "Processing Batch:  17024 17088\n",
      "Processing Batch:  17088 17152\n",
      "Processing Batch:  17152 17216\n",
      "Processing Batch:  17216 17280\n",
      "Processing Batch:  17280 17344\n",
      "Processing Batch:  17344 17408\n",
      "Processing Batch:  17408 17472\n",
      "Processing Batch:  17472 17536\n",
      "Processing Batch:  17536 17600\n",
      "Processing Batch:  17600 17664\n",
      "Processing Batch:  17664 17728\n",
      "Processing Batch:  17728 17792\n",
      "Processing Batch:  17792 17856\n",
      "Processing Batch:  17856 17920\n",
      "Processing Batch:  17920 17984\n",
      "Processing Batch:  17984 18048\n",
      "Processing Batch:  18048 18112\n",
      "Processing Batch:  18112 18176\n",
      "Processing Batch:  18176 18240\n",
      "Processing Batch:  18240 18304\n",
      "Processing Batch:  18304 18368\n",
      "Processing Batch:  18368 18432\n",
      "Processing Batch:  18432 18496\n",
      "Processing Batch:  18496 18560\n",
      "Processing Batch:  18560 18624\n",
      "Processing Batch:  18624 18688\n",
      "Processing Batch:  18688 18752\n",
      "Processing Batch:  18752 18816\n",
      "Processing Batch:  18816 18880\n",
      "Processing Batch:  18880 18944\n",
      "Processing Batch:  18944 19008\n",
      "Processing Batch:  19008 19072\n",
      "Processing Batch:  19072 19136\n",
      "Processing Batch:  19136 19200\n",
      "Processing Batch:  19200 19264\n",
      "Processing Batch:  19264 19328\n",
      "Processing Batch:  19328 19392\n",
      "Processing Batch:  19392 19456\n",
      "Processing Batch:  19456 19520\n",
      "Processing Batch:  19520 19584\n",
      "Processing Batch:  19584 19648\n",
      "Processing Batch:  19648 19712\n",
      "Processing Batch:  19712 19776\n",
      "Processing Batch:  19776 19840\n",
      "Processing Batch:  19840 19904\n",
      "Processing Batch:  19904 19968\n",
      "Processing Batch:  19968 20032\n",
      "Processing Batch:  20032 20096\n",
      "Processing Batch:  20096 20160\n",
      "Processing Batch:  20160 20224\n",
      "Processing Batch:  20224 20288\n",
      "Processing Batch:  20288 20352\n",
      "Processing Batch:  20352 20416\n",
      "Processing Batch:  20416 20480\n",
      "Processing Batch:  20480 20544\n",
      "Processing Batch:  20544 20608\n",
      "Processing Batch:  20608 20672\n",
      "Processing Batch:  20672 20736\n",
      "Processing Batch:  20736 20800\n",
      "Processing Batch:  20800 20864\n",
      "Processing Batch:  20864 20928\n",
      "Processing Batch:  20928 20992\n",
      "Processing Batch:  20992 21056\n",
      "Processing Batch:  21056 21120\n",
      "Processing Batch:  21120 21184\n",
      "Processing Batch:  21184 21248\n",
      "Processing Batch:  21248 21312\n",
      "Processing Batch:  21312 21376\n",
      "Processing Batch:  21376 21440\n",
      "Processing Batch:  21440 21504\n",
      "Processing Batch:  21504 21568\n",
      "Processing Batch:  21568 21632\n",
      "Processing Batch:  21632 21696\n",
      "Processing Batch:  21696 21760\n",
      "Processing Batch:  21760 21824\n",
      "Processing Batch:  21824 21888\n",
      "Processing Batch:  21888 21952\n",
      "Processing Batch:  21952 22016\n",
      "Processing Batch:  22016 22080\n",
      "Processing Batch:  22080 22144\n",
      "Processing Batch:  22144 22208\n",
      "Processing Batch:  22208 22272\n",
      "Processing Batch:  22272 22336\n",
      "Processing Batch:  22336 22400\n",
      "Processing Batch:  22400 22464\n",
      "Processing Batch:  22464 22528\n",
      "Processing Batch:  22528 22592\n",
      "Processing Batch:  22592 22656\n",
      "Processing Batch:  22656 22720\n",
      "Processing Batch:  22720 22784\n",
      "Processing Batch:  22784 22848\n",
      "Processing Batch:  22848 22912\n",
      "Processing Batch:  22912 22976\n",
      "Processing Batch:  22976 23040\n",
      "Processing Batch:  23040 23104\n",
      "Processing Batch:  23104 23168\n",
      "Processing Batch:  23168 23232\n",
      "Processing Batch:  23232 23296\n",
      "Processing Batch:  23296 23360\n",
      "Processing Batch:  23360 23424\n",
      "Processing Batch:  23424 23488\n",
      "Processing Batch:  23488 23552\n",
      "Processing Batch:  23552 23616\n",
      "Processing Batch:  23616 23680\n",
      "Processing Batch:  23680 23744\n",
      "Processing Batch:  23744 23808\n",
      "Processing Batch:  23808 23872\n",
      "Processing Batch:  23872 23936\n",
      "Processing Batch:  23936 24000\n",
      "Processing Batch:  24000 24064\n",
      "Processing Batch:  24064 24128\n",
      "Processing Batch:  24128 24192\n",
      "Processing Batch:  24192 24256\n",
      "Processing Batch:  24256 24320\n",
      "Processing Batch:  24320 24384\n",
      "Processing Batch:  24384 24448\n",
      "Processing Batch:  24448 24512\n",
      "Processing Batch:  24512 24576\n",
      "Processing Batch:  24576 24640\n",
      "Processing Batch:  24640 24704\n",
      "Processing Batch:  24704 24768\n",
      "Processing Batch:  24768 24832\n",
      "Processing Batch:  24832 24896\n",
      "Processing Batch:  24896 24960\n",
      "Processing Batch:  24960 25024\n",
      "Processing Batch:  25024 25088\n",
      "Processing Batch:  25088 25152\n",
      "Processing Batch:  25152 25216\n",
      "Processing Batch:  25216 25280\n",
      "Processing Batch:  25280 25344\n",
      "Processing Batch:  25344 25408\n",
      "Processing Batch:  25408 25472\n",
      "Processing Batch:  25472 25536\n",
      "Processing Batch:  25536 25600\n",
      "Processing Batch:  25600 25664\n",
      "Processing Batch:  25664 25728\n",
      "Processing Batch:  25728 25792\n",
      "Processing Batch:  25792 25856\n",
      "Processing Batch:  25856 25920\n",
      "Processing Batch:  25920 25984\n",
      "Processing Batch:  25984 26048\n",
      "Processing Batch:  26048 26112\n",
      "Processing Batch:  26112 26176\n",
      "Processing Batch:  26176 26240\n",
      "Processing Batch:  26240 26304\n",
      "Processing Batch:  26304 26368\n",
      "Processing Batch:  26368 26432\n",
      "Processing Batch:  26432 26496\n",
      "Processing Batch:  26496 26560\n",
      "Processing Batch:  26560 26624\n",
      "Processing Batch:  26624 26688\n",
      "Processing Batch:  26688 26752\n",
      "Processing Batch:  26752 26816\n",
      "Processing Batch:  26816 26880\n",
      "Processing Batch:  26880 26944\n",
      "Processing Batch:  26944 27008\n",
      "Processing Batch:  27008 27072\n",
      "Processing Batch:  27072 27136\n",
      "Processing Batch:  27136 27200\n",
      "Processing Batch:  27200 27264\n",
      "Processing Batch:  27264 27328\n",
      "Processing Batch:  27328 27392\n",
      "Processing Batch:  27392 27456\n",
      "Processing Batch:  27456 27520\n",
      "Processing Batch:  27520 27584\n",
      "Processing Batch:  27584 27648\n",
      "Processing Batch:  27648 27712\n",
      "Processing Batch:  27712 27776\n",
      "Processing Batch:  27776 27840\n",
      "Processing Batch:  27840 27904\n",
      "Processing Batch:  27904 27968\n",
      "Processing Batch:  27968 28032\n",
      "Processing Batch:  28032 28096\n",
      "Processing Batch:  28096 28160\n",
      "Processing Batch:  28160 28224\n",
      "Processing Batch:  28224 28288\n",
      "Processing Batch:  28288 28352\n",
      "Processing Batch:  28352 28416\n",
      "Processing Batch:  28416 28480\n",
      "Processing Batch:  28480 28544\n",
      "Processing Batch:  28544 28608\n",
      "Processing Batch:  28608 28672\n",
      "Processing Batch:  28672 28736\n",
      "Processing Batch:  28736 28800\n",
      "Processing Batch:  28800 28864\n",
      "Processing Batch:  28864 28928\n",
      "Processing Batch:  28928 28992\n",
      "Processing Batch:  28992 29056\n",
      "Processing Batch:  29056 29120\n",
      "Processing Batch:  29120 29184\n",
      "Processing Batch:  29184 29248\n",
      "Processing Batch:  29248 29312\n",
      "Processing Batch:  29312 29376\n",
      "Processing Batch:  29376 29440\n",
      "Processing Batch:  29440 29504\n",
      "Processing Batch:  29504 29568\n",
      "Processing Batch:  29568 29632\n",
      "Processing Batch:  29632 29696\n",
      "Processing Batch:  29696 29760\n",
      "Processing Batch:  29760 29824\n",
      "Processing Batch:  29824 29888\n",
      "Processing Batch:  29888 29952\n",
      "Processing Batch:  29952 30016\n",
      "Processing Batch:  30016 30080\n",
      "Processing Batch:  30080 30144\n",
      "Processing Batch:  30144 30208\n",
      "Processing Batch:  30208 30272\n",
      "Processing Batch:  30272 30336\n",
      "Processing Batch:  30336 30400\n",
      "Processing Batch:  30400 30464\n",
      "Processing Batch:  30464 30528\n",
      "Processing Batch:  30528 30592\n",
      "Processing Batch:  30592 30656\n",
      "Processing Batch:  30656 30720\n",
      "Using GPU: Tesla T4\n",
      "Loading: Rostlab/prot_bert\n",
      "Model is on device: cuda:0\n",
      "Processing Batch:  0 64\n",
      "Processing Batch:  64 128\n",
      "Processing Batch:  128 192\n",
      "Processing Batch:  192 256\n",
      "Processing Batch:  256 320\n",
      "Processing Batch:  320 384\n",
      "Processing Batch:  384 448\n",
      "Processing Batch:  448 512\n",
      "Processing Batch:  512 576\n",
      "Processing Batch:  576 640\n",
      "Processing Batch:  640 704\n",
      "Processing Batch:  704 768\n",
      "Processing Batch:  768 832\n",
      "Processing Batch:  832 896\n",
      "Processing Batch:  896 960\n",
      "Processing Batch:  960 1024\n",
      "Processing Batch:  1024 1088\n",
      "Processing Batch:  1088 1152\n",
      "Processing Batch:  1152 1216\n",
      "Processing Batch:  1216 1280\n",
      "Processing Batch:  1280 1344\n",
      "Processing Batch:  1344 1408\n",
      "Processing Batch:  1408 1472\n",
      "Processing Batch:  1472 1536\n",
      "Processing Batch:  1536 1600\n",
      "Processing Batch:  1600 1664\n",
      "Processing Batch:  1664 1728\n",
      "Processing Batch:  1728 1792\n",
      "Processing Batch:  1792 1856\n",
      "Processing Batch:  1856 1920\n",
      "Processing Batch:  1920 1984\n",
      "Processing Batch:  1984 2048\n",
      "Processing Batch:  2048 2112\n",
      "Processing Batch:  2112 2176\n",
      "Processing Batch:  2176 2240\n",
      "Processing Batch:  2240 2304\n",
      "Processing Batch:  2304 2368\n",
      "Processing Batch:  2368 2432\n",
      "Processing Batch:  2432 2496\n",
      "Processing Batch:  2496 2560\n",
      "Processing Batch:  2560 2624\n",
      "Processing Batch:  2624 2688\n",
      "Processing Batch:  2688 2752\n",
      "Processing Batch:  2752 2816\n",
      "Processing Batch:  2816 2880\n",
      "Processing Batch:  2880 2944\n",
      "Processing Batch:  2944 3008\n",
      "Processing Batch:  3008 3072\n",
      "Processing Batch:  3072 3136\n",
      "Processing Batch:  3136 3200\n",
      "Processing Batch:  3200 3264\n",
      "Processing Batch:  3264 3328\n",
      "Processing Batch:  3328 3392\n",
      "Processing Batch:  3392 3456\n",
      "Processing Batch:  3456 3520\n",
      "Processing Batch:  3520 3584\n",
      "Processing Batch:  3584 3648\n",
      "Processing Batch:  3648 3712\n",
      "Processing Batch:  3712 3776\n",
      "Processing Batch:  3776 3840\n",
      "Processing Batch:  3840 3904\n",
      "Processing Batch:  3904 3968\n",
      "Processing Batch:  3968 4032\n",
      "Processing Batch:  4032 4096\n",
      "Processing Batch:  4096 4160\n",
      "Processing Batch:  4160 4224\n",
      "Processing Batch:  4224 4288\n",
      "Processing Batch:  4288 4352\n",
      "Processing Batch:  4352 4416\n",
      "Processing Batch:  4416 4480\n",
      "Processing Batch:  4480 4544\n",
      "Processing Batch:  4544 4608\n",
      "Processing Batch:  4608 4672\n",
      "Processing Batch:  4672 4736\n",
      "Processing Batch:  4736 4800\n",
      "Processing Batch:  4800 4864\n",
      "Processing Batch:  4864 4928\n",
      "Processing Batch:  4928 4992\n",
      "Processing Batch:  4992 5056\n",
      "Processing Batch:  5056 5120\n",
      "Processing Batch:  5120 5184\n",
      "Processing Batch:  5184 5248\n",
      "Processing Batch:  5248 5312\n",
      "Processing Batch:  5312 5376\n",
      "Processing Batch:  5376 5440\n",
      "Processing Batch:  5440 5504\n",
      "Processing Batch:  5504 5568\n",
      "Processing Batch:  5568 5632\n",
      "Processing Batch:  5632 5696\n",
      "Processing Batch:  5696 5760\n",
      "Processing Batch:  5760 5824\n",
      "Processing Batch:  5824 5888\n",
      "Processing Batch:  5888 5952\n",
      "Processing Batch:  5952 6016\n",
      "Processing Batch:  6016 6080\n",
      "Processing Batch:  6080 6144\n",
      "Processing Batch:  6144 6208\n",
      "Processing Batch:  6208 6272\n",
      "Processing Batch:  6272 6336\n",
      "Processing Batch:  6336 6400\n",
      "Processing Batch:  6400 6464\n",
      "Processing Batch:  6464 6528\n",
      "Processing Batch:  6528 6592\n",
      "Processing Batch:  6592 6656\n",
      "Processing Batch:  6656 6720\n",
      "Processing Batch:  6720 6784\n",
      "Processing Batch:  6784 6848\n",
      "Processing Batch:  6848 6912\n",
      "Processing Batch:  6912 6976\n",
      "Processing Batch:  6976 7040\n",
      "Processing Batch:  7040 7104\n",
      "Processing Batch:  7104 7168\n",
      "Processing Batch:  7168 7232\n",
      "Processing Batch:  7232 7296\n",
      "Processing Batch:  7296 7360\n",
      "Processing Batch:  7360 7424\n",
      "Processing Batch:  7424 7488\n",
      "Processing Batch:  7488 7552\n",
      "Processing Batch:  7552 7616\n",
      "Processing Batch:  7616 7680\n",
      "Processing Batch:  7680 7744\n",
      "Processing Batch:  7744 7808\n",
      "Processing Batch:  7808 7872\n",
      "Processing Batch:  7872 7936\n",
      "Processing Batch:  7936 8000\n",
      "Processing Batch:  8000 8064\n",
      "Processing Batch:  8064 8128\n",
      "Processing Batch:  8128 8192\n",
      "Processing Batch:  8192 8256\n",
      "Processing Batch:  8256 8320\n",
      "Processing Batch:  8320 8384\n",
      "Processing Batch:  8384 8448\n",
      "Processing Batch:  8448 8512\n",
      "Processing Batch:  8512 8576\n",
      "Processing Batch:  8576 8640\n",
      "Processing Batch:  8640 8704\n",
      "Processing Batch:  8704 8768\n",
      "Processing Batch:  8768 8832\n",
      "Processing Batch:  8832 8896\n",
      "Processing Batch:  8896 8960\n",
      "Processing Batch:  8960 9024\n",
      "Processing Batch:  9024 9088\n",
      "Processing Batch:  9088 9152\n",
      "Processing Batch:  9152 9216\n",
      "Processing Batch:  9216 9280\n",
      "Processing Batch:  9280 9344\n",
      "Processing Batch:  9344 9408\n",
      "Processing Batch:  9408 9472\n",
      "Processing Batch:  9472 9536\n",
      "Processing Batch:  9536 9600\n",
      "Processing Batch:  9600 9664\n",
      "Processing Batch:  9664 9728\n",
      "Processing Batch:  9728 9792\n",
      "Processing Batch:  9792 9856\n",
      "Processing Batch:  9856 9920\n",
      "Processing Batch:  9920 9984\n",
      "Processing Batch:  9984 10048\n",
      "Processing Batch:  10048 10112\n",
      "Processing Batch:  10112 10176\n",
      "Processing Batch:  10176 10240\n",
      "Processing Batch:  10240 10304\n",
      "Processing Batch:  10304 10368\n",
      "Processing Batch:  10368 10432\n",
      "Processing Batch:  10432 10496\n",
      "Processing Batch:  10496 10560\n",
      "Processing Batch:  10560 10624\n",
      "Processing Batch:  10624 10688\n",
      "Processing Batch:  10688 10752\n",
      "Processing Batch:  10752 10816\n",
      "Processing Batch:  10816 10880\n",
      "Processing Batch:  10880 10944\n",
      "Processing Batch:  10944 11008\n",
      "Processing Batch:  11008 11072\n",
      "Processing Batch:  11072 11136\n",
      "Processing Batch:  11136 11200\n",
      "Processing Batch:  11200 11264\n",
      "Processing Batch:  11264 11328\n",
      "Processing Batch:  11328 11392\n",
      "Processing Batch:  11392 11456\n",
      "Processing Batch:  11456 11520\n",
      "Processing Batch:  11520 11584\n",
      "Processing Batch:  11584 11648\n",
      "Processing Batch:  11648 11712\n",
      "Processing Batch:  11712 11776\n",
      "Processing Batch:  11776 11840\n",
      "Processing Batch:  11840 11904\n",
      "Processing Batch:  11904 11968\n",
      "Processing Batch:  11968 12032\n",
      "Processing Batch:  12032 12096\n",
      "Processing Batch:  12096 12160\n",
      "Processing Batch:  12160 12224\n",
      "Processing Batch:  12224 12288\n",
      "Processing Batch:  12288 12352\n",
      "Processing Batch:  12352 12416\n",
      "Processing Batch:  12416 12480\n",
      "Processing Batch:  12480 12544\n",
      "Processing Batch:  12544 12608\n",
      "Processing Batch:  12608 12672\n",
      "Processing Batch:  12672 12736\n",
      "Processing Batch:  12736 12800\n",
      "Processing Batch:  12800 12864\n",
      "Processing Batch:  12864 12928\n",
      "Processing Batch:  12928 12992\n",
      "Processing Batch:  12992 13056\n",
      "Processing Batch:  13056 13120\n",
      "Processing Batch:  13120 13184\n",
      "Processing Batch:  13184 13248\n",
      "Processing Batch:  13248 13312\n",
      "Processing Batch:  13312 13376\n",
      "Processing Batch:  13376 13440\n",
      "Processing Batch:  13440 13504\n",
      "Processing Batch:  13504 13568\n",
      "Processing Batch:  13568 13632\n",
      "Processing Batch:  13632 13696\n",
      "Processing Batch:  13696 13760\n",
      "Processing Batch:  13760 13824\n",
      "Processing Batch:  13824 13888\n",
      "Processing Batch:  13888 13952\n",
      "Processing Batch:  13952 14016\n",
      "Processing Batch:  14016 14080\n",
      "Processing Batch:  14080 14144\n",
      "Processing Batch:  14144 14208\n",
      "Processing Batch:  14208 14272\n",
      "Processing Batch:  14272 14336\n",
      "Processing Batch:  14336 14400\n",
      "Processing Batch:  14400 14464\n",
      "Processing Batch:  14464 14528\n",
      "Processing Batch:  14528 14592\n",
      "Processing Batch:  14592 14656\n",
      "Processing Batch:  14656 14720\n",
      "Processing Batch:  14720 14784\n",
      "Processing Batch:  14784 14848\n",
      "Processing Batch:  14848 14912\n",
      "Processing Batch:  14912 14976\n",
      "Processing Batch:  14976 15040\n",
      "Processing Batch:  15040 15104\n",
      "Processing Batch:  15104 15168\n",
      "Processing Batch:  15168 15232\n",
      "Processing Batch:  15232 15296\n",
      "Processing Batch:  15296 15360\n",
      "Processing Batch:  15360 15424\n",
      "Processing Batch:  15424 15488\n",
      "Processing Batch:  15488 15552\n",
      "Processing Batch:  15552 15616\n",
      "Processing Batch:  15616 15680\n",
      "Processing Batch:  15680 15744\n",
      "Processing Batch:  15744 15808\n",
      "Processing Batch:  15808 15872\n",
      "Processing Batch:  15872 15936\n",
      "Processing Batch:  15936 16000\n",
      "Processing Batch:  16000 16064\n",
      "Processing Batch:  16064 16128\n",
      "Processing Batch:  16128 16192\n",
      "Processing Batch:  16192 16256\n",
      "Processing Batch:  16256 16320\n",
      "Processing Batch:  16320 16384\n",
      "Processing Batch:  16384 16448\n",
      "Processing Batch:  16448 16512\n",
      "Processing Batch:  16512 16576\n",
      "Processing Batch:  16576 16640\n",
      "Processing Batch:  16640 16704\n",
      "Processing Batch:  16704 16768\n",
      "Processing Batch:  16768 16832\n",
      "Processing Batch:  16832 16896\n",
      "Processing Batch:  16896 16960\n",
      "Processing Batch:  16960 17024\n",
      "Processing Batch:  17024 17088\n",
      "Processing Batch:  17088 17152\n",
      "Processing Batch:  17152 17216\n",
      "Processing Batch:  17216 17280\n",
      "Processing Batch:  17280 17344\n",
      "Processing Batch:  17344 17408\n",
      "Processing Batch:  17408 17472\n",
      "Processing Batch:  17472 17536\n",
      "Processing Batch:  17536 17600\n",
      "Processing Batch:  17600 17664\n",
      "Processing Batch:  17664 17728\n",
      "Processing Batch:  17728 17792\n",
      "Processing Batch:  17792 17856\n",
      "Processing Batch:  17856 17920\n",
      "Processing Batch:  17920 17984\n",
      "Processing Batch:  17984 18048\n",
      "Processing Batch:  18048 18112\n",
      "Processing Batch:  18112 18176\n",
      "Processing Batch:  18176 18240\n",
      "Processing Batch:  18240 18304\n",
      "Processing Batch:  18304 18368\n",
      "Processing Batch:  18368 18432\n",
      "Processing Batch:  18432 18496\n",
      "Processing Batch:  18496 18560\n",
      "Processing Batch:  18560 18624\n",
      "Processing Batch:  18624 18688\n",
      "Processing Batch:  18688 18752\n",
      "Processing Batch:  18752 18816\n",
      "Processing Batch:  18816 18880\n",
      "Processing Batch:  18880 18944\n",
      "Processing Batch:  18944 19008\n",
      "Processing Batch:  19008 19072\n",
      "Processing Batch:  19072 19136\n",
      "Processing Batch:  19136 19200\n",
      "Processing Batch:  19200 19264\n",
      "Processing Batch:  19264 19328\n",
      "Processing Batch:  19328 19392\n",
      "Processing Batch:  19392 19456\n",
      "Processing Batch:  19456 19520\n",
      "Processing Batch:  19520 19584\n",
      "Processing Batch:  19584 19648\n",
      "Processing Batch:  19648 19712\n",
      "Processing Batch:  19712 19776\n",
      "Processing Batch:  19776 19840\n",
      "Processing Batch:  19840 19904\n",
      "Processing Batch:  19904 19968\n",
      "Processing Batch:  19968 20032\n",
      "Processing Batch:  20032 20096\n",
      "Processing Batch:  20096 20160\n",
      "Processing Batch:  20160 20224\n",
      "Processing Batch:  20224 20288\n",
      "Processing Batch:  20288 20352\n",
      "Processing Batch:  20352 20416\n",
      "Processing Batch:  20416 20480\n",
      "Processing Batch:  20480 20544\n",
      "Processing Batch:  20544 20608\n",
      "Processing Batch:  20608 20672\n",
      "Processing Batch:  20672 20736\n",
      "Processing Batch:  20736 20800\n",
      "Processing Batch:  20800 20864\n",
      "Processing Batch:  20864 20928\n",
      "Processing Batch:  20928 20992\n",
      "Processing Batch:  20992 21056\n",
      "Processing Batch:  21056 21120\n",
      "Processing Batch:  21120 21184\n",
      "Processing Batch:  21184 21248\n",
      "Processing Batch:  21248 21312\n",
      "Processing Batch:  21312 21376\n",
      "Processing Batch:  21376 21440\n",
      "Processing Batch:  21440 21504\n",
      "Processing Batch:  21504 21568\n",
      "Processing Batch:  21568 21632\n",
      "Processing Batch:  21632 21696\n",
      "Processing Batch:  21696 21760\n",
      "Processing Batch:  21760 21824\n",
      "Processing Batch:  21824 21888\n",
      "Processing Batch:  21888 21952\n",
      "Processing Batch:  21952 22016\n",
      "Processing Batch:  22016 22080\n",
      "Processing Batch:  22080 22144\n",
      "Processing Batch:  22144 22208\n",
      "Processing Batch:  22208 22272\n",
      "Processing Batch:  22272 22336\n",
      "Processing Batch:  22336 22400\n",
      "Processing Batch:  22400 22464\n",
      "Processing Batch:  22464 22528\n",
      "Processing Batch:  22528 22592\n",
      "Processing Batch:  22592 22656\n",
      "Processing Batch:  22656 22720\n",
      "Processing Batch:  22720 22784\n",
      "Processing Batch:  22784 22848\n",
      "Processing Batch:  22848 22912\n",
      "Processing Batch:  22912 22976\n",
      "Processing Batch:  22976 23040\n",
      "Processing Batch:  23040 23104\n",
      "Processing Batch:  23104 23168\n",
      "Processing Batch:  23168 23232\n",
      "Processing Batch:  23232 23296\n",
      "Processing Batch:  23296 23360\n",
      "Processing Batch:  23360 23424\n",
      "Processing Batch:  23424 23488\n",
      "Processing Batch:  23488 23552\n",
      "Processing Batch:  23552 23616\n",
      "Processing Batch:  23616 23680\n",
      "Processing Batch:  23680 23744\n",
      "Processing Batch:  23744 23808\n",
      "Processing Batch:  23808 23872\n",
      "Processing Batch:  23872 23936\n",
      "Processing Batch:  23936 24000\n",
      "Processing Batch:  24000 24064\n",
      "Processing Batch:  24064 24128\n",
      "Processing Batch:  24128 24192\n",
      "Processing Batch:  24192 24256\n",
      "Processing Batch:  24256 24320\n",
      "Processing Batch:  24320 24384\n",
      "Processing Batch:  24384 24448\n",
      "Processing Batch:  24448 24512\n",
      "Processing Batch:  24512 24576\n",
      "Processing Batch:  24576 24640\n",
      "Processing Batch:  24640 24704\n",
      "Processing Batch:  24704 24768\n"
     ]
    }
   ],
   "source": [
    "path_paired_validation = f\"{pipeline_data_splitted}/{precision}/paired/validation.tsv\"\n",
    "path_paired_train = f\"{pipeline_data_splitted}/{precision}/paired/train.tsv\"\n",
    "path_beta_validation = f\"{pipeline_data_splitted}/{precision}/beta/validation.tsv\"\n",
    "path_beta_train = f\"{pipeline_data_splitted}/{precision}/beta/train.tsv\"\n",
    "\n",
    "\n",
    "path_paired = f\"{pipeline_data}/embeddings/temp/{precision}/paired_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_paired)])\n",
    "df_paired_validation = pd.read_csv(path_paired_validation, sep=\"\\t\", index_col=False)\n",
    "df_paired_train = pd.read_csv(path_paired_train, sep=\"\\t\", index_col=False)\n",
    "df_paired = pd.concat([df_paired_validation, df_paired_train])\n",
    "df_paired.to_csv(path_paired, sep=\"\\t\", index=False)\n",
    "\n",
    "# paired\n",
    "%run data_scripts/generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz TRA_CDR3\n",
    "%run data_scripts/generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz TRB_CDR3\n",
    "%run data_scripts/generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz Epitope\n",
    "\n",
    "path_beta = f\"{pipeline_data}/embeddings/temp/{precision}/beta_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_beta)])\n",
    "df_beta_validation = pd.read_csv(path_beta_validation, sep=\"\\t\", index_col=False)\n",
    "df_beta_train = pd.read_csv(path_beta_train, sep=\"\\t\", index_col=False)\n",
    "df_beta = pd.concat([df_beta_validation, df_beta_train])\n",
    "df_beta.to_csv(path_beta, sep=\"\\t\", index=False)\n",
    "\n",
    "# beta\n",
    "%run data_scripts/generateEmbeddingsProtBERT.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/TRB_beta_embeddings.npz TRB_CDR3\n",
    "%run data_scripts/generateEmbeddingsProtBERT.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/Epitope_beta_embeddings.npz Epitope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 TRA Embedding Shape: (596417, 1024)\n",
      "📌 TRB Embedding Shape: (694427, 1024)\n",
      "📌 Epitope Embedding Shape: (12870, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Funktion, um Embeddings korrekt zu laden\n",
    "def load_embeddings(file_path):\n",
    "    npz_data = np.load(file_path)\n",
    "    all_keys = list(npz_data.keys())\n",
    "\n",
    "    # Falls Embeddings als einzelne Sequenzen gespeichert sind\n",
    "    if len(all_keys) > 1:\n",
    "        all_values = [npz_data[k] for k in all_keys]\n",
    "        return np.vstack(all_values)  # Alles zusammenfügen\n",
    "    else:\n",
    "        return npz_data[all_keys[0]]\n",
    "\n",
    "# Embeddings für TRA, TRB und Epitope laden\n",
    "tra_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz\")\n",
    "trb_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz\")\n",
    "epitope_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz\")\n",
    "\n",
    "# Ausgabe der finalen Shapes\n",
    "print(f\"📌 TRA Embedding Shape: {tra_embeddings.shape}\")\n",
    "print(f\"📌 TRB Embedding Shape: {trb_embeddings.shape}\")\n",
    "print(f\"📌 Epitope Embedding Shape: {epitope_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93065/3207364397.py:24: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 67422 (Binding=1: 33711, Binding=0: 33711, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 43356 (Binding=1: 7226, Binding=0: 36130, TPP1: 27972, TPP2: 15095, TPP3: 289)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 43344 (Binding=1: 7224, Binding=0: 36120, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 154122\n",
      "\n",
      "--- Beta Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 251750 (Binding=1: 125875, Binding=0: 125875, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 161844 (Binding=1: 26974, Binding=0: 134870, TPP1: 140896, TPP2: 20645, TPP3: 299)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 161838 (Binding=1: 26973, Binding=0: 134865, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 575432\n",
      "\n",
      "       Dataset   Train  Train_Binding_1  Train_Binding_0  Train_TPP1  \\\n",
      "0  Paired Gene   67422            33711            33711           0   \n",
      "1    Beta Gene  251750           125875           125875           0   \n",
      "\n",
      "   Train_TPP2  Train_TPP3  Train_TPP4    Test  Test_Binding_1  ...  Test_TPP3  \\\n",
      "0           0           0           0   43356            7226  ...        289   \n",
      "1           0           0           0  161844           26974  ...        299   \n",
      "\n",
      "   Test_TPP4  Validation  Validation_Binding_1  Validation_Binding_0  \\\n",
      "0          0       43344                  7224                 36120   \n",
      "1          4      161838                 26973                134865   \n",
      "\n",
      "   Validation_TPP1  Validation_TPP2  Validation_TPP3  Validation_TPP4   Total  \n",
      "0                0                0                0                0  154122  \n",
      "1                0                0                0                0  575432  \n",
      "\n",
      "[2 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = pipeline_data_splitted\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen für jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "    \n",
    "    # Zähle die Anzahl der Bindings 1 und 0 in jedem Datensatz\n",
    "    train_binding_counts = train_df['Binding'].value_counts()\n",
    "    test_binding_counts = test_df['Binding'].value_counts()\n",
    "    validation_binding_counts = validation_df['Binding'].value_counts()\n",
    "    \n",
    "    # Zähle die Anzahl der TPP1, TPP2, TPP3 Einträge in jedem Datensatz\n",
    "    train_task_counts = train_df['task'].value_counts()\n",
    "    test_task_counts = test_df['task'].value_counts()\n",
    "    validation_task_counts = validation_df['task'].value_counts()\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Train_Binding_1\": train_binding_counts.get(1, 0),\n",
    "        \"Train_Binding_0\": train_binding_counts.get(0, 0),\n",
    "        \"Train_TPP1\": train_task_counts.get(\"TPP1\", 0),\n",
    "        \"Train_TPP2\": train_task_counts.get(\"TPP2\", 0),\n",
    "        \"Train_TPP3\": train_task_counts.get(\"TPP3\", 0),\n",
    "        \"Train_TPP4\": train_task_counts.get(\"TPP4\", 0),\n",
    "        \"Test\": test_length,\n",
    "        \"Test_Binding_1\": test_binding_counts.get(1, 0),\n",
    "        \"Test_Binding_0\": test_binding_counts.get(0, 0),\n",
    "        \"Test_TPP1\": test_task_counts.get(\"TPP1\", 0),\n",
    "        \"Test_TPP2\": test_task_counts.get(\"TPP2\", 0),\n",
    "        \"Test_TPP3\": test_task_counts.get(\"TPP3\", 0),\n",
    "        \"Test_TPP4\": test_task_counts.get(\"TPP4\", 0),\n",
    "        \"Validation\": validation_length,\n",
    "        \"Validation_Binding_1\": validation_binding_counts.get(1, 0),\n",
    "        \"Validation_Binding_0\": validation_binding_counts.get(0, 0),\n",
    "        \"Validation_TPP1\": validation_task_counts.get(\"TPP1\", 0),\n",
    "        \"Validation_TPP2\": validation_task_counts.get(\"TPP2\", 0),\n",
    "        \"Validation_TPP3\": validation_task_counts.get(\"TPP3\", 0),\n",
    "        \"Validation_TPP4\": validation_task_counts.get(\"TPP4\", 0),\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]} (Binding=1: {lengths[\"Train_Binding_1\"]}, Binding=0: {lengths[\"Train_Binding_0\"]}, TPP1: {lengths[\"Train_TPP1\"]}, TPP2: {lengths[\"Train_TPP2\"]}, TPP3: {lengths[\"Train_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]} (Binding=1: {lengths[\"Test_Binding_1\"]}, Binding=0: {lengths[\"Test_Binding_0\"]}, TPP1: {lengths[\"Test_TPP1\"]}, TPP2: {lengths[\"Test_TPP2\"]}, TPP3: {lengths[\"Test_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]} (Binding=1: {lengths[\"Validation_Binding_1\"]}, Binding=0: {lengths[\"Validation_Binding_0\"]}, TPP1: {lengths[\"Validation_TPP1\"]}, TPP2: {lengths[\"Validation_TPP2\"]}, TPP3: {lengths[\"Validation_TPP3\"]})')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')\n",
    "\n",
    "# Optional: Ergebnisse in einer Übersichtstabelle darstellen\n",
    "summary_data = []\n",
    "for dataset, lengths in results.items():\n",
    "    summary_data.append({\n",
    "        \"Dataset\": dataset.replace(\"_\", \" \").title(),\n",
    "        \"Train\": lengths[\"Train\"],\n",
    "        \"Train_Binding_1\": lengths[\"Train_Binding_1\"],\n",
    "        \"Train_Binding_0\": lengths[\"Train_Binding_0\"],\n",
    "        \"Train_TPP1\": lengths[\"Train_TPP1\"],\n",
    "        \"Train_TPP2\": lengths[\"Train_TPP2\"],\n",
    "        \"Train_TPP3\": lengths[\"Train_TPP3\"],\n",
    "        \"Train_TPP4\": lengths[\"Train_TPP4\"],\n",
    "        \"Test\": lengths[\"Test\"],\n",
    "        \"Test_Binding_1\": lengths[\"Test_Binding_1\"],\n",
    "        \"Test_Binding_0\": lengths[\"Test_Binding_0\"],\n",
    "        \"Test_TPP1\": lengths[\"Test_TPP1\"],\n",
    "        \"Test_TPP2\": lengths[\"Test_TPP2\"],\n",
    "        \"Test_TPP3\": lengths[\"Test_TPP3\"],\n",
    "        \"Test_TPP4\": lengths[\"Test_TPP4\"],\n",
    "        \"Validation\": lengths[\"Validation\"],\n",
    "        \"Validation_Binding_1\": lengths[\"Validation_Binding_1\"],\n",
    "        \"Validation_Binding_0\": lengths[\"Validation_Binding_0\"],\n",
    "        \"Validation_TPP1\": lengths[\"Validation_TPP1\"],\n",
    "        \"Validation_TPP2\": lengths[\"Validation_TPP2\"],\n",
    "        \"Validation_TPP3\": lengths[\"Validation_TPP3\"],\n",
    "        \"Validation_TPP4\": lengths[\"Validation_TPP4\"],\n",
    "        \"Total\": lengths[\"Total\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/splitted_datasets/allele/paired/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, paths \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Daten laden\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m     validation_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/splitted_datasets/allele/paired/train.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = pipeline_data_splitted\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"paired_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    },\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen für jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "    \n",
    "    # Zähle die Anzahl der Bindings 1 und 0 in jedem Datensatz\n",
    "    train_binding_counts = train_df['Binding'].value_counts()\n",
    "    test_binding_counts = test_df['Binding'].value_counts()\n",
    "    validation_binding_counts = validation_df['Binding'].value_counts()\n",
    "    \n",
    "    # Zähle die Anzahl der TPP1, TPP2, TPP3 Einträge in jedem Datensatz\n",
    "    train_task_counts = train_df['task'].value_counts()\n",
    "    test_task_counts = test_df['task'].value_counts()\n",
    "    validation_task_counts = validation_df['task'].value_counts()\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Train_Binding_1\": train_binding_counts.get(1, 0),\n",
    "        \"Train_Binding_0\": train_binding_counts.get(0, 0),\n",
    "        \"Train_TPP1\": train_task_counts.get(\"TPP1\", 0),\n",
    "        \"Train_TPP2\": train_task_counts.get(\"TPP2\", 0),\n",
    "        \"Train_TPP3\": train_task_counts.get(\"TPP3\", 0),\n",
    "        \"Train_TPP4\": train_task_counts.get(\"TPP4\", 0),\n",
    "        \"Test\": test_length,\n",
    "        \"Test_Binding_1\": test_binding_counts.get(1, 0),\n",
    "        \"Test_Binding_0\": test_binding_counts.get(0, 0),\n",
    "        \"Test_TPP1\": test_task_counts.get(\"TPP1\", 0),\n",
    "        \"Test_TPP2\": test_task_counts.get(\"TPP2\", 0),\n",
    "        \"Test_TPP3\": test_task_counts.get(\"TPP3\", 0),\n",
    "        \"Test_TPP4\": test_task_counts.get(\"TPP4\", 0),\n",
    "        \"Validation\": validation_length,\n",
    "        \"Validation_Binding_1\": validation_binding_counts.get(1, 0),\n",
    "        \"Validation_Binding_0\": validation_binding_counts.get(0, 0),\n",
    "        \"Validation_TPP1\": validation_task_counts.get(\"TPP1\", 0),\n",
    "        \"Validation_TPP2\": validation_task_counts.get(\"TPP2\", 0),\n",
    "        \"Validation_TPP3\": validation_task_counts.get(\"TPP3\", 0),\n",
    "        \"Validation_TPP4\": validation_task_counts.get(\"TPP4\", 0),\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]} (Binding=1: {lengths[\"Train_Binding_1\"]}, Binding=0: {lengths[\"Train_Binding_0\"]}, TPP1: {lengths[\"Train_TPP1\"]}, TPP2: {lengths[\"Train_TPP2\"]}, TPP3: {lengths[\"Train_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]} (Binding=1: {lengths[\"Test_Binding_1\"]}, Binding=0: {lengths[\"Test_Binding_0\"]}, TPP1: {lengths[\"Test_TPP1\"]}, TPP2: {lengths[\"Test_TPP2\"]}, TPP3: {lengths[\"Test_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]} (Binding=1: {lengths[\"Validation_Binding_1\"]}, Binding=0: {lengths[\"Validation_Binding_0\"]}, TPP1: {lengths[\"Validation_TPP1\"]}, TPP2: {lengths[\"Validation_TPP2\"]}, TPP3: {lengths[\"Validation_TPP3\"]})')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')\n",
    "\n",
    "# Optional: Ergebnisse in einer Übersichtstabelle darstellen\n",
    "summary_data = []\n",
    "for dataset, lengths in results.items():\n",
    "    summary_data.append({\n",
    "        \"Dataset\": dataset.replace(\"_\", \" \").title(),\n",
    "        \"Train\": lengths[\"Train\"],\n",
    "        \"Train_Binding_1\": lengths[\"Train_Binding_1\"],\n",
    "        \"Train_Binding_0\": lengths[\"Train_Binding_0\"],\n",
    "        \"Train_TPP1\": lengths[\"Train_TPP1\"],\n",
    "        \"Train_TPP2\": lengths[\"Train_TPP2\"],\n",
    "        \"Train_TPP3\": lengths[\"Train_TPP3\"],\n",
    "        \"Train_TPP4\": lengths[\"Train_TPP4\"],\n",
    "        \"Test\": lengths[\"Test\"],\n",
    "        \"Test_Binding_1\": lengths[\"Test_Binding_1\"],\n",
    "        \"Test_Binding_0\": lengths[\"Test_Binding_0\"],\n",
    "        \"Test_TPP1\": lengths[\"Test_TPP1\"],\n",
    "        \"Test_TPP2\": lengths[\"Test_TPP2\"],\n",
    "        \"Test_TPP3\": lengths[\"Test_TPP3\"],\n",
    "        \"Test_TPP4\": lengths[\"Test_TPP4\"],\n",
    "        \"Validation\": lengths[\"Validation\"],\n",
    "        \"Validation_Binding_1\": lengths[\"Validation_Binding_1\"],\n",
    "        \"Validation_Binding_0\": lengths[\"Validation_Binding_0\"],\n",
    "        \"Validation_TPP1\": lengths[\"Validation_TPP1\"],\n",
    "        \"Validation_TPP2\": lengths[\"Validation_TPP2\"],\n",
    "        \"Validation_TPP3\": lengths[\"Validation_TPP3\"],\n",
    "        \"Validation_TPP4\": lengths[\"Validation_TPP4\"],\n",
    "        \"Total\": lengths[\"Total\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
