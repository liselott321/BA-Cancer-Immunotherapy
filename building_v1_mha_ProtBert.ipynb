{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short inspection in train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218269/381989945.py:8: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 280252\n",
      "Validation dataset length: 210192\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "\n",
    "# # Print the head (first few rows) and length (number of rows) of both datasets\n",
    "# print(\"Train dataset head:\")\n",
    "# print(train_df.head())  # First few rows of the training set\n",
    "print(f\"Train dataset length: {len(train_df)}\")\n",
    "\n",
    "# print(\"\\nValidation dataset head:\")\n",
    "# print(valid_df.head())  # First few rows of the validation set\n",
    "print(f\"Validation dataset length: {len(valid_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218269/3552120905.py:10: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset - Binding: 140126, Non-Binding: 140126\n",
      "Validation Dataset - Binding: 35032, Non-Binding: 175160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "t_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "v_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Function to count binding and non-binding rows\n",
    "def count_binding_rows(file_path):\n",
    "    # Read the TSV file\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Count the number of binding (1) and non-binding (0) rows\n",
    "    binding_count = df[df['Binding'] == 1].shape[0]\n",
    "    non_binding_count = df[df['Binding'] == 0].shape[0]\n",
    "    \n",
    "    return binding_count, non_binding_count\n",
    "\n",
    "# Count for training dataset\n",
    "train_binding, train_non_binding = count_binding_rows(t_path)\n",
    "print(f\"Training Dataset - Binding: {train_binding}, Non-Binding: {train_non_binding}\")\n",
    "\n",
    "# Count for validation dataset\n",
    "val_binding, val_non_binding = count_binding_rows(v_path)\n",
    "print(f\"Validation Dataset - Binding: {val_binding}, Non-Binding: {val_non_binding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218269/1833262170.py:9: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File paths\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "\n",
    "# Combine datasets\n",
    "data = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "\n",
    "# Ensure the columns exist\n",
    "if 'TRB_CDR3' not in data.columns or 'Epitope' not in data.columns:\n",
    "    raise ValueError(\"Columns 'TRB_CDR3' and 'Epitope' must exist in the dataset\")\n",
    "\n",
    "# Compute lengths\n",
    "data['tcr_length'] = data['TRB_CDR3'].astype(str).apply(len)\n",
    "data['epitope_length'] = data['Epitope'].astype(str).apply(len)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data['tcr_length'], bins=20, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('TCR Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('TCR Length Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data['epitope_length'], bins=20, color='green', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Epitope Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Epitope Length Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epitopes longer than 26: 3112\n",
      "Epitopes longer than 26:\n",
      "                                            Epitope  epitope_length\n",
      "100                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "114                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "119                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "164                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "306                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "...                                             ...             ...\n",
      "315193  LPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERD              43\n",
      "315197  ILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASA              43\n",
      "315255  MIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGV              43\n",
      "315271  ILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASA              43\n",
      "315279       WESGVKDCVVLHSYFTSDYYQLYSTQLSTDTGVEHVTF              38\n",
      "\n",
      "[3112 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter epitopes longer than 26\n",
    "long_epitopes = data[data['epitope_length'] > 26]\n",
    "\n",
    "# Count the number of long epitopes\n",
    "num_long_epitopes = long_epitopes.shape[0]\n",
    "\n",
    "# Display the long epitopes\n",
    "print(f\"Number of epitopes longer than 26: {num_long_epitopes}\")\n",
    "print(\"Epitopes longer than 26:\")\n",
    "print(long_epitopes[['Epitope', 'epitope_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigations on ProtBERT-Embeddings in oder to get shape, type, etc., and be able to use them in the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 1896\n",
      "\n",
      "Key: NLTTRTQL\n",
      "Shape: (8, 1024)\n",
      "Size: 8192\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.13486663  0.16832928  0.01105008 -0.2017876   0.21350099]\n",
      " [ 0.06380487  0.13675283  0.00786234 -0.12408309  0.31762993]\n",
      " [ 0.01229951  0.05960181  0.00698517 -0.0671403   0.08832411]\n",
      " [ 0.03610628  0.0980993   0.03087564 -0.11637626  0.03888167]\n",
      " [ 0.04467435  0.12075185  0.01568509 -0.10579667  0.06191917]]\n",
      "\n",
      "Key: FIYIFHTL\n",
      "Shape: (8, 1024)\n",
      "Size: 8192\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.10352622  0.19366597  0.01785575 -0.14839908  0.25590894]\n",
      " [-0.05904485  0.1726287   0.08648816  0.00153423  0.19045785]\n",
      " [-0.03943206  0.14551184  0.07295718  0.01637407  0.07709593]\n",
      " [-0.04497753  0.11167102 -0.02338432  0.08326313  0.1050052 ]\n",
      " [-0.02016015  0.17720129  0.05871242  0.01146071  0.08313055]]\n",
      "\n",
      "Key: YMHHMELPT\n",
      "Shape: (9, 1024)\n",
      "Size: 9216\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.06514921  0.11687686  0.04630813 -0.13314888  0.2626437 ]\n",
      " [ 0.03239926  0.09270856 -0.06613344  0.10825594  0.22238302]\n",
      " [-0.04459083  0.11944033  0.08202285  0.00903853  0.20343044]\n",
      " [-0.06591333  0.04584923 -0.0100627   0.03922262  0.07404124]\n",
      " [-0.06201258  0.0835928  -0.00328284  0.02340558  0.08391251]]\n",
      "\n",
      "Key: ILLDWAANI\n",
      "Shape: (9, 1024)\n",
      "Size: 9216\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.1645789   0.16454928  0.05097169 -0.18367876  0.22337942]\n",
      " [ 0.05207512  0.18722653  0.02419805 -0.058014    0.17848566]\n",
      " [-0.09758498 -0.02297855 -0.02284254  0.11435525  0.06869881]\n",
      " [-0.05901216 -0.02084499  0.03752796  0.11448745  0.06468341]\n",
      " [ 0.01347261  0.01105556  0.01085016  0.00422142  0.00759499]]\n",
      "\n",
      "Key: SMWALVISV\n",
      "Shape: (9, 1024)\n",
      "Size: 9216\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.13382603  0.17833568  0.05681703 -0.16807695  0.28101236]\n",
      " [ 0.0540608   0.02565792  0.01853698 -0.09408475  0.14658062]\n",
      " [ 0.03769435  0.03907148  0.00128629 -0.04829998  0.09988906]\n",
      " [ 0.01763713 -0.01054653 -0.04083464 -0.01348473 -0.00274556]\n",
      " [ 0.07827983  0.0613616  -0.03358259 -0.11400963  0.00127117]]\n",
      "\n",
      "Key: LYALVYFLQ\n",
      "Shape: (9, 1024)\n",
      "Size: 9216\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.08476291  0.20222266  0.06196603 -0.14673246  0.23006561]\n",
      " [-0.14318103  0.05591617 -0.00127477  0.11112293  0.14740887]\n",
      " [-0.06780659  0.08569615 -0.08425176  0.11367673  0.0485346 ]\n",
      " [-0.08426807  0.09359098 -0.00910662  0.06339386  0.02945401]\n",
      " [-0.17015631  0.00465459  0.00782919  0.14257677  0.07460892]]\n",
      "\n",
      "Key: WLPTGTLLV\n",
      "Shape: (9, 1024)\n",
      "Size: 9216\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 7.11493343e-02  1.65525571e-01  1.26987230e-02 -1.45161450e-01\n",
      "   2.32726499e-01]\n",
      " [ 8.07066783e-02  1.16721235e-01 -3.00281271e-02  1.76357739e-02\n",
      "   1.69594646e-01]\n",
      " [-5.53761609e-02  3.14663863e-03 -4.00667004e-02  7.39174262e-02\n",
      "   6.55133873e-02]\n",
      " [ 2.84234341e-02 -9.72590316e-03 -5.74579909e-02 -3.21520418e-02\n",
      "  -1.36363000e-01]\n",
      " [ 2.73284502e-02  1.30570233e-01 -2.20451001e-02 -8.79020765e-02\n",
      "  -1.00181984e-04]]\n",
      "\n",
      "Key: YVDDVVLGA\n",
      "Shape: (9, 1024)\n",
      "Size: 9216\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[-0.0024154   0.12843473  0.0225231  -0.14156406  0.28215185]\n",
      " [ 0.02715496  0.05313357 -0.09671949  0.07621586  0.05791095]\n",
      " [-0.04939542  0.0488166   0.03799681 -0.0058029  -0.05534845]\n",
      " [ 0.01083976  0.00643365 -0.03478385 -0.06003731 -0.10587035]\n",
      " [-0.01217522  0.01530864 -0.04609848 -0.05830718 -0.10603323]]\n",
      "\n",
      "Key: GTSGSPIVAR\n",
      "Shape: (10, 1024)\n",
      "Size: 10240\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.06974209  0.14879663  0.10697252 -0.17935663  0.24209325]\n",
      " [ 0.05517931 -0.03474814  0.06169648 -0.06058516  0.04527742]\n",
      " [ 0.08977984  0.03090146  0.00056062 -0.16347373  0.01186237]\n",
      " [ 0.0510754   0.01041487  0.10906803 -0.13072549  0.19618535]\n",
      " [ 0.10519237 -0.05887794  0.03077317 -0.04610283  0.01222026]]\n",
      "\n",
      "Key: LTGHMLDMY\n",
      "Shape: (9, 1024)\n",
      "Size: 9216\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.09455532  0.15318629  0.05579497 -0.13188438  0.2803143 ]\n",
      " [-0.09015333  0.02826832  0.01212277  0.1419175   0.25118333]\n",
      " [-0.07790209  0.13092007  0.01606615  0.01464872  0.06838296]\n",
      " [-0.05344576  0.07159731 -0.00455313  0.05558321  0.11078076]\n",
      " [-0.09847418  0.06165167 -0.0413266   0.02965492  0.07923073]]\n",
      "Number of keys in the NPZ file: 199653\n",
      "\n",
      "Key: CASSSTASRNTGELFF\n",
      "Shape: (16, 1024)\n",
      "Size: 16384\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.07469119  0.16057323 -0.02494635 -0.12556049  0.19221778]\n",
      " [-0.00656565  0.14308369 -0.03793573 -0.1554746   0.24289103]\n",
      " [ 0.0885237   0.06751317  0.04995612 -0.15602483 -0.00942309]\n",
      " [ 0.07394025  0.0131586   0.08071905 -0.12810881  0.08166827]\n",
      " [ 0.07159891  0.01876852  0.073502   -0.12457485  0.08144016]]\n",
      "\n",
      "Key: CASSLVTGEQYF\n",
      "Shape: (12, 1024)\n",
      "Size: 12288\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.07010764  0.12251393  0.02140945 -0.16270812  0.24841776]\n",
      " [ 0.05499206  0.11281241  0.03106381 -0.078941    0.20540527]\n",
      " [ 0.05064077  0.02822007 -0.00722912 -0.09067001  0.00904552]\n",
      " [ 0.02219009 -0.00742153  0.07775165 -0.05271496  0.12367377]\n",
      " [ 0.03937658  0.01345454  0.08577791 -0.06563974  0.11149757]]\n",
      "\n",
      "Key: CASSAHRGGYGYTF\n",
      "Shape: (14, 1024)\n",
      "Size: 14336\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.0259957   0.14121705  0.02583774 -0.16263919  0.21499923]\n",
      " [ 0.06158676  0.03402864  0.04027538 -0.07507148  0.15073666]\n",
      " [ 0.06063946  0.06699179  0.06803514 -0.07316865  0.07636785]\n",
      " [ 0.03237424 -0.00231018  0.11510685 -0.02450134  0.13999277]\n",
      " [ 0.03153361  0.01758576  0.11263457 -0.02732282  0.15621635]]\n",
      "\n",
      "Key: CASSLGRTGGNIQYF\n",
      "Shape: (15, 1024)\n",
      "Size: 15360\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.09804129  0.09820717  0.02049457 -0.12856779  0.21390699]\n",
      " [ 0.08903929  0.16557583  0.01409251 -0.13702932  0.2974042 ]\n",
      " [ 0.06773366  0.0675644   0.0435829  -0.11334272  0.06439715]\n",
      " [ 0.03478089  0.01890391  0.10661771 -0.05421938  0.16203704]\n",
      " [ 0.03930545  0.04148522  0.10854344 -0.04548183  0.14797124]]\n",
      "\n",
      "Key: CSARGQEGQYISYEQYF\n",
      "Shape: (17, 1024)\n",
      "Size: 17408\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.01932946  0.1382388  -0.00449134 -0.08856098  0.19165596]\n",
      " [ 0.02416678  0.1431091   0.04314493 -0.08867458  0.27244067]\n",
      " [ 0.00480766  0.02736266  0.06795171 -0.01584252  0.16667004]\n",
      " [ 0.06505071  0.10113262 -0.01426045 -0.04396826  0.0521609 ]\n",
      " [-0.02731858  0.04183019 -0.00057096  0.03345158  0.04350732]]\n",
      "\n",
      "Key: CASSGKQGCDTEAFF\n",
      "Shape: (15, 1024)\n",
      "Size: 15360\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.03795411  0.17988847  0.01271268 -0.13178729  0.21033947]\n",
      " [ 0.06393876  0.0085198   0.00458275 -0.03879464  0.00813764]\n",
      " [ 0.07353879  0.10551791  0.03774535 -0.13709953  0.02511349]\n",
      " [ 0.04934791  0.04270718  0.10120159 -0.10174271  0.10751786]\n",
      " [ 0.04988315  0.0786605   0.0916657  -0.08712282  0.11972284]]\n",
      "\n",
      "Key: CASHQTGGRDTEAFF\n",
      "Shape: (15, 1024)\n",
      "Size: 15360\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.04551863  0.14790724 -0.02047832 -0.15338682  0.21382557]\n",
      " [ 0.07782343  0.05539408 -0.04363305 -0.04090966  0.0998164 ]\n",
      " [ 0.05377225  0.11179271  0.01632312 -0.11428415  0.06922784]\n",
      " [ 0.01825636  0.04956283  0.09200361 -0.05137397  0.17221723]\n",
      " [-0.01347755  0.04456992  0.00423876 -0.01620701  0.06052136]]\n",
      "\n",
      "Key: CSASSPRLTSNQPQHF\n",
      "Shape: (16, 1024)\n",
      "Size: 16384\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.07574212  0.10970163  0.00150833 -0.13109115  0.12338831]\n",
      " [ 0.01861601  0.09567336  0.03823717 -0.10818074  0.16575798]\n",
      " [ 0.04485331  0.01937971  0.09304145 -0.09059825  0.1102045 ]\n",
      " [ 0.0963963   0.05458871  0.04254222 -0.11261372 -0.00557708]\n",
      " [ 0.06543909  0.02160901  0.11010893 -0.08391578  0.09368671]]\n",
      "\n",
      "Key: CASSIIDGINLSYNEQFF\n",
      "Shape: (18, 1024)\n",
      "Size: 18432\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 0.08439212  0.14210248  0.04210078 -0.12809177  0.25006005]\n",
      " [ 0.063409    0.16236842  0.00173241 -0.0832165   0.2772287 ]\n",
      " [-0.01487497  0.01825052  0.00953387 -0.09049478  0.02372776]\n",
      " [-0.04881179 -0.00047715  0.08077581 -0.03949557  0.12681617]\n",
      " [-0.04605204  0.02459216  0.07439072 -0.08289557  0.103013  ]]\n",
      "\n",
      "Key: CASSLVGGELFF\n",
      "Shape: (12, 1024)\n",
      "Size: 12288\n",
      "Data Type: float32\n",
      "Sample Data (first 5 elements):\n",
      "[[ 5.9192471e-02  1.5573145e-01  9.1706561e-03 -1.2763464e-01\n",
      "   2.5335079e-01]\n",
      " [-3.7754830e-02 -3.4782767e-02  1.4735702e-02 -6.7909606e-02\n",
      "   4.7309119e-02]\n",
      " [ 4.9934052e-02  1.9687060e-02 -1.8533036e-03 -8.5300505e-02\n",
      "  -1.5940756e-02]\n",
      " [ 2.9749207e-02  2.2412003e-04  7.5775057e-02 -4.1687630e-02\n",
      "   9.1509439e-02]\n",
      " [ 4.2623851e-02  3.2320082e-02  7.7929325e-02 -4.0884953e-02\n",
      "   7.5231127e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to the embeddings file\n",
    "paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "\n",
    "beta_all_epi_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "beta_all_trb_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "paths = [paired_all_epi_path, paired_all_tra_path,paired_all_trb_path ]\n",
    "\n",
    "paths_beta = [beta_all_epi_path, beta_all_trb_path]\n",
    "for path in paths_beta:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    # Inspect the shape and size of each stored array\n",
    "    for key in data.files[:10]:\n",
    "        array = data[key]\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        print(f\"Shape: {array.shape}\")\n",
    "        print(f\"Size: {array.size}\")\n",
    "        print(f\"Data Type: {array.dtype}\")\n",
    "        print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 1383\n",
      "Number of keys in the NPZ file: 41519\n",
      "Number of keys in the NPZ file: 45261\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to the embeddings file\n",
    "paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "paths = [paired_all_epi_path, paired_all_tra_path,paired_all_trb_path ]\n",
    "for path in paths:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    # Inspect the shape and size of each stored array\n",
    "    # for key in data.files[:10]:\n",
    "    #     array = data[key]\n",
    "    #     print(f\"\\nKey: {key}\")\n",
    "    #     print(f\"Shape: {array.shape}\")\n",
    "    #     print(f\"Size: {array.size}\")\n",
    "    #     print(f\"Data Type: {array.dtype}\")\n",
    "    #     print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 41519 but got size 45261 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m padded_trb \u001b[38;5;241m=\u001b[39m pad_embeddings(trb_embeddings, max_len)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Concatenate along sequence dimension\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m padded_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([padded_tra, padded_trb, padded_epi], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save padded embeddings\u001b[39;00m\n\u001b[1;32m     37\u001b[0m padd_paired_all_epi_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/paired/allele/padded_Epitope_paired_embeddings.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 41519 but got size 45261 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# raising error, because length tra != length trb.  To be solved.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Paths to the embeddings files\n",
    "paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "\n",
    "# Load NPZ files\n",
    "epi_data = np.load(paired_all_epi_path, allow_pickle=True)\n",
    "tra_data = np.load(paired_all_tra_path, allow_pickle=True)\n",
    "trb_data = np.load(paired_all_trb_path, allow_pickle=True)\n",
    "\n",
    "# Extract embeddings\n",
    "epi_embeddings = [torch.tensor(epi_data[key]) for key in epi_data]\n",
    "tra_embeddings = [torch.tensor(tra_data[key]) for key in tra_data]\n",
    "trb_embeddings = [torch.tensor(trb_data[key]) for key in trb_data]\n",
    "\n",
    "# Find max sequence length\n",
    "max_len = max(max(e.shape[0] for e in epi_embeddings), \n",
    "              max(e.shape[0] for e in tra_embeddings), \n",
    "              max(e.shape[0] for e in trb_embeddings))\n",
    "\n",
    "# Pad sequences\n",
    "def pad_embeddings(embeddings, max_len):\n",
    "    return pad_sequence([torch.nn.functional.pad(e, (0, 0, 0, max_len - e.shape[0])) for e in embeddings], batch_first=True, padding_value=0.0)\n",
    "\n",
    "padded_epi = pad_embeddings(epi_embeddings, max_len)\n",
    "padded_tra = pad_embeddings(tra_embeddings, max_len)\n",
    "padded_trb = pad_embeddings(trb_embeddings, max_len)\n",
    "\n",
    "# Concatenate along sequence dimension\n",
    "padded_combined = torch.cat([padded_tra, padded_trb, padded_epi], dim=1)\n",
    "\n",
    "# Save padded embeddings\n",
    "padd_paired_all_epi_path = '../../data/embeddings/paired/allele/padded_Epitope_paired_embeddings.npz'\n",
    "padd_paired_all_tra_path = '../../data/embeddings/paired/allele/padded_TRA_paired_embeddings.npz'\n",
    "padd_paired_all_trb_path = '../../data/embeddings/paired/allele/padded_TRB_paired_embeddings.npz'\n",
    "padd_paired_combined_path = '../../data/embeddings/paired/allele/padded_Combined_paired_embeddings.npz'\n",
    "\n",
    "np.savez(padd_paired_all_epi_path, **{key: padded_epi[i].numpy() for i, key in enumerate(epi_data)})\n",
    "np.savez(padd_paired_all_tra_path, **{key: padded_tra[i].numpy() for i, key in enumerate(tra_data)})\n",
    "np.savez(padd_paired_all_trb_path, **{key: padded_trb[i].numpy() for i, key in enumerate(trb_data)})\n",
    "np.savez(padd_paired_combined_path, combined=padded_combined.numpy())\n",
    "\n",
    "print(\"Padded and concatenated embeddings saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_all_epi_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "beta_all_trb_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from data_scripts.pca_analysis import perform_pca_on_embeddings\n",
    "\n",
    "def process_chunk(chunk, chunk_index, output_path, device='cuda', n_components=512):\n",
    "    \"\"\"\n",
    "    Fhrt PCA auf einem Chunk durch und speichert das reduzierte Ergebnis.\n",
    "    \"\"\"\n",
    "    # Auf CUDA verschieben\n",
    "    chunk_tensor = torch.tensor(chunk, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Zurck auf CPU und in NumPy konvertieren fr PCA\n",
    "    chunk_cpu = chunk_tensor.cpu().numpy()\n",
    "\n",
    "    # PCA durchfhren\n",
    "    pca_df = perform_pca_on_embeddings([chunk_cpu], n_components=n_components)\n",
    "\n",
    "    # Speichern des reduzierten Chunks\n",
    "    chunk_output_path = f\"{output_path}_chunk_{chunk_index}.npz\"\n",
    "    np.savez_compressed(chunk_output_path, embeddings=pca_df.values)\n",
    "    print(f\" Reduzierter Chunk {chunk_index} gespeichert unter: {chunk_output_path}\")\n",
    "\n",
    "def load_and_process_embeddings_in_chunks(file_path, output_path, chunk_size=100_000, device='cuda'):\n",
    "    \"\"\"\n",
    "    Ldt Embeddings in Blcken, fhrt PCA durch und speichert sie sofort.\n",
    "    \"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    print(f\" Verfgbare Keys: {len(data.files)}\")\n",
    "\n",
    "    current_chunk = []\n",
    "    chunk_index = 0\n",
    "\n",
    "    for idx, key in enumerate(data.files):\n",
    "        current_chunk.append(data[key])\n",
    "\n",
    "        # Wenn Chunk voll, dann verarbeiten und speichern\n",
    "        if len(current_chunk) * 8 >= chunk_size:  # Weil jeder Key 8 Samples hat\n",
    "            combined_chunk = np.concatenate(current_chunk, axis=0)\n",
    "            process_chunk(combined_chunk, chunk_index, output_path, device)\n",
    "            current_chunk = []  # Speicher freigeben\n",
    "            chunk_index += 1\n",
    "\n",
    "    # Verarbeite den letzten, unvollstndigen Chunk\n",
    "    if current_chunk:\n",
    "        combined_chunk = np.concatenate(current_chunk, axis=0)\n",
    "        process_chunk(combined_chunk, chunk_index, output_path, device)\n",
    "        print(f\" Letzter Chunk verarbeitet mit Shape: {combined_chunk.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verfgbare Keys: 211529\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183794, 1024)\n",
      "Gesamte Datenform vor PCA: (183794, 1024)\n",
      "Gesamte Datenform nach PCA: (183794, 512)\n",
      "Erklrte Varianz pro Komponente: [1.32249368e-01 9.58742275e-02 7.89532647e-02 5.11123302e-02\n",
      " 4.80737901e-02 4.30722391e-02 4.12162620e-02 3.67663143e-02\n",
      " 3.44270099e-02 3.00138312e-02 2.75371255e-02 2.53238905e-02\n",
      " 2.09061986e-02 1.88502574e-02 1.82200435e-02 1.76574678e-02\n",
      " 1.67295576e-02 1.39572318e-02 1.24031297e-02 1.13262613e-02\n",
      " 1.04606786e-02 9.60687032e-03 8.80338193e-03 8.47489202e-03\n",
      " 8.19804270e-03 7.24789258e-03 6.62234060e-03 6.46977093e-03\n",
      " 6.05263837e-03 5.62309360e-03 5.19391063e-03 4.63077482e-03\n",
      " 4.57076437e-03 4.37590878e-03 4.22315599e-03 3.85675877e-03\n",
      " 3.70877228e-03 3.50729256e-03 3.36813172e-03 3.19846308e-03\n",
      " 3.02063081e-03 2.81538770e-03 2.59925467e-03 2.51338564e-03\n",
      " 2.38639413e-03 2.29968182e-03 2.23009345e-03 2.17231169e-03\n",
      " 2.08578046e-03 1.99314245e-03 1.89377655e-03 1.78039604e-03\n",
      " 1.69840526e-03 1.66242810e-03 1.60023223e-03 1.50027460e-03\n",
      " 1.45836685e-03 1.40550723e-03 1.38354467e-03 1.30071209e-03\n",
      " 1.22352720e-03 1.17135390e-03 1.14210816e-03 1.09223570e-03\n",
      " 1.06541732e-03 1.00208841e-03 9.88587380e-04 9.66312127e-04\n",
      " 9.48534597e-04 9.07367833e-04 8.98717355e-04 8.62741397e-04\n",
      " 8.25732197e-04 7.99276721e-04 7.63160787e-04 7.54369732e-04\n",
      " 7.36028923e-04 7.30822350e-04 6.78140344e-04 6.65148766e-04\n",
      " 6.59425804e-04 6.32058288e-04 6.20421845e-04 5.92764806e-04\n",
      " 5.80061272e-04 5.71998489e-04 5.52067357e-04 5.27997250e-04\n",
      " 5.24235475e-04 5.16245570e-04 4.97477990e-04 4.80747064e-04\n",
      " 4.65446140e-04 4.56599779e-04 4.53016870e-04 4.41956930e-04\n",
      " 4.28339137e-04 4.22203600e-04 4.19148039e-04 4.10068709e-04\n",
      " 3.94203350e-04 3.90213183e-04 3.80060590e-04 3.74439962e-04\n",
      " 3.66189527e-04 3.56702455e-04 3.55999477e-04 3.41677829e-04\n",
      " 3.36665866e-04 3.35212976e-04 3.30815298e-04 3.22832746e-04\n",
      " 3.18989630e-04 3.14833246e-04 3.05139631e-04 2.96561199e-04\n",
      " 2.88623641e-04 2.81425753e-04 2.80811376e-04 2.76561697e-04\n",
      " 2.74668371e-04 2.72339625e-04 2.62400417e-04 2.54755527e-04\n",
      " 2.53086222e-04 2.47829429e-04 2.46898824e-04 2.43470688e-04\n",
      " 2.38360599e-04 2.37120678e-04 2.33134812e-04 2.29676341e-04\n",
      " 2.25141763e-04 2.23836941e-04 2.20442122e-04 2.17229615e-04\n",
      " 2.11744570e-04 2.09139665e-04 2.07593064e-04 2.05150862e-04\n",
      " 2.00507568e-04 1.96684915e-04 1.95408971e-04 1.93661620e-04\n",
      " 1.89769719e-04 1.87748185e-04 1.84336824e-04 1.81109565e-04\n",
      " 1.78549896e-04 1.75441746e-04 1.74692018e-04 1.73733355e-04\n",
      " 1.71824560e-04 1.69543676e-04 1.66107123e-04 1.62391186e-04\n",
      " 1.61769295e-04 1.61371564e-04 1.60635361e-04 1.57998040e-04\n",
      " 1.55910819e-04 1.54358461e-04 1.53969608e-04 1.50173637e-04\n",
      " 1.49692982e-04 1.48973276e-04 1.46293435e-04 1.44811065e-04\n",
      " 1.43333701e-04 1.42951611e-04 1.41029730e-04 1.40153320e-04\n",
      " 1.39130646e-04 1.37250198e-04 1.34972608e-04 1.33773750e-04\n",
      " 1.32691192e-04 1.31035597e-04 1.29954773e-04 1.28409952e-04\n",
      " 1.27596814e-04 1.25049238e-04 1.23333008e-04 1.23185888e-04\n",
      " 1.21958533e-04 1.20159541e-04 1.18940787e-04 1.18080377e-04\n",
      " 1.17006432e-04 1.16466887e-04 1.15390409e-04 1.15020239e-04\n",
      " 1.14196488e-04 1.13735011e-04 1.12560315e-04 1.11397145e-04\n",
      " 1.10185686e-04 1.09691308e-04 1.07350597e-04 1.06698429e-04\n",
      " 1.06341611e-04 1.04710525e-04 1.03929883e-04 1.03132398e-04\n",
      " 1.02526322e-04 1.01521231e-04 1.01104124e-04 1.00450037e-04\n",
      " 9.87141079e-05 9.80707833e-05 9.73115752e-05 9.60420356e-05\n",
      " 9.56048135e-05 9.46452774e-05 9.37319491e-05 9.31051793e-05\n",
      " 9.21008343e-05 9.12831757e-05 9.10631369e-05 9.09332824e-05\n",
      " 8.94095541e-05 8.87031895e-05 8.84329083e-05 8.76011978e-05\n",
      " 8.69328214e-05 8.65039576e-05 8.60259092e-05 8.53331052e-05\n",
      " 8.50488299e-05 8.45394580e-05 8.31376825e-05 8.22110421e-05\n",
      " 8.21087365e-05 8.17999123e-05 8.12756157e-05 8.07364926e-05\n",
      " 7.99666082e-05 7.92107411e-05 7.87746751e-05 7.86245144e-05\n",
      " 7.81833096e-05 7.77557233e-05 7.75314533e-05 7.66396855e-05\n",
      " 7.58083276e-05 7.49885477e-05 7.46844055e-05 7.39702895e-05\n",
      " 7.36486619e-05 7.32864276e-05 7.24267521e-05 7.23897639e-05\n",
      " 7.13687774e-05 7.11523051e-05 7.06331009e-05 7.04109350e-05\n",
      " 6.97196916e-05 6.88054442e-05 6.86669713e-05 6.82499282e-05\n",
      " 6.79955660e-05 6.68010952e-05 6.65641202e-05 6.58116634e-05\n",
      " 6.54294409e-05 6.52274309e-05 6.50737442e-05 6.47137931e-05\n",
      " 6.43868187e-05 6.41439477e-05 6.35529233e-05 6.32491684e-05\n",
      " 6.30386613e-05 6.22518291e-05 6.18925602e-05 6.12005596e-05\n",
      " 6.05164434e-05 6.03410170e-05 5.99106734e-05 5.93408049e-05\n",
      " 5.89244381e-05 5.87526591e-05 5.84414476e-05 5.80553060e-05\n",
      " 5.75424544e-05 5.72881210e-05 5.68863437e-05 5.63384808e-05\n",
      " 5.63042903e-05 5.60479570e-05 5.57723753e-05 5.51455419e-05\n",
      " 5.50746580e-05 5.48370760e-05 5.45470898e-05 5.38189855e-05\n",
      " 5.37137897e-05 5.36803333e-05 5.33007177e-05 5.30049743e-05\n",
      " 5.29144316e-05 5.25497754e-05 5.24284295e-05 5.18886186e-05\n",
      " 5.14568242e-05 5.11334682e-05 5.08372335e-05 5.06842346e-05\n",
      " 5.05278023e-05 4.99996271e-05 4.97299933e-05 4.95114458e-05\n",
      " 4.93145803e-05 4.93091179e-05 4.89130053e-05 4.86653135e-05\n",
      " 4.80139312e-05 4.78027969e-05 4.76622980e-05 4.74435280e-05\n",
      " 4.71454638e-05 4.69189770e-05 4.64865092e-05 4.60363305e-05\n",
      " 4.58168582e-05 4.57373097e-05 4.51052422e-05 4.50274480e-05\n",
      " 4.48829520e-05 4.45298362e-05 4.42332517e-05 4.41705701e-05\n",
      " 4.39982275e-05 4.36944668e-05 4.35905514e-05 4.30351654e-05\n",
      " 4.29667757e-05 4.26682722e-05 4.25229207e-05 4.23615665e-05\n",
      " 4.22390587e-05 4.19738295e-05 4.17639814e-05 4.15442518e-05\n",
      " 4.14766396e-05 4.10943102e-05 4.10744288e-05 4.08071129e-05\n",
      " 4.04264915e-05 4.02555882e-05 4.00675376e-05 3.99650181e-05\n",
      " 3.96617285e-05 3.95832002e-05 3.91486833e-05 3.90867069e-05\n",
      " 3.87236316e-05 3.84920813e-05 3.82616755e-05 3.81238182e-05\n",
      " 3.80217987e-05 3.77110497e-05 3.75932500e-05 3.73406392e-05\n",
      " 3.72915875e-05 3.71346176e-05 3.69501074e-05 3.68728392e-05\n",
      " 3.65888385e-05 3.63620425e-05 3.61535499e-05 3.60605419e-05\n",
      " 3.60044354e-05 3.58361650e-05 3.56467214e-05 3.54430668e-05\n",
      " 3.52566087e-05 3.49801802e-05 3.49156113e-05 3.45245854e-05\n",
      " 3.44216439e-05 3.42781392e-05 3.41148572e-05 3.40719558e-05\n",
      " 3.38643620e-05 3.38249143e-05 3.37568513e-05 3.35729047e-05\n",
      " 3.33974985e-05 3.32332570e-05 3.31314976e-05 3.27393503e-05\n",
      " 3.25866000e-05 3.25385628e-05 3.22943941e-05 3.22039353e-05\n",
      " 3.21326902e-05 3.18307156e-05 3.15757840e-05 3.14728744e-05\n",
      " 3.13904010e-05 3.13276645e-05 3.11853679e-05 3.10635971e-05\n",
      " 3.08304544e-05 3.07730935e-05 3.05546270e-05 3.04188621e-05\n",
      " 3.03525591e-05 3.02119619e-05 2.98079506e-05 2.97399597e-05\n",
      " 2.95692674e-05 2.94663751e-05 2.93447749e-05 2.91941951e-05\n",
      " 2.90991611e-05 2.90764098e-05 2.88628016e-05 2.87185339e-05\n",
      " 2.86412571e-05 2.85016570e-05 2.83484905e-05 2.82583438e-05\n",
      " 2.81085906e-05 2.79081962e-05 2.78462371e-05 2.77399980e-05\n",
      " 2.76506403e-05 2.74181421e-05 2.73537899e-05 2.72553485e-05\n",
      " 2.71956322e-05 2.71219593e-05 2.68623948e-05 2.67864243e-05\n",
      " 2.65704606e-05 2.64725567e-05 2.63530634e-05 2.62384545e-05\n",
      " 2.61055624e-05 2.60344069e-05 2.58893791e-05 2.58387870e-05\n",
      " 2.56221441e-05 2.55446505e-05 2.54522176e-05 2.53246579e-05\n",
      " 2.50694402e-05 2.50223365e-05 2.49106697e-05 2.47774164e-05\n",
      " 2.46809576e-05 2.45673168e-05 2.43960349e-05 2.43066656e-05\n",
      " 2.41833703e-05 2.40160840e-05 2.39062424e-05 2.38601690e-05\n",
      " 2.34681561e-05 2.34413297e-05 2.32404858e-05 2.31891813e-05\n",
      " 2.30594005e-05 2.29829777e-05 2.27778795e-05 2.26755869e-05\n",
      " 2.25951439e-05 2.24469109e-05 2.23388019e-05 2.21877973e-05\n",
      " 2.20793126e-05 2.20459443e-05 2.19125306e-05 2.18362595e-05\n",
      " 2.16245400e-05 2.15294685e-05 2.14183826e-05 2.13354699e-05\n",
      " 2.11948698e-05 2.11543253e-05 2.10945469e-05 2.09591462e-05\n",
      " 2.08500372e-05 2.08022037e-05 2.06741411e-05 2.05506363e-05\n",
      " 2.04585315e-05 2.03921678e-05 2.02261329e-05 2.01593777e-05\n",
      " 2.00619434e-05 1.99562506e-05 1.97359835e-05 1.96640317e-05\n",
      " 1.95811176e-05 1.94621257e-05 1.92684364e-05 1.91874139e-05\n",
      " 1.91309779e-05 1.89619879e-05 1.88031408e-05 1.86715768e-05\n",
      " 1.86401016e-05 1.85672279e-05 1.84634266e-05 1.81899966e-05\n",
      " 1.80857518e-05 1.79561617e-05 1.78468388e-05 1.76249836e-05]\n",
      "Gesamte erklrte Varianz: 0.995497975928254\n",
      " Reduzierter Chunk 0 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_0.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183495, 1024)\n",
      "Gesamte Datenform vor PCA: (183495, 1024)\n",
      "Gesamte Datenform nach PCA: (183495, 512)\n",
      "Erklrte Varianz pro Komponente: [1.31756638e-01 9.61254466e-02 7.89165674e-02 5.14080141e-02\n",
      " 4.82924308e-02 4.33002527e-02 4.12940918e-02 3.66310449e-02\n",
      " 3.42996750e-02 2.95654184e-02 2.75189168e-02 2.53992416e-02\n",
      " 2.09440484e-02 1.87875455e-02 1.80446636e-02 1.73623310e-02\n",
      " 1.65152838e-02 1.38279082e-02 1.24752531e-02 1.13219732e-02\n",
      " 1.04393919e-02 9.71366194e-03 8.80060443e-03 8.45327955e-03\n",
      " 8.23607385e-03 7.23191910e-03 6.59071427e-03 6.48976518e-03\n",
      " 6.04608266e-03 5.63241455e-03 5.23254885e-03 4.65122332e-03\n",
      " 4.55993263e-03 4.41691479e-03 4.21486747e-03 3.91425301e-03\n",
      " 3.74500444e-03 3.52088628e-03 3.35504484e-03 3.19062288e-03\n",
      " 3.00811977e-03 2.78825010e-03 2.64057675e-03 2.52915026e-03\n",
      " 2.41278714e-03 2.30185497e-03 2.24770657e-03 2.19085543e-03\n",
      " 2.10638786e-03 1.98181278e-03 1.90315642e-03 1.77471679e-03\n",
      " 1.70087283e-03 1.68038944e-03 1.62516785e-03 1.48331051e-03\n",
      " 1.47562527e-03 1.42660473e-03 1.38698435e-03 1.31381311e-03\n",
      " 1.24732518e-03 1.18238658e-03 1.15192334e-03 1.09213300e-03\n",
      " 1.06531722e-03 1.01973888e-03 9.88839370e-04 9.69194200e-04\n",
      " 9.55687141e-04 9.14584882e-04 9.08658401e-04 8.70767146e-04\n",
      " 8.33756193e-04 8.01913064e-04 7.70514561e-04 7.59882142e-04\n",
      " 7.37799345e-04 7.29722688e-04 6.79072003e-04 6.72369928e-04\n",
      " 6.59762922e-04 6.41565892e-04 6.29929727e-04 5.95391564e-04\n",
      " 5.89527935e-04 5.70611242e-04 5.59348730e-04 5.36973084e-04\n",
      " 5.32460502e-04 5.13774360e-04 5.00320647e-04 4.79452672e-04\n",
      " 4.66523652e-04 4.57315422e-04 4.50605364e-04 4.45969015e-04\n",
      " 4.30823921e-04 4.17253457e-04 4.16703522e-04 4.11004269e-04\n",
      " 4.04469384e-04 3.93632579e-04 3.83546932e-04 3.74974932e-04\n",
      " 3.65789292e-04 3.60087985e-04 3.58351702e-04 3.47677565e-04\n",
      " 3.41079759e-04 3.37757656e-04 3.33535154e-04 3.23978491e-04\n",
      " 3.20711880e-04 3.12555641e-04 3.05994545e-04 2.97033826e-04\n",
      " 2.89940308e-04 2.84196361e-04 2.81510487e-04 2.79435323e-04\n",
      " 2.74877425e-04 2.73538353e-04 2.66624832e-04 2.58173298e-04\n",
      " 2.57433446e-04 2.53407530e-04 2.46956449e-04 2.45537727e-04\n",
      " 2.40584302e-04 2.39098459e-04 2.32730881e-04 2.29760787e-04\n",
      " 2.27960914e-04 2.22935846e-04 2.21969757e-04 2.16739127e-04\n",
      " 2.12974381e-04 2.11261402e-04 2.08177741e-04 2.06156661e-04\n",
      " 2.03107679e-04 1.98235957e-04 1.95510743e-04 1.94372572e-04\n",
      " 1.92866252e-04 1.89475447e-04 1.87404425e-04 1.84042392e-04\n",
      " 1.81089823e-04 1.77563888e-04 1.76145685e-04 1.75391435e-04\n",
      " 1.74058673e-04 1.72028641e-04 1.67271226e-04 1.65914999e-04\n",
      " 1.63552719e-04 1.62493564e-04 1.60896495e-04 1.59174852e-04\n",
      " 1.56869195e-04 1.55261489e-04 1.54469352e-04 1.51534469e-04\n",
      " 1.50841940e-04 1.49653584e-04 1.47686150e-04 1.45593520e-04\n",
      " 1.44994300e-04 1.43106298e-04 1.42718743e-04 1.41266368e-04\n",
      " 1.39677744e-04 1.38570677e-04 1.36606023e-04 1.35124749e-04\n",
      " 1.33805289e-04 1.30985243e-04 1.29972707e-04 1.28943386e-04\n",
      " 1.28185236e-04 1.26819214e-04 1.25269538e-04 1.23681606e-04\n",
      " 1.22481287e-04 1.21943397e-04 1.20867847e-04 1.19188314e-04\n",
      " 1.18012879e-04 1.17167326e-04 1.16473459e-04 1.15488381e-04\n",
      " 1.14202886e-04 1.13825172e-04 1.13249787e-04 1.11634341e-04\n",
      " 1.10928591e-04 1.08858745e-04 1.07945968e-04 1.07551780e-04\n",
      " 1.06679912e-04 1.06232436e-04 1.05628694e-04 1.04109013e-04\n",
      " 1.03104923e-04 1.02729909e-04 1.01923800e-04 1.01412641e-04\n",
      " 1.00687290e-04 9.86428833e-05 9.82355778e-05 9.68612378e-05\n",
      " 9.56032056e-05 9.48151927e-05 9.42342417e-05 9.40182388e-05\n",
      " 9.32251440e-05 9.29150704e-05 9.24285536e-05 9.20162527e-05\n",
      " 9.02249279e-05 8.95936361e-05 8.93024771e-05 8.86397823e-05\n",
      " 8.73888568e-05 8.69047511e-05 8.64949595e-05 8.58484796e-05\n",
      " 8.53471265e-05 8.44691038e-05 8.39179810e-05 8.31204215e-05\n",
      " 8.25884324e-05 8.17923092e-05 8.15647581e-05 8.12729645e-05\n",
      " 7.99365400e-05 7.99053100e-05 7.92406886e-05 7.84739726e-05\n",
      " 7.83935613e-05 7.78712113e-05 7.76247860e-05 7.68414340e-05\n",
      " 7.63362102e-05 7.57935901e-05 7.55884895e-05 7.50100766e-05\n",
      " 7.42278436e-05 7.35270140e-05 7.31924537e-05 7.27793279e-05\n",
      " 7.23027904e-05 7.17860991e-05 7.12577095e-05 7.04789722e-05\n",
      " 7.00378698e-05 6.98311656e-05 6.93693548e-05 6.88808190e-05\n",
      " 6.77976668e-05 6.75467537e-05 6.68729144e-05 6.65979068e-05\n",
      " 6.63214629e-05 6.59244021e-05 6.49524586e-05 6.48365026e-05\n",
      " 6.43407102e-05 6.42031170e-05 6.40887357e-05 6.39716491e-05\n",
      " 6.36328720e-05 6.27242250e-05 6.23571252e-05 6.18190101e-05\n",
      " 6.16657096e-05 6.05884124e-05 6.03796085e-05 6.00771261e-05\n",
      " 5.99097565e-05 5.94242779e-05 5.92363729e-05 5.87671266e-05\n",
      " 5.83272586e-05 5.81011669e-05 5.72146589e-05 5.69255533e-05\n",
      " 5.66473212e-05 5.64280477e-05 5.59805482e-05 5.58230310e-05\n",
      " 5.54503935e-05 5.50420267e-05 5.48752685e-05 5.45088205e-05\n",
      " 5.41861546e-05 5.37014836e-05 5.34261991e-05 5.32222983e-05\n",
      " 5.30708322e-05 5.28246492e-05 5.24609123e-05 5.20107536e-05\n",
      " 5.17272087e-05 5.14696216e-05 5.12685359e-05 5.11195674e-05\n",
      " 5.07382716e-05 5.03658014e-05 5.02551292e-05 4.97124399e-05\n",
      " 4.95255270e-05 4.93092300e-05 4.91035295e-05 4.84939442e-05\n",
      " 4.81987790e-05 4.80432692e-05 4.78429707e-05 4.76829328e-05\n",
      " 4.73917045e-05 4.69858152e-05 4.68265126e-05 4.64715149e-05\n",
      " 4.62303879e-05 4.61330695e-05 4.56124615e-05 4.54655466e-05\n",
      " 4.51848254e-05 4.49318645e-05 4.46126596e-05 4.45087451e-05\n",
      " 4.42137212e-05 4.39997517e-05 4.38578841e-05 4.38426556e-05\n",
      " 4.35900120e-05 4.33398633e-05 4.32133425e-05 4.28496460e-05\n",
      " 4.25183418e-05 4.20602112e-05 4.20165359e-05 4.18022088e-05\n",
      " 4.15898660e-05 4.13200124e-05 4.12010309e-05 4.09327435e-05\n",
      " 4.07915565e-05 4.05282068e-05 4.03950293e-05 4.02111823e-05\n",
      " 3.99254455e-05 3.94789382e-05 3.91682618e-05 3.90980923e-05\n",
      " 3.87858757e-05 3.86807672e-05 3.86104737e-05 3.83863836e-05\n",
      " 3.82380266e-05 3.81691809e-05 3.78956846e-05 3.78458602e-05\n",
      " 3.73685209e-05 3.72614309e-05 3.71831914e-05 3.68846921e-05\n",
      " 3.68139111e-05 3.65845949e-05 3.64337286e-05 3.62729321e-05\n",
      " 3.61203324e-05 3.60475412e-05 3.59770747e-05 3.57071288e-05\n",
      " 3.55154253e-05 3.53231998e-05 3.51847182e-05 3.50234429e-05\n",
      " 3.48745091e-05 3.47184714e-05 3.44403720e-05 3.43981157e-05\n",
      " 3.41549871e-05 3.38625734e-05 3.38073815e-05 3.35910008e-05\n",
      " 3.34865527e-05 3.33292634e-05 3.32443759e-05 3.30317908e-05\n",
      " 3.28898107e-05 3.27207395e-05 3.25144794e-05 3.24622871e-05\n",
      " 3.20627632e-05 3.18622110e-05 3.16939762e-05 3.16193160e-05\n",
      " 3.15025005e-05 3.13941328e-05 3.11968109e-05 3.11180696e-05\n",
      " 3.09908825e-05 3.09334900e-05 3.08619274e-05 3.07071589e-05\n",
      " 3.04167122e-05 3.03360183e-05 3.01517791e-05 3.00516169e-05\n",
      " 3.00150886e-05 2.97934357e-05 2.95729421e-05 2.94034296e-05\n",
      " 2.93514565e-05 2.91624324e-05 2.91288460e-05 2.89285187e-05\n",
      " 2.88194011e-05 2.86651315e-05 2.84403319e-05 2.84308198e-05\n",
      " 2.81957871e-05 2.81555901e-05 2.78999614e-05 2.78283181e-05\n",
      " 2.77866070e-05 2.76246279e-05 2.74874558e-05 2.73835297e-05\n",
      " 2.73127690e-05 2.71916935e-05 2.70395755e-05 2.69787883e-05\n",
      " 2.67696297e-05 2.67117469e-05 2.64688172e-05 2.63639452e-05\n",
      " 2.62415026e-05 2.61439968e-05 2.60327679e-05 2.58617123e-05\n",
      " 2.56754167e-05 2.55792664e-05 2.54580814e-05 2.53715816e-05\n",
      " 2.52621640e-05 2.51582841e-05 2.50617849e-05 2.49706877e-05\n",
      " 2.47811617e-05 2.47220877e-05 2.45388349e-05 2.44051180e-05\n",
      " 2.43382740e-05 2.42058378e-05 2.40538005e-05 2.38819921e-05\n",
      " 2.38490302e-05 2.37758366e-05 2.35808451e-05 2.34718761e-05\n",
      " 2.32354604e-05 2.31427033e-05 2.30864011e-05 2.29420055e-05\n",
      " 2.27281802e-05 2.26396788e-05 2.25546197e-05 2.24537437e-05\n",
      " 2.23366412e-05 2.21452997e-05 2.20075276e-05 2.18686841e-05\n",
      " 2.17862020e-05 2.17186802e-05 2.16624890e-05 2.15043387e-05\n",
      " 2.14733134e-05 2.13002937e-05 2.11618439e-05 2.10344780e-05\n",
      " 2.08767647e-05 2.08188905e-05 2.07655806e-05 2.06510941e-05\n",
      " 2.04607086e-05 2.04426795e-05 2.02519985e-05 2.00768776e-05\n",
      " 1.99967851e-05 1.98391972e-05 1.97936227e-05 1.96812387e-05\n",
      " 1.96499236e-05 1.94883166e-05 1.93785602e-05 1.92496888e-05\n",
      " 1.92329893e-05 1.91418979e-05 1.89142429e-05 1.88417257e-05\n",
      " 1.88315359e-05 1.85774488e-05 1.85516237e-05 1.84346496e-05\n",
      " 1.83046634e-05 1.81900226e-05 1.80840271e-05 1.80159559e-05]\n",
      "Gesamte erklrte Varianz: 0.99547169052299\n",
      " Reduzierter Chunk 1 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_1.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183992, 1024)\n",
      "Gesamte Datenform vor PCA: (183992, 1024)\n",
      "Gesamte Datenform nach PCA: (183992, 512)\n",
      "Erklrte Varianz pro Komponente: [1.31722005e-01 9.60018661e-02 7.85979188e-02 5.09952533e-02\n",
      " 4.79842820e-02 4.35047058e-02 4.12788401e-02 3.66105572e-02\n",
      " 3.44090390e-02 2.98924303e-02 2.75794466e-02 2.51826362e-02\n",
      " 2.12486970e-02 1.90877161e-02 1.80849450e-02 1.76061757e-02\n",
      " 1.67440072e-02 1.39912520e-02 1.23939912e-02 1.13875126e-02\n",
      " 1.05711129e-02 9.65205067e-03 8.80799389e-03 8.49272866e-03\n",
      " 8.19668311e-03 7.20923482e-03 6.58322364e-03 6.41539261e-03\n",
      " 6.04578024e-03 5.60221690e-03 5.28152484e-03 4.62801708e-03\n",
      " 4.55024662e-03 4.38804057e-03 4.19476892e-03 3.85519197e-03\n",
      " 3.73787937e-03 3.53317285e-03 3.33976381e-03 3.20292987e-03\n",
      " 3.00898898e-03 2.78414841e-03 2.62848759e-03 2.52167821e-03\n",
      " 2.39747066e-03 2.30558837e-03 2.22657317e-03 2.15544279e-03\n",
      " 2.10896169e-03 1.98555302e-03 1.88070881e-03 1.77088251e-03\n",
      " 1.68845777e-03 1.66988945e-03 1.61185249e-03 1.50058201e-03\n",
      " 1.45163859e-03 1.39221426e-03 1.36971946e-03 1.28925953e-03\n",
      " 1.21483617e-03 1.17752432e-03 1.14372669e-03 1.09601025e-03\n",
      " 1.07281334e-03 1.00703668e-03 9.90402872e-04 9.68698770e-04\n",
      " 9.49578342e-04 9.20047986e-04 9.03170537e-04 8.69160762e-04\n",
      " 8.38951868e-04 8.01951516e-04 7.70669106e-04 7.65453046e-04\n",
      " 7.39829786e-04 7.24605635e-04 6.82238350e-04 6.72076945e-04\n",
      " 6.59288174e-04 6.32387257e-04 6.30039721e-04 5.89823703e-04\n",
      " 5.85539907e-04 5.69168024e-04 5.56137865e-04 5.29782522e-04\n",
      " 5.28011684e-04 5.16387403e-04 4.95122385e-04 4.84736535e-04\n",
      " 4.70804774e-04 4.55200424e-04 4.52503813e-04 4.44619089e-04\n",
      " 4.31345608e-04 4.19552268e-04 4.17257679e-04 4.09857203e-04\n",
      " 3.98316025e-04 3.88569073e-04 3.81314890e-04 3.76754371e-04\n",
      " 3.63474675e-04 3.59963500e-04 3.57095896e-04 3.48985852e-04\n",
      " 3.39528630e-04 3.37176905e-04 3.32936479e-04 3.25423090e-04\n",
      " 3.16279401e-04 3.14319223e-04 3.07968322e-04 2.95542255e-04\n",
      " 2.90718018e-04 2.86773653e-04 2.81739567e-04 2.77311413e-04\n",
      " 2.74442060e-04 2.73458644e-04 2.64679454e-04 2.59700408e-04\n",
      " 2.53566264e-04 2.52126223e-04 2.47966920e-04 2.45803315e-04\n",
      " 2.40699754e-04 2.37414935e-04 2.33130472e-04 2.29814853e-04\n",
      " 2.27896986e-04 2.24113209e-04 2.21802713e-04 2.16848782e-04\n",
      " 2.13422551e-04 2.10196111e-04 2.06926095e-04 2.04337356e-04\n",
      " 2.02483853e-04 1.99206723e-04 1.97369495e-04 1.94933540e-04\n",
      " 1.92604616e-04 1.88907831e-04 1.86794927e-04 1.83941654e-04\n",
      " 1.79313203e-04 1.77436818e-04 1.75727248e-04 1.75141998e-04\n",
      " 1.73820753e-04 1.70842285e-04 1.67801973e-04 1.66283895e-04\n",
      " 1.64277344e-04 1.62914858e-04 1.60939591e-04 1.59686450e-04\n",
      " 1.57995169e-04 1.54884635e-04 1.54240144e-04 1.50448079e-04\n",
      " 1.49816962e-04 1.49437489e-04 1.47011018e-04 1.46160612e-04\n",
      " 1.44978062e-04 1.44136347e-04 1.42882124e-04 1.40098808e-04\n",
      " 1.37906209e-04 1.36631832e-04 1.34966921e-04 1.34503597e-04\n",
      " 1.33507625e-04 1.32639850e-04 1.30833401e-04 1.28864176e-04\n",
      " 1.28309302e-04 1.26373721e-04 1.24217723e-04 1.22945027e-04\n",
      " 1.22159055e-04 1.20338010e-04 1.19230091e-04 1.18740237e-04\n",
      " 1.18231494e-04 1.17390481e-04 1.16197998e-04 1.15698613e-04\n",
      " 1.15331478e-04 1.14362681e-04 1.13199917e-04 1.11254184e-04\n",
      " 1.09819991e-04 1.08968492e-04 1.08254376e-04 1.07350852e-04\n",
      " 1.06711920e-04 1.05984844e-04 1.05165804e-04 1.04765244e-04\n",
      " 1.02913352e-04 1.01850484e-04 1.00721801e-04 1.00373289e-04\n",
      " 9.90024123e-05 9.79709774e-05 9.71834856e-05 9.64495211e-05\n",
      " 9.50265156e-05 9.44129112e-05 9.42746358e-05 9.36541829e-05\n",
      " 9.29986476e-05 9.24654983e-05 9.15600634e-05 9.12030171e-05\n",
      " 8.99043485e-05 8.93156288e-05 8.90428877e-05 8.84933825e-05\n",
      " 8.74642381e-05 8.67697241e-05 8.64700553e-05 8.57647967e-05\n",
      " 8.50444831e-05 8.45073225e-05 8.42430642e-05 8.34252205e-05\n",
      " 8.27853618e-05 8.19381330e-05 8.14505402e-05 8.10559714e-05\n",
      " 8.04431670e-05 7.99675504e-05 7.95238050e-05 7.87499180e-05\n",
      " 7.85938653e-05 7.79017281e-05 7.71362607e-05 7.69380066e-05\n",
      " 7.62619662e-05 7.56415824e-05 7.54568813e-05 7.47311234e-05\n",
      " 7.38789972e-05 7.35449594e-05 7.30716506e-05 7.27778749e-05\n",
      " 7.23987985e-05 7.20323716e-05 7.11548888e-05 7.10263680e-05\n",
      " 7.01238739e-05 6.98153193e-05 6.89442073e-05 6.82990771e-05\n",
      " 6.80100722e-05 6.70534639e-05 6.69130417e-05 6.64731177e-05\n",
      " 6.58563596e-05 6.56786796e-05 6.50792383e-05 6.46396768e-05\n",
      " 6.42747807e-05 6.40290478e-05 6.38386147e-05 6.31305592e-05\n",
      " 6.28888030e-05 6.22994387e-05 6.20709132e-05 6.16062772e-05\n",
      " 6.12248009e-05 6.08125987e-05 6.04913716e-05 6.02053306e-05\n",
      " 5.96293410e-05 5.94821798e-05 5.90325700e-05 5.85901138e-05\n",
      " 5.81737507e-05 5.80825569e-05 5.73328812e-05 5.69030226e-05\n",
      " 5.63644578e-05 5.60831935e-05 5.60608007e-05 5.56293193e-05\n",
      " 5.54646974e-05 5.50643159e-05 5.49255973e-05 5.44841424e-05\n",
      " 5.41851008e-05 5.40826215e-05 5.38290445e-05 5.33937589e-05\n",
      " 5.29184704e-05 5.27766325e-05 5.22585785e-05 5.18228729e-05\n",
      " 5.16158021e-05 5.14022741e-05 5.10512994e-05 5.09599790e-05\n",
      " 5.07593480e-05 5.05490774e-05 5.03174327e-05 5.00118765e-05\n",
      " 4.97442630e-05 4.92432036e-05 4.88513510e-05 4.84681483e-05\n",
      " 4.82182200e-05 4.80720256e-05 4.77907526e-05 4.76549834e-05\n",
      " 4.74505081e-05 4.68893232e-05 4.66289467e-05 4.63363191e-05\n",
      " 4.61714526e-05 4.61025019e-05 4.55100961e-05 4.52585074e-05\n",
      " 4.52015041e-05 4.49378616e-05 4.46011880e-05 4.43391569e-05\n",
      " 4.41482203e-05 4.39798691e-05 4.36796766e-05 4.35153079e-05\n",
      " 4.32380922e-05 4.31229029e-05 4.28016959e-05 4.25262729e-05\n",
      " 4.23484749e-05 4.20992746e-05 4.20710664e-05 4.18164276e-05\n",
      " 4.15261538e-05 4.12641055e-05 4.10696525e-05 4.10037088e-05\n",
      " 4.07541890e-05 4.04887164e-05 4.04395600e-05 3.99937170e-05\n",
      " 3.97601041e-05 3.95124058e-05 3.94605791e-05 3.92748852e-05\n",
      " 3.90462993e-05 3.88063703e-05 3.86648864e-05 3.85036110e-05\n",
      " 3.84543050e-05 3.82073319e-05 3.79293220e-05 3.76123852e-05\n",
      " 3.74794360e-05 3.71594059e-05 3.69817288e-05 3.67834890e-05\n",
      " 3.65785246e-05 3.63443649e-05 3.62744445e-05 3.60760983e-05\n",
      " 3.60466959e-05 3.57551532e-05 3.56275503e-05 3.55273240e-05\n",
      " 3.53941734e-05 3.52473057e-05 3.50148150e-05 3.48248279e-05\n",
      " 3.46381586e-05 3.44164384e-05 3.43899941e-05 3.41605392e-05\n",
      " 3.40686203e-05 3.40086733e-05 3.37357019e-05 3.36270129e-05\n",
      " 3.35653164e-05 3.32936657e-05 3.31770233e-05 3.28973704e-05\n",
      " 3.25902028e-05 3.24642315e-05 3.23842110e-05 3.21888487e-05\n",
      " 3.21113346e-05 3.20486309e-05 3.18379978e-05 3.17873882e-05\n",
      " 3.15281627e-05 3.15168109e-05 3.12994156e-05 3.10790047e-05\n",
      " 3.10480342e-05 3.09734033e-05 3.07302169e-05 3.05054178e-05\n",
      " 3.04120170e-05 3.03189844e-05 3.00442434e-05 2.99712987e-05\n",
      " 2.98773569e-05 2.97169994e-05 2.95172001e-05 2.94282852e-05\n",
      " 2.92718210e-05 2.90852323e-05 2.88989255e-05 2.87959742e-05\n",
      " 2.86780024e-05 2.85252934e-05 2.84545413e-05 2.83523929e-05\n",
      " 2.80806876e-05 2.80612731e-05 2.79371002e-05 2.78155171e-05\n",
      " 2.77053462e-05 2.74845727e-05 2.73763096e-05 2.73315558e-05\n",
      " 2.72578083e-05 2.70741430e-05 2.69743196e-05 2.68574326e-05\n",
      " 2.65961525e-05 2.65498449e-05 2.64654074e-05 2.62459864e-05\n",
      " 2.62234210e-05 2.60217714e-05 2.59959832e-05 2.58204067e-05\n",
      " 2.57504028e-05 2.56581443e-05 2.56238071e-05 2.55204731e-05\n",
      " 2.52697275e-05 2.52031795e-05 2.50194942e-05 2.48919172e-05\n",
      " 2.46821646e-05 2.46542960e-05 2.45239652e-05 2.43450191e-05\n",
      " 2.41158923e-05 2.40760273e-05 2.40180658e-05 2.38691378e-05\n",
      " 2.36330747e-05 2.35323348e-05 2.35063567e-05 2.33301888e-05\n",
      " 2.32709123e-05 2.31101045e-05 2.30593137e-05 2.27551387e-05\n",
      " 2.27379183e-05 2.25297080e-05 2.24072314e-05 2.23994334e-05\n",
      " 2.23073360e-05 2.20781904e-05 2.19592100e-05 2.18467241e-05\n",
      " 2.17838766e-05 2.16774004e-05 2.15666899e-05 2.14828179e-05\n",
      " 2.14072101e-05 2.13621830e-05 2.12839926e-05 2.10735119e-05\n",
      " 2.08497070e-05 2.07987450e-05 2.07284275e-05 2.06382768e-05\n",
      " 2.05588246e-05 2.04639864e-05 2.03764571e-05 2.02634648e-05\n",
      " 2.02241294e-05 2.01186229e-05 1.99996928e-05 1.98693664e-05\n",
      " 1.97008412e-05 1.96180310e-05 1.95285392e-05 1.94278828e-05\n",
      " 1.93094016e-05 1.90723803e-05 1.90636600e-05 1.89535092e-05\n",
      " 1.89267787e-05 1.87828331e-05 1.85375936e-05 1.84507002e-05\n",
      " 1.83277244e-05 1.81662836e-05 1.81267424e-05 1.80443897e-05]\n",
      "Gesamte erklrte Varianz: 0.9954788024792924\n",
      " Reduzierter Chunk 2 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_2.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183726, 1024)\n",
      "Gesamte Datenform vor PCA: (183726, 1024)\n",
      "Gesamte Datenform nach PCA: (183726, 512)\n",
      "Erklrte Varianz pro Komponente: [1.32405806e-01 9.63381820e-02 7.84095011e-02 5.14826679e-02\n",
      " 4.79844361e-02 4.33826356e-02 4.11837117e-02 3.67562801e-02\n",
      " 3.42264615e-02 2.97791585e-02 2.78092476e-02 2.53811984e-02\n",
      " 2.11469737e-02 1.87715211e-02 1.80004718e-02 1.73718024e-02\n",
      " 1.67193293e-02 1.39215149e-02 1.23034026e-02 1.13663863e-02\n",
      " 1.04044598e-02 9.62449779e-03 8.75348407e-03 8.43343052e-03\n",
      " 8.25480484e-03 7.25266778e-03 6.60698157e-03 6.47785542e-03\n",
      " 6.05368493e-03 5.60749030e-03 5.23709090e-03 4.62919126e-03\n",
      " 4.60949729e-03 4.33824922e-03 4.29524831e-03 3.80967721e-03\n",
      " 3.71536659e-03 3.51946212e-03 3.34636197e-03 3.17794767e-03\n",
      " 3.00048523e-03 2.78117607e-03 2.65683796e-03 2.52581034e-03\n",
      " 2.37476204e-03 2.28114235e-03 2.20872837e-03 2.18596231e-03\n",
      " 2.07500118e-03 1.98931499e-03 1.90462536e-03 1.78651294e-03\n",
      " 1.68912542e-03 1.64238145e-03 1.59691621e-03 1.47664380e-03\n",
      " 1.43269919e-03 1.39785778e-03 1.36962984e-03 1.29408186e-03\n",
      " 1.22644021e-03 1.17269463e-03 1.14311718e-03 1.09748859e-03\n",
      " 1.08178411e-03 1.01032770e-03 9.84796283e-04 9.66436857e-04\n",
      " 9.49052188e-04 9.15955365e-04 8.96486102e-04 8.57070546e-04\n",
      " 8.30045025e-04 7.98973940e-04 7.70783747e-04 7.52968336e-04\n",
      " 7.39681156e-04 7.28513955e-04 6.82103712e-04 6.70375588e-04\n",
      " 6.63891654e-04 6.35839867e-04 6.28715551e-04 5.87643463e-04\n",
      " 5.84176147e-04 5.68514949e-04 5.46674256e-04 5.29530827e-04\n",
      " 5.22226850e-04 5.10910896e-04 4.96283771e-04 4.83745121e-04\n",
      " 4.70332839e-04 4.54945958e-04 4.48932772e-04 4.45796319e-04\n",
      " 4.31287274e-04 4.16589792e-04 4.15407749e-04 4.09108696e-04\n",
      " 3.98405267e-04 3.87279506e-04 3.82550619e-04 3.72482607e-04\n",
      " 3.65669104e-04 3.60480489e-04 3.54442287e-04 3.47214351e-04\n",
      " 3.37084943e-04 3.35687738e-04 3.29272338e-04 3.24105711e-04\n",
      " 3.18631991e-04 3.14428203e-04 3.09316597e-04 2.96731634e-04\n",
      " 2.88589390e-04 2.84861431e-04 2.80984579e-04 2.75347413e-04\n",
      " 2.72397573e-04 2.70930865e-04 2.63970663e-04 2.57360150e-04\n",
      " 2.53065400e-04 2.49733187e-04 2.46232217e-04 2.42909567e-04\n",
      " 2.40119962e-04 2.35829439e-04 2.31649696e-04 2.29846719e-04\n",
      " 2.26397813e-04 2.24332345e-04 2.19714886e-04 2.16568709e-04\n",
      " 2.11188168e-04 2.09353754e-04 2.03931682e-04 2.03360086e-04\n",
      " 2.02843026e-04 1.98573939e-04 1.95676856e-04 1.93966666e-04\n",
      " 1.90973470e-04 1.88363455e-04 1.84496455e-04 1.83084224e-04\n",
      " 1.79449236e-04 1.76851752e-04 1.75257944e-04 1.74252132e-04\n",
      " 1.73295299e-04 1.68934049e-04 1.65058421e-04 1.63924365e-04\n",
      " 1.62023717e-04 1.61831526e-04 1.59938535e-04 1.59117984e-04\n",
      " 1.57360211e-04 1.55286312e-04 1.53406187e-04 1.50652973e-04\n",
      " 1.49575312e-04 1.47717384e-04 1.46053854e-04 1.45464207e-04\n",
      " 1.43656784e-04 1.43418430e-04 1.42460536e-04 1.41110821e-04\n",
      " 1.38747602e-04 1.38183306e-04 1.34858519e-04 1.34330418e-04\n",
      " 1.32837031e-04 1.30622740e-04 1.29481258e-04 1.28624512e-04\n",
      " 1.26799534e-04 1.26021946e-04 1.24311075e-04 1.22333878e-04\n",
      " 1.22106092e-04 1.20877518e-04 1.19480336e-04 1.18397062e-04\n",
      " 1.17637757e-04 1.17074084e-04 1.16288215e-04 1.15441644e-04\n",
      " 1.14370301e-04 1.12632520e-04 1.11948689e-04 1.11025303e-04\n",
      " 1.10194739e-04 1.08200207e-04 1.07934978e-04 1.06165414e-04\n",
      " 1.05270312e-04 1.05174049e-04 1.03524135e-04 1.02818880e-04\n",
      " 1.02742874e-04 1.01656795e-04 1.01358789e-04 1.00742822e-04\n",
      " 9.89755092e-05 9.71188981e-05 9.65825394e-05 9.63411196e-05\n",
      " 9.54364438e-05 9.42937668e-05 9.37436876e-05 9.31224651e-05\n",
      " 9.24120880e-05 9.20731176e-05 9.17768158e-05 9.08864553e-05\n",
      " 8.98242708e-05 8.90065204e-05 8.83480457e-05 8.77037246e-05\n",
      " 8.73493764e-05 8.63512440e-05 8.54564024e-05 8.53350371e-05\n",
      " 8.49452268e-05 8.43077140e-05 8.36934730e-05 8.30695132e-05\n",
      " 8.25910509e-05 8.18698984e-05 8.17226744e-05 8.02478121e-05\n",
      " 7.97478567e-05 7.95566762e-05 7.90347078e-05 7.85191262e-05\n",
      " 7.78687764e-05 7.75201508e-05 7.65591782e-05 7.62233433e-05\n",
      " 7.55956820e-05 7.55733458e-05 7.51899800e-05 7.43259460e-05\n",
      " 7.42710004e-05 7.37517519e-05 7.30802151e-05 7.22789859e-05\n",
      " 7.22275397e-05 7.13715902e-05 7.06895493e-05 7.02897778e-05\n",
      " 7.00143570e-05 6.92317453e-05 6.86977080e-05 6.81471090e-05\n",
      " 6.76288769e-05 6.75406522e-05 6.67531666e-05 6.61253436e-05\n",
      " 6.56745937e-05 6.55304187e-05 6.47097059e-05 6.44692332e-05\n",
      " 6.41683695e-05 6.37745170e-05 6.36948098e-05 6.32311594e-05\n",
      " 6.27723886e-05 6.23852173e-05 6.16820759e-05 6.15265826e-05\n",
      " 6.11341622e-05 6.09332399e-05 6.04805440e-05 5.96212913e-05\n",
      " 5.94691358e-05 5.87257856e-05 5.86223448e-05 5.83522945e-05\n",
      " 5.77574446e-05 5.77105604e-05 5.70024066e-05 5.68118556e-05\n",
      " 5.65030517e-05 5.63161098e-05 5.58309778e-05 5.50956371e-05\n",
      " 5.50396174e-05 5.46145646e-05 5.44576854e-05 5.40351330e-05\n",
      " 5.38555884e-05 5.34556610e-05 5.31262360e-05 5.30149705e-05\n",
      " 5.27021706e-05 5.21999746e-05 5.18767631e-05 5.16636852e-05\n",
      " 5.15669371e-05 5.13666038e-05 5.10465915e-05 5.05025121e-05\n",
      " 5.02988584e-05 4.98706065e-05 4.97675063e-05 4.96108754e-05\n",
      " 4.93828874e-05 4.91154010e-05 4.89806444e-05 4.84795398e-05\n",
      " 4.83567626e-05 4.77215048e-05 4.74605755e-05 4.73483370e-05\n",
      " 4.70080297e-05 4.68993108e-05 4.62040749e-05 4.60907941e-05\n",
      " 4.59648351e-05 4.57697134e-05 4.51880932e-05 4.50588513e-05\n",
      " 4.48173911e-05 4.47177875e-05 4.46153168e-05 4.43918619e-05\n",
      " 4.41415002e-05 4.40206171e-05 4.36216078e-05 4.33975899e-05\n",
      " 4.30600718e-05 4.28954055e-05 4.25690525e-05 4.22764985e-05\n",
      " 4.21679586e-05 4.19325965e-05 4.18235658e-05 4.16650754e-05\n",
      " 4.14830246e-05 4.12656678e-05 4.08957684e-05 4.07254285e-05\n",
      " 4.05026060e-05 4.03733958e-05 4.02045833e-05 3.99881446e-05\n",
      " 3.97993981e-05 3.96133859e-05 3.95363755e-05 3.91441659e-05\n",
      " 3.87290916e-05 3.85481611e-05 3.82407547e-05 3.81260736e-05\n",
      " 3.78526364e-05 3.77623300e-05 3.74971996e-05 3.73214171e-05\n",
      " 3.72490524e-05 3.70659766e-05 3.68856553e-05 3.67631091e-05\n",
      " 3.67084754e-05 3.65358920e-05 3.61912827e-05 3.61555955e-05\n",
      " 3.59720144e-05 3.56936053e-05 3.55185244e-05 3.54351417e-05\n",
      " 3.52628500e-05 3.50431949e-05 3.47279465e-05 3.45426042e-05\n",
      " 3.44824268e-05 3.43241617e-05 3.41399800e-05 3.39846224e-05\n",
      " 3.39245432e-05 3.37349132e-05 3.37219318e-05 3.35517075e-05\n",
      " 3.32461749e-05 3.31843865e-05 3.30002308e-05 3.28901491e-05\n",
      " 3.27787739e-05 3.26068893e-05 3.24538704e-05 3.21898055e-05\n",
      " 3.18665680e-05 3.17575691e-05 3.15670036e-05 3.14857257e-05\n",
      " 3.14153302e-05 3.12370473e-05 3.10669759e-05 3.08712479e-05\n",
      " 3.07792293e-05 3.07104535e-05 3.05246434e-05 3.03682030e-05\n",
      " 3.02880252e-05 3.02228644e-05 3.01103487e-05 2.97515713e-05\n",
      " 2.96838378e-05 2.95671557e-05 2.93867074e-05 2.91641100e-05\n",
      " 2.91467255e-05 2.90581688e-05 2.89354926e-05 2.87259199e-05\n",
      " 2.85086064e-05 2.82814095e-05 2.82427484e-05 2.81190502e-05\n",
      " 2.80813996e-05 2.79608774e-05 2.77879159e-05 2.77625508e-05\n",
      " 2.75765415e-05 2.73851358e-05 2.73425999e-05 2.72502897e-05\n",
      " 2.71123772e-05 2.70255990e-05 2.68843459e-05 2.67309546e-05\n",
      " 2.65482223e-05 2.63954200e-05 2.63419816e-05 2.62594218e-05\n",
      " 2.61242956e-05 2.60543678e-05 2.59500896e-05 2.57943913e-05\n",
      " 2.57112482e-05 2.53800013e-05 2.53261327e-05 2.51610160e-05\n",
      " 2.51001543e-05 2.50131047e-05 2.48981811e-05 2.48249675e-05\n",
      " 2.46217498e-05 2.45498210e-05 2.45092341e-05 2.43498140e-05\n",
      " 2.42421057e-05 2.41275834e-05 2.40444389e-05 2.39788983e-05\n",
      " 2.38662672e-05 2.35728989e-05 2.35135343e-05 2.33221387e-05\n",
      " 2.31237226e-05 2.30027817e-05 2.29050419e-05 2.28208175e-05\n",
      " 2.27310727e-05 2.24926644e-05 2.24291998e-05 2.22571203e-05\n",
      " 2.20814476e-05 2.20051042e-05 2.19927522e-05 2.18600687e-05\n",
      " 2.17028184e-05 2.15739490e-05 2.14567631e-05 2.13619698e-05\n",
      " 2.11912401e-05 2.11623670e-05 2.10253640e-05 2.09098253e-05\n",
      " 2.07939430e-05 2.06880249e-05 2.05711508e-05 2.04568724e-05\n",
      " 2.04147350e-05 2.03562452e-05 2.01355160e-05 2.00864432e-05\n",
      " 1.99627926e-05 1.98570188e-05 1.97516622e-05 1.96150288e-05\n",
      " 1.95591362e-05 1.95326221e-05 1.93454145e-05 1.91993338e-05\n",
      " 1.91226324e-05 1.89579213e-05 1.89095473e-05 1.88058364e-05\n",
      " 1.86790979e-05 1.84757574e-05 1.84191849e-05 1.83744417e-05\n",
      " 1.82227148e-05 1.81409418e-05 1.79724801e-05 1.79466401e-05]\n",
      "Gesamte erklrte Varianz: 0.9954871730816074\n",
      " Reduzierter Chunk 3 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_3.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183396, 1024)\n",
      "Gesamte Datenform vor PCA: (183396, 1024)\n",
      "Gesamte Datenform nach PCA: (183396, 512)\n",
      "Erklrte Varianz pro Komponente: [1.32701745e-01 9.63894465e-02 7.92096142e-02 5.12135132e-02\n",
      " 4.81423486e-02 4.34124626e-02 4.11329077e-02 3.67094262e-02\n",
      " 3.43188180e-02 2.96224483e-02 2.77444148e-02 2.51777114e-02\n",
      " 2.08857510e-02 1.88560193e-02 1.81359810e-02 1.75355663e-02\n",
      " 1.66431454e-02 1.39866960e-02 1.22468843e-02 1.13198978e-02\n",
      " 1.04212582e-02 9.61556769e-03 8.81597350e-03 8.41628320e-03\n",
      " 8.19018804e-03 7.29229751e-03 6.66446883e-03 6.46443641e-03\n",
      " 6.00571929e-03 5.58800600e-03 5.21883015e-03 4.63188146e-03\n",
      " 4.53189417e-03 4.38970382e-03 4.19726210e-03 3.82726413e-03\n",
      " 3.68854503e-03 3.51660004e-03 3.33412412e-03 3.18273185e-03\n",
      " 2.95943824e-03 2.79099750e-03 2.61272947e-03 2.53436231e-03\n",
      " 2.39431215e-03 2.28392428e-03 2.21620857e-03 2.16389118e-03\n",
      " 2.06231605e-03 1.98141193e-03 1.88226381e-03 1.77033945e-03\n",
      " 1.67816368e-03 1.64581998e-03 1.60268114e-03 1.46030685e-03\n",
      " 1.40876374e-03 1.40005827e-03 1.36865191e-03 1.29448300e-03\n",
      " 1.23447674e-03 1.16823532e-03 1.14959863e-03 1.09004542e-03\n",
      " 1.07420462e-03 1.00206354e-03 9.93056223e-04 9.70774178e-04\n",
      " 9.50171673e-04 9.11410090e-04 8.93660676e-04 8.55492221e-04\n",
      " 8.34377890e-04 8.01149134e-04 7.70526410e-04 7.50920902e-04\n",
      " 7.30626733e-04 7.15271847e-04 6.75246090e-04 6.66854517e-04\n",
      " 6.55852274e-04 6.35327640e-04 6.27552463e-04 5.87307969e-04\n",
      " 5.83435955e-04 5.71125967e-04 5.48932410e-04 5.29964805e-04\n",
      " 5.23143888e-04 5.13465613e-04 4.95257124e-04 4.69123878e-04\n",
      " 4.56791688e-04 4.53275476e-04 4.49567410e-04 4.41490663e-04\n",
      " 4.28616101e-04 4.20075180e-04 4.14071040e-04 4.08178750e-04\n",
      " 3.97592475e-04 3.87767495e-04 3.81836106e-04 3.67679456e-04\n",
      " 3.65018379e-04 3.57756602e-04 3.54769898e-04 3.44513955e-04\n",
      " 3.38060752e-04 3.34788053e-04 3.28815086e-04 3.19883065e-04\n",
      " 3.17803156e-04 3.11656135e-04 3.02519490e-04 2.92889353e-04\n",
      " 2.88232311e-04 2.83370948e-04 2.81101643e-04 2.77114744e-04\n",
      " 2.74574253e-04 2.69612791e-04 2.63730003e-04 2.55192418e-04\n",
      " 2.52723820e-04 2.50537950e-04 2.46571098e-04 2.41556748e-04\n",
      " 2.39186302e-04 2.31938547e-04 2.30175921e-04 2.27442349e-04\n",
      " 2.24811215e-04 2.22737912e-04 2.17230718e-04 2.14920414e-04\n",
      " 2.09392710e-04 2.07386988e-04 2.04949470e-04 2.02412780e-04\n",
      " 2.00742442e-04 1.96133005e-04 1.93577658e-04 1.93314793e-04\n",
      " 1.88832628e-04 1.87896967e-04 1.86022667e-04 1.80462736e-04\n",
      " 1.78406073e-04 1.75932538e-04 1.75675745e-04 1.72396707e-04\n",
      " 1.72004773e-04 1.68200816e-04 1.65484278e-04 1.64055704e-04\n",
      " 1.63036435e-04 1.60957082e-04 1.60083221e-04 1.57163450e-04\n",
      " 1.55961667e-04 1.54213886e-04 1.53705642e-04 1.50018968e-04\n",
      " 1.49213141e-04 1.47440341e-04 1.45379089e-04 1.43988246e-04\n",
      " 1.43421586e-04 1.42035297e-04 1.41141515e-04 1.39859670e-04\n",
      " 1.38993700e-04 1.35447162e-04 1.34468984e-04 1.32774079e-04\n",
      " 1.31461978e-04 1.30573550e-04 1.28935346e-04 1.27720596e-04\n",
      " 1.27066434e-04 1.25811520e-04 1.23557696e-04 1.23251434e-04\n",
      " 1.21026453e-04 1.19751920e-04 1.19097862e-04 1.18321886e-04\n",
      " 1.16928157e-04 1.16015325e-04 1.15097532e-04 1.14703142e-04\n",
      " 1.13047880e-04 1.12802525e-04 1.11603708e-04 1.09868338e-04\n",
      " 1.09258304e-04 1.08765796e-04 1.07206230e-04 1.06967792e-04\n",
      " 1.06012049e-04 1.04507248e-04 1.03916404e-04 1.02281781e-04\n",
      " 1.02005299e-04 1.01284865e-04 1.01069314e-04 1.00303859e-04\n",
      " 9.86105309e-05 9.69680049e-05 9.53656666e-05 9.48316620e-05\n",
      " 9.44249589e-05 9.38328513e-05 9.29870505e-05 9.22519395e-05\n",
      " 9.16576070e-05 9.08136139e-05 9.01747424e-05 8.93357670e-05\n",
      " 8.90701552e-05 8.86276100e-05 8.75451735e-05 8.73772035e-05\n",
      " 8.69117369e-05 8.65724362e-05 8.55679589e-05 8.47898849e-05\n",
      " 8.42827358e-05 8.34413384e-05 8.25595622e-05 8.24987998e-05\n",
      " 8.23032847e-05 8.13400612e-05 8.06042492e-05 7.99886119e-05\n",
      " 7.94500590e-05 7.87691979e-05 7.82269657e-05 7.78339655e-05\n",
      " 7.73900065e-05 7.68156346e-05 7.59377172e-05 7.53102948e-05\n",
      " 7.51895464e-05 7.47869919e-05 7.44170637e-05 7.35488398e-05\n",
      " 7.30256237e-05 7.29068047e-05 7.20048999e-05 7.14904561e-05\n",
      " 7.10344976e-05 7.09171271e-05 7.02843799e-05 6.95175812e-05\n",
      " 6.94137631e-05 6.85582282e-05 6.81535066e-05 6.76399029e-05\n",
      " 6.69661685e-05 6.68943626e-05 6.59273266e-05 6.56336948e-05\n",
      " 6.50734662e-05 6.49662470e-05 6.44287949e-05 6.42183195e-05\n",
      " 6.38352330e-05 6.35318266e-05 6.34330087e-05 6.29019301e-05\n",
      " 6.24321470e-05 6.18921860e-05 6.14647749e-05 6.12736575e-05\n",
      " 6.07020784e-05 6.04169059e-05 5.97010080e-05 5.90958233e-05\n",
      " 5.86770163e-05 5.83741777e-05 5.82660314e-05 5.79488756e-05\n",
      " 5.75969786e-05 5.72727251e-05 5.66763242e-05 5.63221095e-05\n",
      " 5.58785214e-05 5.56990153e-05 5.53846349e-05 5.49704922e-05\n",
      " 5.44702786e-05 5.42447328e-05 5.41469463e-05 5.39000378e-05\n",
      " 5.37624635e-05 5.29727839e-05 5.28683343e-05 5.26985217e-05\n",
      " 5.23951616e-05 5.21706240e-05 5.17082399e-05 5.14156799e-05\n",
      " 5.12224825e-05 5.06953844e-05 5.04418591e-05 5.01498959e-05\n",
      " 4.98106879e-05 4.97757960e-05 4.94381235e-05 4.92374633e-05\n",
      " 4.89234710e-05 4.87265712e-05 4.84883567e-05 4.79098421e-05\n",
      " 4.78218504e-05 4.75379932e-05 4.74433268e-05 4.70534612e-05\n",
      " 4.68723792e-05 4.61577705e-05 4.61271141e-05 4.59689881e-05\n",
      " 4.57763933e-05 4.55919767e-05 4.52501150e-05 4.47372443e-05\n",
      " 4.45672318e-05 4.43423002e-05 4.40622236e-05 4.39481587e-05\n",
      " 4.37834333e-05 4.35377647e-05 4.34363395e-05 4.32457291e-05\n",
      " 4.26005943e-05 4.22594250e-05 4.21472373e-05 4.21165432e-05\n",
      " 4.18377384e-05 4.17728390e-05 4.16079688e-05 4.14694587e-05\n",
      " 4.09069994e-05 4.06749614e-05 4.05761870e-05 4.03175658e-05\n",
      " 4.00930861e-05 3.99957892e-05 3.96277355e-05 3.93986293e-05\n",
      " 3.93379219e-05 3.91893996e-05 3.88242400e-05 3.86777340e-05\n",
      " 3.84436855e-05 3.81805884e-05 3.79935500e-05 3.77213505e-05\n",
      " 3.75991361e-05 3.75617934e-05 3.74224779e-05 3.72567762e-05\n",
      " 3.70965209e-05 3.69644480e-05 3.68456029e-05 3.64147183e-05\n",
      " 3.61768862e-05 3.60907400e-05 3.58821100e-05 3.57798939e-05\n",
      " 3.56338515e-05 3.53921056e-05 3.51776989e-05 3.51057453e-05\n",
      " 3.49382011e-05 3.47680756e-05 3.46775433e-05 3.45649066e-05\n",
      " 3.43794963e-05 3.41596056e-05 3.41240532e-05 3.39079721e-05\n",
      " 3.38355375e-05 3.36727184e-05 3.35412509e-05 3.33644159e-05\n",
      " 3.32192368e-05 3.28721112e-05 3.27825465e-05 3.25911452e-05\n",
      " 3.23302933e-05 3.23126996e-05 3.20268289e-05 3.20120048e-05\n",
      " 3.16671997e-05 3.15271571e-05 3.14752538e-05 3.12981870e-05\n",
      " 3.11742258e-05 3.10514263e-05 3.08567253e-05 3.06939090e-05\n",
      " 3.06227376e-05 3.05740761e-05 3.04084410e-05 3.01228398e-05\n",
      " 3.00015641e-05 2.98446491e-05 2.96163454e-05 2.95882963e-05\n",
      " 2.95271515e-05 2.94011131e-05 2.93301415e-05 2.91438883e-05\n",
      " 2.89285691e-05 2.87538576e-05 2.85678158e-05 2.84439560e-05\n",
      " 2.82935014e-05 2.82384809e-05 2.80710931e-05 2.80016483e-05\n",
      " 2.77428533e-05 2.76745818e-05 2.74944789e-05 2.74679595e-05\n",
      " 2.73227166e-05 2.70873731e-05 2.70424112e-05 2.69424200e-05\n",
      " 2.69021947e-05 2.66750816e-05 2.66093798e-05 2.64923859e-05\n",
      " 2.64650148e-05 2.63534297e-05 2.60994553e-05 2.60575236e-05\n",
      " 2.59898315e-05 2.56780845e-05 2.55547520e-05 2.54596858e-05\n",
      " 2.53722648e-05 2.53487786e-05 2.52249970e-05 2.50369534e-05\n",
      " 2.49531827e-05 2.49153127e-05 2.47980870e-05 2.47124391e-05\n",
      " 2.46030616e-05 2.45195372e-05 2.42721014e-05 2.40817141e-05\n",
      " 2.40402604e-05 2.37595986e-05 2.36645831e-05 2.36153292e-05\n",
      " 2.34547784e-05 2.33749926e-05 2.32928660e-05 2.32234038e-05\n",
      " 2.30644189e-05 2.29833150e-05 2.28197759e-05 2.26612690e-05\n",
      " 2.24450749e-05 2.23824381e-05 2.22927908e-05 2.21845257e-05\n",
      " 2.20711720e-05 2.19274255e-05 2.18672526e-05 2.17748285e-05\n",
      " 2.15741509e-05 2.14759125e-05 2.13306537e-05 2.12547717e-05\n",
      " 2.12254001e-05 2.11527613e-05 2.10689095e-05 2.07699718e-05\n",
      " 2.07066744e-05 2.05653571e-05 2.04070790e-05 2.03361321e-05\n",
      " 2.02018343e-05 2.01750671e-05 2.01035683e-05 2.00877433e-05\n",
      " 1.99548939e-05 1.99097900e-05 1.98197907e-05 1.96668678e-05\n",
      " 1.95928066e-05 1.92809234e-05 1.92366292e-05 1.90324694e-05\n",
      " 1.88765828e-05 1.87969665e-05 1.86667448e-05 1.86311243e-05\n",
      " 1.85209892e-05 1.84497091e-05 1.83070548e-05 1.81766578e-05\n",
      " 1.80714708e-05 1.79310182e-05 1.77382264e-05 1.75264444e-05]\n",
      "Gesamte erklrte Varianz: 0.9955425936638228\n",
      " Reduzierter Chunk 4 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_4.npz\n"
     ]
    }
   ],
   "source": [
    "# === Lade, verarbeite und speichere TCR-Embeddings ===\n",
    "load_and_process_embeddings_in_chunks(\n",
    "    file_path='../../data/embeddings/beta/allele/TRB_beta_embeddings.npz',\n",
    "    output_path='../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced',\n",
    "    chunk_size=100_000,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# === Lade, verarbeite und speichere Epitope-Embeddings ===\n",
    "load_and_process_embeddings_in_chunks(\n",
    "    file_path='../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz',\n",
    "    output_path='../../data/embeddings/beta/allele/Epitope_beta_embeddings_reduced',\n",
    "    chunk_size=100_000,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def combine_reduced_chunks(output_file_path, chunk_base_path, total_chunks):\n",
    "    \"\"\"\n",
    "    Kombiniert alle reduzierten Chunks in eine finale Datei.\n",
    "    \"\"\"\n",
    "    combined_chunks = []\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        chunk_path = f\"{chunk_base_path}_chunk_{i}.npz\"\n",
    "        chunk = np.load(chunk_path)['embeddings']\n",
    "        combined_chunks.append(chunk)\n",
    "        print(f\" Chunk {i} geladen von {chunk_path} mit Shape: {chunk.shape}\")\n",
    "\n",
    "    # Zusammenfgen aller Chunks\n",
    "    final_combined = np.vstack(combined_chunks)\n",
    "    np.savez_compressed(output_file_path, embeddings=final_combined)\n",
    "    print(f\" Finale reduzierte Embeddings gespeichert unter: {output_file_path}\")\n",
    "\n",
    "\n",
    "# === TRB Chunks zusammenfhren ===\n",
    "combine_reduced_chunks(\n",
    "    output_file_path='../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_final.npz',\n",
    "    chunk_base_path='../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced',\n",
    "    total_chunks=2  # <- Anzahl der TRB Chunks\n",
    ")\n",
    "\n",
    "# === Epitope Chunks zusammenfhren ===\n",
    "combine_reduced_chunks(\n",
    "    output_file_path='../../data/embeddings/beta/allele/Epitope_beta_embeddings_reduced_final.npz',\n",
    "    chunk_base_path='../../data/embeddings/beta/allele/Epitope_beta_embeddings_reduced',\n",
    "    total_chunks=2  # <- Anzahl der Epitope Chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_final_embeddings(file_path):\n",
    "    data = np.load(file_path, allow_pickle=True)['embeddings']\n",
    "    print(f\" Finale Shape: {data.shape}\")\n",
    "    print(f\" Erste 5 Embeddings:\\n{data[:5]}\")\n",
    "\n",
    "# berprfe TRB\n",
    "check_final_embeddings('../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_final.npz')\n",
    "\n",
    "# berprfe Epitope\n",
    "check_final_embeddings('../../data/embeddings/beta/allele/Epitope_beta_embeddings_reduced_final.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beta and paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding embeddings using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 2.75 MiB is free. Including non-PyTorch memory, this process has 14.56 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 64.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# epi_embeddings = load_embeddings_to_gpu(paired_all_epi_path)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# tra_embeddings = load_embeddings_to_gpu(paired_all_tra_path)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# trb_embeddings = load_embeddings_to_gpu(paired_all_trb_path)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m beta_epi_embeddings \u001b[38;5;241m=\u001b[39m load_embeddings_to_gpu(beta_all_epi_path)\n\u001b[0;32m---> 27\u001b[0m beta_trb_embeddings \u001b[38;5;241m=\u001b[39m load_embeddings_to_gpu(beta_all_trb_path)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Find max sequence length\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# max_len = max(max(e.shape[0] for e in epi_embeddings.values()), \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#               max(e.shape[0] for e in tra_embeddings.values()), \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#               max(e.shape[0] for e in trb_embeddings.values()))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m beta_max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmax\u001b[39m(e\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m beta_epi_embeddings\u001b[38;5;241m.\u001b[39mvalues()), \n\u001b[1;32m     35\u001b[0m                     \u001b[38;5;28mmax\u001b[39m(e\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m beta_trb_embeddings\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mload_embeddings_to_gpu\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_embeddings_to_gpu\u001b[39m(path):\n\u001b[1;32m     19\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: torch\u001b[38;5;241m.\u001b[39mtensor(data[key], device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 2.75 MiB is free. Including non-PyTorch memory, this process has 14.56 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 64.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to embeddings files\n",
    "# paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "# paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "# paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "\n",
    "beta_all_epi_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "beta_all_trb_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "\n",
    "# Load NPZ files into GPU\n",
    "def load_embeddings_to_gpu(path):\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    return {key: torch.tensor(data[key], device=device) for key in data}\n",
    "\n",
    "# epi_embeddings = load_embeddings_to_gpu(paired_all_epi_path)\n",
    "# tra_embeddings = load_embeddings_to_gpu(paired_all_tra_path)\n",
    "# trb_embeddings = load_embeddings_to_gpu(paired_all_trb_path)\n",
    "\n",
    "beta_epi_embeddings = load_embeddings_to_gpu(beta_all_epi_path)\n",
    "beta_trb_embeddings = load_embeddings_to_gpu(beta_all_trb_path)\n",
    "\n",
    "# Find max sequence length\n",
    "# max_len = max(max(e.shape[0] for e in epi_embeddings.values()), \n",
    "#               max(e.shape[0] for e in tra_embeddings.values()), \n",
    "#               max(e.shape[0] for e in trb_embeddings.values()))\n",
    "\n",
    "beta_max_len = max(max(e.shape[0] for e in beta_epi_embeddings.values()), \n",
    "                    max(e.shape[0] for e in beta_trb_embeddings.values()))\n",
    "\n",
    "# Function to pad embeddings on GPU\n",
    "def pad_embeddings(embeddings, max_len):\n",
    "    return pad_sequence(\n",
    "        [torch.nn.functional.pad(e, (0, 0, 0, max_len - e.shape[0])) for e in embeddings.values()],\n",
    "        batch_first=True, padding_value=0.0\n",
    "    )\n",
    "\n",
    "# Pad embeddings (all computations on GPU)\n",
    "# padded_epi = pad_embeddings(epi_embeddings, max_len)\n",
    "# padded_tra = pad_embeddings(tra_embeddings, max_len)\n",
    "# padded_trb = pad_embeddings(trb_embeddings, max_len)\n",
    "\n",
    "padded_beta_epi = pad_embeddings(beta_epi_embeddings, beta_max_len)\n",
    "# padded_beta_trb = pad_embeddings(beta_trb_embeddings, beta_max_len)\n",
    "\n",
    "# Move back to CPU before saving\n",
    "# padded_epi = padded_epi.cpu().numpy()\n",
    "# padded_tra = padded_tra.cpu().numpy()\n",
    "# padded_trb = padded_trb.cpu().numpy()\n",
    "\n",
    "padded_beta_epi = padded_beta_epi.cpu().numpy()\n",
    "# padded_beta_trb = padded_beta_trb.cpu().numpy()\n",
    "\n",
    "# Save padded embeddings\n",
    "# padd_paired_all_epi_path = '../../data/embeddings/paired/allele/padded_Epitope_paired_embeddings.npz'\n",
    "# padd_paired_all_tra_path = '../../data/embeddings/paired/allele/padded_TRA_paired_embeddings.npz'\n",
    "# padd_paired_all_trb_path = '../../data/embeddings/paired/allele/padded_TRB_paired_embeddings.npz'\n",
    "\n",
    "padd_beta_all_epi_path = '../../data/embeddings/beta/allele/padded_Epitope_beta_embeddings.npz'\n",
    "# padd_beta_all_trb_path = '../../data/embeddings/beta/allele/padded_TRB_beta_embeddings.npz'\n",
    "\n",
    "# np.savez(padd_paired_all_epi_path, **{key: padded_epi[i] for i, key in enumerate(epi_embeddings)})\n",
    "# np.savez(padd_paired_all_tra_path, **{key: padded_tra[i] for i, key in enumerate(tra_embeddings)})\n",
    "# np.savez(padd_paired_all_trb_path, **{key: padded_trb[i] for i, key in enumerate(trb_embeddings)})\n",
    "\n",
    "np.savez(padd_beta_all_epi_path, **{key: padded_beta_epi[i] for i, key in enumerate(beta_epi_embeddings)})\n",
    "# np.savez(padd_beta_all_trb_path, **{key: padded_beta_trb[i] for i, key in enumerate(beta_trb_embeddings)})\n",
    "\n",
    "print(\"Padded embedding saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### short inspection into padded embeddings in directory prov.\n",
    "#### Just to check the Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 1896\n",
      "\n",
      "Key: NLTTRTQL\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: FIYIFHTL\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: YMHHMELPT\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: ILLDWAANI\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: SMWALVISV\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: LYALVYFLQ\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: WLPTGTLLV\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: YVDDVVLGA\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: GTSGSPIVAR\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: LTGHMLDMY\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "Number of keys in the NPZ file: 10000\n",
      "\n",
      "Key: CASSSTASRNTGELFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSLVTGEQYF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSAHRGGYGYTF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSLGRTGGNIQYF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CSARGQEGQYISYEQYF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSGKQGCDTEAFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASHQTGGRDTEAFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CSASSPRLTSNQPQHF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSIIDGINLSYNEQFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSLVGGELFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n"
     ]
    }
   ],
   "source": [
    "paddedepiembpath = '../../data/embeddings/beta/gene/prov/padded_epitope_embeddings_batch_0.npz'\n",
    "paddedtcrembpath = '../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_0.npz'\n",
    "paths = [paddedepiembpath, paddedtcrembpath]\n",
    "for path in paths:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    # Inspect the shape and size of each stored array\n",
    "    for key in data.files[:10]:\n",
    "        array = data[key]\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        print(f\"Shape: {array.shape}\")\n",
    "        print(f\"Size: {array.size}\")\n",
    "        print(f\"Data Type: {array.dtype}\")\n",
    "        # print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove all files in folder 'prov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in ../../data/embeddings/beta/gene/prov have been removed.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define the directory path\n",
    "# directory_path = '../../data/embeddings/beta/gene/prov'\n",
    "\n",
    "# # Check if the directory exists\n",
    "# if os.path.exists(directory_path):\n",
    "#     # Iterate over all files in the directory and remove them\n",
    "#     for filename in os.listdir(directory_path):\n",
    "#         file_path = os.path.join(directory_path, filename)\n",
    "#         try:\n",
    "#             if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "#                 os.unlink(file_path)  # Remove the file or symbolic link\n",
    "#             elif os.path.isdir(file_path):\n",
    "#                 shutil.rmtree(file_path)  # Remove the subdirectory\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "#     print(f\"All files in {directory_path} have been removed.\")\n",
    "# else:\n",
    "#     print(f\"The directory {directory_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding all embeddings and saving in batches\n",
    "\n",
    "to same length (max(max(len(epitope)), max(len(tcr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185630/2300908741.py:8: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(train_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the training and validation data\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "validation_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "train_data = pd.read_csv(train_path, sep='\\t')\n",
    "validation_data = pd.read_csv(validation_path, sep='\\t')\n",
    "\n",
    "# Load the embeddings\n",
    "tcr_embeddings_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "epitope_embeddings_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "\n",
    "tcr_embeddings = np.load(tcr_embeddings_path, allow_pickle=True)\n",
    "epitope_embeddings = np.load(epitope_embeddings_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_0.npz\n",
      "Saved batch 2/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_1.npz\n",
      "Saved batch 3/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_2.npz\n",
      "Saved batch 4/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_3.npz\n",
      "Saved batch 5/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_4.npz\n",
      "Saved batch 6/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_5.npz\n",
      "Saved batch 7/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_6.npz\n",
      "Saved batch 8/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_7.npz\n",
      "Saved batch 9/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_8.npz\n",
      "Saved batch 10/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_9.npz\n",
      "Saved batch 11/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_10.npz\n",
      "Saved batch 12/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_11.npz\n",
      "Saved batch 13/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_12.npz\n",
      "Saved batch 14/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_13.npz\n",
      "Saved batch 15/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_14.npz\n",
      "Saved batch 16/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_15.npz\n",
      "Saved batch 17/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_16.npz\n",
      "Saved batch 18/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_17.npz\n",
      "Saved batch 19/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_18.npz\n",
      "Saved batch 20/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_19.npz\n",
      "Saved batch 21/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_20.npz\n",
      "Saved batch 22/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_21.npz\n",
      "All batches saved successfully!\n",
      "Saved batch 1/1 to ../../data/embeddings/beta/gene/prov/padded_epitope_embeddings_batch_0.npz\n",
      "All batches saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def pad_embeddings_in_batches(embeddings_dict, max_length, batch_size, save_path):\n",
    "    \"\"\"\n",
    "    Pad embeddings in batches and save them incrementally to disk.\n",
    "    \"\"\"\n",
    "    keys = list(embeddings_dict.keys())\n",
    "    num_batches = (len(keys) + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "\n",
    "    # Create a directory to save the batches\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_keys = keys[i * batch_size : (i + 1) * batch_size]\n",
    "        padded_batch = {}\n",
    "\n",
    "        for key in batch_keys:\n",
    "            embedding = embeddings_dict[key]\n",
    "            padded_embedding = np.zeros((max_length, embedding.shape[1]), dtype=embedding.dtype)\n",
    "            padded_embedding[:embedding.shape[0], :] = embedding\n",
    "            padded_batch[key] = padded_embedding\n",
    "\n",
    "        # Save the batch to disk\n",
    "        batch_save_path = f\"{save_path}_batch_{i}.npz\"\n",
    "        np.savez_compressed(batch_save_path, **padded_batch)\n",
    "        print(f\"Saved batch {i + 1}/{num_batches} to {batch_save_path}\")\n",
    "\n",
    "    print(\"All batches saved successfully!\")\n",
    "\n",
    "# Define batch size (adjust based on memory availability)\n",
    "batch_size = 10000  # Process 10000 embeddings at a time\n",
    "\n",
    "# Determine the maximum length for TCR and Epitope embeddings\n",
    "max_tcr_length = max([embedding.shape[0] for embedding in tcr_embeddings.values()])\n",
    "max_epitope_length = max([embedding.shape[0] for embedding in epitope_embeddings.values()])\n",
    "\n",
    "# Calculate the global max_length\n",
    "max_length = max(max_tcr_length, max_epitope_length)\n",
    "\n",
    "# Pad and save TCR embeddings in batches using the global max_length\n",
    "pad_embeddings_in_batches(tcr_embeddings, max_length, batch_size, '../../data/embeddings/beta/allele/padded/padded_tcr_embeddings')\n",
    "\n",
    "# Pad and save Epitope embeddings in batches using the global max_length\n",
    "pad_embeddings_in_batches(epitope_embeddings, max_length, batch_size, '../../data/embeddings/beta/allele/padded/padded_epitope_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking dimensions of subset padded emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if the dimensions in the version2 were padded correctly for both epitopes and tcrs\n",
    "padd_beta_all_epi_path = './dummy_data/subset_padded_epitope_embeddings.npz'\n",
    "padd_beta_all_trb_path = './dummy_data/subset_padded_tcr_embeddings.npz'\n",
    "\n",
    "paths = [padd_beta_all_epi_path, padd_beta_all_trb_path]\n",
    "for path in paths:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    # Inspect the shape and size of each stored array\n",
    "    for key in data.files[:5]:\n",
    "        array = data[key]\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        print(f\"Shape: {array.shape}\")\n",
    "        print(f\"Size: {array.size}\")\n",
    "        print(f\"Data Type: {array.dtype}\")\n",
    "        # print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_padded_embeddings(save_path_pattern):\n",
    "    \"\"\"\n",
    "    Load padded embeddings saved in batches.\n",
    "    \"\"\"\n",
    "    padded_embeddings = {}\n",
    "    i = 0\n",
    "\n",
    "    while True:\n",
    "        batch_path = f\"{save_path_pattern}_batch_{i}.npz\"\n",
    "        if not os.path.exists(batch_path):\n",
    "            break  # Stop if no more batches exist\n",
    "\n",
    "        batch = np.load(batch_path, allow_pickle=True)\n",
    "        for key in batch.files:\n",
    "            padded_embeddings[key] = batch[key]\n",
    "        \n",
    "        # if i == 3: # prov\n",
    "        #     break\n",
    "        i += 1\n",
    "\n",
    "    print(f\"Loaded {len(padded_embeddings)} embeddings from {i} batches.\")\n",
    "    return padded_embeddings\n",
    "\n",
    "# Load TCR embeddings\n",
    "padded_tcr_embeddings = load_padded_embeddings('../../data/embeddings/beta/allele/padded/padded_tcr_embeddings')\n",
    "\n",
    "# Load Epitope embeddings\n",
    "padded_epitope_embeddings = load_padded_embeddings('../../data/embeddings/beta/allele/padded/padded_epitope_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TCR_name         TRBV        TRBJ            TRB_CDR3 TRBC    Epitope  \\\n",
    "# 0      257  TRBV20-1*01  TRBJ2-5*01  CSARIWPYPAGGEETQYF  NaN  KLGGALQAK   \n",
    "# 1      358    TRBV28*01  TRBJ2-1*01     CASSKGLAGLDEQFF  NaN   RAKFKQLL   \n",
    "# 2      382   TRBV6-6*01  TRBJ2-5*01     CATQTPDSRRETQYF  NaN  KLGGALQAK   \n",
    "# 3      393  TRBV20-1*01  TRBJ1-2*01     CSARDLDSLTNGYTF  NaN  KLGGALQAK   \n",
    "# 4      396  TRBV10-2*01  TRBJ2-7*01       CASSEDREDEQYF  NaN  KLGGALQAK   \n",
    "\n",
    "#            MHC  Binding  task  \n",
    "# 0  HLA-A*03:01        1   NaN  \n",
    "# 1  HLA-B*08:01        1   NaN  \n",
    "# 2  HLA-A*03:01        1   NaN  \n",
    "# 3  HLA-A*03:01        1   NaN  \n",
    "# 4  HLA-A*03:01        1   NaN  \n",
    "# Train dataset length: 319226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy\n",
      "['configs', 'models_scripts', 'test-wandb.ipynb', 'O__building_V1.ipynb', '.gitignore', 'dummy_data', 'data_pipeline_TCRPeg.ipynb', 'data_scripts', 'data_pipeline_ProtBERT_fulldata.ipynb', 'datacheck_for_testfile.ipynb', 'data_pipeline_ProtBERT_testfileonly.ipynb', 'results', 'models', 'prov_emb_tcrpeg', 'physicochemical-properties.ipynb', '.git', 'V1_rough_out.py', 'building_V1_ProtBERT.ipynb', 'README.md', 'ENV.yml', 'data_pipeline_10x-allrows50-datacheck.ipynb', 'data_pipeline_ProtBERT.ipynb', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating dummy/subsets to try the model and pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### discarding from subset_train and subset_validation all the missing tcrs in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185630/3547741114.py:12: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling subsets\n",
      "Saving subsets\n",
      "Loading epitope embeddings\n",
      "Extracting TCR embeddings\n",
      "Processing TCR embeddings batch 0\n",
      "Processing TCR embeddings batch 1\n",
      "Processing TCR embeddings batch 2\n",
      "Processing TCR embeddings batch 3\n",
      "Processing TCR embeddings batch 4\n",
      "Processing TCR embeddings batch 5\n",
      "Processing TCR embeddings batch 6\n",
      "Processing TCR embeddings batch 7\n",
      "Processing TCR embeddings batch 8\n",
      "Processing TCR embeddings batch 9\n",
      "Processing TCR embeddings batch 10\n",
      "Processing TCR embeddings batch 11\n",
      "Processing TCR embeddings batch 12\n",
      "Processing TCR embeddings batch 13\n",
      "Processing TCR embeddings batch 14\n",
      "Processing TCR embeddings batch 15\n",
      "Processing TCR embeddings batch 16\n",
      "Processing TCR embeddings batch 17\n",
      "Processing TCR embeddings batch 18\n",
      "Processing TCR embeddings batch 19\n",
      "Processing TCR embeddings batch 20\n",
      "Processing TCR embeddings batch 21\n",
      "All TCRs were found in the batches.\n",
      "Saving subset TCR embeddings\n",
      "Filtering subsets to remove missing TCRs\n",
      "Saving filtered subsets\n",
      "Extracting epitope embeddings\n",
      "Saving subset epitope embeddings\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "subset_dir = './dummy_data'\n",
    "os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "\n",
    "subset_size = 50000\n",
    "\n",
    "# Randomly sample  rows\n",
    "print('Sampling subsets')\n",
    "subset_train = train_df.sample(n=subset_size, random_state=42)\n",
    "subset_valid = valid_df.sample(n=subset_size, random_state=42)\n",
    "\n",
    "# Save subsets\n",
    "print('Saving subsets')\n",
    "subset_train.to_csv(os.path.join(subset_dir, 'subset_train.tsv'), sep='\\t', index=False)\n",
    "subset_valid.to_csv(os.path.join(subset_dir, 'subset_validation.tsv'), sep='\\t', index=False)\n",
    "\n",
    "# Load epitope embeddings\n",
    "print('Loading epitope embeddings')\n",
    "epitope_embeddings = np.load('../../data/embeddings/beta/gene/prov/padded_epitope_embeddings_batch_0.npz')\n",
    "\n",
    "# Combine TCRs and epitopes from both training and validation subsets\n",
    "all_tcrs = set(subset_train['TRB_CDR3']).union(set(subset_valid['TRB_CDR3']))\n",
    "all_epitopes = set(subset_train['Epitope']).union(set(subset_valid['Epitope']))\n",
    "\n",
    "# Extract subset TCR embeddings\n",
    "print('Extracting TCR embeddings')\n",
    "subset_tcr_embeddings = {}\n",
    "missing_tcrs = set(all_tcrs)  # Initialize with all TCRs; remove found ones\n",
    "\n",
    "for i in range(22):\n",
    "    print(f'Processing TCR embeddings batch {i}')\n",
    "    batch_path = f'../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_{i}.npz'\n",
    "    batch_embeddings = np.load(batch_path)\n",
    "    for key in all_tcrs:\n",
    "        if key in batch_embeddings:\n",
    "            subset_tcr_embeddings[key] = batch_embeddings[key]\n",
    "            missing_tcrs.discard(key)  # Remove found TCR from missing set\n",
    "    batch_embeddings.close()  # Free up memory\n",
    "\n",
    "# Log missing TCRs\n",
    "if missing_tcrs:\n",
    "    print(f\"Missing TCRs in all batches: \", len(missing_tcrs)) # {missing_tcrs}\n",
    "else:\n",
    "    print(\"All TCRs were found in the batches.\")\n",
    "\n",
    "# Save subset TCR embeddings\n",
    "print('Saving subset TCR embeddings')\n",
    "np.savez(os.path.join(subset_dir, 'subset_padded_tcr_embeddings.npz'), **subset_tcr_embeddings)\n",
    "\n",
    "# Filter subset_train and subset_valid to keep only rows with TCRs in the embeddings\n",
    "print('Filtering subsets to remove missing TCRs')\n",
    "subset_train_filtered = subset_train[subset_train['TRB_CDR3'].isin(subset_tcr_embeddings.keys())]\n",
    "subset_valid_filtered = subset_valid[subset_valid['TRB_CDR3'].isin(subset_tcr_embeddings.keys())]\n",
    "\n",
    "# Save filtered subsets\n",
    "print('Saving filtered subsets')\n",
    "subset_train_filtered.to_csv(os.path.join(subset_dir, 'subset_train_filtered.tsv'), sep='\\t', index=False)\n",
    "subset_valid_filtered.to_csv(os.path.join(subset_dir, 'subset_validation_filtered.tsv'), sep='\\t', index=False)\n",
    "\n",
    "# Extract subset epitope embeddings\n",
    "print('Extracting epitope embeddings')\n",
    "subset_epitope_embeddings = {key: epitope_embeddings[key] for key in all_epitopes if key in epitope_embeddings}\n",
    "\n",
    "# Save subset epitope embeddings\n",
    "print('Saving subset epitope embeddings')\n",
    "np.savez(os.path.join(subset_dir, 'subset_padded_epitope_embeddings.npz'), **subset_epitope_embeddings)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "trf = './dummy_data/subset_train_filtered.tsv'\n",
    "valf = './dummy_data/subset_validation_filtered.tsv'\n",
    "tr = './dummy_data/subset_train.tsv'\n",
    "val = './dummy_data/subset_validation.tsv'\n",
    "\n",
    "paths = [tr, trf, val, valf]\n",
    "\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check if all keys in tcrs and epitopes are in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All TCRs in training set are present in TCR embeddings.\n",
      "All epitopes in training set are present in epitope embeddings.\n",
      "All TCRs in validation set are present in TCR embeddings.\n",
      "All epitopes in validation set are present in epitope embeddings.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "t_path = \"./dummy_data/subset_train_filtered.tsv\"\n",
    "v_path = \"./dummy_data/subset_validation_filtered.tsv\"\n",
    "t_e_path = \"./dummy_data/subset_padded_tcr_embeddings.npz\"\n",
    "e_e_path = \"./dummy_data/subset_padded_epitope_embeddings.npz\"\n",
    "\n",
    "# Load the training and validation data\n",
    "train_data = pd.read_csv(t_path, sep='\\t')\n",
    "validation_data = pd.read_csv(v_path, sep='\\t')\n",
    "\n",
    "# Load the TCR and epitope embeddings\n",
    "tcr_embeddings = np.load(t_e_path)\n",
    "epitope_embeddings = np.load(e_e_path)\n",
    "\n",
    "# Extract TCR and epitope sequences from the datasets\n",
    "train_tcrs = set(train_data['TRB_CDR3'])\n",
    "train_epitopes = set(train_data['Epitope'])\n",
    "validation_tcrs = set(validation_data['TRB_CDR3'])\n",
    "validation_epitopes = set(validation_data['Epitope'])\n",
    "\n",
    "# Extract keys from the embeddings\n",
    "tcr_keys = set(tcr_embeddings.keys())\n",
    "epitope_keys = set(epitope_embeddings.keys())\n",
    "\n",
    "# Check if all TCRs in the training set are in the TCR embeddings\n",
    "missing_train_tcrs = train_tcrs - tcr_keys\n",
    "if missing_train_tcrs:\n",
    "    print(f\"Missing TCRs in training set: {missing_train_tcrs}\")\n",
    "    print(len(missing_train_tcrs))\n",
    "else:\n",
    "    print(\"All TCRs in training set are present in TCR embeddings.\")\n",
    "\n",
    "# Check if all epitopes in the training set are in the epitope embeddings\n",
    "missing_train_epitopes = train_epitopes - epitope_keys\n",
    "if missing_train_epitopes:\n",
    "    print(f\"Missing epitopes in training set: {missing_train_epitopes}\")\n",
    "else:\n",
    "    print(\"All epitopes in training set are present in epitope embeddings.\")\n",
    "\n",
    "# Check if all TCRs in the validation set are in the TCR embeddings\n",
    "missing_validation_tcrs = validation_tcrs - tcr_keys\n",
    "if missing_validation_tcrs:\n",
    "    print(f\"Missing TCRs in validation set: {missing_validation_tcrs}\")\n",
    "    print(len(missing_validation_tcrs))\n",
    "else:\n",
    "    print(\"All TCRs in validation set are present in TCR embeddings.\")\n",
    "\n",
    "# Check if all epitopes in the validation set are in the epitope embeddings\n",
    "missing_validation_epitopes = validation_epitopes - epitope_keys\n",
    "if missing_validation_epitopes:\n",
    "    print(f\"Missing epitopes in validation set: {missing_validation_epitopes}\")\n",
    "else:\n",
    "    print(\"All epitopes in validation set are present in epitope embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binding not Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset - Binding: 24984, Non-Binding: 25016\n",
      "Validation Dataset - Binding: 8287, Non-Binding: 41713\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "t_path = \"./dummy_data/subset_train_filtered.tsv\"\n",
    "v_path = \"./dummy_data/subset_validation_filtered.tsv\"\n",
    "\n",
    "# Function to count binding and non-binding rows\n",
    "def count_binding_rows(file_path):\n",
    "    # Read the TSV file\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Count the number of binding (1) and non-binding (0) rows\n",
    "    binding_count = df[df['Binding'] == 1].shape[0]\n",
    "    non_binding_count = df[df['Binding'] == 0].shape[0]\n",
    "    \n",
    "    return binding_count, non_binding_count\n",
    "\n",
    "# Count for training dataset\n",
    "train_binding, train_non_binding = count_binding_rows(t_path)\n",
    "print(f\"Training Dataset - Binding: {train_binding}, Non-Binding: {train_non_binding}\")\n",
    "\n",
    "# Count for validation dataset\n",
    "val_binding, val_non_binding = count_binding_rows(v_path)\n",
    "print(f\"Validation Dataset - Binding: {val_binding}, Non-Binding: {val_non_binding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed batch 0 successfully.\n",
      " Processed batch 1 successfully.\n",
      " Processed batch 2 successfully.\n",
      " Processed batch 3 successfully.\n",
      " Processed batch 4 successfully.\n",
      " Processed batch 5 successfully.\n",
      " Processed batch 6 successfully.\n",
      " Processed batch 7 successfully.\n",
      " Processed batch 8 successfully.\n",
      " Processed batch 9 successfully.\n",
      " Processed batch 10 successfully.\n",
      " Processed batch 11 successfully.\n",
      " Processed batch 12 successfully.\n",
      " Processed batch 13 successfully.\n",
      " Processed batch 14 successfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Write directly to disk in an incremental fashion\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 30\u001b[0m     np\u001b[38;5;241m.\u001b[39msavez(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m load_batches()})\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Combined embeddings saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mload_batches\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(file_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 23\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m key, data[key]  \u001b[38;5;66;03m# Yield one key-value pair at a time\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Processed batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mread_array(\u001b[38;5;28mbytes\u001b[39m,\n\u001b[1;32m    257\u001b[0m                              allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_pickle,\n\u001b[1;32m    258\u001b[0m                              pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpickle_kwargs,\n\u001b[1;32m    259\u001b[0m                              max_header_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_header_size)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py:831\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m    830\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 831\u001b[0m             data \u001b[38;5;241m=\u001b[39m _read_bytes(fp, read_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    832\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    833\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py:966\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m         r \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m    967\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/zipfile/__init__.py:981\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 981\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read1(n)\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/zipfile/__init__.py:1057\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m   1056\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m-> 1057\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(data, n)\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the base directory and file pattern\n",
    "base_dir = \"../../data/embeddings/beta/gene/prov\"\n",
    "file_pattern = \"padded_tcr_embeddings_batch_{}.npz\"\n",
    "\n",
    "# Output file path\n",
    "output_path = \"../../data/embeddings/beta/allele/padded_tcr_embeddings.npz\"\n",
    "\n",
    "# Create a generator function to yield key-value pairs\n",
    "def load_batches():\n",
    "    for i in range(22):\n",
    "        file_path = os.path.join(base_dir, file_pattern.format(i))\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: File {file_path} not found. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with np.load(file_path, allow_pickle=True) as data:\n",
    "                for key in data.keys():\n",
    "                    yield key, data[key]  # Yield one key-value pair at a time\n",
    "            print(f\" Processed batch {i} successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error loading {file_path}: {e}\")\n",
    "\n",
    "# Write directly to disk in an incremental fashion\n",
    "with open(output_path, 'wb') as f:\n",
    "    np.savez(f, **{key: value for key, value in load_batches()})\n",
    "\n",
    "print(f\" Combined embeddings saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the v1_mha and v1_mha_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(configs_path='./configs/v1_mha_config.yaml', train='./dummy_data/subset_train_filtered.tsv', val='./dummy_data/subset_validation_filtered.tsv', tcr_embeddings='./dummy_data/subset_padded_tcr_embeddings.npz', epitope_embeddings='./dummy_data/subset_padded_epitope_embeddings.npz', epochs=3, batch_size=8, learning_rate=None, embed_dim=None, num_heads=None, num_layers=None, max_tcr_length=None, max_epitope_length=None) \n",
      " {'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embed_dim': 128, 'num_heads': 8, 'num_layers': 2, 'max_tcr_length': 43, 'max_epitope_length': 43, 'model_path': 'results/trained_models/v1_mha.pth', 'data_paths': {'train': '../../data/splitted_datasets/allele/beta/train.tsv', 'val': '../../data/splitted_datasets/allele/beta/validation.tsv', 'test': '../../data/splitted_datasets/allele/beta/test.tsv'}, 'embeddings': {'tcr': '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz', 'epitope': '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'}}\n",
      "embed_dim:  128\n",
      "max_tcr_length:  43\n",
      "max_epitope_length:  43\n",
      "Epoch [1/3], Loss: 0.5120, Val AUC: 0.9997\n",
      "Epoch [2/3], Loss: 0.5086, Val AUC: 0.9952\n",
      "Epoch [3/3], Loss: 0.5136, Val AUC: 0.9974\n",
      "Best model saved with AUC: 0.9996612561329208\n"
     ]
    }
   ],
   "source": [
    "# overfit the model \n",
    "# train and validation a 50.000 samples\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/subset_train_filtered.tsv \\\n",
    "    --val ./dummy_data/subset_validation_filtered.tsv \\\n",
    "        --tcr_embeddings ./dummy_data/subset_padded_tcr_embeddings.npz \\\n",
    "            --epitope_embeddings ./dummy_data/subset_padded_epitope_embeddings.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 \n",
    "                        #-- config_path ./configs/v1_mha_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.001\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.5128, Val AUC: 1.0000, Val Accuracy: 0.1657\n",
      "Epoch [2/3], Loss: 0.5092, Val AUC: 0.9999, Val Accuracy: 0.1657\n",
      "Epoch [3/3], Loss: 0.5053, Val AUC: 1.0000, Val Accuracy: 0.1657\n",
      "Best model saved with AUC: 0.9999773631714294\n"
     ]
    }
   ],
   "source": [
    "# check memory\n",
    "# train and validation a 50.000 samples\n",
    "# batch_size = 16\n",
    "# accuracy calculation added\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/subset_train_filtered.tsv \\\n",
    "    --val ./dummy_data/subset_validation_filtered.tsv \\\n",
    "        --tcr_embeddings ./dummy_data/subset_padded_tcr_embeddings.npz \\\n",
    "            --epitope_embeddings ./dummy_data/subset_padded_epitope_embeddings.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/subset_train_filtered.tsv\n",
      "val_path: ./dummy_data/subset_validation_filtered.tsv\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.5151, Val AUC: 0.9467, Val Accuracy: 0.1657\n",
      "Epoch [2/3], Loss: 0.5066, Val AUC: 0.9994, Val Accuracy: 0.1657\n",
      "Epoch [3/3], Loss: 0.5041, Val AUC: 0.9990, Val Accuracy: 0.1657\n",
      "Best model saved with AUC: 0.9994373945324483\n"
     ]
    }
   ],
   "source": [
    "# overfit the model \n",
    "# train and validation a 50.000 samples\n",
    "# \n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/subset_train_filtered.tsv \\\n",
    "    --val ./dummy_data/subset_validation_filtered.tsv \\\n",
    "        --tcr_embeddings ./dummy_data/subset_padded_tcr_embeddings.npz \\\n",
    "            --epitope_embeddings ./dummy_data/subset_padded_epitope_embeddings.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 \n",
    "\n",
    "# 17m 26.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/subset_train_filtered.tsv\n",
      "val_path: ./dummy_data/subset_validation_filtered.tsv\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.5075, Val AUC: 0.9995, Val Accuracy: 0.9939\n",
      "Epoch [2/3], Loss: 0.5034, Val AUC: 0.9999, Val Accuracy: 0.9999\n",
      "Epoch [3/3], Loss: 0.5033, Val AUC: 0.9998, Val Accuracy: 0.9999\n",
      "Best model saved with AUC: 0.999853282975565\n"
     ]
    }
   ],
   "source": [
    "# new USING:\n",
    "# pooled = combined.mean(dim=1)  # Average across all tokens, shape: (B, D)\n",
    "# output = torch.sigmoid(self.output_layer(pooled)).squeeze(1)\n",
    "\n",
    "# subset = 50.000 samples\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/subset_train_filtered.tsv \\\n",
    "    --val ./dummy_data/subset_validation_filtered.tsv \\\n",
    "        --tcr_embeddings ./dummy_data/subset_padded_tcr_embeddings.npz \\\n",
    "            --epitope_embeddings ./dummy_data/subset_padded_epitope_embeddings.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 \n",
    "\n",
    "# 14m 13.5s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_batches.py:58: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(train_path, sep='\\t')\n",
      "Sample 0 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 1 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 2 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 3 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 4 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 0 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 1 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 2 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 3 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 4 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "starting epoch  0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_batches.py\", line 103, in <module>\n",
      "    epoch_loss = 0\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models/morning_stars_v1/beta/v1_mha_batches.py\", line 52, in __getitem__\n",
      "    tcr_embedding = torch.tensor(tcr_batch_data[tcr_id], dtype=torch.float32)\n",
      "                                 ~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py\", line 256, in __getitem__\n",
      "    return format.read_array(bytes,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py\", line 782, in read_array\n",
      "    version = read_magic(fp)\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py\", line 235, in read_magic\n",
      "    magic_str = _read_bytes(fp, MAGIC_LEN, \"magic string\")\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py\", line 966, in _read_bytes\n",
      "    r = fp.read(size - len(data))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 981, in read\n",
      "    data = self._read1(n)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1057, in _read1\n",
      "    data = self._decompressor.decompress(data, n)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# train with all the data (tran/validation) and all batches of embeddings\n",
    "! python models_scripts/v1_mha/train_batches.py \\\n",
    "    --model_path results/trained_models/v1_mha/v1_mha_batches.pth \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 16 \n",
    "\n",
    "# 138m  and still in 'starting epoch 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_batches.py:59: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(train_path, sep='\\t')\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Starting epoch 0\n",
      "Training Epoch 0:   4%|               | 724/17516 [11:18<4:22:11,  1.07batch/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_batches.py\", line 106, in <module>\n",
      "    for tcr, epitope, label in tqdm(train_loader, desc=f\"Training Epoch {epoch}\", unit=\"batch\"):\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models/morning_stars_v1/beta/v1_mha_batches.py\", line 51, in __getitem__\n",
      "    tcr_batch_data = np.load(tcr_batch_file, mmap_mode=\"r\")\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py\", line 444, in load\n",
      "    ret = NpzFile(fid, own_fid=own_fid, allow_pickle=allow_pickle,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py\", line 190, in __init__\n",
      "    _zip = zipfile_factory(fid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n",
      "    return zipfile.ZipFile(file, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1341, in __init__\n",
      "    self._RealGetContents()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1480, in _RealGetContents\n",
      "    for zinfo in sorted(self.filelist,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1481, in <lambda>\n",
      "    key=lambda zinfo: zinfo.header_offset,\n",
      "\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# train with all the data (tran/validation) and all batches of embeddings\n",
    "# with progress bar\n",
    "! python models_scripts/v1_mha/train_batches.py \\\n",
    "    --model_path results/trained_models/v1_mha/v1_mha_batches.pth \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 16 \n",
    "# 11m 25.3s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old attempts to build the model with TCRPeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings.\n",
    "            n_heads: Number of attention heads.\n",
    "            dropout: Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Multihead Attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, n_heads, dropout=dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feedforward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),  # Expand dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)   # Compress back to original dimension\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (seq_len, batch_size, embed_dim).\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (seq_len, batch_size, embed_dim).\n",
    "        \"\"\"\n",
    "        # Multihead Attention\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)  # Self-attention\n",
    "        x = x + self.dropout(attn_output)              # Residual connection\n",
    "        x = self.norm1(x)                              # Layer normalization\n",
    "        \n",
    "        # Feedforward Network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)               # Residual connection\n",
    "        x = self.norm2(x)                              # Layer normalization\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Testing the Updated Dataset\\\n",
    "Let's test the updated dataset to ensure the combined embeddings are in the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in TCR embeddings file: ['embeddings', 'labels']\n",
      "Keys in Epitope embeddings file: ['embeddings']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the .npz files\n",
    "tcr_data = np.load('/home/ubuntu/data/embeddings/beta/gene/TCRPeg_tcr_embeddings.npz')\n",
    "epitope_data = np.load('/home/ubuntu/data/embeddings/beta/gene/TCRPeg_Epitope_embeddings.npz')\n",
    "\n",
    "# Print the keys in each file\n",
    "print(\"Keys in TCR embeddings file:\", list(tcr_data.keys()))\n",
    "print(\"Keys in Epitope embeddings file:\", list(epitope_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK!!  \n",
    "\\\n",
    "Step 1: Verify the Size of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TCR embeddings: 570500\n",
      "Number of epitope embeddings: 570500\n"
     ]
    }
   ],
   "source": [
    "# Load the TCR and epitope embeddings\n",
    "tcr_data = np.load('/home/ubuntu/data/embeddings/beta/gene/TCRPeg_tcr_embeddings.npz')\n",
    "epitope_data = np.load('/home/ubuntu/data/embeddings/beta/gene/TCRPeg_Epitope_embeddings.npz')\n",
    "\n",
    "tcr_embeddings = tcr_data['embeddings']  # Use the correct key\n",
    "epitope_embeddings = epitope_data['embeddings']  # Use the correct key\n",
    "\n",
    "print(f\"Number of TCR embeddings: {len(tcr_embeddings)}\")\n",
    "print(f\"Number of epitope embeddings: {len(epitope_embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Ensure Labels Match the Dataset Size\\\n",
    "If you have labels, ensure the labels array has the same length as the number of TCR/epitope embeddings. If you don't have labels, we can create dummy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 570500\n"
     ]
    }
   ],
   "source": [
    "# If you have labels, load them here\n",
    "path_to_labels = '/home/ubuntu/data/embeddings/beta/gene/TCRPeg_tcr_embeddings.npz'\n",
    "labels_data = np.load(path_to_labels)  # Example: Load labels from a file\n",
    "labels = labels_data['labels']\n",
    "\n",
    "# If you don't have labels, create dummy labels\n",
    "# labels = np.zeros(len(tcr_embeddings))  # Dummy labels (all zeros)\n",
    "\n",
    "print(f\"Number of labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation and test\\\n",
    "This is only a dummy splitting, since for real testing we'll use \n",
    "a separate test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% training\n",
    "val_size = len(dataset) - train_size   # 20% validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Update the Dataset Class \\\n",
    "Update the TCR_Epitope_Dataset class to ensure it uses the correct indices and handles the dataset size properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_Epitope_Dataset(Dataset):\n",
    "    def __init__(self, tcr_embeddings, epitope_embeddings, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tcr_embeddings: Array of TCR embeddings.\n",
    "            epitope_embeddings: Array of epitope embeddings.\n",
    "            labels: Optional array of labels (1 for binding, 0 for non-binding).\n",
    "        \"\"\"\n",
    "        self.tcr_embeddings = tcr_embeddings\n",
    "        self.epitope_embeddings = epitope_embeddings\n",
    "        \n",
    "        # Ensure the number of TCR and epitope embeddings match\n",
    "        assert len(self.tcr_embeddings) == len(self.epitope_embeddings), \\\n",
    "            \"Number of TCR and epitope embeddings must match!\"\n",
    "        \n",
    "        # If labels are provided, use them; otherwise, create dummy labels\n",
    "        self.labels = labels if labels is not None else np.zeros(len(self.tcr_embeddings))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tcr_embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            combined_embedding: Combined TCR and epitope embeddings of shape (2, 1024).\n",
    "            label: Binding label (1 or 0).\n",
    "        \"\"\"\n",
    "        tcr_embedding = self.tcr_embeddings[idx]\n",
    "        epitope_embedding = self.epitope_embeddings[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        tcr_embedding = torch.tensor(tcr_embedding, dtype=torch.float32)\n",
    "        epitope_embedding = torch.tensor(epitope_embedding, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # Combine TCR and epitope embeddings along the sequence dimension\n",
    "        combined_embedding = torch.stack([tcr_embedding, epitope_embedding], dim=0)  # Shape: (2, 1024)\n",
    "        \n",
    "        return combined_embedding, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Create the Dataset and DataLoader \\\n",
    "Now, create the dataset and DataLoader with the correct data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 570500\n",
      "Combined embedding shape: torch.Size([2, 1024])\n",
      "Label: 1.0\n",
      "Combined embeddings batch shape: torch.Size([32, 2, 1024])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TCR_Epitope_Dataset(tcr_embeddings, epitope_embeddings, labels)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "combined_embedding, label = dataset[0]  # Get the first sample\n",
    "print(f\"Combined embedding shape: {combined_embedding.shape}\")  # Should be (2, 1024)\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in dataloader:\n",
    "    combined_embeddings, labels = batch\n",
    "    print(f\"Combined embeddings batch shape: {combined_embeddings.shape}\")  # Should be (batch_size, 2, 1024)\n",
    "    print(f\"Labels batch shape: {labels.shape}\")  # Should be (batch_size,)\n",
    "    break  # Stop after the first batch for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Integrate the Transformer Block\\\n",
    "We'll create a model class that:\n",
    "\n",
    "Takes the combined TCR and epitope embeddings as input.\n",
    "\n",
    "Passes them through the TransformerBlock.\n",
    "\n",
    "Flattens the output and passes it through a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TCR_Epitope_Model(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, dropout=0.1, classifier_hidden_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings (1024 in your case).\n",
    "            n_heads: Number of attention heads in the transformer.\n",
    "            dropout: Dropout rate for regularization.\n",
    "            classifier_hidden_dim: Hidden dimension of the classifier.\n",
    "        \"\"\"\n",
    "        super(TCR_Epitope_Model, self).__init__()\n",
    "        \n",
    "        # Transformer Block\n",
    "        self.transformer_block = TransformerBlock(embed_dim, n_heads, dropout)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2 * embed_dim, classifier_hidden_dim),  # Input: flattened transformer output\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(classifier_hidden_dim, 1)  # Output: binary prediction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 2, embed_dim).\n",
    "        \n",
    "        Returns:\n",
    "            logits: Output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Permute input to (seq_len, batch_size, embed_dim) for the transformer\n",
    "        x = x.permute(1, 0, 2)  # Shape: (2, batch_size, embed_dim)\n",
    "        \n",
    "        # Pass through the transformer block\n",
    "        x = self.transformer_block(x)  # Shape: (2, batch_size, embed_dim)\n",
    "        \n",
    "        # Permute back to (batch_size, seq_len, embed_dim)\n",
    "        x = x.permute(1, 0, 2)  # Shape: (batch_size, 2, embed_dim)\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.reshape(x.size(0), -1)  # Shape: (batch_size, 2 * embed_dim)\n",
    "        \n",
    "        # Pass through the classifier\n",
    "        logits = self.classifier(x)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Test the Model \\\n",
    "Let's test the model with a batch of data to ensure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# # Initialize the model\n",
    "# embed_dim = 1024  # Dimensionality of TCR and epitope embeddings\n",
    "# n_heads = 4       # Number of attention heads\n",
    "# model = TCR_Epitope_Model(embed_dim, n_heads)\n",
    "\n",
    "# # Get a batch of data from the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     combined_embeddings, labels = batch\n",
    "#     break  # Stop after the first batch\n",
    "\n",
    "# # Pass the batch through the model\n",
    "# logits = model(combined_embeddings)\n",
    "# print(f\"Logits shape: {logits.shape}\")  # Should be (batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we forgot to use GPU. Then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: Tesla T4\n",
      "Model is on: cuda:0\n",
      "Data is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")\n",
    "\n",
    "# Move the model to the GPU (if available)\n",
    "model = model.to(device)\n",
    "\n",
    "# Move Data to GPU\n",
    "\n",
    "for batch in dataloader:\n",
    "    combined_embeddings, labels = batch\n",
    "    \n",
    "    # Move data to the GPU (if available)\n",
    "    combined_embeddings = combined_embeddings.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Rest of the training loop...\n",
    "\n",
    "# Verify GPU Usage\n",
    "\n",
    "# Check model device\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")\n",
    "\n",
    "# Check data device\n",
    "print(f\"Data is on: {combined_embeddings.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Add a Training Loop\\\n",
    "Now, let's add a simple training loop to train the model. We'll use binary cross-entropy loss and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.30530472857386276\n",
      "Epoch 2/5, Loss: 0.1719683983865046\n",
      "Epoch 3/5, Loss: 0.157329112976129\n",
      "Epoch 4/5, Loss: 0.14955465956043631\n",
      "Epoch 5/5, Loss: 0.14537664496026595\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TCR_Epitope_Model(embed_dim, n_heads).to(device)  # Move model to GPU\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss with logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        combined_embeddings, labels = batch\n",
    "        \n",
    "        # Move data to the GPU (if available)\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(combined_embeddings).squeeze()  # Shape: (batch_size,)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Prepare a Validation Set\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% training\n",
    "val_size = len(dataset) - train_size   # 20% validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 2: Evaluation Function\\\n",
    "We'll write a function to evaluate the model on the validation set. This function will:\n",
    "\n",
    "Switch the model to evaluation mode.\n",
    "\n",
    "Disable gradient computation.\n",
    "\n",
    "Compute predictions and metrics like accuracy, ROC-AUC, and precision-recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in dataloader:\n",
    "            combined_embeddings, labels = batch\n",
    "            \n",
    "            # Move data to the GPU (if available)\n",
    "            combined_embeddings = combined_embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(combined_embeddings).squeeze()\n",
    "            predictions = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions > 0.5)  # Threshold at 0.5\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions > 0.5, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions > 0.5, zero_division=0)\n",
    "    \n",
    "    return accuracy, roc_auc, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for batch in train_dataloader:\n",
    "        combined_embeddings, labels = batch\n",
    "        \n",
    "        # Move data to the GPU (if available)\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(combined_embeddings).squeeze()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print training loss\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    accuracy, roc_auc, precision, recall = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Validation Metrics - Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"tcr_epitope_model.pth\")\n",
    "\n",
    "# To load the model later:\n",
    "\n",
    "# Load the model\n",
    "model = TCR_Epitope_Model(embed_dim, n_heads).to(device)\n",
    "model.load_state_dict(torch.load(\"tcr_epitope_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with hyperparameter tuning, test set evaluation, and visualizations for your Multihead Attention Model. We'll break this down into clear steps. \n",
    "\n",
    "Step 1: Hyperparameter Tuning \\\n",
    "Hyperparameter tuning involves finding the best set of hyperparameters (e.g., learning rate, number of attention heads, dropout rate) for your model. We'll use a simple grid search approach.\n",
    "\n",
    "Key Hyperparameters to Tune:\n",
    "\n",
    "Learning Rate: Controls the step size during optimization.\n",
    "\n",
    "Number of Attention Heads: Determines how many parallel attention mechanisms to use.\n",
    "\n",
    "Dropout Rate: Regularization to prevent overfitting.\n",
    "\n",
    "Hidden Dimension of the Classifier: Size of the hidden layer in the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define hyperparameter grid\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "n_heads_list = [2, 4, 8]\n",
    "dropout_rates = [0.1, 0.2]\n",
    "classifier_hidden_dims = [64, 128]\n",
    "\n",
    "# Iterate over all combinations\n",
    "best_roc_auc = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for lr, n_heads, dropout, hidden_dim in product(learning_rates, n_heads_list, dropout_rates, classifier_hidden_dims):\n",
    "    print(f\"Testing: lr={lr}, n_heads={n_heads}, dropout={dropout}, hidden_dim={hidden_dim}\")\n",
    "    \n",
    "    # Initialize model with current hyperparameters\n",
    "    model = TCR_Epitope_Model(embed_dim=1024, n_heads=n_heads, dropout=dropout, classifier_hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(5):  # Short training for hyperparameter tuning\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            combined_embeddings, labels = batch\n",
    "            combined_embeddings, labels = combined_embeddings.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(combined_embeddings).squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    accuracy, roc_auc, precision, recall = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Validation Metrics - Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    \n",
    "    # Track the best hyperparameters\n",
    "    if roc_auc > best_roc_auc:\n",
    "        best_roc_auc = roc_auc\n",
    "        best_hyperparams = {\n",
    "            \"learning_rate\": lr,\n",
    "            \"n_heads\": n_heads,\n",
    "            \"dropout\": dropout,\n",
    "            \"classifier_hidden_dim\": hidden_dim\n",
    "        }\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_hyperparams}, Best ROC-AUC: {best_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Test Set Evaluation \\\n",
    "Once you've identified the best hyperparameters, evaluate the model on the test set.\n",
    "\n",
    "Load the Test Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test embeddings (assuming Scenario B: shared embeddings)\n",
    "test_tcr_embeddings = all_tcr_embeddings[test_tcr_indices]\n",
    "test_epitope_embeddings = all_epitope_embeddings[test_epitope_indices]\n",
    "\n",
    "# Create test dataset and DataLoader\n",
    "test_dataset = TCR_Epitope_Dataset(test_tcr_embeddings, test_epitope_embeddings, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the Test Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best hyperparameters\n",
    "best_model = TCR_Epitope_Model(\n",
    "    embed_dim=1024,\n",
    "    n_heads=best_hyperparams[\"n_heads\"],\n",
    "    dropout=best_hyperparams[\"dropout\"],\n",
    "    classifier_hidden_dim=best_hyperparams[\"classifier_hidden_dim\"]\n",
    ").to(device)\n",
    "\n",
    "# Load the trained weights (if you saved the model)\n",
    "best_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy, test_roc_auc, test_precision, test_recall = evaluate(best_model, test_dataloader, device)\n",
    "print(f\"Test Metrics - Accuracy: {test_accuracy:.4f}, ROC-AUC: {test_roc_auc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Visualizations \\\n",
    "Visualizations help you understand the model's performance. We'll create:\n",
    "\n",
    "ROC Curve: To visualize the trade-off between true positive rate (TPR) and false positive rate (FPR).\n",
    "\n",
    "Precision-Recall Curve: To visualize the trade-off between precision and recall.\n",
    "\n",
    "Confusion Matrix: To show the distribution of predictions.\n",
    "\n",
    "ROC Curve:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions and labels\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        combined_embeddings, labels = batch\n",
    "        combined_embeddings, labels = combined_embeddings.to(device), labels.to(device)\n",
    "        \n",
    "        logits = best_model(combined_embeddings).squeeze()\n",
    "        predictions = torch.sigmoid(logits)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(all_labels, all_predictions)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute confusion matrix\n",
    "binary_predictions = (np.array(all_predictions) > 0.5).astype(int)\n",
    "cm = confusion_matrix(all_labels, binary_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Binding', 'Binding'], yticklabels=['Not Binding', 'Binding'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful chunks...  just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'correct' padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to embeddings files\n",
    "# paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "# paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "# paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "\n",
    "beta_all_epi_path = './dummy_data/subset_padded_epitope_embeddings.npz'\n",
    "beta_all_trb_path = './dummy_data/subset_padded_tcr_embeddings.npz'\n",
    "\n",
    "# Load NPZ files into GPU\n",
    "def load_embeddings_to_gpu(path):\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    return {key: torch.tensor(data[key], device=device) for key in data}\n",
    "\n",
    "# epi_embeddings = load_embeddings_to_gpu(paired_all_epi_path)\n",
    "# tra_embeddings = load_embeddings_to_gpu(paired_all_tra_path)\n",
    "# trb_embeddings = load_embeddings_to_gpu(paired_all_trb_path)\n",
    "\n",
    "beta_epi_embeddings = load_embeddings_to_gpu(beta_all_epi_path)\n",
    "beta_trb_embeddings = load_embeddings_to_gpu(beta_all_trb_path)\n",
    "\n",
    "# Find max sequence length\n",
    "# max_len = max(max(e.shape[0] for e in epi_embeddings.values()), \n",
    "#               max(e.shape[0] for e in tra_embeddings.values()), \n",
    "#               max(e.shape[0] for e in trb_embeddings.values()))\n",
    "\n",
    "beta_max_len = max(max(e.shape[0] for e in beta_epi_embeddings.values()), \n",
    "                    max(e.shape[0] for e in beta_trb_embeddings.values()))\n",
    "\n",
    "# Function to pad embeddings on GPU\n",
    "def pad_embeddings(embeddings, max_len):\n",
    "    return pad_sequence(\n",
    "        [torch.nn.functional.pad(e, (0, 0, 0, max_len - e.shape[0])) for e in embeddings.values()],\n",
    "        batch_first=True, padding_value=0.0\n",
    "    )\n",
    "\n",
    "# Pad embeddings (all computations on GPU)\n",
    "# padded_epi = pad_embeddings(epi_embeddings, max_len)\n",
    "# padded_tra = pad_embeddings(tra_embeddings, max_len)\n",
    "# padded_trb = pad_embeddings(trb_embeddings, max_len)\n",
    "\n",
    "padded_beta_epi = pad_embeddings(beta_epi_embeddings, beta_max_len)\n",
    "padded_beta_trb = pad_embeddings(beta_trb_embeddings, beta_max_len)\n",
    "\n",
    "# Move back to CPU before saving\n",
    "# padded_epi = padded_epi.cpu().numpy()\n",
    "# padded_tra = padded_tra.cpu().numpy()\n",
    "# padded_trb = padded_trb.cpu().numpy()\n",
    "\n",
    "padded_beta_epi = padded_beta_epi.cpu().numpy()\n",
    "padded_beta_trb = padded_beta_trb.cpu().numpy()\n",
    "\n",
    "# Save padded embeddings\n",
    "# padd_paired_all_epi_path = '../../data/embeddings/paired/allele/padded_Epitope_paired_embeddings.npz'\n",
    "# padd_paired_all_tra_path = '../../data/embeddings/paired/allele/padded_TRA_paired_embeddings.npz'\n",
    "# padd_paired_all_trb_path = '../../data/embeddings/paired/allele/padded_TRB_paired_embeddings.npz'\n",
    "\n",
    "padd_beta_all_epi_path = './dummy_data/subset_v2_padded_epitope_embeddings.npz'\n",
    "padd_beta_all_trb_path = './dummy_data/subset_v2_padded_tcr_embeddings.npz'\n",
    "\n",
    "# np.savez(padd_paired_all_epi_path, **{key: padded_epi[i] for i, key in enumerate(epi_embeddings)})\n",
    "# np.savez(padd_paired_all_tra_path, **{key: padded_tra[i] for i, key in enumerate(tra_embeddings)})\n",
    "# np.savez(padd_paired_all_trb_path, **{key: padded_trb[i] for i, key in enumerate(trb_embeddings)})\n",
    "\n",
    "np.savez(padd_beta_all_epi_path, **{key: padded_beta_epi[i] for i, key in enumerate(beta_epi_embeddings)})\n",
    "np.savez(padd_beta_all_trb_path, **{key: padded_beta_trb[i] for i, key in enumerate(beta_trb_embeddings)})\n",
    "\n",
    "print(\"Padded embedding saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
