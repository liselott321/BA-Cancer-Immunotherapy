{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tidytcells in /home/ubuntu/anaconda3/lib/python3.12/site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install tidytcells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision of mhc and V/J values (gene or allele)\n",
    "precision = 'gene'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is not thread safe\n",
    "def create_folders_if_not_exists(folders):\n",
    "  for path in folders:\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = '../../data'\n",
    "pipeline_data_plain = f'{pipeline_data}/plain_datasets'\n",
    "pipeline_data_cleaned = f'{pipeline_data}/cleaned_datasets'\n",
    "pipeline_data_concatenated = f'{pipeline_data}/concatenated_datasets'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "pipeline_data_temp_bucket = f'{pipeline_data}/temp'\n",
    "\n",
    "pipeline_folders = [pipeline_data, pipeline_data_plain, pipeline_data_cleaned, pipeline_data_concatenated, pipeline_data_splitted, pipeline_data_temp_bucket]\n",
    "\n",
    "create_folders_if_not_exists(pipeline_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "IEDB_data_plain = f'{pipeline_data_plain}/IEDB'\n",
    "IEDB_data_cleaned = f'{pipeline_data_cleaned}/IEDB'\n",
    "IEDB_data_fitted = f'{pipeline_data_temp_bucket}/IEDB'\n",
    "\n",
    "IEDB_folders = [IEDB_data_plain, IEDB_data_cleaned, IEDB_data_fitted]\n",
    "create_folders_if_not_exists(IEDB_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook IEDB fit data\n",
    "path_prefix_plain = IEDB_data_plain\n",
    "path_prefix_fitted = IEDB_data_fitted\n",
    "mhc_I_input_beta = f\"{path_prefix_plain}/MHCI_IEDB_beta_export.csv\"\n",
    "mhc_I_output_beta = f\"{path_prefix_fitted}/IEDB_beta_fitted.csv\"\n",
    "mhc_I_input_paired = f\"{path_prefix_plain}/MHCI_IEDB_paired_export.csv\"\n",
    "mhc_I_output_paired = f\"{path_prefix_fitted}/IEDB_paired_fitted.csv\"\n",
    "\n",
    "# fit IEDB data\n",
    "%run ../../data_scripts/IEDB/IEDB_fitted_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook IEDB clean data\n",
    "path_prefix_fitted = IEDB_data_fitted\n",
    "path_prefix_cleaned =  IEDB_data_cleaned\n",
    "fitted_file_beta = \"IEDB_beta_fitted.csv\"\n",
    "fitted_file_paired = \"IEDB_paired_fitted.csv\"\n",
    "cleaned_file_beta = \"IEDB_cleaned_data_beta.csv\"\n",
    "cleaned_file_paired = \"IEDB_cleaned_data_paired.csv\"\n",
    "\n",
    "# clean IEDB data\n",
    "%run ../../data_scripts/IEDB/IEDB_clean_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEDB_cleaned_beta_output = f'{IEDB_data_cleaned}/{cleaned_file_beta}'\n",
    "IEDB_cleaned_paired_output = f'{IEDB_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "McPas_data_plain = f'{pipeline_data_plain}/McPas'\n",
    "McPas_data_cleaned = f'{pipeline_data_cleaned}/McPas'\n",
    "McPas_data_fitted = f'{pipeline_data_temp_bucket}/McPas'\n",
    "\n",
    "McPas_folders = [McPas_data_plain, McPas_data_cleaned, McPas_data_fitted]\n",
    "create_folders_if_not_exists(McPas_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook McPAS fit data\n",
    "input_file = f'{McPas_data_plain}/McPAS-TCR.csv'\n",
    "path_prefix_fitted = McPas_data_fitted\n",
    "fitted_file = 'McPAS_fitted.tsv'\n",
    "\n",
    "# fit McPAS data\n",
    "%run ../../data_scripts/McPas-TCR/fit_data_mcpastcr_both.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 10078 entries\n",
      "whole dataframe has 13701 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 10078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25574/2652240660.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  mcpastcr_cleaned_both_df = mcpastcr_cleaned_both_df[~mask]\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook McPAS clean data\n",
    "fitted_input_file = f'{McPas_data_fitted}/{fitted_file}'\n",
    "path_prefix_cleaned = McPas_data_cleaned\n",
    "cleaned_file_paired = 'McPAS_cleaned_data_paired.tsv'\n",
    "cleaned_file_beta = 'McPAS_cleaned_data_beta.tsv'\n",
    "\n",
    "# clean McPAS data\n",
    "%run ../../data_scripts/McPas-TCR/clean_data_mcpastcr_both.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "McPAS_cleaned_beta_output = f'{McPas_data_cleaned}/{cleaned_file_beta}'\n",
    "McPAS_cleaned_paired_output = f'{McPas_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VDJdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "VDJdb_data_plain = f'{pipeline_data_plain}/VDJdb'\n",
    "VDJdb_data_cleaned = f'{pipeline_data_cleaned}/VDJdb'\n",
    "VDJdb_data_fitted = f'{pipeline_data_temp_bucket}/VDJdb'\n",
    "\n",
    "VDJdb_folders = [VDJdb_data_plain, VDJdb_data_cleaned, VDJdb_data_fitted]\n",
    "create_folders_if_not_exists(VDJdb_folders)\n",
    "\n",
    "fitted_beta_file = 'VDJdb_beta_fitted.tsv'\n",
    "fitted_paired_file = 'VDJdb_paired_fitted.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data paired\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_paired_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_paired_file\n",
    "\n",
    "# fit paired VDJdb data\n",
    "%run ../../data_scripts/VDJdb/fit_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data beta\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_beta_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_beta_file\n",
    "\n",
    "# fit beta VDJdb data\n",
    "%run ../../data_scripts/VDJdb/fit_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 27414 entries\n",
      "whole dataframe has 28119 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 27414\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data paired\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_paired_file}'\n",
    "cleaned_file_paired = 'VDJdb_cleaned_data_paired.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'\n",
    "\n",
    "# clean paired VDJdb data\n",
    "%run ../../data_scripts/VDJdb/clean_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 46507 entries\n",
      "whole dataframe has 49042 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 46507\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data beta\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_beta_file}'\n",
    "cleaned_file_beta = 'VDJdb_cleaned_data_beta.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "\n",
    "# clean beta VDJdb data\n",
    "%run ../../data_scripts/VDJdb/clean_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDJdb_cleaned_beta_output = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "VDJdb_cleaned_paired_output = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Concatenation\n",
    "The concatenation includes further cleaning and advanced removal of duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of beta_df: 231627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"H-2Kb\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-H-2KB\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA class I\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-HLACLASSI\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV9-02\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV9-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-7*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV19*04\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV26-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV19*05\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV26-1*03\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRBV26*03\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-6*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV8-4*05\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV1-4*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-03\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV2-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-05\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV2-5\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV13-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV13-06\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV1-05\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV1-5\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV21-03\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV21-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV6-2*02\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRB17-1\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRB17-1\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV21-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV21-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-6\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB06\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB6\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB12\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB12\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV12-3/4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV12-3/4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBB27\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBB27\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"05-07*01\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TR5-7*01\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ10-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ20-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ19-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ17-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ38-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Donor 13\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRDON/OR13\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Negative\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRNEGATIVE\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ37*02\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ8*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ52*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ9*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ33*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ36*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ53*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ54*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ3-1*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ42*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ39*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ45*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ31*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ43*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ1-7\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-6\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ3-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize Donor13: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CALQDXNTGEXFF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CSARTGDRTEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSILGWSEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSLRTRTDTQYX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 36836 entries removed.\n",
      "removed all duplicates (CDR3, Epitope) from distinct values (most_important_columns, keep=False). 47581 entries removed.\n",
      "beta removed entries df length: 47581\n",
      "\n",
      "\n",
      "Number of groups formed: 18337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25574/3398904298.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  duplicates_to_add = pd.concat([duplicates_to_add, group[group['is_duplicated'] == False]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32617 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 231622 entries, 51800 entries have been removed.\n",
      "for beta dataset :\n",
      "size difference is: 51800\n",
      "  179822 information score cleaned: 5.202255563835348\n",
      "  231622 information score dropout: 5.0485964200291855\n",
      "final_beta_df length = 179822\n",
      "length of paired_df: 54338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"H-2Kb\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-H-2KB\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA class I\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-HLACLASSI\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV1-4\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV2-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV251\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV21-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRDAV1*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRA21-01\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRA21-1\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV19-*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV1-4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV1-4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV13/1*02 (F)\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV13/DV1*02\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV2-2\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV2-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV9*02\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRAV9-1*02\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV12-4\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV24*0\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRAV24*00\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV12-4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV12-4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV9-02\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV9-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"CATSESSGQTYEQYF\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRCAT-E--GQTYEQYF\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAJ53*02\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Negative\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRNEGATIVE\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Donor 13\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRDON/OR13\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-6\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ10-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ20-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ38-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ17-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ19-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize TRAJ37: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize XVXNREDKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CVVNNNXDMRF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CVVNGMDSSYKLXF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQDLGY##GGTSYGKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VL*#GGGADGLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVED#ISSGSARQLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRQLGTGAVVTIN*#F: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CTSVQQAL#GGSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVSVLG#SGGSNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize XEXXGGXXRXXX##: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CI##GGADGLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CTPG#SQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRV#NSNSGYALNF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSA#KAAGNKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSAAX#AGNKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSGPS#TDSWGKLQF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAPQTRRLATTVS*#W: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAGLGGT#GGSNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVND#NTDKLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VL*VLPG#ALNF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAIELQAR##: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQAQ#YGGSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAASSWS*#GLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVEDRGEKHXSCL: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVERGL##GSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVS#SNFGNEKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CALPP#YNFNKFYF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAXSGGGSYQLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAG#YGNKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRVA#SGNTPLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIXXL*NDIGENMFLI: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CISP*SGSSNTGKLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQAG#SNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CATLCPQ#NKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAASGXGTYKYIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAXRGGSEKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVN?PPFGNEKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize Donor13: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSILGWSEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CSARTGDRTEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSLRTRTDTQYX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 6090 entries removed.\n",
      "removed all duplicates from distinct values (cultivated columns, keep=False). 32381 entries removed.\n",
      "paired removed entries df length: 32381\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25574/2624603129.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  duplicates_to_add = pd.concat([duplicates_to_add, group[group['is_duplicated'] == False]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32337 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 54295 entries, 6134 entries have been removed.\n",
      "for paired dataset:\n",
      "size difference is: 6134\n",
      "  48161 information score cleaned: 7.147442951765952\n",
      "  54295 information score dropout: 7.243300488074408\n",
      "final_paired_df length: 48161\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "# beta input files\n",
    "vdjdb_beta_read_path = VDJdb_cleaned_beta_output\n",
    "mcpastcr_beta_read_path = McPAS_cleaned_beta_output\n",
    "iedb_beta_read_path = IEDB_cleaned_beta_output\n",
    "# paired input files\n",
    "vdjdb_paired_read_path = VDJdb_cleaned_paired_output\n",
    "mcpastcr_paired_read_path = McPAS_cleaned_paired_output\n",
    "iedb_paired_read_path = IEDB_cleaned_paired_output\n",
    "# output files\n",
    "output_file_beta = 'beta_concatenated.tsv'\n",
    "output_file_paired = 'paired_concatenated.tsv'\n",
    "\n",
    "create_folders_if_not_exists([custom_dataset_path])\n",
    "\n",
    "%run ../../data_scripts/concatDatasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "# output files\n",
    "output_file_beta = 'beta_concatenated.tsv'\n",
    "output_file_paired = 'paired_concatenated.tsv'\n",
    "\n",
    "concatenated_paired = f'{custom_dataset_path}/{output_file_paired}'\n",
    "concatenated_beta = f'{custom_dataset_path}/{output_file_beta}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "The split creates 3 datasets. Train, Validation and Test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct tcr's: 29339 from 48161\n",
      "unique tcr's: 13525 from 48161\n",
      "unique epitopes: 616 from 48161\n",
      "train data has 34636 entries\n",
      "test data has 13525 entries\n",
      "test data has 0 TPP1 tasks (unseen tcr & seen epitopes).\n",
      "test data has 11253 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 2272 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/test ratio is 0.7191711135566121/0.2808288864433878\n",
      "924 entries need to be shifted from train to test so the train/test ratio can be 0.7/0.3\n",
      "924 entries from train will be moved to test (TPP1)\n",
      "df_train size before: 34636\n",
      "number of tpp1 before: 0\n",
      "number of tpp2 before: 11253\n",
      "df_train size after: 33712\n",
      "number of tpp1 after: 924\n",
      "number of tpp2 after: 11253\n",
      "5164 entries will be shifted from test to train so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "5165 entries need to be shifted from train to test so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "5165 entries from train will be moved to test (TPP1)\n",
      "df_train size before: 38876\n",
      "number of tpp1 before: 924\n",
      "number of tpp2 before: 6089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25574/4063483123.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_train = pd.concat([df_train, rows_to_move], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train size after: 33711\n",
      "number of tpp1 after: 6089\n",
      "number of tpp2 after: 6089\n",
      "train data has 33711 entries\n",
      "test data has 14450 entries\n",
      "test data has 6089 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 6089 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 2272 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/test ratio is 0.6999647017296152/0.3000352982703848\n",
      "test data has 7226 entries\n",
      "validation data has 7224 entries\n",
      "train data has 33711 entries\n",
      "test data has 3045 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 3891 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 290 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the test ratio is 0.849961587176346/0.150038412823654\n",
      "the validation ratio is 0.8500031145532693/0.14999688544673076\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for split of paired dataset\n",
    "input_file = concatenated_paired\n",
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "validation_file_name = 'validation.tsv'\n",
    "test_file_name = 'test.tsv'\n",
    "train_file_name = 'train.tsv'\n",
    "aimed_test_ratio = 0.3 # this means 30% of the concatenated dataset will be for test and validation (fifty/fifty)\n",
    "\n",
    "create_folders_if_not_exists([paired_output_folder])\n",
    "\n",
    "# do the split\n",
    "%run ../../data_scripts/data_preparation/split_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct tcr's: 152160 from 179822\n",
      "unique tcr's: 139540 from 179822\n",
      "unique epitopes: 678 from 179822\n",
      "train data has 40282 entries\n",
      "test data has 139540 entries\n",
      "test data has 0 TPP1 tasks (unseen tcr & seen epitopes).\n",
      "test data has 137217 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 2323 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/test ratio is 0.22401041029462465/0.7759895897053753\n",
      "85594 entries will be shifted from test to train so the train/test ratio can be 0.7/0.3\n",
      "25811 entries will be shifted from test to train so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "25812 entries need to be shifted from train to test so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "train data has 125875 entries\n",
      "test data has 53947 entries\n",
      "test data has 25812 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 25812 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 2323 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/test ratio is 0.6999977755780716/0.30000222442192837\n",
      "test data has 26974 entries\n",
      "validation data has 26973 entries\n",
      "train data has 125875 entries\n",
      "test data has 12906 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 13764 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 304 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the test ratio is 0.8499961072616253/0.1500038927383746\n",
      "the validation ratio is 0.8500016683164463/0.14999833168355373\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for split of beta dataset\n",
    "input_file = concatenated_beta\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "aimed_test_ratio = 0.3 # this means 30% of the concatenated dataset will be for test and validation (fifty/fifty)\n",
    "\n",
    "create_folders_if_not_exists([beta_output_folder])\n",
    "\n",
    "# do the split\n",
    "%run ../../data_scripts/data_preparation/split_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus:                 barcode   donor  \\\n",
      "0   AAACCTGAGACAAAGG-4  donor1   \n",
      "1  AAACCTGAGACTGTAA-34  donor1   \n",
      "2   AAACCTGAGAGCCCAA-5  donor1   \n",
      "3  AAACCTGAGAGCTGCA-24  donor1   \n",
      "4   AAACCTGAGAGGGATA-8  donor1   \n",
      "\n",
      "                                  cell_clono_cdr3_aa  \\\n",
      "0  TRA:CAASVSIWTGTASKLTF;TRA:CAAWDMEYGNKLVF;TRB:C...   \n",
      "1                                    TRB:CASDTPVGQFF   \n",
      "2                 TRA:CASYTDKLIF;TRB:CASSGGSISTDTQYF   \n",
      "3                                 TRB:CASSGGQSSYEQYF   \n",
      "4          TRA:CAASGYGNTGRRALTF;TRB:CASSQDPAGGYNEQFF   \n",
      "\n",
      "                                  cell_clono_cdr3_nt     CD3  CD19  CD45RA  \\\n",
      "0  TRA:TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAA...  2125.0   0.0   912.0   \n",
      "1              TRB:TGTGCCAGCGATACCCCGGTTGGGCAGTTCTTC  1023.0   0.0  2028.0   \n",
      "2  TRA:TGTGCTTCCTACACCGACAAGCTCATCTTT;TRB:TGCGCCA...  1598.0   3.0  3454.0   \n",
      "3     TRB:TGCGCCAGCAGTGGCGGACAGAGCTCCTACGAGCAGTACTTC   298.0   1.0   880.0   \n",
      "4  TRA:TGTGCAGCAAGCGGGTATGGAAACACGGGCAGGAGAGCACTT...  1036.0   0.0  2457.0   \n",
      "\n",
      "   CD4    CD8a  CD14  ...  B0702_RPHERNGFTVL_pp65_CMV_binder  \\\n",
      "0  1.0  2223.0   4.0  ...                              False   \n",
      "1  2.0  3485.0   1.0  ...                              False   \n",
      "2  4.0  3383.0   1.0  ...                              False   \n",
      "3  1.0  2389.0   1.0  ...                              False   \n",
      "4  2.0  3427.0   3.0  ...                              False   \n",
      "\n",
      "   B0801_RAKFKQLL_BZLF1_EBV_binder  B0801_ELRRKMMYM_IE-1_CMV_binder  \\\n",
      "0                            False                            False   \n",
      "1                            False                            False   \n",
      "2                            False                            False   \n",
      "3                            False                            False   \n",
      "4                            False                            False   \n",
      "\n",
      "   B0801_FLRGRAYGL_EBNA-3A_EBV_binder  A0101_SLEGGGLGY_NC_binder  \\\n",
      "0                               False                      False   \n",
      "1                               False                      False   \n",
      "2                               False                      False   \n",
      "3                               False                      False   \n",
      "4                               False                      False   \n",
      "\n",
      "   A0101_STEGGGLAY_NC_binder  A0201_ALIAPVHAV_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   A2402_AYSSAGASI_NC_binder  B0702_GPAESAAGL_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   NR(B0801)_AAKGRGAAL_NC_binder  \n",
      "0                          False  \n",
      "1                          False  \n",
      "2                          False  \n",
      "3                          False  \n",
      "4                          False  \n",
      "\n",
      "[5 rows x 118 columns]\n",
      "Meta:                barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_1             True   \n",
      "1  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_2             True   \n",
      "2  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_3             True   \n",
      "3  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_4            False   \n",
      "4  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_5            False   \n",
      "\n",
      "   length chain     v_gene d_gene   j_gene c_gene  full_length productive  \\\n",
      "0     722   TRB   TRBV10-3  TRBD2  TRBJ2-1  TRBC2         True       True   \n",
      "1     605   TRA  TRAV29DV5    NaN   TRAJ44   TRAC         True       True   \n",
      "2     738   TRA    TRAV8-6    NaN   TRAJ47   TRAC         True       True   \n",
      "3     468   TRB        NaN    NaN  TRBJ2-3  TRBC2        False        NaN   \n",
      "4     488   TRB        NaN    NaN  TRBJ2-6  TRBC2        False        NaN   \n",
      "\n",
      "                cdr3                                            cdr3_nt  \\\n",
      "0  CAISDPGLAGGGGEQFF  TGTGCCATCAGTGACCCCGGACTAGCGGGAGGCGGGGGGGAGCAGT...   \n",
      "1  CAASVSIWTGTASKLTF  TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAACTCA...   \n",
      "2     CAAWDMEYGNKLVF         TGTGCCGCCTGGGACATGGAATATGGAAACAAGCTGGTCTTT   \n",
      "3                NaN                                                NaN   \n",
      "4                NaN                                                NaN   \n",
      "\n",
      "   reads  umis raw_clonotype_id         raw_consensus_id  \n",
      "0  32237    18      clonotype19  clonotype19_consensus_1  \n",
      "1   6088     3      clonotype19  clonotype19_consensus_2  \n",
      "2   5358     3      clonotype19  clonotype19_consensus_3  \n",
      "3   2517     1      clonotype19                      NaN  \n",
      "4   2468     1      clonotype19                      NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25574/3850524540.py:9: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n"
     ]
    }
   ],
   "source": [
    "#Daten einlesen\n",
    "\n",
    "combined_donors_path = f'{pipeline_data_plain}/10x/combined_donors_consensus_annotations.csv'\n",
    "all_donors_consensus = pd.read_csv(combined_donors_path, sep=',')\n",
    "\n",
    "print(\"Consensus: \", all_donors_consensus.head())\n",
    "\n",
    "all_donors_meta_path = f'{pipeline_data_plain}/10x/meta.csv'\n",
    "all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n",
    "\n",
    "print(\"Meta: \", all_donors_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Start:  0\n",
      "Batch Start:  1000\n",
      "Batch Start:  2000\n",
      "Batch Start:  3000\n",
      "Batch Start:  4000\n",
      "Batch Start:  5000\n",
      "Batch Start:  6000\n",
      "Batch Start:  7000\n",
      "Batch Start:  8000\n",
      "Batch Start:  9000\n",
      "Batch Start:  10000\n",
      "Batch Start:  11000\n",
      "Batch Start:  12000\n",
      "Batch Start:  13000\n",
      "Batch Start:  14000\n",
      "Batch Start:  15000\n",
      "Batch Start:  16000\n",
      "Batch Start:  17000\n",
      "Batch Start:  18000\n",
      "Batch Start:  19000\n",
      "Batch Start:  20000\n",
      "Batch Start:  21000\n",
      "Batch Start:  22000\n",
      "Batch Start:  23000\n",
      "Batch Start:  24000\n",
      "Batch Start:  25000\n",
      "Batch Start:  26000\n",
      "Batch Start:  27000\n",
      "Batch Start:  28000\n",
      "Batch Start:  29000\n",
      "Batch Start:  30000\n",
      "Batch Start:  31000\n",
      "Batch Start:  32000\n",
      "Batch Start:  33000\n",
      "Batch Start:  34000\n",
      "Batch Start:  35000\n",
      "Batch Start:  36000\n",
      "Batch Start:  37000\n",
      "Batch Start:  38000\n",
      "Batch Start:  39000\n",
      "Batch Start:  40000\n",
      "Batch Start:  41000\n",
      "Batch Start:  42000\n",
      "Batch Start:  43000\n",
      "Batch Start:  44000\n",
      "Batch Start:  45000\n",
      "Batch Start:  46000\n",
      "Batch Start:  47000\n",
      "Batch Start:  48000\n",
      "Batch Start:  49000\n",
      "Batch Start:  50000\n",
      "Batch Start:  51000\n",
      "Batch Start:  52000\n",
      "Batch Start:  53000\n",
      "Batch Start:  54000\n",
      "Batch Start:  55000\n",
      "Batch Start:  56000\n",
      "Batch Start:  57000\n",
      "Batch Start:  58000\n",
      "Batch Start:  59000\n",
      "Batch Start:  60000\n",
      "Batch Start:  61000\n",
      "Batch Start:  62000\n",
      "Batch Start:  63000\n",
      "Batch Start:  64000\n",
      "Batch Start:  65000\n",
      "Batch Start:  66000\n",
      "Batch Start:  67000\n",
      "Batch Start:  68000\n",
      "Batch Start:  69000\n",
      "Batch Start:  70000\n",
      "Batch Start:  71000\n",
      "Batch Start:  72000\n",
      "Batch Start:  73000\n",
      "Batch Start:  74000\n",
      "Batch Start:  75000\n",
      "Batch Start:  76000\n",
      "Batch Start:  77000\n",
      "Batch Start:  78000\n",
      "Batch Start:  79000\n",
      "Batch Start:  80000\n",
      "Batch Start:  81000\n",
      "Batch Start:  82000\n",
      "Batch Start:  83000\n",
      "Batch Start:  84000\n",
      "Batch Start:  85000\n",
      "Batch Start:  86000\n",
      "Batch Start:  87000\n",
      "Batch Start:  88000\n",
      "Batch Start:  89000\n",
      "Batch Start:  90000\n",
      "Batch Start:  91000\n",
      "Batch Start:  92000\n",
      "Batch Start:  93000\n",
      "Batch Start:  94000\n",
      "Batch Start:  95000\n",
      "Batch Start:  96000\n",
      "Batch Start:  97000\n",
      "Batch Start:  98000\n",
      "Batch Start:  99000\n",
      "Batch Start:  100000\n",
      "Batch Start:  101000\n",
      "Batch Start:  102000\n",
      "Batch Start:  103000\n",
      "Batch Start:  104000\n",
      "Batch Start:  105000\n",
      "Batch Start:  106000\n",
      "Batch Start:  107000\n",
      "Batch Start:  108000\n",
      "Batch Start:  109000\n",
      "Batch Start:  110000\n",
      "Batch Start:  111000\n",
      "Batch Start:  112000\n",
      "Batch Start:  113000\n",
      "Batch Start:  114000\n",
      "Batch Start:  115000\n",
      "Batch Start:  116000\n",
      "Batch Start:  117000\n",
      "Batch Start:  118000\n",
      "Batch Start:  119000\n",
      "Batch Start:  120000\n",
      "Batch Start:  121000\n",
      "Batch Start:  122000\n",
      "Batch Start:  123000\n",
      "Batch Start:  124000\n",
      "Batch Start:  125000\n",
      "Batch Start:  126000\n",
      "Batch Start:  127000\n",
      "Batch Start:  128000\n",
      "Batch Start:  129000\n",
      "Batch Start:  130000\n",
      "Batch Start:  131000\n",
      "Batch Start:  132000\n",
      "Batch Start:  133000\n",
      "Batch Start:  134000\n",
      "Batch Start:  135000\n",
      "Batch Start:  136000\n",
      "Batch Start:  137000\n",
      "Batch Start:  138000\n",
      "Batch Start:  139000\n",
      "Batch Start:  140000\n",
      "Batch Start:  141000\n",
      "Batch Start:  142000\n",
      "Batch Start:  143000\n",
      "Batch Start:  144000\n",
      "Batch Start:  145000\n",
      "Batch Start:  146000\n",
      "Batch Start:  147000\n",
      "Batch Start:  148000\n",
      "Batch Start:  149000\n",
      "Batch Start:  150000\n",
      "Batch Start:  151000\n",
      "Batch Start:  152000\n",
      "Batch Start:  153000\n",
      "Batch Start:  154000\n",
      "Batch Start:  155000\n",
      "Batch Start:  156000\n",
      "Batch Start:  157000\n",
      "Batch Start:  158000\n",
      "Batch Start:  159000\n",
      "Batch Start:  160000\n",
      "Batch Start:  161000\n",
      "Batch Start:  162000\n",
      "Batch Start:  163000\n",
      "Batch Start:  164000\n",
      "Batch Start:  165000\n",
      "Batch Start:  166000\n",
      "Batch Start:  167000\n",
      "Batch Start:  168000\n",
      "Batch Start:  169000\n",
      "Batch Start:  170000\n",
      "Batch Start:  171000\n",
      "Batch Start:  172000\n",
      "Batch Start:  173000\n",
      "Batch Start:  174000\n",
      "Batch Start:  175000\n",
      "Batch Start:  176000\n",
      "Batch Start:  177000\n",
      "Batch Start:  178000\n",
      "Batch Start:  179000\n",
      "Batch Start:  180000\n",
      "Batch Start:  181000\n",
      "Batch Start:  182000\n",
      "Batch Start:  183000\n",
      "Batch Start:  184000\n",
      "Batch Start:  185000\n",
      "Batch Start:  186000\n",
      "Batch Start:  187000\n",
      "Batch Start:  188000\n",
      "Batch Start:  189000\n",
      "             TCR_name      TRBV     TRBJ           TRB_CDR3   TRBC  \\\n",
      "0  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "1  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "2  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "3  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "4  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "\n",
      "      Epitope          MHC Binding task  \n",
      "0   VTEHDTLLY  HLA-A*01:01       0  nan  \n",
      "1   KTWGQYWQV  HLA-A*02:01       0  nan  \n",
      "2  ELAGIGILTV  HLA-A*02:01       0  nan  \n",
      "3  CLLWSFQTSA  HLA-A*02:01       0  nan  \n",
      "4   IMDQVPFSV  HLA-A*02:01       0  nan  \n"
     ]
    }
   ],
   "source": [
    "#Dieser Code fr ganzen Datensatz laufen lassen\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Festlegen der Batch-Gre fr die Verarbeitung\n",
    "batch_size = 1000  # Passe diese Zahl je nach Speicherressourcen an\n",
    "\n",
    "# Identifizieren von Epitope-Spalten, aber ohne \"NR(B0801)_AAKGRGAAL_NC_binder\"\n",
    "epitope_columns = [col for col in all_donors_consensus.columns if '_binder' in col and col != \"NR(B0801)_AAKGRGAAL_NC_binder\"]\n",
    "\n",
    "# Liste fr alle Batch-Ergebnisse\n",
    "all_batches = []\n",
    "\n",
    "# Verarbeite `all_donors_consensus` in Batches\n",
    "for batch_start in range(0, len(all_donors_consensus), batch_size):\n",
    "    print(\"Batch Start: \", batch_start)\n",
    "    # Batch definieren\n",
    "    batch = all_donors_consensus.iloc[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Filtern auf Zeilen, die 'TRB:' enthalten\n",
    "    batch_trb = batch[batch['cell_clono_cdr3_aa'].str.contains(\"TRB:\", na=False)]\n",
    "\n",
    "    # Liste, um Zeilen fr diesen Batch zu speichern\n",
    "    expanded_rows = []\n",
    "    \n",
    "    # Iteriere durch jede Zeile im Batch\n",
    "    for _, row in batch_trb.iterrows():\n",
    "        for col in epitope_columns:\n",
    "            # Extrahiere MHC und Epitope\n",
    "            match = re.match(r'([A-Z0-9]+)_([A-Z]+)_.*_binder', col)\n",
    "            if match:\n",
    "                mhc_raw, epitope = match.groups()\n",
    "                mhc_formatted = f'HLA-{mhc_raw[0]}*{mhc_raw[1:3]}:{mhc_raw[3:]}'\n",
    "\n",
    "                # Fge `Epitope` und `MHC` zur Zeile hinzu\n",
    "                new_row = row.copy()\n",
    "                new_row['Epitope'] = epitope\n",
    "                new_row['MHC'] = mhc_formatted\n",
    "\n",
    "                # Fge neue Zeile zur Batch-Liste hinzu\n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    # Erstelle einen DataFrame aus dem Batch\n",
    "    batch_df = pd.DataFrame(expanded_rows)\n",
    "    all_batches.append(batch_df)  # Speichere den Batch in der Liste\n",
    "\n",
    "# Kombiniere alle Batch-Ergebnisse zu einem DataFrame\n",
    "expanded_df = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "# Nur die TRB-Chain-Eintrge in `all_donors_meta` beibehalten\n",
    "all_donors_meta_trb = all_donors_meta[all_donors_meta['chain'] == 'TRB']\n",
    "\n",
    "# Zusammenfhren der beiden DataFrames basierend auf der 'barcode' Spalte\n",
    "merged_df = pd.merge(all_donors_meta_trb, expanded_df[['barcode', 'Epitope', 'MHC']], on='barcode', how='inner')\n",
    "\n",
    "# Spalten umbenennen und Format anpassen\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'barcode': 'TCR_name',\n",
    "    'v_gene': 'TRBV',\n",
    "    'j_gene': 'TRBJ',\n",
    "    'c_gene': 'TRBC',\n",
    "    'cdr3': 'TRB_CDR3'\n",
    "})\n",
    "\n",
    "# Fehlende Spalten auffllen\n",
    "desired_columns = ['TCR_name', 'TRBV', 'TRBJ', 'TRB_CDR3', 'TRBC', 'Epitope', 'MHC', 'Binding', 'task']\n",
    "for col in desired_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = 'nan' if col == 'task' else '0'\n",
    "\n",
    "# Nur die gewnschten Spalten beibehalten und Zeilen mit `None` in `TRB_CDR3` entfernen\n",
    "final_df = merged_df[desired_columns]\n",
    "final_df = final_df[final_df['TRB_CDR3'] != 'None']\n",
    "\n",
    "final_df = final_df[final_df['TRB_CDR3'].notna() & (final_df['TRB_CDR3'] != '')]\n",
    "\n",
    "# Ausgabe des ersten Teils des Ergebnisses zur berprfung\n",
    "print(final_df.head())\n",
    "\n",
    "# Speichern des kombinierten DataFrames\n",
    "output_path = f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv'\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verteilung der Epitope im Train-Datensatz:\n",
      "Epitope\n",
      "GLCTLVAML                    10062\n",
      "MIELSLIDFYLCFLAFLLFLVLIML     9955\n",
      "YVLDHLIVV                     9053\n",
      "NLVPMVATV                     7625\n",
      "GILGFVFTL                     7519\n",
      "                             ...  \n",
      "FVGEFFTDV                        1\n",
      "NLAQTDLATV                       1\n",
      "LLAILPYYV                        1\n",
      "NLFNRYPAL                        1\n",
      "VQYLGMLPV                        1\n",
      "Name: count, Length: 1113, dtype: int64\n",
      "Gesamt: 251750\n",
      "\n",
      "Verteilung der Epitope im Validation-Datensatz:\n",
      "Epitope\n",
      "KLGGALQAK     8663\n",
      "GLCTLVAML     4590\n",
      "YVLDHLIVV     3863\n",
      "GILGFVFTL     3784\n",
      "AVFDRKSDAK    3447\n",
      "              ... \n",
      "YLEPGAVTA        1\n",
      "KLNEEIAII        1\n",
      "AMDEFIERY        1\n",
      "ALNNIINNA        1\n",
      "AVILRGHLR        1\n",
      "Name: count, Length: 1184, dtype: int64\n",
      "Gesamt: 161838\n",
      "\n",
      "Verteilung der Epitope im Test-Datensatz:\n",
      "Epitope\n",
      "KLGGALQAK         8716\n",
      "GLCTLVAML         4566\n",
      "YVLDHLIVV         3912\n",
      "GILGFVFTL         3712\n",
      "AVFDRKSDAK        3476\n",
      "                  ... \n",
      "FTEQPIDLVPNQPY       1\n",
      "GPKKSTNLV            1\n",
      "LDYIINLII            1\n",
      "GLTSFFIAI            1\n",
      "KIVALGINAV           1\n",
      "Name: count, Length: 1178, dtype: int64\n",
      "Gesamt: 161844\n",
      "\n",
      "Alle Datenstze wurden erfolgreich gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Daten laden\n",
    "beta = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv', sep=',')\n",
    "\n",
    "# Schritt 1: Ursprngliche Aufteilung in Train, Validation und Test\n",
    "train_split, test_split = train_test_split(beta, test_size=0.3, random_state=42)\n",
    "validation_split, test_split = train_test_split(test_split, test_size=0.5, random_state=42)\n",
    "\n",
    "# Positive Samples laden >> zuerst noch umbenennen, damit wir positiven sample files nicht verlieren\n",
    "train_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n",
    "validation_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/validation_prenegsamples.tsv', sep='\\t')\n",
    "test_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv', sep='\\t')\n",
    "\n",
    "# Anzahl positiver Samples\n",
    "num_train_pos = len(train_preneg)\n",
    "num_validation_pos = len(validation_preneg)\n",
    "num_test_pos = len(test_preneg)\n",
    "\n",
    "# Zielgren fr negative Samples\n",
    "train_neg_needed = num_train_pos\n",
    "validation_neg_needed = num_validation_pos * 5\n",
    "test_neg_needed = num_test_pos * 5\n",
    "\n",
    "# Funktion zur Sicherstellung, dass alle unique Epitope erhalten bleiben\n",
    "def ensure_unique_epitopes(df, target_count):\n",
    "    unique_epitopes = df['Epitope'].unique()\n",
    "    guaranteed_samples = []\n",
    "    \n",
    "    # Stelle sicher, dass jedes Epitope mindestens einmal vorkommt\n",
    "    for epitope in unique_epitopes:\n",
    "        epitope_group = df[df['Epitope'] == epitope]\n",
    "        if len(epitope_group) > 0:\n",
    "            guaranteed_samples.append(epitope_group.sample(1, random_state=42))\n",
    "    \n",
    "    # Kombiniere garantierte Samples\n",
    "    guaranteed_df = pd.concat(guaranteed_samples, ignore_index=True)\n",
    "    \n",
    "    # Berechne verbleibende Anzahl an Samples\n",
    "    remaining_count = target_count - len(guaranteed_df)\n",
    "    if remaining_count > 0:\n",
    "        remaining_samples = df.sample(remaining_count, random_state=42, replace=True)\n",
    "        return pd.concat([guaranteed_df, remaining_samples], ignore_index=True)\n",
    "    return guaranteed_df\n",
    "\n",
    "# Balancierung der Splits\n",
    "train_balanced_negatives = ensure_unique_epitopes(train_split, train_neg_needed)\n",
    "validation_balanced_negatives = ensure_unique_epitopes(validation_split, validation_neg_needed)\n",
    "test_balanced_negatives = ensure_unique_epitopes(test_split, test_neg_needed)\n",
    "\n",
    "# Positive und negative Samples kombinieren\n",
    "train_combined = pd.concat([train_preneg, train_balanced_negatives], ignore_index=True)\n",
    "validation_combined = pd.concat([validation_preneg, validation_balanced_negatives], ignore_index=True)\n",
    "test_combined = pd.concat([test_preneg, test_balanced_negatives], ignore_index=True)\n",
    "\n",
    "# Verteilungen prfen\n",
    "def check_distribution(df, name):\n",
    "    print(f\"\\nVerteilung der Epitope im {name}-Datensatz:\")\n",
    "    print(df['Epitope'].value_counts())\n",
    "    print(f\"Gesamt: {len(df)}\")\n",
    "\n",
    "check_distribution(train_combined, \"Train\")\n",
    "check_distribution(validation_combined, \"Validation\")\n",
    "check_distribution(test_combined, \"Test\")\n",
    "\n",
    "# Speichern der kombinierten Datenstze\n",
    "output_dir = f'{pipeline_data_splitted}/{precision}/beta/'\n",
    "train_combined.to_csv(output_dir + \"train.tsv\", sep='\\t', index=False)\n",
    "validation_combined.to_csv(output_dir + \"validation.tsv\", sep='\\t', index=False)\n",
    "test_combined.to_csv(output_dir + \"test.tsv\", sep='\\t', index=False)\n",
    "\n",
    "print(\"\\nAlle Datenstze wurden erfolgreich gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Start: 0\n",
      "Batch 0 gespeichert.\n",
      "Batch Start: 1000\n",
      "Batch 1000 gespeichert.\n",
      "Batch Start: 2000\n",
      "Batch 2000 gespeichert.\n",
      "Batch Start: 3000\n",
      "Batch 3000 gespeichert.\n",
      "Batch Start: 4000\n",
      "Batch 4000 gespeichert.\n",
      "Batch Start: 5000\n",
      "Batch 5000 gespeichert.\n",
      "Batch Start: 6000\n",
      "Batch 6000 gespeichert.\n",
      "Batch Start: 7000\n",
      "Batch 7000 gespeichert.\n",
      "Batch Start: 8000\n",
      "Batch 8000 gespeichert.\n",
      "Batch Start: 9000\n",
      "Batch 9000 gespeichert.\n",
      "Batch Start: 10000\n",
      "Batch 10000 gespeichert.\n",
      "Batch Start: 11000\n",
      "Batch 11000 gespeichert.\n",
      "Batch Start: 12000\n",
      "Batch 12000 gespeichert.\n",
      "Batch Start: 13000\n",
      "Batch 13000 gespeichert.\n",
      "Batch Start: 14000\n",
      "Batch 14000 gespeichert.\n",
      "Batch Start: 15000\n",
      "Batch 15000 gespeichert.\n",
      "Batch Start: 16000\n",
      "Batch 16000 gespeichert.\n",
      "Batch Start: 17000\n",
      "Batch 17000 gespeichert.\n",
      "Batch Start: 18000\n",
      "Batch 18000 gespeichert.\n",
      "Batch Start: 19000\n",
      "Batch 19000 gespeichert.\n",
      "Batch Start: 20000\n",
      "Batch 20000 gespeichert.\n",
      "Batch Start: 21000\n",
      "Batch 21000 gespeichert.\n",
      "Batch Start: 22000\n",
      "Batch 22000 gespeichert.\n",
      "Batch Start: 23000\n",
      "Batch 23000 gespeichert.\n",
      "Batch Start: 24000\n",
      "Batch 24000 gespeichert.\n",
      "Batch Start: 25000\n",
      "Batch 25000 gespeichert.\n",
      "Batch Start: 26000\n",
      "Batch 26000 gespeichert.\n",
      "Batch Start: 27000\n",
      "Batch 27000 gespeichert.\n",
      "Batch Start: 28000\n",
      "Batch 28000 gespeichert.\n",
      "Batch Start: 29000\n",
      "Batch 29000 gespeichert.\n",
      "Batch Start: 30000\n",
      "Batch 30000 gespeichert.\n",
      "Batch Start: 31000\n",
      "Batch 31000 gespeichert.\n",
      "Batch Start: 32000\n",
      "Batch 32000 gespeichert.\n",
      "Batch Start: 33000\n",
      "Batch 33000 gespeichert.\n",
      "Batch Start: 34000\n",
      "Batch 34000 gespeichert.\n",
      "Batch Start: 35000\n",
      "Batch 35000 gespeichert.\n",
      "Batch Start: 36000\n",
      "Batch 36000 gespeichert.\n",
      "Batch Start: 37000\n",
      "Batch 37000 gespeichert.\n",
      "Batch Start: 38000\n",
      "Batch 38000 gespeichert.\n",
      "Batch Start: 39000\n",
      "Batch 39000 gespeichert.\n",
      "Batch Start: 40000\n",
      "Batch 40000 gespeichert.\n",
      "Batch Start: 41000\n",
      "Batch 41000 gespeichert.\n",
      "Batch Start: 42000\n",
      "Batch 42000 gespeichert.\n",
      "Batch Start: 43000\n",
      "Batch 43000 gespeichert.\n",
      "Batch Start: 44000\n",
      "Batch 44000 gespeichert.\n",
      "Batch Start: 45000\n",
      "Batch 45000 gespeichert.\n",
      "Batch Start: 46000\n",
      "Batch 46000 gespeichert.\n",
      "Batch Start: 47000\n",
      "Batch 47000 gespeichert.\n",
      "Batch Start: 48000\n",
      "Batch 48000 gespeichert.\n",
      "Batch Start: 49000\n",
      "Batch 49000 gespeichert.\n",
      "Batch Start: 50000\n",
      "Batch 50000 gespeichert.\n",
      "Batch Start: 51000\n",
      "Batch 51000 gespeichert.\n",
      "Batch Start: 52000\n",
      "Batch 52000 gespeichert.\n",
      "Batch Start: 53000\n",
      "Batch 53000 gespeichert.\n",
      "Batch Start: 54000\n",
      "Batch 54000 gespeichert.\n",
      "Batch Start: 55000\n",
      "Batch 55000 gespeichert.\n",
      "Batch Start: 56000\n",
      "Batch 56000 gespeichert.\n",
      "Batch Start: 57000\n",
      "Batch 57000 gespeichert.\n",
      "Batch Start: 58000\n",
      "Batch 58000 gespeichert.\n",
      "Batch Start: 59000\n",
      "Batch 59000 gespeichert.\n",
      "Batch Start: 60000\n",
      "Batch 60000 gespeichert.\n",
      "Batch Start: 61000\n",
      "Batch 61000 gespeichert.\n",
      "Batch Start: 62000\n",
      "Batch 62000 gespeichert.\n",
      "Batch Start: 63000\n",
      "Batch 63000 gespeichert.\n",
      "Batch Start: 64000\n",
      "Batch 64000 gespeichert.\n",
      "Batch Start: 65000\n",
      "Batch 65000 gespeichert.\n",
      "Batch Start: 66000\n",
      "Batch 66000 gespeichert.\n",
      "Batch Start: 67000\n",
      "Batch 67000 gespeichert.\n",
      "Batch Start: 68000\n",
      "Batch 68000 gespeichert.\n",
      "Batch Start: 69000\n",
      "Batch 69000 gespeichert.\n",
      "Batch Start: 70000\n",
      "Batch 70000 gespeichert.\n",
      "Batch Start: 71000\n",
      "Batch 71000 gespeichert.\n",
      "Batch Start: 72000\n",
      "Batch 72000 gespeichert.\n",
      "Batch Start: 73000\n",
      "Batch 73000 gespeichert.\n",
      "Batch Start: 74000\n",
      "Batch 74000 gespeichert.\n",
      "Batch Start: 75000\n",
      "Batch 75000 gespeichert.\n",
      "Batch Start: 76000\n",
      "Batch 76000 gespeichert.\n",
      "Batch Start: 77000\n",
      "Batch 77000 gespeichert.\n",
      "Batch Start: 78000\n",
      "Batch 78000 gespeichert.\n",
      "Batch Start: 79000\n",
      "Batch 79000 gespeichert.\n",
      "Batch Start: 80000\n",
      "Batch 80000 gespeichert.\n",
      "Batch Start: 81000\n",
      "Batch 81000 gespeichert.\n",
      "Batch Start: 82000\n",
      "Batch 82000 gespeichert.\n",
      "Batch Start: 83000\n",
      "Batch 83000 gespeichert.\n",
      "Batch Start: 84000\n",
      "Batch 84000 gespeichert.\n",
      "Batch Start: 85000\n",
      "Batch 85000 gespeichert.\n",
      "Batch Start: 86000\n",
      "Batch 86000 gespeichert.\n",
      "Batch Start: 87000\n",
      "Batch 87000 gespeichert.\n",
      "Batch Start: 88000\n",
      "Batch 88000 gespeichert.\n",
      "Batch Start: 89000\n",
      "Batch 89000 gespeichert.\n",
      "Batch Start: 90000\n",
      "Batch 90000 gespeichert.\n",
      "Batch Start: 91000\n",
      "Batch 91000 gespeichert.\n",
      "Batch Start: 92000\n",
      "Batch 92000 gespeichert.\n",
      "Batch Start: 93000\n",
      "Batch 93000 gespeichert.\n",
      "Batch Start: 94000\n",
      "Batch 94000 gespeichert.\n",
      "Batch Start: 95000\n",
      "Batch 95000 gespeichert.\n",
      "Batch Start: 96000\n",
      "Batch 96000 gespeichert.\n",
      "Batch Start: 97000\n",
      "Batch 97000 gespeichert.\n",
      "Batch Start: 98000\n",
      "Batch 98000 gespeichert.\n",
      "Batch Start: 99000\n",
      "Batch 99000 gespeichert.\n",
      "Batch Start: 100000\n",
      "Batch 100000 gespeichert.\n",
      "Batch Start: 101000\n",
      "Batch 101000 gespeichert.\n",
      "Batch Start: 102000\n",
      "Batch 102000 gespeichert.\n",
      "Batch Start: 103000\n",
      "Batch 103000 gespeichert.\n",
      "Batch Start: 104000\n",
      "Batch 104000 gespeichert.\n",
      "Batch Start: 105000\n",
      "Batch 105000 gespeichert.\n",
      "Batch Start: 106000\n",
      "Batch 106000 gespeichert.\n",
      "Batch Start: 107000\n",
      "Batch 107000 gespeichert.\n",
      "Batch Start: 108000\n",
      "Batch 108000 gespeichert.\n",
      "Batch Start: 109000\n",
      "Batch 109000 gespeichert.\n",
      "Batch Start: 110000\n",
      "Batch 110000 gespeichert.\n",
      "Batch Start: 111000\n",
      "Batch 111000 gespeichert.\n",
      "Batch Start: 112000\n",
      "Batch 112000 gespeichert.\n",
      "Batch Start: 113000\n",
      "Batch 113000 gespeichert.\n",
      "Batch Start: 114000\n",
      "Batch 114000 gespeichert.\n",
      "Batch Start: 115000\n",
      "Batch 115000 gespeichert.\n",
      "Batch Start: 116000\n",
      "Batch 116000 gespeichert.\n",
      "Batch Start: 117000\n",
      "Batch 117000 gespeichert.\n",
      "Batch Start: 118000\n",
      "Batch 118000 gespeichert.\n",
      "Batch Start: 119000\n",
      "Batch 119000 gespeichert.\n",
      "Batch Start: 120000\n",
      "Batch 120000 gespeichert.\n",
      "Batch Start: 121000\n",
      "Batch 121000 gespeichert.\n",
      "Batch Start: 122000\n",
      "Batch 122000 gespeichert.\n",
      "Batch Start: 123000\n",
      "Batch 123000 gespeichert.\n",
      "Batch Start: 124000\n",
      "Batch 124000 gespeichert.\n",
      "Batch Start: 125000\n",
      "Batch 125000 gespeichert.\n",
      "Batch Start: 126000\n",
      "Batch 126000 gespeichert.\n",
      "Batch Start: 127000\n",
      "Batch 127000 gespeichert.\n",
      "Batch Start: 128000\n",
      "Batch 128000 gespeichert.\n",
      "Batch Start: 129000\n",
      "Batch 129000 gespeichert.\n",
      "Batch Start: 130000\n",
      "Batch 130000 gespeichert.\n",
      "Batch Start: 131000\n",
      "Batch 131000 gespeichert.\n",
      "Batch Start: 132000\n",
      "Batch 132000 gespeichert.\n",
      "Batch Start: 133000\n",
      "Batch 133000 gespeichert.\n",
      "Batch Start: 134000\n",
      "Batch 134000 gespeichert.\n",
      "Batch Start: 135000\n",
      "Batch 135000 gespeichert.\n",
      "Batch Start: 136000\n",
      "Batch 136000 gespeichert.\n",
      "Batch Start: 137000\n",
      "Batch 137000 gespeichert.\n",
      "Batch Start: 138000\n",
      "Batch 138000 gespeichert.\n",
      "Batch Start: 139000\n",
      "Batch 139000 gespeichert.\n",
      "Batch Start: 140000\n",
      "Batch 140000 gespeichert.\n",
      "Batch Start: 141000\n",
      "Batch 141000 gespeichert.\n",
      "Batch Start: 142000\n",
      "Batch 142000 gespeichert.\n",
      "Batch Start: 143000\n",
      "Batch 143000 gespeichert.\n",
      "Batch Start: 144000\n",
      "Batch 144000 gespeichert.\n",
      "Batch Start: 145000\n",
      "Batch 145000 gespeichert.\n",
      "Batch Start: 146000\n",
      "Batch 146000 gespeichert.\n",
      "Batch Start: 147000\n",
      "Batch 147000 gespeichert.\n",
      "Batch Start: 148000\n",
      "Batch 148000 gespeichert.\n",
      "Batch Start: 149000\n",
      "Batch 149000 gespeichert.\n",
      "Batch Start: 150000\n",
      "Batch 150000 gespeichert.\n",
      "Batch Start: 151000\n",
      "Batch 151000 gespeichert.\n",
      "Batch Start: 152000\n",
      "Batch 152000 gespeichert.\n",
      "Batch Start: 153000\n",
      "Batch 153000 gespeichert.\n",
      "Batch Start: 154000\n",
      "Batch 154000 gespeichert.\n",
      "Batch Start: 155000\n",
      "Batch 155000 gespeichert.\n",
      "Batch Start: 156000\n",
      "Batch 156000 gespeichert.\n",
      "Batch Start: 157000\n",
      "Batch 157000 gespeichert.\n",
      "Batch Start: 158000\n",
      "Batch 158000 gespeichert.\n",
      "Batch Start: 159000\n",
      "Batch 159000 gespeichert.\n",
      "Batch Start: 160000\n",
      "Batch 160000 gespeichert.\n",
      "Batch Start: 161000\n",
      "Batch 161000 gespeichert.\n",
      "Batch Start: 162000\n",
      "Batch 162000 gespeichert.\n",
      "Batch Start: 163000\n",
      "Batch 163000 gespeichert.\n",
      "Batch Start: 164000\n",
      "Batch 164000 gespeichert.\n",
      "Batch Start: 165000\n",
      "Batch 165000 gespeichert.\n",
      "Batch Start: 166000\n",
      "Batch 166000 gespeichert.\n",
      "Batch Start: 167000\n",
      "Batch 167000 gespeichert.\n",
      "Batch Start: 168000\n",
      "Batch 168000 gespeichert.\n",
      "Batch Start: 169000\n",
      "Batch 169000 gespeichert.\n",
      "Batch Start: 170000\n",
      "Batch 170000 gespeichert.\n",
      "Batch Start: 171000\n",
      "Batch 171000 gespeichert.\n",
      "Batch Start: 172000\n",
      "Batch 172000 gespeichert.\n",
      "Batch Start: 173000\n",
      "Batch 173000 gespeichert.\n",
      "Batch Start: 174000\n",
      "Batch 174000 gespeichert.\n",
      "Batch Start: 175000\n",
      "Batch 175000 gespeichert.\n",
      "Batch Start: 176000\n",
      "Batch 176000 gespeichert.\n",
      "Batch Start: 177000\n",
      "Batch 177000 gespeichert.\n",
      "Batch Start: 178000\n",
      "Batch 178000 gespeichert.\n",
      "Batch Start: 179000\n",
      "Batch 179000 gespeichert.\n",
      "Batch Start: 180000\n",
      "Batch 180000 gespeichert.\n",
      "Batch Start: 181000\n",
      "Batch 181000 gespeichert.\n",
      "Batch Start: 182000\n",
      "Batch 182000 gespeichert.\n",
      "Batch Start: 183000\n",
      "Batch 183000 gespeichert.\n",
      "Batch Start: 184000\n",
      "Batch 184000 gespeichert.\n",
      "Batch Start: 185000\n",
      "Batch 185000 gespeichert.\n",
      "Batch Start: 186000\n",
      "Batch 186000 gespeichert.\n",
      "Batch Start: 187000\n",
      "Batch 187000 gespeichert.\n",
      "Batch Start: 188000\n",
      "Batch 188000 gespeichert.\n",
      "Batch Start: 189000\n",
      "Batch 189000 gespeichert.\n",
      "             TCR_name       TRAV    TRAJ           TRA_CDR3      TRBV  \\\n",
      "0  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "1  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "2  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "3  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "4  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "\n",
      "      TRBJ           TRB_CDR3  TRAC   TRBC     Epitope          MHC Binding  \\\n",
      "0  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   VTEHDTLLY  HLA-A*01:01       0   \n",
      "1  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   KTWGQYWQV  HLA-A*02:01       0   \n",
      "2  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2  ELAGIGILTV  HLA-A*02:01       0   \n",
      "3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2  CLLWSFQTSA  HLA-A*02:01       0   \n",
      "4  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   IMDQVPFSV  HLA-A*02:01       0   \n",
      "\n",
      "  task  \n",
      "0  nan  \n",
      "1  nan  \n",
      "2  nan  \n",
      "3  nan  \n",
      "4  nan  \n",
      "Datei erfolgreich gespeichert!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Annahme: all_donors_consensus und all_donors_meta sind bereits geladen und gefiltert\n",
    "\n",
    "# Festlegen der Batch-Gre fr die Verarbeitung\n",
    "batch_size = 1000\n",
    "\n",
    "# Identifizieren von Epitope-Spalten, aber ohne \"NR(B0801)_AAKGRGAAL_NC_binder\"\n",
    "epitope_columns = [col for col in all_donors_consensus.columns if '_binder' in col and col != \"NR(B0801)_AAKGRGAAL_NC_binder\"]\n",
    "\n",
    "# Ausgabe-Datei fr Batch-Ergebnisse\n",
    "output_batch_file = f'{pipeline_data_plain}/10x/expanded_batches.csv'\n",
    "\n",
    "# Stelle sicher, dass die Ausgabedatei leer ist\n",
    "with open(output_batch_file, 'w') as f:\n",
    "    f.write('')  # Leere Datei erstellen\n",
    "\n",
    "# Verarbeite `all_donors_consensus` in Batches\n",
    "for batch_start in range(0, len(all_donors_consensus), batch_size):\n",
    "    print(f\"Batch Start: {batch_start}\")\n",
    "    # Definiere Batch\n",
    "    batch = all_donors_consensus.iloc[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Filtern auf Zeilen, die sowohl 'TRA:' als auch 'TRB:' in 'cell_clono_cdr3_aa' enthalten\n",
    "    batch_paired = batch[\n",
    "        batch['cell_clono_cdr3_aa'].str.contains(\"TRA:\", na=False) &\n",
    "        batch['cell_clono_cdr3_aa'].str.contains(\"TRB:\", na=False)\n",
    "    ]\n",
    "\n",
    "    # Liste, um Zeilen fr diesen Batch zu speichern\n",
    "    expanded_rows = []\n",
    "    \n",
    "    # Iteriere durch jede Zeile im Batch\n",
    "    for _, row in batch_paired.iterrows():\n",
    "        for col in epitope_columns:\n",
    "            # Extrahiere MHC und Epitope\n",
    "            match = re.match(r'([A-Z0-9]+)_([A-Z]+)_.*_binder', col)\n",
    "            if match:\n",
    "                mhc_raw, epitope = match.groups()\n",
    "                mhc_formatted = f'HLA-{mhc_raw[0]}*{mhc_raw[1:3]}:{mhc_raw[3:]}'\n",
    "\n",
    "                # Fge `Epitope` und `MHC` zur Zeile hinzu\n",
    "                new_row = row.copy()\n",
    "                new_row['Epitope'] = epitope\n",
    "                new_row['MHC'] = mhc_formatted\n",
    "\n",
    "                # Neue Zeile zur Batch-Liste hinzufgen\n",
    "                expanded_rows.append(new_row)\n",
    "\n",
    "    # Erstelle einen DataFrame aus dem Batch\n",
    "    batch_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    # Fge den Batch direkt in die Datei ein\n",
    "    batch_df.to_csv(output_batch_file, mode='a', index=False, header=not batch_start, sep=',')\n",
    "    print(f\"Batch {batch_start} gespeichert.\")\n",
    "\n",
    "# Laden der gespeicherten Batch-Ergebnisse\n",
    "expanded_df = pd.read_csv(output_batch_file)\n",
    "\n",
    "# Nur die Paired-Eintrge in `all_donors_meta` beibehalten\n",
    "# Filtern auf Barcodes, die sowohl eine TRA- als auch eine TRB-Kette haben\n",
    "paired_barcodes = all_donors_meta.groupby('barcode').filter(\n",
    "    lambda x: set(x['chain']) == {'TRA', 'TRB'}\n",
    ")['barcode'].unique()\n",
    "all_donors_meta_paired = all_donors_meta[all_donors_meta['barcode'].isin(paired_barcodes)]\n",
    "\n",
    "# Split `all_donors_meta_paired` nach `chain` in separate DataFrames fr TRA und TRB\n",
    "alpha_chain = all_donors_meta_paired[all_donors_meta_paired['chain'] == 'TRA'].rename(\n",
    "    columns={'v_gene': 'TRAV', 'j_gene': 'TRAJ', 'cdr3': 'TRA_CDR3', 'c_gene': 'TRAC'}\n",
    ")\n",
    "beta_chain = all_donors_meta_paired[all_donors_meta_paired['chain'] == 'TRB'].rename(\n",
    "    columns={'v_gene': 'TRBV', 'j_gene': 'TRBJ', 'cdr3': 'TRB_CDR3', 'c_gene': 'TRBC'}\n",
    ")\n",
    "\n",
    "# Zusammenfhren von alpha_chain und beta_chain anhand der gemeinsamen 'barcode'-Spalte\n",
    "paired_meta = pd.merge(alpha_chain, beta_chain, on='barcode', suffixes=('_alpha', '_beta'))\n",
    "\n",
    "# Zusammenfhren von `paired_meta` mit `expanded_df` anhand der 'barcode'-Spalte\n",
    "merged_df = pd.merge(paired_meta, expanded_df[['barcode', 'Epitope', 'MHC']], on='barcode', how='inner')\n",
    "\n",
    "# Spalten umbenennen und Format anpassen\n",
    "merged_df = merged_df.rename(columns={'barcode': 'TCR_name'})\n",
    "\n",
    "# Fehlende Spalten auffllen\n",
    "desired_columns = [\n",
    "    'TCR_name', 'TRAV', 'TRAJ', 'TRA_CDR3', 'TRBV', 'TRBJ', 'TRB_CDR3', 'TRAC', 'TRBC', \n",
    "    'Epitope', 'MHC', 'Binding', 'task'\n",
    "]\n",
    "for col in desired_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = 'nan' if col == 'task' else '0'\n",
    "\n",
    "# Nur die gewnschten Spalten beibehalten und Zeilen mit `None` in `TRB_CDR3` entfernen\n",
    "final_df = merged_df[desired_columns]\n",
    "final_df = final_df[final_df['TRB_CDR3'] != 'None']\n",
    "\n",
    "final_df = final_df[\n",
    "    final_df['TRB_CDR3'].notna() & (final_df['TRB_CDR3'] != '') &\n",
    "    final_df['TRA_CDR3'].notna() & (final_df['TRA_CDR3'] != '')\n",
    "]\n",
    "\n",
    "# Ausgabe des ersten Teils des Ergebnisses zur berprfung\n",
    "print(final_df.head())\n",
    "\n",
    "# Optional: Speichern des kombinierten DataFrames\n",
    "output_path = f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_paired_only_expanded-all.csv'\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(\"Datei erfolgreich gespeichert!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train-Datensatz:\n",
      "Gesamtanzahl der Daten: 67422\n",
      "Anzahl einzigartiger Epitope: 700\n",
      "\n",
      "Binding=1 (Positive):\n",
      "Epitope\n",
      "KLGGALQAK     20595\n",
      "AVFDRKSDAK     2349\n",
      "GILGFVFTL      2318\n",
      "RAKFKQLL       1125\n",
      "IVTDFSVIK       810\n",
      "              ...  \n",
      "SLLQGSPHL         1\n",
      "LMFWASSSI         1\n",
      "MYVKWPWYVW        1\n",
      "VLNYFEPYL         1\n",
      "YLDELIRNT         1\n",
      "Name: count, Length: 693, dtype: int64\n",
      "Gesamtanzahl Binding=1: 33711\n",
      "\n",
      "Binding=0 (Negative):\n",
      "Epitope\n",
      "RIAAWMATY      746\n",
      "MLDLQPETT      735\n",
      "AYSSAGASI      733\n",
      "SLEGGGLGY      729\n",
      "SLFNTVATL      727\n",
      "KLGGALQAK      727\n",
      "LLFGYPVYV      726\n",
      "ALIAPVHAV      722\n",
      "KVLEYVIKV      719\n",
      "RAKFKQLL       719\n",
      "KLQCVDLHV      716\n",
      "KTWGQYWQV      716\n",
      "FLRGRAYGL      715\n",
      "ELRRKMMYM      713\n",
      "STEGGGLAY      713\n",
      "QPRAPIRPI      712\n",
      "TPRVTGGGAM     709\n",
      "CLLWSFQTSA     708\n",
      "IPSINVHHY      698\n",
      "SLLMWITQV      697\n",
      "LLMGTLGIVC     696\n",
      "QYDPVAALF      695\n",
      "GILGFVFTL      695\n",
      "GPAESAAGL      695\n",
      "KVAELVHFL      693\n",
      "CLLGTYTQDV     692\n",
      "RPHERNGFTVL    689\n",
      "RLRAEAQVK      687\n",
      "CYTWNQMNL      686\n",
      "ELAGIGILTV     683\n",
      "FLYALALLL      681\n",
      "SLYNTVATLY     678\n",
      "GLCTLVAML      673\n",
      "LLDFVRFMGV     670\n",
      "RTLNAWVKV      666\n",
      "CLGGLLTMV      666\n",
      "IMDQVPFSV      665\n",
      "FLASKIGRLV     663\n",
      "SLFNTVATLY     661\n",
      "YLLEMLWRL      660\n",
      "AVFDRKSDAK     659\n",
      "AYAQKIFKI      657\n",
      "VTEHDTLLY      657\n",
      "ILKEPVHGV      646\n",
      "NLVPMVATV      637\n",
      "YLNDHLEPWI     633\n",
      "RPPIFIRRL      630\n",
      "IVTDFSVIK      625\n",
      "RMFPNAPYL      593\n",
      "Name: count, dtype: int64\n",
      "Gesamtanzahl Binding=0: 33711\n",
      "\n",
      "Validation-Datensatz:\n",
      "Gesamtanzahl der Daten: 43344\n",
      "Anzahl einzigartiger Epitope: 751\n",
      "\n",
      "Binding=1 (Positive):\n",
      "Epitope\n",
      "KLGGALQAK     2413\n",
      "GILGFVFTL      608\n",
      "RAKFKQLL       451\n",
      "AVFDRKSDAK     390\n",
      "LLLDRLNQL      252\n",
      "              ... \n",
      "VMYASAVVL        1\n",
      "FEADTFFHFV       1\n",
      "YVFHLYLQY        1\n",
      "RLFARTRSMW       1\n",
      "FLNGSCGSV        1\n",
      "Name: count, Length: 736, dtype: int64\n",
      "Gesamtanzahl Binding=1: 7224\n",
      "\n",
      "Binding=0 (Negative):\n",
      "Epitope\n",
      "KTWGQYWQV      802\n",
      "SLLMWITQV      798\n",
      "FLASKIGRLV     796\n",
      "SLFNTVATL      789\n",
      "STEGGGLAY      788\n",
      "FLYALALLL      784\n",
      "RMFPNAPYL      769\n",
      "QYDPVAALF      762\n",
      "RIAAWMATY      761\n",
      "ALIAPVHAV      761\n",
      "GPAESAAGL      760\n",
      "KLQCVDLHV      757\n",
      "RAKFKQLL       752\n",
      "KVAELVHFL      751\n",
      "SLFNTVATLY     749\n",
      "AYAQKIFKI      745\n",
      "GLCTLVAML      745\n",
      "IMDQVPFSV      743\n",
      "KLGGALQAK      743\n",
      "VTEHDTLLY      742\n",
      "CLGGLLTMV      741\n",
      "YLLEMLWRL      739\n",
      "IVTDFSVIK      737\n",
      "SLYNTVATLY     737\n",
      "ELRRKMMYM      736\n",
      "SLEGGGLGY      735\n",
      "ELAGIGILTV     735\n",
      "MLDLQPETT      734\n",
      "CLLGTYTQDV     734\n",
      "RTLNAWVKV      733\n",
      "LLFGYPVYV      732\n",
      "LLMGTLGIVC     732\n",
      "CLLWSFQTSA     728\n",
      "GILGFVFTL      726\n",
      "FLRGRAYGL      724\n",
      "IPSINVHHY      718\n",
      "NLVPMVATV      715\n",
      "AYSSAGASI      714\n",
      "YLNDHLEPWI     711\n",
      "LLDFVRFMGV     710\n",
      "AVFDRKSDAK     705\n",
      "KVLEYVIKV      705\n",
      "RPHERNGFTVL    703\n",
      "QPRAPIRPI      699\n",
      "RLRAEAQVK      698\n",
      "ILKEPVHGV      696\n",
      "RPPIFIRRL      686\n",
      "TPRVTGGGAM     682\n",
      "CYTWNQMNL      678\n",
      "Name: count, dtype: int64\n",
      "Gesamtanzahl Binding=0: 36120\n",
      "\n",
      "Test-Datensatz:\n",
      "Gesamtanzahl der Daten: 43356\n",
      "Anzahl einzigartiger Epitope: 771\n",
      "\n",
      "Binding=1 (Positive):\n",
      "Epitope\n",
      "KLGGALQAK     2400\n",
      "GILGFVFTL      619\n",
      "RAKFKQLL       448\n",
      "AVFDRKSDAK     378\n",
      "LLLDRLNQL      244\n",
      "              ... \n",
      "FVVPYMIYLL       1\n",
      "FMIGYASAL        1\n",
      "KRKEPLKI         1\n",
      "YGFQPTNGV        1\n",
      "KLSHQPVLL        1\n",
      "Name: count, Length: 758, dtype: int64\n",
      "Gesamtanzahl Binding=1: 7226\n",
      "\n",
      "Binding=0 (Negative):\n",
      "Epitope\n",
      "RTLNAWVKV      816\n",
      "SLEGGGLGY      791\n",
      "ALIAPVHAV      786\n",
      "RLRAEAQVK      783\n",
      "IVTDFSVIK      778\n",
      "FLRGRAYGL      774\n",
      "CLGGLLTMV      774\n",
      "SLLMWITQV      773\n",
      "AYAQKIFKI      766\n",
      "FLYALALLL      766\n",
      "KTWGQYWQV      762\n",
      "LLMGTLGIVC     762\n",
      "ELAGIGILTV     757\n",
      "IMDQVPFSV      756\n",
      "TPRVTGGGAM     754\n",
      "RPPIFIRRL      753\n",
      "KVAELVHFL      748\n",
      "SLYNTVATLY     748\n",
      "CLLWSFQTSA     737\n",
      "VTEHDTLLY      736\n",
      "MLDLQPETT      733\n",
      "YLNDHLEPWI     731\n",
      "QYDPVAALF      730\n",
      "KLQCVDLHV      730\n",
      "CYTWNQMNL      730\n",
      "AVFDRKSDAK     729\n",
      "NLVPMVATV      728\n",
      "ILKEPVHGV      728\n",
      "SLFNTVATLY     727\n",
      "RIAAWMATY      725\n",
      "RAKFKQLL       725\n",
      "LLDFVRFMGV     724\n",
      "RPHERNGFTVL    720\n",
      "AYSSAGASI      719\n",
      "KLGGALQAK      719\n",
      "YLLEMLWRL      718\n",
      "QPRAPIRPI      718\n",
      "RMFPNAPYL      716\n",
      "CLLGTYTQDV     716\n",
      "GPAESAAGL      714\n",
      "LLFGYPVYV      711\n",
      "GILGFVFTL      710\n",
      "STEGGGLAY      710\n",
      "IPSINVHHY      708\n",
      "ELRRKMMYM      706\n",
      "FLASKIGRLV     704\n",
      "KVLEYVIKV      700\n",
      "GLCTLVAML      691\n",
      "SLFNTVATL      690\n",
      "Name: count, dtype: int64\n",
      "Gesamtanzahl Binding=0: 36130\n",
      "\n",
      "Alle Datenstze wurden erfolgreich gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Daten laden\n",
    "paired_df = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_paired_only_expanded-all.csv', sep=',')\n",
    "\n",
    "# Schritt 1: Ursprngliche Aufteilung in Train, Validation und Test\n",
    "train_split, test_split = train_test_split(paired_df, test_size=0.3, random_state=42)\n",
    "validation_split, test_split = train_test_split(test_split, test_size=0.5, random_state=42)\n",
    "\n",
    "# Positive Samples laden >> files zuerst umbenennen\n",
    "train_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/paired/train_prenegsamples.tsv', sep='\\t')\n",
    "validation_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/paired/validation_prenegsamples.tsv', sep='\\t')\n",
    "test_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/paired/test_prenegsamples.tsv', sep='\\t')\n",
    "\n",
    "# Anzahl positiver Samples\n",
    "num_train_pos = len(train_preneg)\n",
    "num_validation_pos = len(validation_preneg)\n",
    "num_test_pos = len(test_preneg)\n",
    "\n",
    "# Zielgren fr negative Samples\n",
    "train_neg_needed = num_train_pos\n",
    "validation_neg_needed = num_validation_pos * 5\n",
    "test_neg_needed = num_test_pos * 5\n",
    "\n",
    "# Funktion zur Sicherstellung, dass alle unique Epitope erhalten bleiben\n",
    "def ensure_unique_epitopes(df, target_count):\n",
    "    unique_epitopes = df['Epitope'].unique()\n",
    "    guaranteed_samples = []\n",
    "    \n",
    "    # Stelle sicher, dass jedes Epitope mindestens einmal vorkommt\n",
    "    for epitope in unique_epitopes:\n",
    "        epitope_group = df[df['Epitope'] == epitope]\n",
    "        if len(epitope_group) > 0:\n",
    "            guaranteed_samples.append(epitope_group.sample(1, random_state=42))\n",
    "    \n",
    "    # Kombiniere garantierte Samples\n",
    "    guaranteed_df = pd.concat(guaranteed_samples, ignore_index=True)\n",
    "    \n",
    "    # Berechne verbleibende Anzahl an Samples\n",
    "    remaining_count = target_count - len(guaranteed_df)\n",
    "    if remaining_count > 0:\n",
    "        remaining_samples = df.sample(remaining_count, random_state=42, replace=True)\n",
    "        return pd.concat([guaranteed_df, remaining_samples], ignore_index=True)\n",
    "    return guaranteed_df\n",
    "\n",
    "# Balancierung der Splits\n",
    "train_balanced_negatives = ensure_unique_epitopes(train_split, train_neg_needed)\n",
    "validation_balanced_negatives = ensure_unique_epitopes(validation_split, validation_neg_needed)\n",
    "test_balanced_negatives = ensure_unique_epitopes(test_split, test_neg_needed)\n",
    "\n",
    "# Positive und negative Samples kombinieren\n",
    "train_combined = pd.concat([train_preneg, train_balanced_negatives], ignore_index=True)\n",
    "validation_combined = pd.concat([validation_preneg, validation_balanced_negatives], ignore_index=True)\n",
    "test_combined = pd.concat([test_preneg, test_balanced_negatives], ignore_index=True)\n",
    "\n",
    "# Verteilungen prfen\n",
    "def check_distribution(df, name):\n",
    "    print(f\"\\n{name}-Datensatz:\")\n",
    "    print(f\"Gesamtanzahl der Daten: {len(df)}\")\n",
    "    print(f\"Anzahl einzigartiger Epitope: {df['Epitope'].nunique()}\")\n",
    "\n",
    "    positive_counts = df[df['Binding'] == 1]['Epitope'].value_counts()\n",
    "    print(f\"\\nBinding=1 (Positive):\")\n",
    "    print(positive_counts)\n",
    "    print(f\"Gesamtanzahl Binding=1: {positive_counts.sum()}\")\n",
    "\n",
    "    negative_counts = df[df['Binding'] == 0]['Epitope'].value_counts()\n",
    "    print(f\"\\nBinding=0 (Negative):\")\n",
    "    print(negative_counts)\n",
    "    print(f\"Gesamtanzahl Binding=0: {negative_counts.sum()}\")\n",
    "\n",
    "check_distribution(train_combined, \"Train\")\n",
    "check_distribution(validation_combined, \"Validation\")\n",
    "check_distribution(test_combined, \"Test\")\n",
    "\n",
    "# Speichern der kombinierten Datenstze\n",
    "output_dir = f'{pipeline_data_splitted}/{precision}/paired/'\n",
    "train_combined.to_csv(output_dir + \"train.tsv\", sep='\\t', index=False)\n",
    "validation_combined.to_csv(output_dir + \"validation.tsv\", sep='\\t', index=False)\n",
    "test_combined.to_csv(output_dir + \"test.tsv\", sep='\\t', index=False)\n",
    "\n",
    "print(\"\\nAlle Datenstze wurden erfolgreich gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Classification \n",
    "The classification in the split notebook correct for positive only data. After adding negative data, some classifications might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "validation_file_name = 'validation.tsv'\n",
    "test_file_name = 'test.tsv'\n",
    "train_file_name = 'train.tsv'\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the classification for paired data\n",
    "paired = True\n",
    "train_data_path = f'{paired_output_folder}/{train_file_name}'\n",
    "test_data_path = f'{paired_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "\n",
    "%run ../../data_scripts/data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene\n",
      "../../data/splitted_datasets/gene/paired/train.tsv\n",
      "                  TCR_name        TRAV    TRAJ           TRA_CDR3      TRBV  \\\n",
      "0                    17414  TRAV29/DV5  TRAJ37     CAASALGNTGKLIF   TRBV4-1   \n",
      "1                    20147       TRAV5  TRAJ20      CAEIRANDYKLSF   TRBV5-1   \n",
      "2                    16532      TRAV17  TRAJ17   CAALDGIKAAGNKLTF    TRBV19   \n",
      "3                    21795         NaN     NaN       CAFLGGANNLFF       NaN   \n",
      "4                     8426      TRAV17   TRAJ7     CATGLYYGNNRLAF  TRBV11-2   \n",
      "...                    ...         ...     ...                ...       ...   \n",
      "43351   TTATGCTAGAGGTACC-6    TRAV26-2  TRAJ41        CILKSGYALNF  TRBV12-4   \n",
      "43352  CCTTCCCGTGGAAAGA-28     TRAV8-2   TRAJ8     CVVSENTGFQKLVF   TRBV7-9   \n",
      "43353  GCTTGAATCCTCATTA-10       TRAV3  TRAJ37  CAVRDGGMGSNTGKLIF  TRBV12-4   \n",
      "43354  ACTTACTTCACGATGT-36     TRAV1-2  TRAJ33       CAVLDSNYQLIW    TRBV27   \n",
      "43355   CCCTCCTGTCATCCCT-7     TRAV1-2  TRAJ12       CAVMDSSYKLIF   TRBV6-4   \n",
      "\n",
      "          TRBJ           TRB_CDR3  TRAC   TRBC    Epitope          MHC  \\\n",
      "0      TRBJ1-1     CASSPGAGDTEAFF   NaN    NaN  KLGGALQAK        HLA-A   \n",
      "1      TRBJ1-2        CASSLGQRYTF   NaN    NaN  KLGGALQAK        HLA-A   \n",
      "2      TRBJ1-1    CASKTGGALNTEAFF   NaN    NaN  KLGGALQAK        HLA-A   \n",
      "3          NaN     CASSLEWGGETQYF   NaN    NaN  KLGGALQAK        HLA-A   \n",
      "4      TRBJ2-2  CASSLGVAGTNTGELFF   NaN    NaN  KLGGALQAK        HLA-A   \n",
      "...        ...                ...   ...    ...        ...          ...   \n",
      "43351  TRBJ2-1    CASSLSGGSYNEQFF  TRAC  TRBC2  MLDLQPETT  HLA-A*02:01   \n",
      "43352  TRBJ2-3    CASSLVRPPTDTQYF  TRAC  TRBC2  GPAESAAGL  HLA-B*07:02   \n",
      "43353  TRBJ2-3        CASSPMDTQYF  TRAC  TRBC2   RAKFKQLL  HLA-B*08:01   \n",
      "43354  TRBJ2-7         CASSFYEQYF  TRAC  TRBC2  NLVPMVATV  HLA-A*02:01   \n",
      "43355  TRBJ2-3    CASSDAGGHTDTQYF  TRAC  TRBC2  RLRAEAQVK  HLA-A*03:01   \n",
      "\n",
      "       Binding  task  \n",
      "0            1  TPP1  \n",
      "1            1  TPP1  \n",
      "2            1  TPP1  \n",
      "3            1  TPP1  \n",
      "4            1  TPP1  \n",
      "...        ...   ...  \n",
      "43351        0  TPP1  \n",
      "43352        0  TPP2  \n",
      "43353        0  TPP1  \n",
      "43354        0  TPP1  \n",
      "43355        0  TPP1  \n",
      "\n",
      "[43356 rows x 13 columns]\n",
      "train+validate data has 110766 entries\n",
      "test data has 43356 entries\n",
      "test data has 37203 TPP1 tasks (old value: 27972) (seen tcr & seen epitopes).\n",
      "test data has 5864 TPP2 tasks (old value: 15095) (unseen tcr & seen epitopes).\n",
      "test data has 222 TPP3 tasks (old value: 289) (unseen tcr & unseen epitope).\n",
      "test data has 67 TPP4 tasks (old value: 0) (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.7186903881340756/0.2813096118659244\n",
      "../../data/splitted_datasets/gene/paired/test_reclassified_paired_specific.tsv\n",
      "uploading dataset to dataset-gene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250301_192835-xffehb0u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/xffehb0u' target=\"_blank\">glamorous-brook-1</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/xffehb0u' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/xffehb0u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/gene/paired)... Done. 0.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26cbcc5be2b4dd3862a4ba14f2b8c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.494 MB of 23.975 MB uploaded\\r'), FloatProgress(value=0.062304217482112415, max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glamorous-brook-1</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/xffehb0u' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/xffehb0u</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250301_192835-xffehb0u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extended classification for paired data\n",
    "test_path = f'{paired_output_folder}/{test_file_name}'\n",
    "train_path = f'{paired_output_folder}/{train_file_name}'\n",
    "validation_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "output_path = f'{paired_output_folder}/test_reclassified_paired_specific.tsv'\n",
    "paired_data_path = paired_output_folder\n",
    "alpha_cdr3_name = 'TRA_CDR3'\n",
    "beta_cdr3_name = 'TRB_CDR3'\n",
    "epitope_name = 'Epitope'\n",
    "task_name = 'task'\n",
    "\n",
    "%run ../../data_scripts/data_preparation/paired_reclassification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25574/1815672389.py:1: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(train_data_path, sep=\"\\t\")\n",
      "/tmp/ipykernel_25574/1815672389.py:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_data_path, sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "# do the classification for beta data\n",
    "paired = False\n",
    "train_data_path = f'{beta_output_folder}/{train_file_name}'\n",
    "test_data_path = f'{beta_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{beta_output_folder}/{validation_file_name}'\n",
    "\n",
    "%run ../../data_scripts/data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells the classification is checked. If the output says \"Classification is correct\", everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train+validate data has 110766 entries\n",
      "test data has 43356 entries\n",
      "test data has 27972 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 15095 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 289 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 0 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.7186903881340756/0.2813096118659244\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    43356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification paired\n",
    "splitted_data_path = paired_output_folder\n",
    "\n",
    "%run ../../data_scripts/data_preparation/check_task_classification_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25574/1838351243.py:18: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(f\"{splitted_data_path}/{train_file_name}\", sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 251750 entries\n",
      "test data has 161844 entries\n",
      "test data has 140896 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 20645 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 299 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 4 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.7187434831570021/0.28125651684299796\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    161844\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification beta\n",
    "splitted_data_path = beta_output_folder\n",
    "\n",
    "%run ../../data_scripts/data_preparation/check_task_classification_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'validation_prenegsamples.tsv', 'test.tsv', 'train.tsv', 'test_reclassified_paired_specific.tsv', 'validation.tsv', 'train_prenegsamples.tsv', 'test_prenegsamples.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(path_to_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-gene\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250301_192944-02cxktoa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/02cxktoa' target=\"_blank\">mild-hill-2</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/02cxktoa' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/02cxktoa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/gene/paired)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-hill-2</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/02cxktoa' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/02cxktoa</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250301_192944-02cxktoa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# upload paired data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "dataset_name = f'paired_{precision}'\n",
    "#main_project_name = os.getenv(\"MAIN_PROJECT_NAME\")\n",
    "main_project_name = f\"dataset-{precision}\"\n",
    "\n",
    "%run ../../data_scripts/upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-gene\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250301_192950-ex4p87q7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/ex4p87q7' target=\"_blank\">toasty-terrain-3</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/ex4p87q7' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/ex4p87q7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/gene/beta)... Done. 0.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd19d36921a467397a896a1ca9a1b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.244 MB of 51.643 MB uploaded\\r'), FloatProgress(value=0.02408326556236381, max=1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-terrain-3</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/ex4p87q7' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene/runs/ex4p87q7</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250301_192950-ex4p87q7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload beta data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "dataset_name = f'beta_{precision}'\n",
    "\n",
    "%run ../../data_scripts/upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings >> ProtBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Sollte True zurckgeben\n",
    "print(torch.version.cuda)  # Sollte die richtige CUDA-Version anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Loading: Rostlab/prot_bert\n",
      "Model is on device: cuda:0\n",
      "Processing Batch:  0 64\n",
      "Processing Batch:  64 128\n",
      "Processing Batch:  128 192\n",
      "Processing Batch:  192 256\n",
      "Processing Batch:  256 320\n",
      "Processing Batch:  320 384\n",
      "Processing Batch:  384 448\n",
      "Processing Batch:  448 512\n",
      "Processing Batch:  512 576\n",
      "Processing Batch:  576 640\n",
      "Processing Batch:  640 704\n",
      "Processing Batch:  704 768\n",
      "Processing Batch:  768 832\n",
      "Processing Batch:  832 896\n",
      "Processing Batch:  896 960\n",
      "Processing Batch:  960 1024\n",
      "Processing Batch:  1024 1088\n",
      "Processing Batch:  1088 1152\n",
      "Processing Batch:  1152 1216\n",
      "Processing Batch:  1216 1280\n",
      "Processing Batch:  1280 1344\n",
      "Processing Batch:  1344 1408\n",
      "Processing Batch:  1408 1472\n",
      "Processing Batch:  1472 1536\n",
      "Processing Batch:  1536 1600\n",
      "Processing Batch:  1600 1664\n",
      "Processing Batch:  1664 1728\n",
      "Processing Batch:  1728 1792\n",
      "Processing Batch:  1792 1856\n",
      "Processing Batch:  1856 1920\n",
      "Processing Batch:  1920 1984\n",
      "Processing Batch:  1984 2048\n",
      "Processing Batch:  2048 2112\n",
      "Processing Batch:  2112 2176\n",
      "Processing Batch:  2176 2240\n",
      "Processing Batch:  2240 2304\n",
      "Processing Batch:  2304 2368\n",
      "Processing Batch:  2368 2432\n",
      "Processing Batch:  2432 2496\n",
      "Processing Batch:  2496 2560\n",
      "Processing Batch:  2560 2624\n",
      "Processing Batch:  2624 2688\n",
      "Processing Batch:  2688 2752\n",
      "Processing Batch:  2752 2816\n",
      "Processing Batch:  2816 2880\n",
      "Processing Batch:  2880 2944\n",
      "Processing Batch:  2944 3008\n",
      "Processing Batch:  3008 3072\n",
      "Processing Batch:  3072 3136\n",
      "Processing Batch:  3136 3200\n",
      "Processing Batch:  3200 3264\n",
      "Processing Batch:  3264 3328\n",
      "Processing Batch:  3328 3392\n",
      "Processing Batch:  3392 3456\n",
      "Processing Batch:  3456 3520\n",
      "Processing Batch:  3520 3584\n",
      "Processing Batch:  3584 3648\n",
      "Processing Batch:  3648 3712\n",
      "Processing Batch:  3712 3776\n",
      "Processing Batch:  3776 3840\n",
      "Processing Batch:  3840 3904\n",
      "Processing Batch:  3904 3968\n",
      "Processing Batch:  3968 4032\n",
      "Processing Batch:  4032 4096\n",
      "Processing Batch:  4096 4160\n",
      "Processing Batch:  4160 4224\n",
      "Processing Batch:  4224 4288\n",
      "Processing Batch:  4288 4352\n",
      "Processing Batch:  4352 4416\n",
      "Processing Batch:  4416 4480\n",
      "Processing Batch:  4480 4544\n",
      "Processing Batch:  4544 4608\n",
      "Processing Batch:  4608 4672\n"
     ]
    }
   ],
   "source": [
    "path_paired_test = f\"{pipeline_data_splitted}/{precision}/paired/test.tsv\"\n",
    "path_paired_validation = f\"{pipeline_data_splitted}/{precision}/paired/validation.tsv\"\n",
    "path_paired_train = f\"{pipeline_data_splitted}/{precision}/paired/train.tsv\"\n",
    "path_beta_test = f\"{pipeline_data_splitted}/{precision}/beta/test.tsv\"\n",
    "path_beta_validation = f\"{pipeline_data_splitted}/{precision}/beta/validation.tsv\"\n",
    "path_beta_train = f\"{pipeline_data_splitted}/{precision}/beta/train.tsv\"\n",
    "\n",
    "\n",
    "path_paired = f\"{pipeline_data}/embeddings/temp/{precision}/paired_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_paired)])\n",
    "df_paired_test = pd.read_csv(path_paired_test, sep=\"\\t\", index_col=False)\n",
    "df_paired_validation = pd.read_csv(path_paired_validation, sep=\"\\t\", index_col=False)\n",
    "df_paired_train = pd.read_csv(path_paired_train, sep=\"\\t\", index_col=False)\n",
    "df_paired = pd.concat([df_paired_test, df_paired_validation, df_paired_train])\n",
    "df_paired.to_csv(path_paired, sep=\"\\t\", index=False)\n",
    "\n",
    "# paired\n",
    "%run ../../data_scripts/generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz TRA_CDR3\n",
    "%run ../../data_scripts/generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz TRB_CDR3\n",
    "%run ../../data_scripts/generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz Epitope\n",
    "\n",
    "path_beta = f\"{pipeline_data}/embeddings/temp/{precision}/beta_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_beta)])\n",
    "df_beta_test = pd.read_csv(path_beta_test, sep=\"\\t\", index_col=False)\n",
    "df_beta_validation = pd.read_csv(path_beta_validation, sep=\"\\t\", index_col=False)\n",
    "df_beta_train = pd.read_csv(path_beta_train, sep=\"\\t\", index_col=False)\n",
    "df_beta = pd.concat([df_beta_test, df_beta_validation, df_beta_train])\n",
    "df_beta.to_csv(path_beta, sep=\"\\t\", index=False)\n",
    "\n",
    "# beta\n",
    "#%run ../../data_scripts/generateEmbeddingsProtBERT.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/TRB_beta_embeddings.npz TRB_CDR3\n",
    "#%run ../../data_scripts/generateEmbeddingsProtBERT.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/Epitope_beta_embeddings.npz Epitope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade fr Train-, Test-, und Validierungsdatenstze fr alle vier Kategorien\n",
    "base_path = 'data/splitted_datasets'\n",
    "\n",
    "# Definierte Pfade fr alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"paired_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    },\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen fr jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "    \n",
    "    # Zhle die Anzahl der Bindings 1 und 0 in jedem Datensatz\n",
    "    train_binding_counts = train_df['Binding'].value_counts()\n",
    "    test_binding_counts = test_df['Binding'].value_counts()\n",
    "    validation_binding_counts = validation_df['Binding'].value_counts()\n",
    "    \n",
    "    # Zhle die Anzahl der TPP1, TPP2, TPP3 Eintrge in jedem Datensatz\n",
    "    train_task_counts = train_df['task'].value_counts()\n",
    "    test_task_counts = test_df['task'].value_counts()\n",
    "    validation_task_counts = validation_df['task'].value_counts()\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Train_Binding_1\": train_binding_counts.get(1, 0),\n",
    "        \"Train_Binding_0\": train_binding_counts.get(0, 0),\n",
    "        \"Train_TPP1\": train_task_counts.get(\"TPP1\", 0),\n",
    "        \"Train_TPP2\": train_task_counts.get(\"TPP2\", 0),\n",
    "        \"Train_TPP3\": train_task_counts.get(\"TPP3\", 0),\n",
    "        \"Train_TPP4\": train_task_counts.get(\"TPP4\", 0),\n",
    "        \"Test\": test_length,\n",
    "        \"Test_Binding_1\": test_binding_counts.get(1, 0),\n",
    "        \"Test_Binding_0\": test_binding_counts.get(0, 0),\n",
    "        \"Test_TPP1\": test_task_counts.get(\"TPP1\", 0),\n",
    "        \"Test_TPP2\": test_task_counts.get(\"TPP2\", 0),\n",
    "        \"Test_TPP3\": test_task_counts.get(\"TPP3\", 0),\n",
    "        \"Test_TPP4\": test_task_counts.get(\"TPP4\", 0),\n",
    "        \"Validation\": validation_length,\n",
    "        \"Validation_Binding_1\": validation_binding_counts.get(1, 0),\n",
    "        \"Validation_Binding_0\": validation_binding_counts.get(0, 0),\n",
    "        \"Validation_TPP1\": validation_task_counts.get(\"TPP1\", 0),\n",
    "        \"Validation_TPP2\": validation_task_counts.get(\"TPP2\", 0),\n",
    "        \"Validation_TPP3\": validation_task_counts.get(\"TPP3\", 0),\n",
    "        \"Validation_TPP4\": validation_task_counts.get(\"TPP4\", 0),\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]} (Binding=1: {lengths[\"Train_Binding_1\"]}, Binding=0: {lengths[\"Train_Binding_0\"]}, TPP1: {lengths[\"Train_TPP1\"]}, TPP2: {lengths[\"Train_TPP2\"]}, TPP3: {lengths[\"Train_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]} (Binding=1: {lengths[\"Test_Binding_1\"]}, Binding=0: {lengths[\"Test_Binding_0\"]}, TPP1: {lengths[\"Test_TPP1\"]}, TPP2: {lengths[\"Test_TPP2\"]}, TPP3: {lengths[\"Test_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]} (Binding=1: {lengths[\"Validation_Binding_1\"]}, Binding=0: {lengths[\"Validation_Binding_0\"]}, TPP1: {lengths[\"Validation_TPP1\"]}, TPP2: {lengths[\"Validation_TPP2\"]}, TPP3: {lengths[\"Validation_TPP3\"]})')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')\n",
    "\n",
    "# Optional: Ergebnisse in einer bersichtstabelle darstellen\n",
    "summary_data = []\n",
    "for dataset, lengths in results.items():\n",
    "    summary_data.append({\n",
    "        \"Dataset\": dataset.replace(\"_\", \" \").title(),\n",
    "        \"Train\": lengths[\"Train\"],\n",
    "        \"Train_Binding_1\": lengths[\"Train_Binding_1\"],\n",
    "        \"Train_Binding_0\": lengths[\"Train_Binding_0\"],\n",
    "        \"Train_TPP1\": lengths[\"Train_TPP1\"],\n",
    "        \"Train_TPP2\": lengths[\"Train_TPP2\"],\n",
    "        \"Train_TPP3\": lengths[\"Train_TPP3\"],\n",
    "        \"Train_TPP4\": lengths[\"Train_TPP4\"],\n",
    "        \"Test\": lengths[\"Test\"],\n",
    "        \"Test_Binding_1\": lengths[\"Test_Binding_1\"],\n",
    "        \"Test_Binding_0\": lengths[\"Test_Binding_0\"],\n",
    "        \"Test_TPP1\": lengths[\"Test_TPP1\"],\n",
    "        \"Test_TPP2\": lengths[\"Test_TPP2\"],\n",
    "        \"Test_TPP3\": lengths[\"Test_TPP3\"],\n",
    "        \"Test_TPP4\": lengths[\"Test_TPP4\"],\n",
    "        \"Validation\": lengths[\"Validation\"],\n",
    "        \"Validation_Binding_1\": lengths[\"Validation_Binding_1\"],\n",
    "        \"Validation_Binding_0\": lengths[\"Validation_Binding_0\"],\n",
    "        \"Validation_TPP1\": lengths[\"Validation_TPP1\"],\n",
    "        \"Validation_TPP2\": lengths[\"Validation_TPP2\"],\n",
    "        \"Validation_TPP3\": lengths[\"Validation_TPP3\"],\n",
    "        \"Validation_TPP4\": lengths[\"Validation_TPP4\"],\n",
    "        \"Total\": lengths[\"Total\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
