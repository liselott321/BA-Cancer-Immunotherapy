{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942454fb-523a-4fe8-a61e-74b998097354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d41e675-c5fd-4568-9240-55cae753e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f'../../../../data/splitted_datasets/allele/beta/new/beta/train.tsv',\n",
    "        \"test\": f'../../../../data/splitted_datasets/allele/beta/new/beta/test.tsv',\n",
    "        \"validation\": f'../../../../data/splitted_datasets/allele/beta/new/beta/validation.tsv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042be97a-6fea-4b07-930f-4e706eca7df8",
   "metadata": {},
   "source": [
    "1. Alle TPP-Teildateien (*_tpp2.tsv, *_tpp3.tsv…) einzulesen\n",
    "\n",
    "2. Zeilen- und Binding-Verteilungen je Datei anzuzeigen\n",
    "\n",
    "3. alle TPP-Chunks zu einem DataFrame zusammenzufassen und mit der Master-Datei (test_neg.tsv bzw. val_neg.tsv, train_neg.tsv) abzugleichen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2dd96c0-7e2b-4613-9e6d-1550fbec0c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- test_neg_tpp2.tsv ---\n",
      "Rows: 27398\n",
      "Binding value counts:\n",
      "Binding\n",
      "0    27398\n",
      "Name: count, dtype: int64\n",
      "Unique (TCR, Epitope) pairs: 25741\n",
      "\n",
      "--- test_neg_tpp3.tsv ---\n",
      "Rows: 43689\n",
      "Binding value counts:\n",
      "Binding\n",
      "0    43689\n",
      "Name: count, dtype: int64\n",
      "Unique (TCR, Epitope) pairs: 40909\n",
      "\n",
      "--- test_neg_tpp4.tsv ---\n",
      "Rows: 165\n",
      "Binding value counts:\n",
      "Binding\n",
      "0    165\n",
      "Name: count, dtype: int64\n",
      "Unique (TCR, Epitope) pairs: 165\n",
      "\n",
      "--- train_neg_tpp3.tsv ---\n",
      "Rows: 0\n",
      "Binding value counts:\n",
      "Series([], Name: count, dtype: int64)\n",
      "Unique (TCR, Epitope) pairs: 0\n",
      "\n",
      "--- val_neg_tpp2.tsv ---\n",
      "Rows: 58344\n",
      "Binding value counts:\n",
      "Binding\n",
      "0    58344\n",
      "Name: count, dtype: int64\n",
      "Unique (TCR, Epitope) pairs: 55888\n",
      "\n",
      "--- val_neg_tpp3.tsv ---\n",
      "Rows: 58280\n",
      "Binding value counts:\n",
      "Binding\n",
      "0    58280\n",
      "Name: count, dtype: int64\n",
      "Unique (TCR, Epitope) pairs: 55820\n",
      "\n",
      "--- val_neg_tpp4.tsv ---\n",
      "Rows: 450\n",
      "Binding value counts:\n",
      "Binding\n",
      "0    450\n",
      "Name: count, dtype: int64\n",
      "Unique (TCR, Epitope) pairs: 450\n",
      "\n",
      "=== Zusammengefasst alle TPP-Teile ===\n",
      "Total rows: 188326\n",
      "Binding value counts:\n",
      "Binding\n",
      "0    188326\n",
      "Name: count, dtype: int64\n",
      "Unique (TCR, Epitope) pairs: 159389\n",
      "\n",
      "=== Master test_neg.tsv ===\n",
      "Total rows: 52775\n",
      "Binding value counts:\n",
      "Binding\n",
      "0    52775\n",
      "Name: count, dtype: int64\n",
      "Unique pairs: 52388\n",
      "\n",
      "Check row-count match: False\n",
      "Check pair-set equality: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Pfad zum Negatives-Ordner\n",
    "neg_dir = \"../../../../data/splitted_datasets/allele/beta/new/negatives\"\n",
    "\n",
    "def summarize_file(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    name = os.path.basename(path)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"Rows: {len(df)}\")\n",
    "    print(\"Binding value counts:\")\n",
    "    print(df[\"Binding\"].value_counts(dropna=False))\n",
    "    print(\"Unique (TCR, Epitope) pairs:\", \n",
    "          df[[\"TRB_CDR3\",\"Epitope\"]].drop_duplicates().shape[0])\n",
    "    return df\n",
    "\n",
    "# 1) alle TPP-Teildateien einlesen und kurz summarizen\n",
    "tpp_files = sorted(glob.glob(os.path.join(neg_dir,\"*_tpp*.tsv\")))\n",
    "tpp_dfs = [summarize_file(f) for f in tpp_files]\n",
    "\n",
    "# 2) zu einem DataFrame zusammenführen\n",
    "all_tpp = pd.concat(tpp_dfs, ignore_index=True)\n",
    "print(\"\\n=== Zusammengefasst alle TPP-Teile ===\")\n",
    "print(\"Total rows:\", len(all_tpp))\n",
    "print(\"Binding value counts:\")\n",
    "print(all_tpp[\"Binding\"].value_counts())\n",
    "print(\"Unique (TCR, Epitope) pairs:\", \n",
    "      all_tpp[[\"TRB_CDR3\",\"Epitope\"]].drop_duplicates().shape[0])\n",
    "\n",
    "# 3) mit der Master-Datei vergleichen (z.B. test_neg.tsv)\n",
    "master = pd.read_csv(os.path.join(neg_dir, \"test_neg.tsv\"), sep=\"\\t\")\n",
    "print(\"\\n=== Master test_neg.tsv ===\")\n",
    "print(\"Total rows:\", len(master))\n",
    "print(\"Binding value counts:\")\n",
    "print(master[\"Binding\"].value_counts())\n",
    "print(\"Unique pairs:\", master[[\"TRB_CDR3\",\"Epitope\"]].drop_duplicates().shape[0])\n",
    "\n",
    "# 4) Stimmen Zeilenanzahl und Paar-Mengen überein?\n",
    "print(\"\\nCheck row-count match:\", len(master)==len(all_tpp))\n",
    "print(\"Check pair-set equality:\", \n",
    "      set(zip(master[\"TRB_CDR3\"],master[\"Epitope\"])) \n",
    "    == set(zip(all_tpp[\"TRB_CDR3\"],all_tpp[\"Epitope\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc300c40-d7f7-48d4-a841-b8ddfa2f13b0",
   "metadata": {},
   "source": [
    "1. Neg-Pairs in validation/test: Wie viele generierten Negatives tatsächlich in den finalen Splits gelandet sind.\n",
    "\n",
    "2. Neg-Pairs als POS: Falls hier etwas > 0 rauskommt, wurden Negative versehentlich als Positive gelabelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c97a8faa-ec8e-4001-b4cc-4fc3b6436643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132584/1540303189.py:16: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val  = pd.read_csv(val_path, sep=\"\\t\", dtype={\"TRB_CDR3\": str, \"Epitope\": str, \"Binding\": int})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg-Pairs in validation.tsv: 62824\n",
      "Neg-Pairs in test.tsv:       31500\n",
      "Neg-Pairs als POS in validation.tsv: 8110\n",
      "Neg-Pairs als POS in test.tsv:       2737\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pandas as pd\n",
    "\n",
    "# 1) Pfade anpassen\n",
    "base_dir = \"../../../../data/splitted_datasets/allele/beta/new\"\n",
    "neg_dir  = os.path.join(base_dir, \"negatives\")\n",
    "val_path = os.path.join(base_dir, \"validation.tsv\")\n",
    "test_path= os.path.join(base_dir, \"test.tsv\")\n",
    "\n",
    "# Alle TPP-neg Dateien einlesen und zusammenführen\n",
    "neg_files = sorted(glob.glob(os.path.join(neg_dir, \"*_neg_tpp*.tsv\")))\n",
    "negs = pd.concat([pd.read_csv(f, sep=\"\\t\", dtype={\"TRB_CDR3\": str, \"Epitope\": str}) \n",
    "                  for f in neg_files],\n",
    "                 ignore_index=True)\n",
    "\n",
    "# validation & test laden (Binding sicher als int lesen)\n",
    "val  = pd.read_csv(val_path, sep=\"\\t\", dtype={\"TRB_CDR3\": str, \"Epitope\": str, \"Binding\": int})\n",
    "test = pd.read_csv(test_path, sep=\"\\t\", dtype={\"TRB_CDR3\": str, \"Epitope\": str, \"Binding\": int})\n",
    "\n",
    "# Sets basteln\n",
    "neg_pairs       = set(zip(negs[\"TRB_CDR3\"], negs[\"Epitope\"]))\n",
    "val_pairs       = set(zip(val[\"TRB_CDR3\"],  val[\"Epitope\"]))\n",
    "test_pairs      = set(zip(test[\"TRB_CDR3\"], test[\"Epitope\"]))\n",
    "\n",
    "# 1) Wie viele Neg-Pairs landen überhaupt in den Splits?\n",
    "print(\"Neg-Pairs in validation.tsv:\", len(neg_pairs & val_pairs))\n",
    "print(\"Neg-Pairs in test.tsv:      \", len(neg_pairs & test_pairs))\n",
    "\n",
    "# 2) Gibt’s Neg-Pairs, die fälschlich als Positives gelabelt wurden?\n",
    "pos_val_pairs  = set(zip(val[val[\"Binding\"] == 1][\"TRB_CDR3\"],  val[val[\"Binding\"] == 1][\"Epitope\"]))\n",
    "pos_test_pairs = set(zip(test[test[\"Binding\"] == 1][\"TRB_CDR3\"], test[test[\"Binding\"] == 1][\"Epitope\"]))\n",
    "\n",
    "false_pos_in_val  = pos_val_pairs  & neg_pairs\n",
    "false_pos_in_test = pos_test_pairs & neg_pairs\n",
    "\n",
    "print(\"Neg-Pairs als POS in validation.tsv:\", len(false_pos_in_val))\n",
    "print(\"Neg-Pairs als POS in test.tsv:      \", len(false_pos_in_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a4c97-920a-4794-89b3-40e9f1866626",
   "metadata": {},
   "source": [
    "GENAUER TEST: \n",
    "1. Neg-Pairs in validation/test: Wie viele generierten Negatives tatsächlich in den finalen Splits gelandet sind.\n",
    "\n",
    "2. Neg-Pairs als POS: Falls hier etwas > 0 rauskommt, wurden Negative versehentlich als Positive gelabelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad68fa5b-9501-443d-b33a-6d6468bd6947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files for pattern '*_neg_tpp2.tsv': ['../../../../data/splitted_datasets/allele/beta/new/negatives/test_neg_tpp2.tsv', '../../../../data/splitted_datasets/allele/beta/new/negatives/val_neg_tpp2.tsv']\n",
      "Found 3 files for pattern '*_neg_tpp3.tsv': ['../../../../data/splitted_datasets/allele/beta/new/negatives/test_neg_tpp3.tsv', '../../../../data/splitted_datasets/allele/beta/new/negatives/train_neg_tpp3.tsv', '../../../../data/splitted_datasets/allele/beta/new/negatives/val_neg_tpp3.tsv']\n",
      "Found 2 files for pattern '*_neg_tpp4.tsv': ['../../../../data/splitted_datasets/allele/beta/new/negatives/test_neg_tpp4.tsv', '../../../../data/splitted_datasets/allele/beta/new/negatives/val_neg_tpp4.tsv']\n",
      "\n",
      " Generierte TPP2+3+4 Negative-Paare (unique): 159389\n",
      "\n",
      " TPP2+3+4 Negatives in validation.tsv: 62824\n",
      " TPP2+3+4 Negatives in test.tsv:       31500\n",
      "\n",
      "  TPP2+3+4 Neg-Paare als POS in validation.tsv: 8110\n",
      "  TPP2+3+4 Neg-Paare als POS in test.tsv:       2737\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Pfade definieren\n",
    "base_dir = \"../../../../data/splitted_datasets/allele/beta/new\"\n",
    "neg_dir  = os.path.join(base_dir, \"negatives\")\n",
    "val_path = os.path.join(base_dir, \"validation.tsv\")\n",
    "test_path= os.path.join(base_dir, \"test.tsv\")\n",
    "\n",
    "# 2) TPP-2, -3 und -4 Negatives laden\n",
    "patterns = [\"*_neg_tpp2.tsv\", \"*_neg_tpp3.tsv\", \"*_neg_tpp4.tsv\"]\n",
    "all_negs = []\n",
    "for pat in patterns:\n",
    "    files = sorted(glob.glob(os.path.join(neg_dir, pat)))\n",
    "    print(f\"Found {len(files)} files for pattern '{pat}':\", files)\n",
    "    for fn in files:\n",
    "        df = pd.read_csv(fn, sep=\"\\t\",\n",
    "                         dtype={\"TRB_CDR3\": str, \"Epitope\": str, \"Binding\": int},\n",
    "                         low_memory=False)\n",
    "        all_negs.append(df)\n",
    "\n",
    "# Falls keine Dateien, leeres DataFrame\n",
    "if all_negs:\n",
    "    negs = pd.concat(all_negs, ignore_index=True)\n",
    "else:\n",
    "    negs = pd.DataFrame(columns=[\"TRB_CDR3\",\"Epitope\",\"Binding\"])\n",
    "\n",
    "# 3) Unique-Paare als Set\n",
    "neg_pairs = set(zip(negs[\"TRB_CDR3\"], negs[\"Epitope\"]))\n",
    "print(f\"\\n Generierte TPP2+3+4 Negative-Paare (unique): {len(neg_pairs)}\")\n",
    "\n",
    "# 4) validation.tsv und test.tsv laden\n",
    "val  = pd.read_csv(val_path,  sep=\"\\t\",\n",
    "                   dtype={\"TRB_CDR3\": str, \"Epitope\": str, \"Binding\": int},\n",
    "                   low_memory=False)\n",
    "test = pd.read_csv(test_path, sep=\"\\t\",\n",
    "                   dtype={\"TRB_CDR3\": str, \"Epitope\": str, \"Binding\": int},\n",
    "                   low_memory=False)\n",
    "\n",
    "# 5) Sets aus den Splits\n",
    "val_pairs  = set(zip(val[\"TRB_CDR3\"],  val[\"Epitope\"]))\n",
    "test_pairs = set(zip(test[\"TRB_CDR3\"], test[\"Epitope\"]))\n",
    "\n",
    "# 6) Schnittmengen zählen\n",
    "in_val  = neg_pairs & val_pairs\n",
    "in_test = neg_pairs & test_pairs\n",
    "\n",
    "print(f\"\\n TPP2+3+4 Negatives in validation.tsv: {len(in_val)}\")\n",
    "print(f\" TPP2+3+4 Negatives in test.tsv:       {len(in_test)}\")\n",
    "\n",
    "# 7) Fälschlich als positiv gelabelt?\n",
    "pos_val_pairs  = set(zip(val[val[\"Binding\"] == 1][\"TRB_CDR3\"],  val[val[\"Binding\"] == 1][\"Epitope\"]))\n",
    "pos_test_pairs = set(zip(test[test[\"Binding\"] == 1][\"TRB_CDR3\"], test[test[\"Binding\"] == 1][\"Epitope\"]))\n",
    "\n",
    "false_pos_val  = neg_pairs & pos_val_pairs\n",
    "false_pos_test = neg_pairs & pos_test_pairs\n",
    "\n",
    "print(f\"\\n  TPP2+3+4 Neg-Paare als POS in validation.tsv: {len(false_pos_val)}\")\n",
    "print(f\"  TPP2+3+4 Neg-Paare als POS in test.tsv:       {len(false_pos_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd9d2c-fa35-400a-a224-f26c2b08ad14",
   "metadata": {},
   "source": [
    "1. die TPP-2, -3 und -4 Negativ-Files jeweils mit einem TPP-Label einliest\n",
    "\n",
    "2. die finalen validation.tsv und test.tsv läd\n",
    "\n",
    "3. aus den Splits die falsch als positiv (Binding == 1) gelabelten Paare ermittelt,\n",
    "\n",
    "4. diese mit den TPP-Labels verbindet\n",
    "\n",
    "5. und dir pro TPP zeigt, wie viele False-Positives aus der jeweiligen Kategorie stammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a26cfa72-951d-4d7b-b598-744aaa444549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== False-Positives in validation.tsv nach TPP ===\n",
      " TPP  FalsePos_in_Val\n",
      "   2             5617\n",
      "   3             5574\n",
      "\n",
      "=== False-Positives in test.tsv nach TPP ===\n",
      " TPP  FalsePos_in_Test\n",
      "   2              1423\n",
      "   3              2047\n",
      "\n",
      "Beispiel False-Positives (TPP=2) in validation.tsv:\n",
      "          TRB_CDR3   Epitope  Binding_split  TPP  Binding_neg\n",
      "  CATSELGGGLTDEQFF KLGGALQAK              1    3            0\n",
      "  CASSSRGTENTGELFF KLGGALQAK              1    3            0\n",
      "  CASSLGGFSSYNEQFF KLGGALQAK              1    3            0\n",
      "  CASSPPRQGANTEAFF KLGGALQAK              1    3            0\n",
      "CATSIRLRPWQGGDEQFF KLGGALQAK              1    3            0\n",
      "    CASSLSGGVTEAFF KLGGALQAK              1    3            0\n",
      "  CASSLGVGDGQETQYF KLGGALQAK              1    3            0\n",
      "   CASSQLGLAGDEQFF KLGGALQAK              1    3            0\n",
      "   CATSRLASSYNEQFF KLGGALQAK              1    3            0\n",
      " CASSFGQGPLYVDGYTF KLGGALQAK              1    3            0\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pandas as pd\n",
    "\n",
    "# 1) Basis-Pfade\n",
    "base_dir = \"../../../../data/splitted_datasets/allele/beta/new\"\n",
    "neg_dir  = os.path.join(base_dir, \"negatives\")\n",
    "val_path = os.path.join(base_dir, \"validation.tsv\")\n",
    "test_path= os.path.join(base_dir, \"test.tsv\")\n",
    "\n",
    "# 2) Alle TPP-Negatives laden und mit TPP-Spalte markieren\n",
    "def load_tpp_negs(tpp):\n",
    "    files = sorted(glob.glob(os.path.join(neg_dir, f\"* _neg_tpp{tpp}.tsv\".replace(\" \",\"\"))))\n",
    "    dfs = []\n",
    "    for fn in files:\n",
    "        df = pd.read_csv(fn, sep=\"\\t\",\n",
    "                         dtype={\"TRB_CDR3\": str, \"Epitope\": str},\n",
    "                         usecols=[\"TRB_CDR3\",\"Epitope\"])\n",
    "        df[\"TPP\"] = tpp\n",
    "        df[\"Binding_neg\"] = 0\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame(columns=[\"TRB_CDR3\",\"Epitope\",\"TPP\",\"Binding_neg\"])\n",
    "\n",
    "negs = pd.concat([load_tpp_negs(2), load_tpp_negs(3), load_tpp_negs(4)], ignore_index=True)\n",
    "neg_pairs = set(zip(negs[\"TRB_CDR3\"], negs[\"Epitope\"]))\n",
    "\n",
    "# 3) validation & test laden (mit dem Split-Label)\n",
    "val  = pd.read_csv(val_path,  sep=\"\\t\", dtype={\"TRB_CDR3\": str, \"Epitope\": str, \"Binding\": int}, low_memory=False)\n",
    "test = pd.read_csv(test_path, sep=\"\\t\", dtype={\"TRB_CDR3\": str, \"Epitope\": str, \"Binding\": int}, low_memory=False)\n",
    "\n",
    "# 4) False-Positives herausfiltern: join Split-Positives (Binding==1) mit negs\n",
    "key = [\"TRB_CDR3\",\"Epitope\"]\n",
    "fp_val = pd.merge(\n",
    "    val[val[\"Binding\"]==1][key+[\"Binding\"]].rename(columns={\"Binding\":\"Binding_split\"}),\n",
    "    negs,\n",
    "    on=key,\n",
    "    how=\"inner\"\n",
    ")\n",
    "fp_test = pd.merge(\n",
    "    test[test[\"Binding\"]==1][key+[\"Binding\"]].rename(columns={\"Binding\":\"Binding_split\"}),\n",
    "    negs,\n",
    "    on=key,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# 5) Zusammenfassung je TPP\n",
    "summary_val  = fp_val.groupby(\"TPP\").size().reset_index(name=\"FalsePos_in_Val\")\n",
    "summary_test = fp_test.groupby(\"TPP\").size().reset_index(name=\"FalsePos_in_Test\")\n",
    "\n",
    "print(\"\\n=== False-Positives in validation.tsv nach TPP ===\")\n",
    "print(summary_val.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== False-Positives in test.tsv nach TPP ===\")\n",
    "print(summary_test.to_string(index=False))\n",
    "\n",
    "# 6) Beispiel-Tabelle ausgeben, um zu sehen, welche Labels stehen\n",
    "print(\"\\nBeispiel False-Positives (TPP=2) in validation.tsv:\")\n",
    "print(fp_val[fp_val[\"TPP\"]==3].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1239d302-fb20-4bf5-b0fc-abb90a7e58cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132584/3013827268.py:27: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val  = pd.read_csv(val_path,  sep=\"\\t\", dtype={\"TRB_CDR3\":str,\"Epitope\":str,\"Binding\":int})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Erste 5 False-Positives in validation.tsv:\n",
      "          TRB_CDR3   Epitope  Binding  TPP\n",
      "  CATSELGGGLTDEQFF KLGGALQAK        1    3\n",
      "  CASSSRGTENTGELFF KLGGALQAK        1    3\n",
      "  CASSLGGFSSYNEQFF KLGGALQAK        1    3\n",
      "  CASSPPRQGANTEAFF KLGGALQAK        1    3\n",
      "CATSIRLRPWQGGDEQFF KLGGALQAK        1    3\n",
      "\n",
      "Erste 5 False-Positives in test.tsv:\n",
      "        TRB_CDR3    Epitope  Binding  TPP\n",
      "CASSQDGGSSYNEQFF  GLCTLVAML        1    3\n",
      "  CASSRTGSDYGYTF ELAGIGILTV        1    2\n",
      "CASSSPQGVSNTEAFF  GILGFVFTL        1    3\n",
      " CASSIMALGRSEAFF  KLGGALQAK        1    3\n",
      " CASSLDRGVNTEAFF  GILGFVFTL        1    2\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Basis-Pfade\n",
    "base_dir = \"../../../../data/splitted_datasets/allele/beta/new\"\n",
    "neg_dir  = os.path.join(base_dir, \"negatives\")\n",
    "val_path = os.path.join(base_dir, \"validation.tsv\")\n",
    "test_path= os.path.join(base_dir, \"test.tsv\")\n",
    "\n",
    "# 2) Alle TPP-Negatives einlesen und mapping pair -> TPP bauen\n",
    "all_negs = []\n",
    "for tpp in [2,3,4]:\n",
    "    files = sorted(glob.glob(os.path.join(neg_dir, f\"* _neg_tpp{tpp}.tsv\".replace(\" \",\"\"))))\n",
    "    for fn in files:\n",
    "        df = pd.read_csv(fn, sep=\"\\t\", dtype=str, usecols=[\"TRB_CDR3\",\"Epitope\"])\n",
    "        df[\"TPP\"] = tpp\n",
    "        all_negs.append(df)\n",
    "negs = pd.concat(all_negs, ignore_index=True)\n",
    "\n",
    "# Mapping (TCR,Epitope) -> TPP (bei Duplikaten wird der erste TPP behalten)\n",
    "pair_to_tpp = {\n",
    "    (r.TRB_CDR3, r.Epitope): r.TPP\n",
    "    for r in negs.itertuples()\n",
    "}\n",
    "\n",
    "# 3) validation & test laden\n",
    "val  = pd.read_csv(val_path,  sep=\"\\t\", dtype={\"TRB_CDR3\":str,\"Epitope\":str,\"Binding\":int})\n",
    "test = pd.read_csv(test_path, sep=\"\\t\", dtype={\"TRB_CDR3\":str,\"Epitope\":str,\"Binding\":int})\n",
    "\n",
    "# 4) Maske für False-Positives in den Original-DFs\n",
    "mask_val = val[\"Binding\"].eq(1) & val.apply(\n",
    "    lambda r: (r[\"TRB_CDR3\"], r[\"Epitope\"]) in pair_to_tpp, axis=1\n",
    ")\n",
    "mask_test = test[\"Binding\"].eq(1) & test.apply(\n",
    "    lambda r: (r[\"TRB_CDR3\"], r[\"Epitope\"]) in pair_to_tpp, axis=1\n",
    ")\n",
    "\n",
    "# 5) Subsets ziehen und TPP-Spalte hinzufügen\n",
    "fp_val_orig = val.loc[mask_val].copy()\n",
    "fp_val_orig[\"TPP\"]  = fp_val_orig.apply(\n",
    "    lambda r: pair_to_tpp[(r[\"TRB_CDR3\"], r[\"Epitope\"])], axis=1\n",
    ")\n",
    "\n",
    "fp_test_orig = test.loc[mask_test].copy()\n",
    "fp_test_orig[\"TPP\"] = fp_test_orig.apply(\n",
    "    lambda r: pair_to_tpp[(r[\"TRB_CDR3\"], r[\"Epitope\"])], axis=1\n",
    ")\n",
    "\n",
    "# 6) Ausgabe – im Original-Format, nur die False-Positives\n",
    "print(\"\\nErste 5 False-Positives in validation.tsv:\")\n",
    "print(\n",
    "    fp_val_orig\n",
    "    .head(5)[[\"TRB_CDR3\",\"Epitope\",\"Binding\",\"TPP\"]]\n",
    "    .to_string(index=False)\n",
    ")\n",
    "\n",
    "print(\"\\nErste 5 False-Positives in test.tsv:\")\n",
    "print(\n",
    "    fp_test_orig\n",
    "    .head(5)[[\"TRB_CDR3\",\"Epitope\",\"Binding\",\"TPP\"]]\n",
    "    .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1f3c2-b4c6-42b5-a7c0-872829973589",
   "metadata": {},
   "source": [
    "# CHECK COS_Siminlarity von generierten negativen zu positiven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fbac5ed-a3df-4f4e-8b50-814affaef606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation source categories: ['datasets' 'generated' '10X']\n",
      "Test source categories:       ['datasets' 'generated' '10X']\n"
     ]
    }
   ],
   "source": [
    "# prüfe, welche Source-Kategorien es gibt\n",
    "print(\"Validation source categories:\", val_df[\"source\"].unique())\n",
    "print(\"Test source categories:      \", test_df[\"source\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d4d4d-df77-4d75-8329-a27fd919bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation source categories: ['datasets' 'generated' '10X']\n",
      "Test       source categories: ['datasets' 'generated' '10X']\n",
      "→ Einzigartige positive Epitopes (datasets): 1598\n",
      "→ Generierte Negatives (Validation): 124790\n",
      "→ Generierte Negatives (Test):       45277\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# du hast val_df und test_df schon geladen, also überspring das Einlesen hier —\n",
    "# ansonsten hier anpassen:\n",
    "val_df  = pd.read_csv(\"../../../../data/splitted_datasets/allele/beta/new/validation.tsv\", sep=\"\\t\", dtype=str)\n",
    "test_df = pd.read_csv(\"../../../../data/splitted_datasets/allele/beta/new/test.tsv\",       sep=\"\\t\", dtype=str)\n",
    "\n",
    "# Binding als int casten\n",
    "val_df [\"Binding\"] = val_df[\"Binding\"].astype(int)\n",
    "test_df[\"Binding\"] = test_df[\"Binding\"].astype(int)\n",
    "\n",
    "# --- Alle positiven Epitope sammeln (source=='datasets') ---\n",
    "pos_val_epitopes  = val_df [ val_df [\"source\"]==\"datasets\" ][\"Epitope\"]\n",
    "pos_test_epitopes = test_df[ test_df[\"source\"]==\"datasets\" ][\"Epitope\"]\n",
    "pos_epitopes = pd.concat([pos_val_epitopes, pos_test_epitopes]).unique().tolist()\n",
    "print(f\"→ Einzigartige positive Epitopes (datasets): {len(pos_epitopes)}\")\n",
    "\n",
    "# --- Die generierten Negatives herausfiltern ---\n",
    "neg_val   = val_df [ (val_df [\"source\"]==\"generated\") & (val_df [\"Binding\"]==0) ].copy()\n",
    "neg_test  = test_df[ (test_df[\"source\"]==\"generated\") & (test_df[\"Binding\"]==0) ].copy()\n",
    "print(f\"→ Generierte Negatives (Validation): {len(neg_val)}\")\n",
    "print(f\"→ Generierte Negatives (Test):       {len(neg_test)}\")\n",
    "\n",
    "# --- Levenshtein-Funktionen ---\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    prev = list(range(len(s2)+1))\n",
    "    for i,c1 in enumerate(s1,1):\n",
    "        curr = [i]\n",
    "        for j,c2 in enumerate(s2,1):\n",
    "            ins = prev[j] + 1\n",
    "            dele= curr[j-1] + 1\n",
    "            sub = prev[j-1] + (c1 != c2)\n",
    "            curr.append(min(ins, dele, sub))\n",
    "        prev = curr\n",
    "    return prev[-1]\n",
    "\n",
    "def similarity_ratio(s1, s2):\n",
    "    d = levenshtein_distance(s1, s2)\n",
    "    return (len(s1) + len(s2) - d) / (len(s1) + len(s2))\n",
    "\n",
    "# Max-Similarity gegen alle positiven Epitopes\n",
    "def max_sim_to_positives(seq):\n",
    "    # Achtung: das ist O(len(pos_epitopes)), kann langsam sein\n",
    "    return max(similarity_ratio(seq, pos) for pos in pos_epitopes)\n",
    "\n",
    "# wende an\n",
    "neg_val  [\"max_pos_sim\"] = neg_val [\"Epitope\"].apply(max_sim_to_positives)\n",
    "neg_test [\"max_pos_sim\"] = neg_test[\"Epitope\"].apply(max_sim_to_positives)\n",
    "\n",
    "# Kurze Zusammenfassung + Histogramm\n",
    "for df, name in [(neg_val, \"Validation\"), (neg_test, \"Test\")]:\n",
    "    print(f\"\\n{name} – Ähnlichkeits-Statistik generierte Negatives → Positives\")\n",
    "    print(df[\"max_pos_sim\"].describe())\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(df[\"max_pos_sim\"], bins=30, alpha=0.7)\n",
    "    plt.axvline(0.75, color=\"red\", linestyle=\"--\", label=\"Beispiel-Threshold 0.75\")\n",
    "    plt.title(f\"Max. Levenshtein-Ratio generated Negatives → Positives ({name})\")\n",
    "    plt.xlabel(\"Similarity-Ratio\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Auf Wunsch: pro TPP aufschlüsseln\n",
    "if \"task_predicted\" in neg_val.columns:\n",
    "    for tpp in sorted(neg_val[\"task_predicted\"].unique()):\n",
    "        sub = neg_val[neg_val[\"task_predicted\"]==tpp]\n",
    "        print(f\"\\n{name} – TPP={tpp}: n={len(sub)}, max_pos_sim median={sub['max_pos_sim'].median():.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4e340-ea97-4232-9e3e-871d9ee9ec65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
