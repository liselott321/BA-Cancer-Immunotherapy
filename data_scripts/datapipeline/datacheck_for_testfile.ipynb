{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d8838a-8ea6-4e2f-862a-e9fcede10484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c85211-ea01-468f-a542-716482a9d87a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vdjdb_beta_read_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVDJdb_beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: vdjdb_beta_read_path,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMcPAS_beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: mcpastcr_beta_read_path,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIEDB_beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: iedb_beta_read_path,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpMTnet_beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: pmtnet_beta_read_path,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVDJdb_paired\u001b[39m\u001b[38;5;124m\"\u001b[39m: vdjdb_paired_read_path,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMcPAS_paired\u001b[39m\u001b[38;5;124m\"\u001b[39m: mcpastcr_paired_read_path,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIEDB_paired\u001b[39m\u001b[38;5;124m\"\u001b[39m: iedb_paired_read_path\n\u001b[1;32m      9\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vdjdb_beta_read_path' is not defined"
     ]
    }
   ],
   "source": [
    "file_paths = {\n",
    "    \"VDJdb_beta\": vdjdb_beta_read_path,\n",
    "    \"McPAS_beta\": mcpastcr_beta_read_path,\n",
    "    \"IEDB_beta\": iedb_beta_read_path,\n",
    "    \"pMTnet_beta\": pmtnet_beta_read_path,\n",
    "    \"VDJdb_paired\": vdjdb_paired_read_path,\n",
    "    \"McPAS_paired\": mcpastcr_paired_read_path,\n",
    "    \"IEDB_paired\": iedb_paired_read_path\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387222f1-411e-46bf-91da-afd9682cb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to classify tasks based on TCR and epitope presence\n",
    "def calculate_task(row, known_epitopes, known_tcr, paired=False):\n",
    "    if paired:\n",
    "        tra_cdr3 = str(row['TRA_CDR3']) if pd.notna(row['TRA_CDR3']) else ''\n",
    "        trb_cdr3 = str(row['TRB_CDR3']) if pd.notna(row['TRB_CDR3']) else ''\n",
    "        tcr = tra_cdr3 + '_' + trb_cdr3\n",
    "    else:\n",
    "        tcr = row['TRB_CDR3']\n",
    "    \n",
    "    epitope_exists = row['Epitope'] in known_epitopes\n",
    "    cdr3_exists = tcr in known_tcr\n",
    "    \n",
    "    if epitope_exists and cdr3_exists:\n",
    "        return 'TPP1'\n",
    "    elif epitope_exists and not cdr3_exists:\n",
    "        return 'TPP2'\n",
    "    elif not epitope_exists and not cdr3_exists:\n",
    "        return 'TPP3'\n",
    "    elif not epitope_exists and cdr3_exists:\n",
    "        return 'TPP4'\n",
    "    raise Exception(\"Something seems wrong\")\n",
    "\n",
    "# Placeholder for the data\n",
    "all_data = {}\n",
    "\n",
    "# Load and prepare data\n",
    "for file_name, path in file_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "\n",
    "            # Create tcr_key\n",
    "            if \"TRA_CDR3\" in df.columns:\n",
    "                paired = True\n",
    "                df[\"tcr_key\"] = df[\"TRA_CDR3\"].astype(str) + '_' + df[\"TRB_CDR3\"]\n",
    "            else:\n",
    "                paired = False\n",
    "                df[\"tcr_key\"] = df[\"TRB_CDR3\"]\n",
    "\n",
    "            all_data[file_name] = df\n",
    "            print(f\"{file_name} geladen mit {len(df)} Einträgen.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Datei nicht gefunden: {file_name}\")\n",
    "\n",
    "# Analyse: Classify TPP tasks\n",
    "for test_file_name, test_df in all_data.items():\n",
    "    # Define training data (excluding the current test set)\n",
    "    train_df = pd.concat([data for name, data in all_data.items() if name != test_file_name]).drop_duplicates()\n",
    "\n",
    "    seen_tcrs = set(train_df[\"tcr_key\"])\n",
    "    seen_epitopes = set(train_df[\"Epitope\"])\n",
    "\n",
    "    # Determine if it's a paired dataset\n",
    "    paired = \"TRA_CDR3\" in test_df.columns\n",
    "\n",
    "    # Apply classification\n",
    "    test_df['task'] = test_df.apply(lambda row: calculate_task(row, seen_epitopes, seen_tcrs, paired=paired), axis=1)\n",
    "\n",
    "    # Count TPP3 pairs\n",
    "    tpp3_pairs = (test_df['task'] == 'TPP3').sum()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"\\n**Wenn {test_file_name} als Testset verwendet wird:**\")\n",
    "    print(f\"  - TPP3-Paare im Testset: {tpp3_pairs}\")\n",
    "    print(f\"  - Gesamt Test-Paare: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05921807-1831-40e1-8fec-66332de4a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to classify TPP tasks based on TCR and epitope presence\n",
    "def calculate_task(row, known_epitopes, known_tcr, paired=False):\n",
    "    if paired:\n",
    "        tra_cdr3 = str(row['TRA_CDR3']) if pd.notna(row['TRA_CDR3']) else ''\n",
    "        trb_cdr3 = str(row['TRB_CDR3']) if pd.notna(row['TRB_CDR3']) else ''\n",
    "        tcr = tra_cdr3 + '_' + trb_cdr3\n",
    "    else:\n",
    "        tcr = row['TRB_CDR3']\n",
    "    \n",
    "    epitope_exists = row['Epitope'] in known_epitopes\n",
    "    cdr3_exists = tcr in known_tcr\n",
    "    \n",
    "    if epitope_exists and cdr3_exists:\n",
    "        return 'TPP1'\n",
    "    elif epitope_exists and not cdr3_exists:\n",
    "        return 'TPP2'\n",
    "    elif not epitope_exists and not cdr3_exists:\n",
    "        return 'TPP3'\n",
    "    elif not epitope_exists and cdr3_exists:\n",
    "        return 'TPP4'\n",
    "    raise Exception(\"Something seems wrong\")\n",
    "\n",
    "\n",
    "# Load train and validation data\n",
    "train_file = f'{pipeline_data_splitted}/{precision}/beta/train.tsv'\n",
    "validation_file = f'{pipeline_data_splitted}/{precision}/beta/validation.tsv'\n",
    "test_file = f'{pipeline_data_splitted}/{precision}/beta/test.tsv'\n",
    "vdjdb_test_file = vdjdb_beta_read_path  # Path to the VDJdb test dataset\n",
    "\n",
    "df_train = pd.read_csv(train_file, sep='\\t')\n",
    "df_validation = pd.read_csv(validation_file, sep='\\t')\n",
    "df_test = pd.read_csv(test_file, sep='\\t')\n",
    "\n",
    "# Combine train and validation datasets\n",
    "trainval_df = pd.concat([df_train, df_validation], ignore_index=True)\n",
    "\n",
    "# Load VDJdb test data\n",
    "vdjdb_df = pd.read_csv(vdjdb_test_file, sep='\\t') #f'{pipeline_data_cleaned}/VDJdb/VDJdb_cleaned_data_beta.tsv'\n",
    "\n",
    "# Create tcr_key and clean data\n",
    "trainval_df[\"tcr_key\"] = trainval_df[\"TRB_CDR3\"].astype(str).str.strip()\n",
    "trainval_df[\"Epitope\"] = trainval_df[\"Epitope\"].astype(str).str.strip()\n",
    "\n",
    "vdjdb_df[\"tcr_key\"] = vdjdb_df[\"TRB_CDR3\"].astype(str).str.strip()\n",
    "vdjdb_df[\"Epitope\"] = vdjdb_df[\"Epitope\"].astype(str).str.strip()\n",
    "\n",
    "df_test[\"tcr_key\"] = df_test[\"TRB_CDR3\"].astype(str).str.strip()\n",
    "df_test[\"Epitope\"] = df_test[\"Epitope\"].astype(str).str.strip()\n",
    "\n",
    "# Generate lookup sets for fast comparison\n",
    "seen_tcrs = set(trainval_df[\"tcr_key\"])\n",
    "seen_epitopes = set(trainval_df[\"Epitope\"])\n",
    "\n",
    "# Apply classification to the VDJdb dataset\n",
    "vdjdb_df['task'] = vdjdb_df.apply(lambda row: calculate_task(row, seen_epitopes, seen_tcrs, paired=False), axis=1)\n",
    "\n",
    "# Count TPP3 pairs\n",
    "tpp3_pairs = (vdjdb_df['task'] == 'TPP3').sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\n**TPP Analysis for VDJdb with Train + Validation**\")\n",
    "print(f\"  - TPP3-Paare im Testset: {tpp3_pairs}\")\n",
    "print(f\"  - Gesamt Test-Paare: {len(vdjdb_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f78f3-8952-4f5d-b888-d1e88adef63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original TPP3 Paare vor dem Hinzufügen negativer Daten\n",
    "original_tpp3 = vdjdb_df[(vdjdb_df['task'] == 'TPP3')][[\"tcr_key\", \"Epitope\"]]\n",
    "\n",
    "# Prüfen, ob diese TPP3-Paare in den negativen Daten des Train/Validation-Sets auftauchen\n",
    "negative_in_trainval = trainval_df[trainval_df['Binding'] == 0][[\"tcr_key\", \"Epitope\"]]\n",
    "\n",
    "# Vergleichen\n",
    "tpp3_now_seen = original_tpp3.merge(negative_in_trainval, on=[\"tcr_key\", \"Epitope\"], how=\"inner\")\n",
    "\n",
    "print(f\"Anzahl der ursprünglichen TPP3-Paare, die jetzt in negativen Daten des Train/Validation-Sets vorkommen: {len(tpp3_now_seen)}\")\n",
    "\n",
    "# Zeige einige Beispiele\n",
    "if not tpp3_now_seen.empty:\n",
    "    print(tpp3_now_seen.head(10))\n",
    "else:\n",
    "    print(\"Keine der ursprünglichen TPP3-Paare wurden in den negativen Daten gefunden.\")\n",
    "\n",
    "# Prüfen, ob die ursprünglichen TPP3-Paare in den positiven Daten des Train/Validation-Sets enthalten sind\n",
    "positive_in_trainval = trainval_df[trainval_df['Binding'] == 1][[\"tcr_key\", \"Epitope\"]]\n",
    "\n",
    "# Vergleich durchführen\n",
    "tpp3_in_pos_trainval = original_tpp3.merge(positive_in_trainval, on=[\"tcr_key\", \"Epitope\"], how=\"inner\")\n",
    "\n",
    "print(f\"Anzahl der ursprünglichen TPP3-Paare, die jetzt in den positiven Train/Validation-Daten vorkommen: {len(tpp3_in_pos_trainval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf564727-b7ff-4151-aa61-917733fc5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the concatenated files\n",
    "concatenated_beta_file = f'{pipeline_data_concatenated}/{precision}/beta_concatenated.tsv'\n",
    "vdjdb_test_file = vdjdb_beta_read_path  # Path to the original VDJdb data\n",
    "\n",
    "# Load concatenated beta data\n",
    "concatenated_beta_df = pd.read_csv(concatenated_beta_file, sep='\\t')\n",
    "\n",
    "# Load original VDJdb dataset\n",
    "vdjdb_df = pd.read_csv(vdjdb_test_file, sep='\\t')\n",
    "\n",
    "# Create tcr_key and clean data\n",
    "concatenated_beta_df[\"tcr_key\"] = concatenated_beta_df[\"TRB_CDR3\"].astype(str).str.strip()\n",
    "concatenated_beta_df[\"Epitope\"] = concatenated_beta_df[\"Epitope\"].astype(str).str.strip()\n",
    "\n",
    "vdjdb_df[\"tcr_key\"] = vdjdb_df[\"TRB_CDR3\"].astype(str).str.strip()\n",
    "vdjdb_df[\"Epitope\"] = vdjdb_df[\"Epitope\"].astype(str).str.strip()\n",
    "\n",
    "# Generate lookup sets from concatenated data\n",
    "seen_tcrs = set(concatenated_beta_df[\"tcr_key\"])\n",
    "seen_epitopes = set(concatenated_beta_df[\"Epitope\"])\n",
    "\n",
    "# Apply classification to the VDJdb dataset\n",
    "vdjdb_df['task'] = vdjdb_df.apply(lambda row: calculate_task(row, seen_epitopes, seen_tcrs, paired=False), axis=1)\n",
    "\n",
    "# Count TPP3 pairs\n",
    "tpp3_pairs = (vdjdb_df['task'] == 'TPP3').sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\n**TPP Analysis for VDJdb with Concatenated Beta Data**\")\n",
    "print(f\"  - TPP3-Paare im Testset: {tpp3_pairs}\")\n",
    "print(f\"  - Gesamt Test-Paare: {len(vdjdb_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2cae14-7f9e-4202-898e-1c099bc786f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Daten aus Train/Validation\n",
    "negative_in_trainval = trainval_df[trainval_df['Binding'] == 0][[\"tcr_key\", \"Epitope\"]]\n",
    "negative_in_test = df_test[df_test['Binding'] == 0][[\"tcr_key\", \"Epitope\"]]\n",
    "\n",
    "# Prüfen, ob VDJdb-TCRs in den negativen Daten vorkommen\n",
    "tpp3_tcr_overlap = vdjdb_df[~vdjdb_df['tcr_key'].isin(seen_tcrs) & vdjdb_df['tcr_key'].isin(set(negative_in_trainval['tcr_key']))]\n",
    "tpp3_tcr_overlap_test = vdjdb_df[~vdjdb_df['tcr_key'].isin(seen_tcrs) & vdjdb_df['tcr_key'].isin(set(negative_in_test['tcr_key']))]\n",
    "\n",
    "# Prüfen, ob VDJdb-Epitope in den negativen Daten vorkommen\n",
    "tpp3_epitope_overlap = vdjdb_df[~vdjdb_df['Epitope'].isin(seen_epitopes) & vdjdb_df['Epitope'].isin(set(negative_in_trainval['Epitope']))]\n",
    "tpp3_epitope_overlap_test = vdjdb_df[~vdjdb_df['Epitope'].isin(seen_epitopes) & vdjdb_df['Epitope'].isin(set(negative_in_trainval['Epitope']))]\n",
    "\n",
    "print(f\"VDJdb TPP3-Paare, deren TCR jetzt in negativen Daten aus Train/Validation vorkommt: {len(tpp3_tcr_overlap)}\")\n",
    "print(f\"VDJdb TPP3-Paare, deren Epitope jetzt in negativen Daten aus Train/Validation vorkommt: {len(tpp3_epitope_overlap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545d6e5-a0de-494e-b378-0183b28d879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VDJdb test data\n",
    "vdjdb_df = pd.read_csv(vdjdb_test_file, sep='\\t') #f'{pipeline_data_cleaned}/VDJdb/VDJdb_cleaned_data_beta.tsv'\n",
    "vdjdb_df[\"tcr_key\"] = vdjdb_df[\"TRB_CDR3\"].astype(str).str.strip()\n",
    "vdjdb_df[\"Epitope\"] = vdjdb_df[\"Epitope\"].astype(str).str.strip()\n",
    "vdjdb_df['task'] = vdjdb_df.apply(lambda row: calculate_task(row, seen_epitopes, seen_tcrs, paired=False), axis=1)\n",
    "# TPP3-Paare aus dem vdjdb File\n",
    "original_tpp3 = vdjdb_df[vdjdb_df['task'] == 'TPP3'][[\"tcr_key\", \"Epitope\"]]\n",
    "# Prüfen, ob diese TPP3-Paare im finalen Testset noch vorhanden sind\n",
    "tpp3_in_test = original_tpp3.merge(df_test, on=[\"tcr_key\", \"Epitope\"], how=\"inner\")\n",
    "print(f\"TPP3-Paare von cleaned data, die noch im finalen Testset vorhanden sind: {len(tpp3_in_test)}\")\n",
    "\n",
    "# Prüfen, welche TPP-Klasse die TPP3-Paare jetzt im finalen Testset haben\n",
    "tpp3_in_test_with_task = original_tpp3.merge(df_test, on=[\"tcr_key\", \"Epitope\"], how=\"inner\")\n",
    "\n",
    "# Zählen, wie viele der ursprünglichen TPP3-Paare jetzt welcher TPP-Klasse zugeordnet wurden\n",
    "task_distribution = tpp3_in_test_with_task['task'].value_counts()\n",
    "\n",
    "print(f\"\\n✅ TPP3-Paare von cleaned data, die noch im finalen Testset vorhanden sind: {len(tpp3_in_test_with_task)}\")\n",
    "print(f\"🔄 Aktuelle TPP-Klassen dieser Paare im finalen Testset:\")\n",
    "print(task_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddd1bd-445f-4e27-8894-79fa3345c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfen, ob TCRs in negativen Daten des Train/Validation-Sets vorkommen\n",
    "tcrs_in_negative_trainval = original_tpp3[original_tpp3['tcr_key'].isin(trainval_df[trainval_df['Binding'] == 0]['tcr_key'])]\n",
    "\n",
    "# Prüfen, ob Epitope in negativen Daten des Train/Validation-Sets vorkommen\n",
    "epitopes_in_negative_trainval = original_tpp3[original_tpp3['Epitope'].isin(trainval_df[trainval_df['Binding'] == 0]['Epitope'])]\n",
    "\n",
    "print(f\"⚠️ Anzahl TPP3-Paare, deren TCRs in den negativen Train/Validation-Daten vorkommen: {len(tcrs_in_negative_trainval)}\")\n",
    "print(f\"⚠️ Anzahl TPP3-Paare, deren Epitope in den negativen Train/Validation-Daten vorkommen: {len(epitopes_in_negative_trainval)}\")\n",
    "\n",
    "# Prüfen, ob TCRs in den positiven Daten vorkommen\n",
    "tcrs_in_positive_trainval = original_tpp3[original_tpp3['tcr_key'].isin(trainval_df[trainval_df['Binding'] == 1]['tcr_key'])]\n",
    "\n",
    "# Prüfen, ob Epitope in den positiven Daten vorkommen\n",
    "epitopes_in_positive_trainval = original_tpp3[original_tpp3['Epitope'].isin(trainval_df[trainval_df['Binding'] == 1]['Epitope'])]\n",
    "\n",
    "print(f\"✅ Anzahl TPP3-Paare, deren TCRs in den positiven Train/Validation-Daten vorkommen: {len(tcrs_in_positive_trainval)}\")\n",
    "print(f\"✅ Anzahl TPP3-Paare, deren Epitope in den positiven Train/Validation-Daten vorkommen: {len(epitopes_in_positive_trainval)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd32847-ee64-4455-b066-968342ab958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130246/566910811.py:17: DtypeWarning: Columns (7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binding  non-binding  binding\n",
      "split                        \n",
      "test             695      293\n",
      "train            937      981\n",
      "val             1270     1443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130246/566910811.py:17: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "pipeline_data = '../../../../data'\n",
    "pipeline_data_plain = f'{pipeline_data}/plain_datasets'\n",
    "pipeline_data_cleaned = f'{pipeline_data}/cleaned_datasets'\n",
    "pipeline_data_concatenated = f'{pipeline_data}/concatenated_datasets'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "pipeline_data_temp_bucket = f'{pipeline_data}/temp'\n",
    "precision = \"allele\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_file = f'{pipeline_data_splitted}/{precision}/beta/new/train.tsv'\n",
    "validation_file = f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv'\n",
    "test_file = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "# Helper-Funktion\n",
    "def count_unique_epitopes(file_path, split_name):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df['split'] = split_name\n",
    "    return df.groupby('Binding')['Epitope'].nunique().reset_index(name='unique_epitopes').assign(split=split_name)\n",
    "\n",
    "# Ergebnisse sammeln\n",
    "results = pd.concat([\n",
    "    count_unique_epitopes(train_file, 'train'),\n",
    "    count_unique_epitopes(validation_file, 'val'),\n",
    "    count_unique_epitopes(test_file, 'test')\n",
    "])\n",
    "\n",
    "# Umstrukturieren für bessere Übersicht\n",
    "summary = results.pivot(index='split', columns='Binding', values='unique_epitopes').rename(columns={0: 'non-binding', 1: 'binding'}).fillna(0).astype(int)\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d925fd-90c1-481b-8391-4f33fb2a6960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130246/3538348797.py:4: DtypeWarning: Columns (7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_val = pd.read_csv(validation_file, sep='\\t')\n",
      "/tmp/ipykernel_130246/3538348797.py:5: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_file, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negative Verteilung in Validation:\n",
      "   task     source  count\n",
      "0  TPP1        10X  78758\n",
      "1  TPP1  generated  73250\n",
      "2  TPP2        10X     27\n",
      "3  TPP2  generated  46648\n",
      "4  TPP3  generated   6976\n",
      "5  TPP4  generated    194\n",
      "\n",
      "Negative Verteilung in Test:\n",
      "   task     source  count\n",
      "0  TPP1        10X  12548\n",
      "1  TPP1  generated  22265\n",
      "2  TPP2        10X     66\n",
      "3  TPP2  generated  20935\n",
      "4  TPP3  generated    881\n",
      "5  TPP4  generated    281\n",
      "\n",
      "Validation Pos/Neg Verhältnisse pro Task:\n",
      "Binding  binder_ratio  nonbinder_ratio\n",
      "task                                  \n",
      "TPP1         0.077979         0.922021\n",
      "TPP2         0.242805         0.757195\n",
      "TPP3         0.200000         0.800000\n",
      "TPP4         0.160173         0.839827\n",
      "\n",
      "Benötigte Anzahl Negativer im Test pro Task (um Validation-Verhältnis zu matchen):\n",
      "TPP1: 14224 Negative\n",
      "TPP2: 23676 Negative\n",
      "TPP3: 1032 Negative\n",
      "TPP4: 273 Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Einlesen\n",
    "df_val = pd.read_csv(validation_file, sep='\\t')\n",
    "df_test = pd.read_csv(test_file, sep='\\t')\n",
    "\n",
    "# Schritt 1: Übersicht pro Task & Source (nur Negative)\n",
    "def negative_overview(df, name):\n",
    "    negatives = df[df['Binding'] == 0]\n",
    "    overview = negatives.groupby(['task', 'source']).size().reset_index(name='count')\n",
    "    print(f\"\\nNegative Verteilung in {name}:\")\n",
    "    print(overview)\n",
    "    return overview\n",
    "\n",
    "neg_overview_val = negative_overview(df_val, 'Validation')\n",
    "neg_overview_test = negative_overview(df_test, 'Test')\n",
    "\n",
    "# Schritt 2: Verhältnis Pos/Neg in Validation bestimmen\n",
    "val_stats = df_val.groupby('task')['Binding'].value_counts().unstack().fillna(0)\n",
    "val_stats['binder_ratio'] = val_stats[1] / (val_stats[0] + val_stats[1])\n",
    "val_stats['nonbinder_ratio'] = val_stats[0] / (val_stats[0] + val_stats[1])\n",
    "print(\"\\nValidation Pos/Neg Verhältnisse pro Task:\")\n",
    "print(val_stats[['binder_ratio', 'nonbinder_ratio']])\n",
    "\n",
    "# Schritt 3: Für Test - neue gewünschte Anzahl an Negativen berechnen\n",
    "test_stats = df_test.groupby('task')['Binding'].value_counts().unstack().fillna(0)\n",
    "needed_negatives = {}\n",
    "\n",
    "for task in test_stats.index:\n",
    "    num_positive = test_stats.loc[task, 1] if 1 in test_stats.columns else 0\n",
    "    binder_ratio_val = val_stats.loc[task, 'binder_ratio']\n",
    "    if binder_ratio_val > 0:\n",
    "        needed_total = num_positive / binder_ratio_val\n",
    "        needed_negative = int(round(needed_total - num_positive))\n",
    "        needed_negatives[task] = needed_negative\n",
    "    else:\n",
    "        needed_negatives[task] = 0\n",
    "\n",
    "print(\"\\nBenötigte Anzahl Negativer im Test pro Task (um Validation-Verhältnis zu matchen):\")\n",
    "for task, needed in needed_negatives.items():\n",
    "    print(f\"{task}: {needed} Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e67fe2-f4d2-4cb0-976c-48e68d8ae2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f'{pipeline_data_splitted}/{precision}/beta/train.tsv',\n",
    "        \"test\": f'{pipeline_data_splitted}/{precision}/beta/test.tsv',\n",
    "        \"validation\": f'{pipeline_data_splitted}/{precision}/beta/validation.tsv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb754c7d-1d70-4f0f-be91-6000ea549e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130246/272919926.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs[split] = pd.read_csv(path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BETA_ALLELE - TRAIN OVERVIEW ---\n",
      "Total rows: 749667\n",
      "Unique TCRs: 187298\n",
      "Unique Epitopes: 1165\n",
      "Unique (TCR, Epitope) pairs: 749667\n",
      "Binding distribution:\n",
      " Binding\n",
      "0    0.831308\n",
      "1    0.168692\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- BETA_ALLELE - TRAIN DUPLICATE & INCONSISTENCY CHECK ---\n",
      "Exact duplicates: 0\n",
      "Exact duplicates by Binding:\n",
      "Series([], dtype: int64)\n",
      "Inconsistent (TCR, Epitope) label pairs (0 and 1): 0\n",
      "\n",
      "--- BETA_ALLELE - VALIDATION OVERVIEW ---\n",
      "Total rows: 186951\n",
      "Unique TCRs: 65331\n",
      "Unique Epitopes: 1468\n",
      "Unique (TCR, Epitope) pairs: 186881\n",
      "Binding distribution:\n",
      " Binding\n",
      "0    0.841648\n",
      "1    0.158352\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- BETA_ALLELE - VALIDATION DUPLICATE & INCONSISTENCY CHECK ---\n",
      "Exact duplicates: 70\n",
      "Exact duplicates by Binding:\n",
      "Binding\n",
      "0    26\n",
      "1    44\n",
      "dtype: int64\n",
      "Inconsistent (TCR, Epitope) label pairs (0 and 1): 0\n",
      "\n",
      "--- BETA_ALLELE - TEST OVERVIEW ---\n",
      "Total rows: 56460\n",
      "Unique TCRs: 14322\n",
      "Unique Epitopes: 385\n",
      "Unique (TCR, Epitope) pairs: 56432\n",
      "Binding distribution:\n",
      " Binding\n",
      "0    0.840135\n",
      "1    0.159865\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- BETA_ALLELE - TEST DUPLICATE & INCONSISTENCY CHECK ---\n",
      "Exact duplicates: 28\n",
      "Exact duplicates by Binding:\n",
      "Binding\n",
      "0    10\n",
      "1    18\n",
      "dtype: int64\n",
      "Inconsistent (TCR, Epitope) label pairs (0 and 1): 0\n",
      "\n",
      "--- OVERLAP BETWEEN TRAIN AND TEST ---\n",
      "Shared (TCR, Epitope) pairs: 0\n",
      "\n",
      "--- OVERLAP BETWEEN TRAIN AND VALIDATION ---\n",
      "Shared (TCR, Epitope) pairs: 0\n",
      "\n",
      "--- OVERLAP BETWEEN VALIDATION AND TEST ---\n",
      "Shared (TCR, Epitope) pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Analyse-Funktionen (ergänzt mit Visualisierung & weiteren Checks)\n",
    "\n",
    "def load_datasets(paths):\n",
    "    dfs = {}\n",
    "    for split, path in paths.items():\n",
    "        if os.path.exists(path):\n",
    "            dfs[split] = pd.read_csv(path, sep=\"\\t\")\n",
    "        else:\n",
    "            dfs[split] = None\n",
    "    return dfs\n",
    "\n",
    "def dataset_overview(df, name):\n",
    "    print(f\"\\n--- {name.upper()} OVERVIEW ---\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(\"Unique TCRs:\", df[\"TRB_CDR3\"].nunique())\n",
    "    print(\"Unique Epitopes:\", df[\"Epitope\"].nunique())\n",
    "    print(\"Unique (TCR, Epitope) pairs:\", df[[\"TRB_CDR3\", \"Epitope\"]].drop_duplicates().shape[0])\n",
    "    print(\"Binding distribution:\\n\", df[\"Binding\"].value_counts(normalize=True))\n",
    "\n",
    "def check_duplicates_and_inconsistencies(df, name):\n",
    "    print(f\"\\n--- {name.upper()} DUPLICATE & INCONSISTENCY CHECK ---\")\n",
    "    # Exakte Duplikate zählen\n",
    "    exact_duplicates = df.duplicated(subset=[\"TRB_CDR3\", \"Epitope\", \"Binding\"])\n",
    "    total_duplicates = exact_duplicates.sum()\n",
    "    print(f\"Exact duplicates: {total_duplicates}\")\n",
    "\n",
    "    # Aufspalten nach Binding-Klasse\n",
    "    dup_by_binding = df[exact_duplicates].groupby(\"Binding\").size()\n",
    "    print(\"Exact duplicates by Binding:\")\n",
    "    print(dup_by_binding)\n",
    "\n",
    "    # Inkonsistente Labels erkennen\n",
    "    inconsistent_labels = df.groupby([\"TRB_CDR3\", \"Epitope\"])[\"Binding\"].nunique()\n",
    "    inconsistent_count = (inconsistent_labels == 2).sum()\n",
    "    print(\"Inconsistent (TCR, Epitope) label pairs (0 and 1):\", inconsistent_count)\n",
    "\n",
    "def check_overlap(df1, df2, name1, name2):\n",
    "    set1 = set(zip(df1[\"TRB_CDR3\"], df1[\"Epitope\"]))\n",
    "    set2 = set(zip(df2[\"TRB_CDR3\"], df2[\"Epitope\"]))\n",
    "    overlap = set1 & set2\n",
    "    print(f\"\\n--- OVERLAP BETWEEN {name1.upper()} AND {name2.upper()} ---\")\n",
    "    print(f\"Shared (TCR, Epitope) pairs: {len(overlap)}\")\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f'{pipeline_data_splitted}/{precision}/beta/new/train.tsv',\n",
    "        \"test\": f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv',\n",
    "        \"validation\": f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv'\n",
    "    }\n",
    "}\n",
    "\n",
    "data = {key: load_datasets(paths) for key, paths in datasets.items()}\n",
    "\n",
    "for dataset_name, splits in data.items():\n",
    "    train, val, test = splits[\"train\"], splits[\"validation\"], splits[\"test\"]\n",
    "\n",
    "    if train is not None:\n",
    "        dataset_overview(train, f\"{dataset_name} - train\")\n",
    "        check_duplicates_and_inconsistencies(train, f\"{dataset_name} - train\")\n",
    "    \n",
    "    if val is not None:\n",
    "        dataset_overview(val, f\"{dataset_name} - validation\")\n",
    "        check_duplicates_and_inconsistencies(val, f\"{dataset_name} - validation\")\n",
    "        \n",
    "    if test is not None:\n",
    "        dataset_overview(test, f\"{dataset_name} - test\")\n",
    "        check_duplicates_and_inconsistencies(test, f\"{dataset_name} - test\")\n",
    "       \n",
    "    if train is not None and test is not None:\n",
    "        check_overlap(train, test, \"train\", \"test\")\n",
    "    if train is not None and val is not None:\n",
    "        check_overlap(train, val, \"train\", \"validation\")\n",
    "    if val is not None and test is not None:\n",
    "        check_overlap(val, test, \"validation\", \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e2b999d-b136-4b8c-abd6-8a04f0aea290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Duplikate nach Task in Validation:\n",
      "task\n",
      "TPP1    88\n",
      "TPP3    52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📊 Duplikate nach Task in Test:\n",
      "task\n",
      "TPP3    28\n",
      "TPP1    25\n",
      "TPP4     2\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130246/142913063.py:7: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(val_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Lade Validation und Test ---\n",
    "val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "\n",
    "val_df = pd.read_csv(val_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# --- Duplikate auf (\"TCR_CDR3\", \"Epitope\", \"Binding\") finden ---\n",
    "val_duplicates = val_df[val_df.duplicated(subset=[\"TRB_CDR3\", \"Epitope\", \"Binding\"], keep=False)]\n",
    "test_duplicates = test_df[test_df.duplicated(subset=[\"TRB_CDR3\", \"Epitope\", \"Binding\"], keep=False)]\n",
    "\n",
    "# --- Gruppieren nach Task ---\n",
    "print(\"\\n📊 Duplikate nach Task in Validation:\")\n",
    "print(val_duplicates['task'].value_counts())\n",
    "\n",
    "print(\"\\n📊 Duplikate nach Task in Test:\")\n",
    "print(test_duplicates['task'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e51ca7e-24e7-4f00-a206-be9e5c4d9a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Alte TPP2 im Test: 47912\n",
      "✅ Neue TPP2 im Test nach Entfernen von Duplikaten: 47912\n",
      "✅ Testset erfolgreich aktualisiert (TPP2-Duplikate entfernt).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Test laden ---\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# --- Nur TPP2 herausfiltern ---\n",
    "tpp2_test = test_df[test_df['task'] == 'TPP2']\n",
    "other_test = test_df[test_df['task'] != 'TPP2']\n",
    "\n",
    "# --- Duplikate innerhalb TPP2 finden ---\n",
    "tpp2_duplicates = tpp2_test.duplicated(subset=[\"TRB_CDR3\", \"Epitope\", \"Binding\"], keep='first')\n",
    "\n",
    "# --- Nur einmalige TPP2 behalten (erste Vorkommen) ---\n",
    "tpp2_test_cleaned = tpp2_test[~tpp2_duplicates]\n",
    "\n",
    "print(f\"✅ Alte TPP2 im Test: {len(tpp2_test)}\")\n",
    "print(f\"✅ Neue TPP2 im Test nach Entfernen von Duplikaten: {len(tpp2_test_cleaned)}\")\n",
    "\n",
    "# --- Neues Testset zusammensetzen ---\n",
    "test_df_cleaned = pd.concat([other_test, tpp2_test_cleaned], ignore_index=True)\n",
    "\n",
    "# --- Speichern ---\n",
    "test_df_cleaned.to_csv(test_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Testset erfolgreich aktualisiert (TPP2-Duplikate entfernt).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36f21040-e119-4180-bc45-14bdde218a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130246/2665809561.py:5: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(val_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Alte TPP2 in Validation: 102377\n",
      "✅ Neue TPP2 in Validation nach Entfernen von Duplikaten: 102377\n",
      "✅ Validation-Set erfolgreich aktualisiert (TPP2-Duplikate entfernt).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Validation laden ---\n",
    "val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "val_df = pd.read_csv(val_path, sep='\\t')\n",
    "\n",
    "# --- Nur TPP2 herausfiltern ---\n",
    "tpp2_val = val_df[val_df['task'] == 'TPP2']\n",
    "other_val = val_df[val_df['task'] != 'TPP2']\n",
    "\n",
    "# --- Duplikate innerhalb TPP2 finden ---\n",
    "tpp2_duplicates_val = tpp2_val.duplicated(subset=[\"TRB_CDR3\", \"Epitope\", \"Binding\"], keep='first')\n",
    "\n",
    "# --- Nur einmalige TPP2 behalten (erste Vorkommen) ---\n",
    "tpp2_val_cleaned = tpp2_val[~tpp2_duplicates_val]\n",
    "\n",
    "print(f\"✅ Alte TPP2 in Validation: {len(tpp2_val)}\")\n",
    "print(f\"✅ Neue TPP2 in Validation nach Entfernen von Duplikaten: {len(tpp2_val_cleaned)}\")\n",
    "\n",
    "# --- Neues Validation-Set zusammensetzen ---\n",
    "val_df_cleaned = pd.concat([other_val, tpp2_val_cleaned], ignore_index=True)\n",
    "\n",
    "# --- Speichern ---\n",
    "val_df_cleaned.to_csv(val_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Validation-Set erfolgreich aktualisiert (TPP2-Duplikate entfernt).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4191f8dd-09cb-4e47-afdf-086f1aaebd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train ---\n",
      "Binding = 0: 623204 Beispiele (83.13%)\n",
      "Binding = 1: 126463 Beispiele (16.87%)\n",
      "Total: 749667 Beispiele\n",
      "\n",
      "--- Validation ---\n",
      "Binding = 0: 157347 Beispiele (84.16%)\n",
      "Binding = 1: 29604 Beispiele (15.84%)\n",
      "Total: 186951 Beispiele\n",
      "\n",
      "--- Test ---\n",
      "Binding = 0: 47434 Beispiele (84.01%)\n",
      "Binding = 1: 9026 Beispiele (15.99%)\n",
      "Total: 56460 Beispiele\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130246/3626647992.py:9: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(validation_file, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_file = f'{pipeline_data_splitted}/{precision}/beta/new/train.tsv'\n",
    "validation_file = f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv'\n",
    "test_file = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "# Dateien einlesen\n",
    "train_df = pd.read_csv(train_file, sep='\\t')\n",
    "val_df = pd.read_csv(validation_file, sep='\\t')\n",
    "test_df = pd.read_csv(test_file, sep='\\t')\n",
    "\n",
    "# Funktion zur Ausgabe der Verteilung\n",
    "def show_binding_distribution(df, name):\n",
    "    counts = df['Binding'].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    print(f'--- {name} ---')\n",
    "    for b in counts.index:\n",
    "        percentage = (counts[b] / total) * 100\n",
    "        print(f'Binding = {b}: {counts[b]} Beispiele ({percentage:.2f}%)')\n",
    "    print(f'Total: {total} Beispiele\\n')\n",
    "\n",
    "# Ausgabe\n",
    "show_binding_distribution(train_df, 'Train')\n",
    "show_binding_distribution(val_df, 'Validation')\n",
    "show_binding_distribution(test_df, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77116007-b2e5-4caa-b10f-c2a4cdceb6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8eb49-bca5-4e6f-bc11-90f9ebeb63e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed8918-fe48-4b1e-980a-e61ddf3cdf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "002feb06-57e6-4f75-bf6b-a81a76561afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"beta_allele\": {\n",
    "        \"test\": f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv',\n",
    "        \"validation\": f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14780a4a-fb27-4d86-8d6b-692cc68d3310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset: beta_allele ###\n",
      "\n",
      "--- TEST ---\n",
      "      Non-Binder  Binder  Total  Binder %  Non-Binder %\n",
      "task                                                   \n",
      "TPP1        5873    1203   7076     17.00         83.00\n",
      "TPP2       40399    7513  47912     15.68         84.32\n",
      "TPP3         881     258   1139     22.65         77.35\n",
      "TPP4         281      52    333     15.62         84.38\n",
      "\n",
      "--- VALIDATION ---\n",
      "      Non-Binder  Binder   Total  Binder %  Non-Binder %\n",
      "task                                                    \n",
      "TPP1       62767   12856   75623     17.00         83.00\n",
      "TPP2       89664   14967  104631     14.30         85.70\n",
      "TPP3        6976    1744    8720     20.00         80.00\n",
      "TPP4         194      37     231     16.02         83.98\n"
     ]
    }
   ],
   "source": [
    "# Analyse-Funktion\n",
    "def analyze_binding_distribution(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Spaltennamen vereinheitlichen\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    if 'binding' not in df.columns or 'task' not in df.columns:\n",
    "        raise ValueError(f\"'binding' oder 'task' Spalte fehlt in {file_path}\")\n",
    "\n",
    "    # Gruppieren nach task und binding\n",
    "    stats = df.groupby(['task', 'binding']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Spalten konsistent benennen (wenn z.B. nur 0 oder nur 1 vorkommt)\n",
    "    if 0 in stats.columns and 1 in stats.columns:\n",
    "        stats.columns = ['Non-Binder', 'Binder']\n",
    "    elif 0 in stats.columns:\n",
    "        stats['Binder'] = 0\n",
    "        stats.columns = ['Non-Binder', 'Binder']\n",
    "    elif 1 in stats.columns:\n",
    "        stats['Non-Binder'] = 0\n",
    "        stats.columns = ['Binder', 'Non-Binder']\n",
    "    else:\n",
    "        raise ValueError(\"Keine Binder/Non-Binder-Werte gefunden\")\n",
    "\n",
    "    # Berechnungen\n",
    "    stats['Total'] = stats.sum(axis=1)\n",
    "    stats['Binder %'] = (stats['Binder'] / stats['Total'] * 100).round(2)\n",
    "    stats['Non-Binder %'] = (stats['Non-Binder'] / stats['Total'] * 100).round(2)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Alle Splits durchgehen\n",
    "for dataset_name, splits in datasets.items():\n",
    "    print(f\"\\n### Dataset: {dataset_name} ###\")\n",
    "    for split_name, path in splits.items():\n",
    "        if os.path.exists(path):\n",
    "            print(f\"\\n--- {split_name.upper()} ---\")\n",
    "            stats = analyze_binding_distribution(path)\n",
    "            print(stats)\n",
    "        else:\n",
    "            print(f\"{split_name} not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22902601-fe26-4709-868a-f62a7a11036b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bca8e-92a0-4670-8f06-dd05c93e89a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110bfc1f-6372-4a51-b7ed-edd5618b0633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102b5a2-2a7e-4a4b-84e4-9f19bafa0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfad (ersetze durch deine Variable bei Bedarf)\n",
    "test_path = datasets[\"beta_allele\"][\"test\"]\n",
    "\n",
    "# Lade das Test-TSV\n",
    "df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Wieviele Zeilen hat die Datei insgesamt?\n",
    "print(f\"Anzahl Zeilen im Testset: {len(df)}\")\n",
    "\n",
    "# Duplikate anhand von TCR + Epitope + Binding (exakt gleiches Beispiel)\n",
    "duplicates = df[df.duplicated(subset=[\"TRB_CDR3\", \"Epitope\", \"Binding\"], keep=False)]\n",
    "\n",
    "# Ausgabe: alle Duplikate\n",
    "print(f\"\\nGesamtanzahl an Duplikaten: {len(duplicates)}\")\n",
    "\n",
    "# Gruppiert nach Bindungsklasse\n",
    "dup_counts = duplicates.groupby(\"Binding\").size()\n",
    "print(\"\\nAnzahl Duplikate pro Binding-Klasse:\")\n",
    "print(dup_counts)\n",
    "\n",
    "# Optional: doppelte Kombinationen anzeigen\n",
    "#print(\"\\nBeispielhafte Duplikate:\")\n",
    "#print(duplicates.sort_values(by=[\"TRB_CDR3\", \"Epitope\"]).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6122a84c-2c9e-4428-b2bf-c05042138e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"beta_allele\": {\n",
    "        \"test\": f'{pipeline_data_splitted}/{precision}/beta/test.tsv',\n",
    "        \"validation\": f'{pipeline_data_splitted}/{precision}/beta/validation.tsv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb9f2981-d9eb-4c7d-8e89-993e5ff776ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset: beta_allele ###\n",
      "\n",
      "--- TEST ---\n",
      "      Non-Binder  Binder  Total  Binder %  Non-Binder %\n",
      "task                                                   \n",
      "TPP1       44422    1059  45481      2.33         97.67\n",
      "TPP2          68    7728   7796     99.13          0.87\n",
      "TPP3           0      88     88    100.00          0.00\n",
      "TPP4          10      25     35     71.43         28.57\n",
      "\n",
      "--- VALIDATION ---\n",
      "      Non-Binder  Binder   Total  Binder %  Non-Binder %\n",
      "task                                                    \n",
      "TPP1      170206   18291  188497      9.70         90.30\n",
      "TPP2        4634   15207   19841     76.64         23.36\n",
      "TPP3          36    1509    1545     97.67          2.33\n",
      "TPP4         284      25     309      8.09         91.91\n"
     ]
    }
   ],
   "source": [
    "# Analyse-Funktion\n",
    "def analyze_binding_distribution(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Spaltennamen vereinheitlichen\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    if 'binding' not in df.columns or 'task' not in df.columns:\n",
    "        raise ValueError(f\"'binding' oder 'task' Spalte fehlt in {file_path}\")\n",
    "\n",
    "    # Gruppieren nach task und binding\n",
    "    stats = df.groupby(['task', 'binding']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Spalten konsistent benennen (wenn z.B. nur 0 oder nur 1 vorkommt)\n",
    "    if 0 in stats.columns and 1 in stats.columns:\n",
    "        stats.columns = ['Non-Binder', 'Binder']\n",
    "    elif 0 in stats.columns:\n",
    "        stats['Binder'] = 0\n",
    "        stats.columns = ['Non-Binder', 'Binder']\n",
    "    elif 1 in stats.columns:\n",
    "        stats['Non-Binder'] = 0\n",
    "        stats.columns = ['Binder', 'Non-Binder']\n",
    "    else:\n",
    "        raise ValueError(\"Keine Binder/Non-Binder-Werte gefunden\")\n",
    "\n",
    "    # Berechnungen\n",
    "    stats['Total'] = stats.sum(axis=1)\n",
    "    stats['Binder %'] = (stats['Binder'] / stats['Total'] * 100).round(2)\n",
    "    stats['Non-Binder %'] = (stats['Non-Binder'] / stats['Total'] * 100).round(2)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Alle Splits durchgehen\n",
    "for dataset_name, splits in datasets.items():\n",
    "    print(f\"\\n### Dataset: {dataset_name} ###\")\n",
    "    for split_name, path in splits.items():\n",
    "        if os.path.exists(path):\n",
    "            print(f\"\\n--- {split_name.upper()} ---\")\n",
    "            stats = analyze_binding_distribution(path)\n",
    "            print(stats)\n",
    "        else:\n",
    "            print(f\"{split_name} not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a59e090-bc46-4cfb-8793-874088ead0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f'{pipeline_data_splitted}/{precision}/beta/new/train.tsv',\n",
    "        \"test\": f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv',\n",
    "        \"validation\": f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d27689bc-ef03-4a25-be24-c103b29ec12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset: beta_allele ###\n",
      "\n",
      "--- TEST ---\n",
      "      Non-Binder  Binder  Total  Binder %  Non-Binder %\n",
      "task                                                   \n",
      "TPP1       41276    1059  42335      2.50         97.50\n",
      "TPP2        3135    7728  10863     71.14         28.86\n",
      "TPP3          10      88     98     89.80         10.20\n",
      "TPP4          79      25    104     24.04         75.96\n",
      "\n",
      "--- VALIDATION ---\n",
      "      Non-Binder  Binder   Total  Binder %  Non-Binder %\n",
      "task                                                    \n",
      "TPP1      170206   18291  188497      9.70         90.30\n",
      "TPP2        4634   15207   19841     76.64         23.36\n",
      "TPP3          36    1509    1545     97.67          2.33\n",
      "TPP4         284      25     309      8.09         91.91\n"
     ]
    }
   ],
   "source": [
    "# Analyse-Funktion\n",
    "def analyze_binding_distribution(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Spaltennamen vereinheitlichen\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    if 'binding' not in df.columns or 'task' not in df.columns:\n",
    "        raise ValueError(f\"'binding' oder 'task' Spalte fehlt in {file_path}\")\n",
    "\n",
    "    # Gruppieren nach task und binding\n",
    "    stats = df.groupby(['task', 'binding']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Spalten konsistent benennen (wenn z.B. nur 0 oder nur 1 vorkommt)\n",
    "    if 0 in stats.columns and 1 in stats.columns:\n",
    "        stats.columns = ['Non-Binder', 'Binder']\n",
    "    elif 0 in stats.columns:\n",
    "        stats['Binder'] = 0\n",
    "        stats.columns = ['Non-Binder', 'Binder']\n",
    "    elif 1 in stats.columns:\n",
    "        stats['Non-Binder'] = 0\n",
    "        stats.columns = ['Binder', 'Non-Binder']\n",
    "    else:\n",
    "        raise ValueError(\"Keine Binder/Non-Binder-Werte gefunden\")\n",
    "\n",
    "    # Berechnungen\n",
    "    stats['Total'] = stats.sum(axis=1)\n",
    "    stats['Binder %'] = (stats['Binder'] / stats['Total'] * 100).round(2)\n",
    "    stats['Non-Binder %'] = (stats['Non-Binder'] / stats['Total'] * 100).round(2)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Alle Splits durchgehen\n",
    "for dataset_name, splits in datasets.items():\n",
    "    print(f\"\\n### Dataset: {dataset_name} ###\")\n",
    "    for split_name, path in splits.items():\n",
    "        if os.path.exists(path):\n",
    "            print(f\"\\n--- {split_name.upper()} ---\")\n",
    "            stats = analyze_binding_distribution(path)\n",
    "            print(stats)\n",
    "        else:\n",
    "            print(f\"{split_name} not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329399dc-f6d4-473a-8342-4931daa3c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 MHCs im TRAINING:\n",
      "MHC\n",
      "HLA-A*02:01    0.654873\n",
      "HLA-B*07:02    0.080040\n",
      "HLA-A*24:02    0.051066\n",
      "HLA-A*03:01    0.047830\n",
      "HLA-A*01:01    0.040218\n",
      "HLA-B*08:01    0.038804\n",
      "HLA-A*11:01    0.032549\n",
      "HLA-A*02       0.027665\n",
      "HLA-B*35:01    0.013045\n",
      "HLA-B*07       0.009232\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Top 10 MHCs im TEST:\n",
      "MHC\n",
      "HLA-A*02:01    0.484202\n",
      "HLA-A*03:01    0.109405\n",
      "HLA-B*07:02    0.088974\n",
      "HLA-A*24:02    0.059859\n",
      "HLA-A*02       0.053385\n",
      "HLA-A*11:01    0.051407\n",
      "HLA-B*08:01    0.049895\n",
      "HLA-A*01:01    0.048015\n",
      "HLA-B*35:01    0.015837\n",
      "HLA-B*08       0.005796\n",
      "Name: proportion, dtype: float64\n",
      "MHC\n",
      "HLA-A*02:01    413216\n",
      "HLA-B*07:02     50504\n",
      "HLA-A*24:02     32222\n",
      "HLA-A*03:01     30180\n",
      "HLA-A*01:01     25377\n",
      "HLA-B*08:01     24485\n",
      "HLA-A*11:01     20538\n",
      "HLA-A*02        17456\n",
      "HLA-B*35:01      8231\n",
      "HLA-B*07         5825\n",
      "Name: count, dtype: int64\n",
      "MHC\n",
      "HLA-A*02:01    24979\n",
      "HLA-A*03:01     5644\n",
      "HLA-B*07:02     4590\n",
      "HLA-A*24:02     3088\n",
      "HLA-A*02        2754\n",
      "HLA-A*11:01     2652\n",
      "HLA-B*08:01     2574\n",
      "HLA-A*01:01     2477\n",
      "HLA-B*35:01      817\n",
      "HLA-B*08         299\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Daten\n",
    "train_data = pd.read_csv(datasets[\"beta_allele\"][\"train\"], sep=\"\\t\")\n",
    "test_data = pd.read_csv(datasets[\"beta_allele\"][\"test\"], sep=\"\\t\")\n",
    "\n",
    "# Vergleiche MHC-Verteilungen (Top 10 häufigste MHCs, anteilig)\n",
    "print(\"\\nTop 10 MHCs im TRAINING:\")\n",
    "print(train_data[\"MHC\"].value_counts(normalize=True).head(10))\n",
    "\n",
    "print(\"\\nTop 10 MHCs im TEST:\")\n",
    "print(test_data[\"MHC\"].value_counts(normalize=True).head(10))\n",
    "\n",
    "print(train_data[\"MHC\"].value_counts().head(10))\n",
    "print(test_data[\"MHC\"].value_counts().head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a32ba01-0862-4ecf-bfec-60098e26b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Beta Allele ---\n",
      "Anzahl einzigartiger Epitope im Trainingsdatensatz: 988\n",
      "Anzahl einzigartiger Epitope im Testdatensatz: 878\n",
      "Anzahl einzigartiger Epitope im Validierungsdatensatz: 1462\n",
      "Gesamtanzahl einzigartiger Epitope (Train + Test + Validation): 1896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zusätzliche Analyse für einzigartige Epitope\n",
    "unique_epitopes = {}\n",
    "\n",
    "for dataset_name, paths in datasets.items():\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Extrahieren einzigartiger Epitope\n",
    "    train_unique = set(train_df[\"Epitope\"].unique())\n",
    "    test_unique = set(test_df[\"Epitope\"].unique())\n",
    "    validation_unique = set(validation_df[\"Epitope\"].unique())\n",
    "    \n",
    "    # Gesamtanzahl einzigartiger Epitope\n",
    "    all_unique = train_unique | test_unique | validation_unique  # Vereinigung der Sets\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    unique_epitopes[dataset_name] = {\n",
    "        \"Train_Unique\": len(train_unique),\n",
    "        \"Test_Unique\": len(test_unique),\n",
    "        \"Validation_Unique\": len(validation_unique),\n",
    "        \"Total_Unique\": len(all_unique),\n",
    "        \"Train_Epitopes\": train_unique,\n",
    "        \"Test_Epitopes\": test_unique,\n",
    "        \"Validation_Epitopes\": validation_unique\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, epitopes in unique_epitopes.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl einzigartiger Epitope im Trainingsdatensatz: {epitopes[\"Train_Unique\"]}')\n",
    "    print(f'Anzahl einzigartiger Epitope im Testdatensatz: {epitopes[\"Test_Unique\"]}')\n",
    "    print(f'Anzahl einzigartiger Epitope im Validierungsdatensatz: {epitopes[\"Validation_Unique\"]}')\n",
    "    print(f'Gesamtanzahl einzigartiger Epitope (Train + Test + Validation): {epitopes[\"Total_Unique\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6eb3f53-4fa5-4679-a1a9-2f14d65d50bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Beta Allele Unique Epitope Counts ---\n",
      "Train (Binding=0): 713, Train (Binding=1): 981\n",
      "Test (Binding=0): 650, Test (Binding=1): 293\n",
      "Validation (Binding=0): 645, Validation (Binding=1): 1450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anzahl der einzigartigen Epitope pro Binding und pro Datei\n",
    "unique_epitopes_count = {}\n",
    "\n",
    "for dataset, paths in datasets.items():\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Einzigartige Epitope für Binding=0 und Binding=1\n",
    "    train_unique = {\n",
    "        0: len(set(train_df[train_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(train_df[train_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    test_unique = {\n",
    "        0: len(set(test_df[test_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(test_df[test_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    validation_unique = {\n",
    "        0: len(set(validation_df[validation_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(validation_df[validation_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    unique_epitopes_count[dataset] = {\n",
    "        \"Train_Binding_0\": train_unique[0],\n",
    "        \"Train_Binding_1\": train_unique[1],\n",
    "        \"Test_Binding_0\": test_unique[0],\n",
    "        \"Test_Binding_1\": test_unique[1],\n",
    "        \"Validation_Binding_0\": validation_unique[0],\n",
    "        \"Validation_Binding_1\": validation_unique[1]\n",
    "    }\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for dataset, counts in unique_epitopes_count.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} Unique Epitope Counts ---')\n",
    "    print(f'Train (Binding=0): {counts[\"Train_Binding_0\"]}, Train (Binding=1): {counts[\"Train_Binding_1\"]}')\n",
    "    print(f'Test (Binding=0): {counts[\"Test_Binding_0\"]}, Test (Binding=1): {counts[\"Test_Binding_1\"]}')\n",
    "    print(f'Validation (Binding=0): {counts[\"Validation_Binding_0\"]}, Validation (Binding=1): {counts[\"Validation_Binding_1\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c160f2c-b751-4f7d-b9a7-99397d362773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train ---\n",
      "Binding = 0: 700630 Beispiele (83.33%)\n",
      "Binding = 1: 140126 Beispiele (16.67%)\n",
      "Total: 840756 Beispiele\n",
      "\n",
      "--- Validation ---\n",
      "Binding = 0: 175160 Beispiele (83.33%)\n",
      "Binding = 1: 35032 Beispiele (16.67%)\n",
      "Total: 210192 Beispiele\n",
      "\n",
      "--- Test ---\n",
      "Binding = 0: 44500 Beispiele (83.33%)\n",
      "Binding = 1: 8900 Beispiele (16.67%)\n",
      "Total: 53400 Beispiele\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_file = f'{pipeline_data_splitted}/{precision}/beta/new/train.tsv'\n",
    "validation_file = f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv'\n",
    "test_file = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "# Dateien einlesen\n",
    "train_df = pd.read_csv(train_file, sep='\\t')\n",
    "val_df = pd.read_csv(validation_file, sep='\\t')\n",
    "test_df = pd.read_csv(test_file, sep='\\t')\n",
    "\n",
    "# Funktion zur Ausgabe der Verteilung\n",
    "def show_binding_distribution(df, name):\n",
    "    counts = df['Binding'].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    print(f'--- {name} ---')\n",
    "    for b in counts.index:\n",
    "        percentage = (counts[b] / total) * 100\n",
    "        print(f'Binding = {b}: {counts[b]} Beispiele ({percentage:.2f}%)')\n",
    "    print(f'Total: {total} Beispiele\\n')\n",
    "\n",
    "# Ausgabe\n",
    "show_binding_distribution(train_df, 'Train')\n",
    "show_binding_distribution(val_df, 'Validation')\n",
    "show_binding_distribution(test_df, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ce96829-a014-4a40-bd7b-943f8eba8442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Matching Rows im Test-Set (kommen auch in Train/Validation vor): 0\n",
      "Verteilung nach Binding:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "train_path = '../../../../data/splitted_datasets/allele/beta/new/train.tsv'\n",
    "valid_path = '../../../../data/splitted_datasets/allele/beta/new/validation.tsv'\n",
    "test_path  = '../../../../data/splitted_datasets/allele/beta/new/test.tsv'\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "test_df  = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Relevante Spalten\n",
    "target_columns = ['Epitope', 'TRB_CDR3']\n",
    "\n",
    "# Erzeuge Sets von Paaren aus Train und Validation\n",
    "train_pairs = set(map(tuple, train_df[target_columns].values))\n",
    "valid_pairs = set(map(tuple, valid_df[target_columns].values))\n",
    "\n",
    "# Kombiniere Train & Val Paare\n",
    "combined_pairs = train_pairs.union(valid_pairs)\n",
    "\n",
    "# Erzeuge Pair-Spalte im Test-Set\n",
    "test_df['Pair'] = test_df[target_columns].apply(tuple, axis=1)\n",
    "\n",
    "# Filtere Testzeilen, deren Paare in Train/Val vorkommen\n",
    "matching_test_df = test_df[test_df['Pair'].isin(combined_pairs)]\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"Anzahl Matching Rows im Test-Set (kommen auch in Train/Validation vor): {len(matching_test_df)}\")\n",
    "print(\"Verteilung nach Binding:\")\n",
    "print(matching_test_df['Binding'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1a58096-998e-4e64-a12c-96236f3c9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f'{pipeline_data_splitted}/{precision}/beta/train.tsv',\n",
    "        \"test\": f'{pipeline_data_splitted}/{precision}/beta/test.tsv',\n",
    "        \"validation\": f'{pipeline_data_splitted}/{precision}/beta/validation.tsv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a036b1e-fe60-47c5-ac1c-c84ff749deb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Dataset: BETA_ALLELE\n",
      "\n",
      "📄 TRAIN (840756 Zeilen)\n",
      "🔢 Source-Verteilung:\n",
      "  - generated: 350647 (41.71%)\n",
      "  - 10X: 349983 (41.63%)\n",
      "  - datasets: 140126 (16.67%)\n",
      "\n",
      "📄 TEST (53400 Zeilen)\n",
      "🔢 Source-Verteilung:\n",
      "  - generated: 22551 (42.23%)\n",
      "  - 10X: 21949 (41.10%)\n",
      "  - datasets: 8900 (16.67%)\n",
      "\n",
      "📄 VALIDATION (210192 Zeilen)\n",
      "🔢 Source-Verteilung:\n",
      "  - generated: 87878 (41.81%)\n",
      "  - 10X: 87282 (41.52%)\n",
      "  - datasets: 35032 (16.67%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for dataset_name, paths in datasets.items():\n",
    "    print(f\"\\n📂 Dataset: {dataset_name.upper()}\")\n",
    "    \n",
    "    for split_name, path in paths.items():\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep='\\t')\n",
    "            print(f\"\\n📄 {split_name.upper()} ({len(df)} Zeilen)\")\n",
    "\n",
    "            if 'source' in df.columns:\n",
    "                source_counts = df['source'].value_counts(dropna=False)\n",
    "                source_percent = df['source'].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "                print(\"🔢 Source-Verteilung:\")\n",
    "                for src in source_counts.index:\n",
    "                    count = source_counts[src]\n",
    "                    perc = source_percent[src]\n",
    "                    print(f\"  - {src}: {count} ({perc:.2f}%)\")\n",
    "            else:\n",
    "                print(\"⚠️ Spalte 'source' nicht vorhanden.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ Datei nicht gefunden: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❗ Fehler beim Lesen von {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30c0002-fea6-4013-ae65-926408ad4739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Matching Rows im Test-Set (kommen auch in Train/Validation vor): 56748\n",
      "Verteilung nach Binding:\n",
      "Binding\n",
      "0    47555\n",
      "1     9193\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "train_path = '../../../../data/ba_splitted/train.tsv'\n",
    "valid_path = '../../../../data/ba_splitted/validation.tsv'\n",
    "test_path  = '../../../../data/ba_splitted/test.tsv'\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "test_df  = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Relevante Spalten\n",
    "target_columns = ['Epitope', 'TRB_CDR3']\n",
    "\n",
    "# Erzeuge Sets von Paaren aus Train und Validation\n",
    "train_pairs = set(map(tuple, train_df[target_columns].values))\n",
    "valid_pairs = set(map(tuple, valid_df[target_columns].values))\n",
    "\n",
    "# Kombiniere Train & Val Paare\n",
    "combined_pairs = train_pairs.union(valid_pairs)\n",
    "\n",
    "# Erzeuge Pair-Spalte im Test-Set\n",
    "test_df['Pair'] = test_df[target_columns].apply(tuple, axis=1)\n",
    "\n",
    "# Filtere Testzeilen, deren Paare in Train/Val vorkommen\n",
    "matching_test_df = test_df[test_df['Pair'].isin(combined_pairs)]\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"Anzahl Matching Rows im Test-Set (kommen auch in Train/Validation vor): {len(matching_test_df)}\")\n",
    "print(\"Verteilung nach Binding:\")\n",
    "print(matching_test_df['Binding'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "410f5a7e-7134-4c5e-a1ec-383223915893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Inhalt von data.zip:\n",
      "File Name                                             Modified             Size\n",
      "data/                                          2024-12-20 00:12:38            0\n",
      "data/cleaned_datasets/                         2024-12-20 00:11:34            0\n",
      "data/cleaned_datasets/IEDB/                    2024-12-20 00:11:32            0\n",
      "data/cleaned_datasets/IEDB/IEDB_cleaned_data_beta.csv 2024-12-20 00:11:32     13347360\n",
      "data/cleaned_datasets/IEDB/IEDB_cleaned_data_paired.csv 2024-12-20 00:11:32      2009011\n",
      "data/cleaned_datasets/McPas/                   2024-12-20 00:11:36            0\n",
      "data/cleaned_datasets/McPas/McPAS_cleaned_data_beta.tsv 2024-12-20 00:11:36       752763\n",
      "data/cleaned_datasets/McPas/McPAS_cleaned_data_paired.tsv 2024-12-20 00:11:36       185984\n",
      "data/cleaned_datasets/VDJdb/                   2024-12-20 00:11:34            0\n",
      "data/cleaned_datasets/VDJdb/VDJdb_cleaned_data_beta.tsv 2024-12-20 00:11:34      3850836\n",
      "data/cleaned_datasets/VDJdb/VDJdb_cleaned_data_paired.tsv 2024-12-20 00:11:34      3223415\n",
      "data/concatenated_datasets/                    2024-12-20 00:11:42            0\n",
      "data/concatenated_datasets/allele/             2024-12-20 00:11:40            0\n",
      "data/concatenated_datasets/allele/.ipynb_checkpoints/ 2024-12-20 00:11:36            0\n",
      "data/concatenated_datasets/allele/.ipynb_checkpoints/beta_concatenated-checkpoint.tsv 2024-12-20 00:11:38     12941348\n",
      "data/concatenated_datasets/allele/beta_concatenated.tsv 2024-12-20 00:11:40     12941348\n",
      "data/concatenated_datasets/allele/paired_concatenated.tsv 2024-12-20 00:11:42      4963626\n",
      "data/concatenated_datasets/gene/               2024-12-20 00:11:48            0\n",
      "data/concatenated_datasets/gene/.ipynb_checkpoints/ 2024-12-20 00:11:44            0\n",
      "data/concatenated_datasets/gene/.ipynb_checkpoints/beta_concatenated-checkpoint.tsv 2024-12-20 00:11:44     10566826\n",
      "data/concatenated_datasets/gene/.ipynb_checkpoints/paired_concatenated-checkpoint.tsv 2024-12-20 00:11:46      3932360\n",
      "data/concatenated_datasets/gene/beta_concatenated.tsv 2024-12-20 00:11:48     10566826\n",
      "data/concatenated_datasets/gene/paired_concatenated.tsv 2024-12-20 00:11:48      3932360\n",
      "data/embeddings/                               2024-12-19 23:56:38            0\n",
      "data/embeddings/beta/                          2024-12-19 23:32:00            0\n",
      "data/embeddings/beta/allele/                   2024-12-19 23:31:50            0\n",
      "data/embeddings/beta/allele/.ipynb_checkpoints/ 2024-12-19 23:06:44            0\n",
      "data/embeddings/beta/allele/Epitope_beta_embeddings.npz 2024-12-19 23:32:00     79140592\n",
      "data/embeddings/beta/allele/TRB_beta_embeddings.npz 2024-12-19 23:31:50   9160092650\n",
      "data/embeddings/beta/gene/                     2024-12-19 23:56:26            0\n",
      "data/embeddings/beta/gene/Epitope_beta_embeddings.npz 2024-12-19 23:56:38     79140592\n",
      "data/embeddings/beta/gene/TRB_beta_embeddings.npz 2024-12-19 23:56:26   9160092182\n",
      "data/embeddings/paired/                        2024-12-20 00:04:04            0\n",
      "data/embeddings/paired/allele/                 2024-12-20 00:00:22            0\n",
      "data/embeddings/paired/allele/Epitope_paired_embeddings.npz 2024-12-19 23:56:46     52835356\n",
      "data/embeddings/paired/allele/TRA_paired_embeddings.npz 2024-12-20 00:04:04   1295576398\n",
      "data/embeddings/paired/allele/TRB_paired_embeddings.npz 2024-12-20 00:00:22   1490238426\n",
      "data/embeddings/paired/gene/                   2024-12-20 00:07:56            0\n",
      "data/embeddings/paired/gene/Epitope_paired_embeddings.npz 2024-12-20 00:04:14     52835356\n",
      "data/embeddings/paired/gene/TRA_paired_embeddings.npz 2024-12-20 00:11:28   1295576398\n",
      "data/embeddings/paired/gene/TRB_paired_embeddings.npz 2024-12-20 00:07:56   1490238426\n",
      "data/embeddings/temp/                          2024-12-19 23:06:38            0\n",
      "data/embeddings/temp/allele/                   2024-12-19 23:06:36            0\n",
      "data/embeddings/temp/allele/.ipynb_checkpoints/ 2024-12-19 23:06:28            0\n",
      "data/embeddings/temp/allele/.ipynb_checkpoints/beta_concatenated-checkpoint.tsv 2024-12-19 23:06:32     25536745\n",
      "data/embeddings/temp/allele/beta_concatenated.tsv 2024-12-19 23:06:36     25536745\n",
      "data/embeddings/temp/allele/paired_concatenated.tsv 2024-12-19 23:06:38      9538220\n",
      "data/embeddings/temp/gene/                     2024-12-19 23:06:42            0\n",
      "data/embeddings/temp/gene/beta_concatenated.tsv 2024-12-19 23:06:42     21206244\n",
      "data/embeddings/temp/gene/paired_concatenated.tsv 2024-12-19 23:06:44      7552300\n",
      "data/physicoProperties/                        2024-12-19 23:05:54            0\n",
      "data/physicoProperties/scaled_test_beta_epitope_allele_physico.npz 2024-12-19 23:03:50       711450\n",
      "data/physicoProperties/scaled_test_beta_epitope_gene_physico.npz 2024-12-19 23:05:36       767234\n",
      "data/physicoProperties/scaled_test_beta_TRB_allele_physico.npz 2024-12-19 23:05:08     16781740\n",
      "data/physicoProperties/scaled_test_beta_TRB_gene_physico.npz 2024-12-19 23:05:50     14091488\n",
      "data/physicoProperties/scaled_test_paired_epitope_allele_physico.npz 2024-12-19 23:04:52       367960\n",
      "data/physicoProperties/scaled_test_paired_epitope_gene_physico.npz 2024-12-19 23:04:50       496170\n",
      "data/physicoProperties/scaled_test_paired_TRA_allele_physico.npz 2024-12-19 23:05:46      4061794\n",
      "data/physicoProperties/scaled_test_paired_TRA_gene_physico.npz 2024-12-19 23:03:58      3830904\n",
      "data/physicoProperties/scaled_test_paired_TRB_allele_physico.npz 2024-12-19 23:03:58      4248448\n",
      "data/physicoProperties/scaled_test_paired_TRB_gene_physico.npz 2024-12-19 23:05:36      3985172\n",
      "data/physicoProperties/scaled_train_beta_epitope_allele_physico.npz 2024-12-19 23:03:42      1051422\n",
      "data/physicoProperties/scaled_train_beta_epitope_gene_physico.npz 2024-12-19 23:04:50       723752\n",
      "data/physicoProperties/scaled_train_beta_TRB_allele_physico.npz 2024-12-19 23:05:32     80969104\n",
      "data/physicoProperties/scaled_train_beta_TRB_gene_physico.npz 2024-12-19 23:04:44     82503766\n",
      "data/physicoProperties/scaled_train_paired_epitope_allele_physico.npz 2024-12-19 23:03:50       773036\n",
      "data/physicoProperties/scaled_train_paired_epitope_gene_physico.npz 2024-12-19 23:05:46       453852\n",
      "data/physicoProperties/scaled_train_paired_TRA_allele_physico.npz 2024-12-19 23:05:42     11548540\n",
      "data/physicoProperties/scaled_train_paired_TRA_gene_physico.npz 2024-12-19 23:05:44     11284604\n",
      "data/physicoProperties/scaled_train_paired_TRB_allele_physico.npz 2024-12-19 23:04:54     12388284\n",
      "data/physicoProperties/scaled_train_paired_TRB_gene_physico.npz 2024-12-19 23:05:02     12119406\n",
      "data/physicoProperties/scaled_validation_beta_epitope_allele_physico.npz 2024-12-19 23:03:58       715064\n",
      "data/physicoProperties/scaled_validation_beta_epitope_gene_physico.npz 2024-12-19 23:05:46       772412\n",
      "data/physicoProperties/scaled_validation_beta_TRB_allele_physico.npz 2024-12-19 23:05:00     16720554\n",
      "data/physicoProperties/scaled_validation_beta_TRB_gene_physico.npz 2024-12-19 23:05:12     14111274\n",
      "data/physicoProperties/scaled_validation_paired_epitope_allele_physico.npz 2024-12-19 23:04:52       373764\n",
      "data/physicoProperties/scaled_validation_paired_epitope_gene_physico.npz 2024-12-19 23:05:02       481702\n",
      "data/physicoProperties/scaled_validation_paired_TRA_allele_physico.npz 2024-12-19 23:03:42      4061098\n",
      "data/physicoProperties/scaled_validation_paired_TRA_gene_physico.npz 2024-12-19 23:03:50      3843458\n",
      "data/physicoProperties/scaled_validation_paired_TRB_allele_physico.npz 2024-12-19 23:04:56      4262972\n",
      "data/physicoProperties/scaled_validation_paired_TRB_gene_physico.npz 2024-12-19 23:05:44      4005876\n",
      "data/physicoProperties/ssl_beta/               2024-12-19 23:03:46            0\n",
      "data/physicoProperties/ssl_beta/allele/        2024-12-19 23:03:46            0\n",
      "data/physicoProperties/ssl_beta/gene/          2024-12-19 23:03:46            0\n",
      "data/physicoProperties/ssl_paired/             2024-12-19 23:03:56            0\n",
      "data/physicoProperties/ssl_paired/allele/      2024-12-19 23:03:54            0\n",
      "data/physicoProperties/ssl_paired/allele/.ipynb_checkpoints/ 2024-12-19 23:03:50            0\n",
      "data/physicoProperties/ssl_paired/allele/Epitope_physico.npz 2024-12-19 23:03:54       829820\n",
      "data/physicoProperties/ssl_paired/allele/TRA_CDR3_physico.npz 2024-12-19 23:03:54     14044782\n",
      "data/physicoProperties/ssl_paired/allele/TRB_CDR3_physico.npz 2024-12-19 23:03:56     15209466\n",
      "data/physicoProperties/ssl_paired/gene/        2024-12-19 23:03:56            0\n",
      "data/physicoProperties/test_beta_epitope_allele_physico.npz 2024-12-19 23:05:16       711450\n",
      "data/physicoProperties/test_beta_epitope_gene_physico.npz 2024-12-19 23:05:44       767234\n",
      "data/physicoProperties/test_beta_TRB_allele_physico.npz 2024-12-19 23:03:44     16781740\n",
      "data/physicoProperties/test_beta_TRB_gene_physico.npz 2024-12-19 23:05:54     14091488\n",
      "data/physicoProperties/test_paired_epitope_allele_physico.npz 2024-12-19 23:04:44       367960\n",
      "data/physicoProperties/test_paired_epitope_gene_physico.npz 2024-12-19 23:04:16       496170\n",
      "data/physicoProperties/test_paired_TRA_allele_physico.npz 2024-12-19 23:05:50      4061794\n",
      "data/physicoProperties/test_paired_TRA_gene_physico.npz 2024-12-19 23:05:56      3830904\n",
      "data/physicoProperties/test_paired_TRB_allele_physico.npz 2024-12-19 23:03:46      4248448\n",
      "data/physicoProperties/test_paired_TRB_gene_physico.npz 2024-12-19 23:05:34      3985172\n",
      "data/physicoProperties/train_beta_epitope_allele_physico.npz 2024-12-19 23:05:32      1051422\n",
      "data/physicoProperties/train_beta_epitope_gene_physico.npz 2024-12-19 23:05:12       723752\n",
      "data/physicoProperties/train_beta_TRB_allele_physico.npz 2024-12-19 23:04:28     80969104\n",
      "data/physicoProperties/train_beta_TRB_gene_physico.npz 2024-12-19 23:04:14     82503766\n",
      "data/physicoProperties/train_paired_epitope_allele_physico.npz 2024-12-19 23:05:32       773036\n",
      "data/physicoProperties/train_paired_epitope_gene_physico.npz 2024-12-19 23:04:52       453852\n",
      "data/physicoProperties/train_paired_TRA_allele_physico.npz 2024-12-19 23:05:16     11548540\n",
      "data/physicoProperties/train_paired_TRA_gene_physico.npz 2024-12-19 23:05:10     11284604\n",
      "data/physicoProperties/train_paired_TRB_allele_physico.npz 2024-12-19 23:03:48     12388284\n",
      "data/physicoProperties/train_paired_TRB_gene_physico.npz 2024-12-19 23:04:50     12119406\n",
      "data/physicoProperties/validation_beta_epitope_allele_physico.npz 2024-12-19 23:04:50       715064\n",
      "data/physicoProperties/validation_beta_epitope_gene_physico.npz 2024-12-19 23:04:16       772412\n",
      "data/physicoProperties/validation_beta_TRB_allele_physico.npz 2024-12-19 23:05:40     16720554\n",
      "data/physicoProperties/validation_beta_TRB_gene_physico.npz 2024-12-19 23:04:48     14111274\n",
      "data/physicoProperties/validation_paired_epitope_allele_physico.npz 2024-12-19 23:05:50       373764\n",
      "data/physicoProperties/validation_paired_epitope_gene_physico.npz 2024-12-19 23:05:34       481702\n",
      "data/physicoProperties/validation_paired_TRA_allele_physico.npz 2024-12-19 23:04:44      4061098\n",
      "data/physicoProperties/validation_paired_TRA_gene_physico.npz 2024-12-19 23:05:14      3843458\n",
      "data/physicoProperties/validation_paired_TRB_allele_physico.npz 2024-12-19 23:05:54      4262972\n",
      "data/physicoProperties/validation_paired_TRB_gene_physico.npz 2024-12-19 23:04:56      4005876\n",
      "data/plain_data/                               2024-12-19 23:06:24            0\n",
      "data/plain_data/IEDB/                          2024-12-19 23:06:04            0\n",
      "data/plain_data/IEDB/.ipynb_checkpoints/       2024-12-19 23:05:56            0\n",
      "data/plain_data/IEDB/.ipynb_checkpoints/MHCI_IEDB_beta_export-checkpoint.csv 2024-12-19 23:06:00     13487313\n",
      "data/plain_data/IEDB/.ipynb_checkpoints/MHCI_IEDB_paired_export-checkpoint.csv 2024-12-19 23:05:56      2300842\n",
      "data/plain_data/IEDB/MHCI_IEDB_beta_export.csv 2024-12-19 23:06:04     13487313\n",
      "data/plain_data/IEDB/MHCI_IEDB_paired_export.csv 2024-12-19 23:06:04      2300842\n",
      "data/plain_data/McPas/                         2024-12-19 23:06:26            0\n",
      "data/plain_data/McPas/.ipynb_checkpoints/      2024-12-19 23:06:26            0\n",
      "data/plain_data/McPas/McPAS-TCR.csv            2024-12-19 23:06:28      8459192\n",
      "data/plain_data/VDJdb/                         2024-12-19 23:06:18            0\n",
      "data/plain_data/VDJdb/.ipynb_checkpoints/      2024-12-19 23:06:04            0\n",
      "data/plain_data/VDJdb/.ipynb_checkpoints/VDJdb_beta_only-checkpoint.tsv 2024-12-19 23:06:10     40780772\n",
      "data/plain_data/VDJdb/VDJdb_beta_only.tsv      2024-12-19 23:06:18     40780772\n",
      "data/plain_data/VDJdb/VDJdb_paired_only.tsv    2024-12-19 23:06:24     48418321\n",
      "data/plain_datasets/                           2024-12-20 00:13:12            0\n",
      "data/plain_datasets/IEDB/                      2024-12-20 00:12:44            0\n",
      "data/plain_datasets/IEDB/.ipynb_checkpoints/   2024-12-20 00:12:40            0\n",
      "data/plain_datasets/IEDB/.ipynb_checkpoints/MHCI_IEDB_beta_export-checkpoint.csv 2024-12-20 00:12:42     13487313\n",
      "data/plain_datasets/IEDB/.ipynb_checkpoints/MHCI_IEDB_paired_export-checkpoint.csv 2024-12-20 00:12:40      2300842\n",
      "data/plain_datasets/IEDB/MHCI_IEDB_beta_export.csv 2024-12-20 00:12:44     13487313\n",
      "data/plain_datasets/IEDB/MHCI_IEDB_paired_export.csv 2024-12-20 00:12:46      2300842\n",
      "data/plain_datasets/McPas/                     2024-12-20 00:13:12            0\n",
      "data/plain_datasets/McPas/.ipynb_checkpoints/  2024-12-20 00:13:12            0\n",
      "data/plain_datasets/McPas/McPAS-TCR.csv        2024-12-20 00:13:14      8459192\n",
      "data/plain_datasets/VDJdb/                     2024-12-20 00:13:02            0\n",
      "data/plain_datasets/VDJdb/.ipynb_checkpoints/  2024-12-20 00:12:46            0\n",
      "data/plain_datasets/VDJdb/.ipynb_checkpoints/VDJdb_beta_only-checkpoint.tsv 2024-12-20 00:12:54     40780772\n",
      "data/plain_datasets/VDJdb/VDJdb_beta_only.tsv  2024-12-20 00:13:02     40780772\n",
      "data/plain_datasets/VDJdb/VDJdb_paired_only.tsv 2024-12-20 00:13:12     48418321\n",
      "data/splitted_datasets/                        2024-12-20 00:12:02            0\n",
      "data/splitted_datasets/allele/                 2024-12-20 00:11:56            0\n",
      "data/splitted_datasets/allele/beta/            2024-12-20 00:11:54            0\n",
      "data/splitted_datasets/allele/beta/.ipynb_checkpoints/ 2024-12-20 00:11:50            0\n",
      "data/splitted_datasets/allele/beta/.ipynb_checkpoints/test-checkpoint.tsv 2024-12-20 00:11:50      4021730\n",
      "data/splitted_datasets/allele/beta/test.tsv    2024-12-20 00:11:52      4021730\n",
      "data/splitted_datasets/allele/beta/train.tsv   2024-12-20 00:11:54     17728345\n",
      "data/splitted_datasets/allele/beta/validation.tsv 2024-12-20 00:11:56      3786786\n",
      "data/splitted_datasets/allele/paired/          2024-12-20 00:12:02            0\n",
      "data/splitted_datasets/allele/paired/.ipynb_checkpoints/ 2024-12-20 00:11:58            0\n",
      "data/splitted_datasets/allele/paired/.ipynb_checkpoints/test-checkpoint.tsv 2024-12-20 00:11:58      1595944\n",
      "data/splitted_datasets/allele/paired/.ipynb_checkpoints/test_reclassified_paired_specific-checkpoint.tsv 2024-12-20 00:11:58      1595944\n",
      "data/splitted_datasets/allele/paired/.ipynb_checkpoints/train-checkpoint.tsv 2024-12-20 00:11:58      6408159\n",
      "data/splitted_datasets/allele/paired/.ipynb_checkpoints/validation-checkpoint.tsv 2024-12-20 00:12:00      1534281\n",
      "data/splitted_datasets/allele/paired/test.tsv  2024-12-20 00:12:00      1595944\n",
      "data/splitted_datasets/allele/paired/test_reclassified_paired_specific.tsv 2024-12-20 00:12:02      1595944\n",
      "data/splitted_datasets/allele/paired/train.tsv 2024-12-20 00:12:02      6408159\n",
      "data/splitted_datasets/allele/paired/validation.tsv 2024-12-20 00:12:02      1534281\n",
      "data/splitted_datasets/gene/                   2024-12-20 00:12:08            0\n",
      "data/splitted_datasets/gene/beta/              2024-12-20 00:12:06            0\n",
      "data/splitted_datasets/gene/beta/.ipynb_checkpoints/ 2024-12-20 00:12:04            0\n",
      "data/splitted_datasets/gene/beta/.ipynb_checkpoints/test-checkpoint.tsv 2024-12-20 00:12:04      3265199\n",
      "data/splitted_datasets/gene/beta/test.tsv      2024-12-20 00:12:04      3265199\n",
      "data/splitted_datasets/gene/beta/train.tsv     2024-12-20 00:12:06     14889523\n",
      "data/splitted_datasets/gene/beta/validation.tsv 2024-12-20 00:12:08      3051638\n",
      "data/splitted_datasets/gene/paired/            2024-12-20 00:12:12            0\n",
      "data/splitted_datasets/gene/paired/.ipynb_checkpoints/ 2024-12-20 00:12:08            0\n",
      "data/splitted_datasets/gene/paired/.ipynb_checkpoints/test-checkpoint.tsv 2024-12-20 00:12:08      1262658\n",
      "data/splitted_datasets/gene/paired/test.tsv    2024-12-20 00:12:10      1262658\n",
      "data/splitted_datasets/gene/paired/test_reclassified_paired_specific.tsv 2024-12-20 00:12:10      1262658\n",
      "data/splitted_datasets/gene/paired/train.tsv   2024-12-20 00:12:10      4415764\n",
      "data/splitted_datasets/gene/paired/validation.tsv 2024-12-20 00:12:12      1206274\n",
      "data/temp/                                     2024-12-20 00:12:38            0\n",
      "data/temp/IEDB/                                2024-12-20 00:12:16            0\n",
      "data/temp/IEDB/IEDB_beta_fitted.csv            2024-12-20 00:12:16     14076015\n",
      "data/temp/IEDB/IEDB_paired_fitted.csv          2024-12-20 00:12:16      2300441\n",
      "data/temp/McPas/                               2024-12-20 00:12:38            0\n",
      "data/temp/McPas/.ipynb_checkpoints/            2024-12-20 00:12:36            0\n",
      "data/temp/McPas/.ipynb_checkpoints/McPAS_fitted-checkpoint.tsv 2024-12-20 00:12:36      2541653\n",
      "data/temp/McPas/McPAS_fitted.tsv               2024-12-20 00:12:38      2541653\n",
      "data/temp/negative_samples/                    2024-12-20 00:12:38            0\n",
      "data/temp/negative_samples/paired/             2024-12-20 00:12:38            0\n",
      "data/temp/negative_samples/paired/.ipynb_checkpoints/ 2024-12-20 00:12:38            0\n",
      "data/temp/VDJdb/                               2024-12-20 00:12:24            0\n",
      "data/temp/VDJdb/VDJdb_beta_fitted.tsv          2024-12-20 00:12:24     46144811\n",
      "data/temp/VDJdb/VDJdb_paired_fitted.tsv        2024-12-20 00:12:36     52817180\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = f\"{pipeline_data_plain}/data.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    print(\"📦 Inhalt von data.zip:\")\n",
    "    zip_ref.printdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c48c9e01-84a2-492e-8eaa-f533f7a4c40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dateien erfolgreich extrahiert nach: ../../../../data/ba_splitted\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Pfade und Zielverzeichnis definieren\n",
    "zip_path = f\"{pipeline_data_plain}/data.zip\"\n",
    "output_dir = f\"{pipeline_data}/ba_splitted\"  # Zielordner\n",
    "\n",
    "# Die drei gewünschten Dateien im ZIP\n",
    "target_files = [\n",
    "    \"data/splitted_datasets/allele/beta/train.tsv\",\n",
    "    \"data/splitted_datasets/allele/beta/validation.tsv\",\n",
    "    \"data/splitted_datasets/allele/beta/test.tsv\"\n",
    "]\n",
    "\n",
    "# Zielverzeichnis erstellen, falls nicht vorhanden\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extraktion\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    for file in target_files:\n",
    "        # Temporär an ursprünglichem Pfad extrahieren\n",
    "        extracted_path = zip_ref.extract(file)\n",
    "\n",
    "        # Nur den Dateinamen extrahieren (z.B. \"train.tsv\")\n",
    "        filename = os.path.basename(file)\n",
    "\n",
    "        # Zielpfad festlegen und Datei verschieben\n",
    "        final_path = os.path.join(output_dir, filename)\n",
    "        shutil.move(extracted_path, final_path)\n",
    "\n",
    "print(f\"✅ Dateien erfolgreich extrahiert nach: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
