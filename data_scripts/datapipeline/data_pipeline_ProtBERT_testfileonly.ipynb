{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tidytcells in /home/ubuntu/anaconda3/lib/python3.12/site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install tidytcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision of mhc and V/J values (gene or allele)\n",
    "precision = 'allele'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is not thread safe\n",
    "def create_folders_if_not_exists(folders):\n",
    "  for path in folders:\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = '../../../../data'\n",
    "pipeline_data_plain = f'{pipeline_data}/plain_datasets'\n",
    "pipeline_data_cleaned = f'{pipeline_data}/cleaned_datasets'\n",
    "pipeline_data_concatenated = f'{pipeline_data}/concatenated_datasets'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "pipeline_data_temp_bucket = f'{pipeline_data}/temp'\n",
    "\n",
    "pipeline_folders = [pipeline_data, pipeline_data_plain, pipeline_data_cleaned, pipeline_data_concatenated, pipeline_data_splitted, pipeline_data_temp_bucket]\n",
    "\n",
    "create_folders_if_not_exists(pipeline_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VDJdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "VDJdb_data_plain = f'{pipeline_data_plain}/VDJdb'\n",
    "VDJdb_data_cleaned = f'{pipeline_data_cleaned}/VDJdb'\n",
    "VDJdb_data_fitted = f'{pipeline_data_temp_bucket}/VDJdb'\n",
    "\n",
    "VDJdb_folders = [VDJdb_data_plain, VDJdb_data_cleaned, VDJdb_data_fitted]\n",
    "create_folders_if_not_exists(VDJdb_folders)\n",
    "\n",
    "fitted_beta_file = 'VDJdb_beta_fitted.tsv'\n",
    "fitted_paired_file = 'VDJdb_paired_fitted.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data paired\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_paired_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_paired_file\n",
    "\n",
    "# fit paired VDJdb data\n",
    "%run ../VDJdb/fit_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data beta\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_beta_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_beta_file\n",
    "\n",
    "# fit beta VDJdb data\n",
    "%run ../VDJdb/fit_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 27414 entries\n",
      "whole dataframe has 28119 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 27414\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data paired\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_paired_file}'\n",
    "cleaned_file_paired = 'VDJdb_cleaned_data_paired.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'\n",
    "\n",
    "# clean paired VDJdb data\n",
    "%run ../VDJdb/clean_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 46507 entries\n",
      "whole dataframe has 49042 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 46507\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data beta\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_beta_file}'\n",
    "cleaned_file_beta = 'VDJdb_cleaned_data_beta.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "\n",
    "# clean beta VDJdb data\n",
    "%run ../VDJdb/clean_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDJdb_cleaned_beta_output = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "VDJdb_cleaned_paired_output = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of beta_df: 46507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 7188 entries removed.\n",
      "removed all duplicates (CDR3, Epitope) from distinct values (most_important_columns, keep=False). 1435 entries removed.\n",
      "beta removed entries df length: 1435\n",
      "\n",
      "\n",
      "Number of groups formed: 655\n",
      "1435 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 46507 entries, 7188 entries have been removed.\n",
      "for beta dataset :\n",
      "size difference is: 7188\n",
      "  39319 information score cleaned: 6.0\n",
      "  46507 information score dropout: 6.0\n",
      "✅ Nach Duplikat-Filter (Train/Val): final_beta_df enthält 9105 Einträge.\n",
      "final_beta_df length = 9105\n",
      "length of paired_df: 27414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 687 entries removed.\n",
      "removed all duplicates from distinct values (cultivated columns, keep=False). 246 entries removed.\n",
      "paired removed entries df length: 246\n",
      "\n",
      "\n",
      "246 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 27414 entries, 687 entries have been removed.\n",
      "for paired dataset:\n",
      "size difference is: 687\n",
      "  26727 information score cleaned: 8.976241254162458\n",
      "  27414 information score dropout: 8.975888232290071\n",
      "final_paired_df length: 26727\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "vdjdb_beta_read_path = VDJdb_cleaned_beta_output\n",
    "vdjdb_paired_read_path = VDJdb_cleaned_paired_output\n",
    "\n",
    "output_file_beta = 'beta_concatenated_test.tsv'\n",
    "output_file_paired = 'paired_concatenated_test.tsv'\n",
    "\n",
    "create_folders_if_not_exists([custom_dataset_path])\n",
    "\n",
    "%run ../concatDatasets_onlytest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta file copied successfully to ../../../../data/splitted_datasets/allele/beta/test_prenegsamples.tsv\n",
      "Paired file copied successfully to ../../../../data/splitted_datasets/allele/paired/test_prenegsamples.tsv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define source folder where the files are currently stored\n",
    "source_folder = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "# Define file names\n",
    "output_file_beta = 'beta_concatenated_test.tsv'\n",
    "output_file_paired = 'paired_concatenated_test.tsv'\n",
    "\n",
    "# Define destination folders\n",
    "destination_beta_folder = f'{pipeline_data_splitted}/{precision}/beta/'\n",
    "destination_paired_folder = f'{pipeline_data_splitted}/{precision}/paired/'\n",
    "\n",
    "# Ensure destination folders exist\n",
    "os.makedirs(destination_beta_folder, exist_ok=True)\n",
    "os.makedirs(destination_paired_folder, exist_ok=True)\n",
    "\n",
    "# Copy files\n",
    "shutil.copy(os.path.join(source_folder, output_file_beta), os.path.join(destination_beta_folder, 'test_prenegsamples.tsv'))\n",
    "shutil.copy(os.path.join(source_folder, output_file_paired), os.path.join(destination_paired_folder, 'test_prenegsamples.tsv'))\n",
    "\n",
    "print(f'Beta file copied successfully to {destination_beta_folder}test_prenegsamples.tsv')\n",
    "print(f'Paired file copied successfully to {destination_paired_folder}test_prenegsamples.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beta Dataset:\n",
      "- Unique TCRs: 8612\n",
      "- Unique Epitope: 293\n",
      "\n",
      "Paired Dataset:\n",
      "- Unique TCRs: 21101\n",
      "- Unique Epitope: 825\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "beta_file_path = f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv'\n",
    "paired_file_path = f'{pipeline_data_splitted}/{precision}/paired/test_prenegsamples.tsv'\n",
    "\n",
    "# Load beta dataset\n",
    "beta_df = pd.read_csv(beta_file_path, sep='\\t')\n",
    "\n",
    "# Load paired dataset\n",
    "paired_df = pd.read_csv(paired_file_path, sep='\\t')\n",
    "\n",
    "# Calculate unique values for beta dataset\n",
    "unique_tcr_beta = beta_df['TRB_CDR3'].nunique()\n",
    "unique_epitope_beta = beta_df['Epitope'].nunique()\n",
    "\n",
    "# Calculate unique values for paired dataset\n",
    "unique_tcr_paired = paired_df['TRB_CDR3'].nunique()\n",
    "unique_epitope_paired = paired_df['Epitope'].nunique()\n",
    "\n",
    "# Print results for beta dataset\n",
    "print(\"\\nBeta Dataset:\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_beta}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_beta}\")\n",
    "\n",
    "# Print results for paired dataset\n",
    "print(\"\\nPaired Dataset:\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_paired}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_paired}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/210144174.py:10: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(val_path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task\n",
      "TPP2    6803\n",
      "TPP1    1129\n",
      "TPP3    1047\n",
      "TPP4     126\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pfade\n",
    "train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv\"\n",
    "\n",
    "# Einlesen\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "val_df = pd.read_csv(val_path, sep=\"\\t\")\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\")\n",
    "\n",
    "# Kombiniere bekannte TCRs & Epitopes aus Train + Validation\n",
    "known_tcrs = set(train_df[\"TRB_CDR3\"]) | set(val_df[\"TRB_CDR3\"])\n",
    "known_epitopes = set(train_df[\"Epitope\"]) | set(val_df[\"Epitope\"])\n",
    "\n",
    "# Task-Tagging-Funktion\n",
    "def calculate_task(row):\n",
    "    epitope_exists = row['Epitope'] in known_epitopes\n",
    "    trb_cdr3_exists = row['TRB_CDR3'] in known_tcrs\n",
    "\n",
    "    if epitope_exists and trb_cdr3_exists:\n",
    "        return 'TPP1'\n",
    "    elif epitope_exists and not trb_cdr3_exists:\n",
    "        return 'TPP2'\n",
    "    elif not epitope_exists and not trb_cdr3_exists:\n",
    "        return 'TPP3'\n",
    "    elif not epitope_exists and trb_cdr3_exists:\n",
    "        return 'TPP4'\n",
    "    return \"UNDEFINED\"\n",
    "\n",
    "# TPP-Spalte hinzufügen\n",
    "test_df[\"task\"] = test_df.apply(calculate_task, axis=1)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(test_df[\"task\"].value_counts())\n",
    "\n",
    "# speichern\n",
    "test_df.to_csv(test_path, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus:                 barcode   donor  \\\n",
      "0   AAACCTGAGACAAAGG-4  donor1   \n",
      "1  AAACCTGAGACTGTAA-34  donor1   \n",
      "2   AAACCTGAGAGCCCAA-5  donor1   \n",
      "3  AAACCTGAGAGCTGCA-24  donor1   \n",
      "4   AAACCTGAGAGGGATA-8  donor1   \n",
      "\n",
      "                                  cell_clono_cdr3_aa  \\\n",
      "0  TRA:CAASVSIWTGTASKLTF;TRA:CAAWDMEYGNKLVF;TRB:C...   \n",
      "1                                    TRB:CASDTPVGQFF   \n",
      "2                 TRA:CASYTDKLIF;TRB:CASSGGSISTDTQYF   \n",
      "3                                 TRB:CASSGGQSSYEQYF   \n",
      "4          TRA:CAASGYGNTGRRALTF;TRB:CASSQDPAGGYNEQFF   \n",
      "\n",
      "                                  cell_clono_cdr3_nt     CD3  CD19  CD45RA  \\\n",
      "0  TRA:TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAA...  2125.0   0.0   912.0   \n",
      "1              TRB:TGTGCCAGCGATACCCCGGTTGGGCAGTTCTTC  1023.0   0.0  2028.0   \n",
      "2  TRA:TGTGCTTCCTACACCGACAAGCTCATCTTT;TRB:TGCGCCA...  1598.0   3.0  3454.0   \n",
      "3     TRB:TGCGCCAGCAGTGGCGGACAGAGCTCCTACGAGCAGTACTTC   298.0   1.0   880.0   \n",
      "4  TRA:TGTGCAGCAAGCGGGTATGGAAACACGGGCAGGAGAGCACTT...  1036.0   0.0  2457.0   \n",
      "\n",
      "   CD4    CD8a  CD14  ...  B0702_RPHERNGFTVL_pp65_CMV_binder  \\\n",
      "0  1.0  2223.0   4.0  ...                              False   \n",
      "1  2.0  3485.0   1.0  ...                              False   \n",
      "2  4.0  3383.0   1.0  ...                              False   \n",
      "3  1.0  2389.0   1.0  ...                              False   \n",
      "4  2.0  3427.0   3.0  ...                              False   \n",
      "\n",
      "   B0801_RAKFKQLL_BZLF1_EBV_binder  B0801_ELRRKMMYM_IE-1_CMV_binder  \\\n",
      "0                            False                            False   \n",
      "1                            False                            False   \n",
      "2                            False                            False   \n",
      "3                            False                            False   \n",
      "4                            False                            False   \n",
      "\n",
      "   B0801_FLRGRAYGL_EBNA-3A_EBV_binder  A0101_SLEGGGLGY_NC_binder  \\\n",
      "0                               False                      False   \n",
      "1                               False                      False   \n",
      "2                               False                      False   \n",
      "3                               False                      False   \n",
      "4                               False                      False   \n",
      "\n",
      "   A0101_STEGGGLAY_NC_binder  A0201_ALIAPVHAV_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   A2402_AYSSAGASI_NC_binder  B0702_GPAESAAGL_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   NR(B0801)_AAKGRGAAL_NC_binder  \n",
      "0                          False  \n",
      "1                          False  \n",
      "2                          False  \n",
      "3                          False  \n",
      "4                          False  \n",
      "\n",
      "[5 rows x 118 columns]\n",
      "Meta:                barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_1             True   \n",
      "1  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_2             True   \n",
      "2  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_3             True   \n",
      "3  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_4            False   \n",
      "4  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_5            False   \n",
      "\n",
      "   length chain     v_gene d_gene   j_gene c_gene  full_length productive  \\\n",
      "0     722   TRB   TRBV10-3  TRBD2  TRBJ2-1  TRBC2         True       True   \n",
      "1     605   TRA  TRAV29DV5    NaN   TRAJ44   TRAC         True       True   \n",
      "2     738   TRA    TRAV8-6    NaN   TRAJ47   TRAC         True       True   \n",
      "3     468   TRB        NaN    NaN  TRBJ2-3  TRBC2        False        NaN   \n",
      "4     488   TRB        NaN    NaN  TRBJ2-6  TRBC2        False        NaN   \n",
      "\n",
      "                cdr3                                            cdr3_nt  \\\n",
      "0  CAISDPGLAGGGGEQFF  TGTGCCATCAGTGACCCCGGACTAGCGGGAGGCGGGGGGGAGCAGT...   \n",
      "1  CAASVSIWTGTASKLTF  TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAACTCA...   \n",
      "2     CAAWDMEYGNKLVF         TGTGCCGCCTGGGACATGGAATATGGAAACAAGCTGGTCTTT   \n",
      "3                NaN                                                NaN   \n",
      "4                NaN                                                NaN   \n",
      "\n",
      "   reads  umis raw_clonotype_id         raw_consensus_id  \n",
      "0  32237    18      clonotype19  clonotype19_consensus_1  \n",
      "1   6088     3      clonotype19  clonotype19_consensus_2  \n",
      "2   5358     3      clonotype19  clonotype19_consensus_3  \n",
      "3   2517     1      clonotype19                      NaN  \n",
      "4   2468     1      clonotype19                      NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231714/3850524540.py:9: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n"
     ]
    }
   ],
   "source": [
    "#Daten einlesen\n",
    "\n",
    "combined_donors_path = f'{pipeline_data_plain}/10x/combined_donors_consensus_annotations.csv'\n",
    "all_donors_consensus = pd.read_csv(combined_donors_path, sep=',')\n",
    "\n",
    "print(\"Consensus: \", all_donors_consensus.head())\n",
    "\n",
    "all_donors_meta_path = f'{pipeline_data_plain}/10x/meta.csv'\n",
    "all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n",
    "\n",
    "print(\"Meta: \", all_donors_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Start:  0\n",
      "Batch Start:  1000\n",
      "Batch Start:  2000\n",
      "Batch Start:  3000\n",
      "Batch Start:  4000\n",
      "Batch Start:  5000\n",
      "Batch Start:  6000\n",
      "Batch Start:  7000\n",
      "Batch Start:  8000\n",
      "Batch Start:  9000\n",
      "Batch Start:  10000\n",
      "Batch Start:  11000\n",
      "Batch Start:  12000\n",
      "Batch Start:  13000\n",
      "Batch Start:  14000\n",
      "Batch Start:  15000\n",
      "Batch Start:  16000\n",
      "Batch Start:  17000\n",
      "Batch Start:  18000\n",
      "Batch Start:  19000\n",
      "Batch Start:  20000\n",
      "Batch Start:  21000\n",
      "Batch Start:  22000\n",
      "Batch Start:  23000\n",
      "Batch Start:  24000\n",
      "Batch Start:  25000\n",
      "Batch Start:  26000\n",
      "Batch Start:  27000\n",
      "Batch Start:  28000\n",
      "Batch Start:  29000\n",
      "Batch Start:  30000\n",
      "Batch Start:  31000\n",
      "Batch Start:  32000\n",
      "Batch Start:  33000\n",
      "Batch Start:  34000\n",
      "Batch Start:  35000\n",
      "Batch Start:  36000\n",
      "Batch Start:  37000\n",
      "Batch Start:  38000\n",
      "Batch Start:  39000\n",
      "Batch Start:  40000\n",
      "Batch Start:  41000\n",
      "Batch Start:  42000\n",
      "Batch Start:  43000\n",
      "Batch Start:  44000\n",
      "Batch Start:  45000\n",
      "Batch Start:  46000\n",
      "Batch Start:  47000\n",
      "Batch Start:  48000\n",
      "Batch Start:  49000\n",
      "Batch Start:  50000\n",
      "Batch Start:  51000\n",
      "Batch Start:  52000\n",
      "Batch Start:  53000\n",
      "Batch Start:  54000\n",
      "Batch Start:  55000\n",
      "Batch Start:  56000\n",
      "Batch Start:  57000\n",
      "Batch Start:  58000\n",
      "Batch Start:  59000\n",
      "Batch Start:  60000\n",
      "Batch Start:  61000\n",
      "Batch Start:  62000\n",
      "Batch Start:  63000\n",
      "Batch Start:  64000\n",
      "Batch Start:  65000\n",
      "Batch Start:  66000\n",
      "Batch Start:  67000\n",
      "Batch Start:  68000\n",
      "Batch Start:  69000\n",
      "Batch Start:  70000\n",
      "Batch Start:  71000\n",
      "Batch Start:  72000\n",
      "Batch Start:  73000\n",
      "Batch Start:  74000\n",
      "Batch Start:  75000\n",
      "Batch Start:  76000\n",
      "Batch Start:  77000\n",
      "Batch Start:  78000\n",
      "Batch Start:  79000\n",
      "Batch Start:  80000\n",
      "Batch Start:  81000\n",
      "Batch Start:  82000\n",
      "Batch Start:  83000\n",
      "Batch Start:  84000\n",
      "Batch Start:  85000\n",
      "Batch Start:  86000\n",
      "Batch Start:  87000\n",
      "Batch Start:  88000\n",
      "Batch Start:  89000\n",
      "Batch Start:  90000\n",
      "Batch Start:  91000\n",
      "Batch Start:  92000\n",
      "Batch Start:  93000\n",
      "Batch Start:  94000\n",
      "Batch Start:  95000\n",
      "Batch Start:  96000\n",
      "Batch Start:  97000\n",
      "Batch Start:  98000\n",
      "Batch Start:  99000\n",
      "Batch Start:  100000\n",
      "Batch Start:  101000\n",
      "Batch Start:  102000\n",
      "Batch Start:  103000\n",
      "Batch Start:  104000\n",
      "Batch Start:  105000\n",
      "Batch Start:  106000\n",
      "Batch Start:  107000\n",
      "Batch Start:  108000\n",
      "Batch Start:  109000\n",
      "Batch Start:  110000\n",
      "Batch Start:  111000\n",
      "Batch Start:  112000\n",
      "Batch Start:  113000\n",
      "Batch Start:  114000\n",
      "Batch Start:  115000\n",
      "Batch Start:  116000\n",
      "Batch Start:  117000\n",
      "Batch Start:  118000\n",
      "Batch Start:  119000\n",
      "Batch Start:  120000\n",
      "Batch Start:  121000\n",
      "Batch Start:  122000\n",
      "Batch Start:  123000\n",
      "Batch Start:  124000\n",
      "Batch Start:  125000\n",
      "Batch Start:  126000\n",
      "Batch Start:  127000\n",
      "Batch Start:  128000\n",
      "Batch Start:  129000\n",
      "Batch Start:  130000\n",
      "Batch Start:  131000\n",
      "Batch Start:  132000\n",
      "Batch Start:  133000\n",
      "Batch Start:  134000\n",
      "Batch Start:  135000\n",
      "Batch Start:  136000\n",
      "Batch Start:  137000\n",
      "Batch Start:  138000\n",
      "Batch Start:  139000\n",
      "Batch Start:  140000\n",
      "Batch Start:  141000\n",
      "Batch Start:  142000\n",
      "Batch Start:  143000\n",
      "Batch Start:  144000\n",
      "Batch Start:  145000\n",
      "Batch Start:  146000\n",
      "Batch Start:  147000\n",
      "Batch Start:  148000\n",
      "Batch Start:  149000\n",
      "Batch Start:  150000\n",
      "Batch Start:  151000\n",
      "Batch Start:  152000\n",
      "Batch Start:  153000\n",
      "Batch Start:  154000\n",
      "Batch Start:  155000\n",
      "Batch Start:  156000\n",
      "Batch Start:  157000\n",
      "Batch Start:  158000\n",
      "Batch Start:  159000\n",
      "Batch Start:  160000\n",
      "Batch Start:  161000\n",
      "Batch Start:  162000\n",
      "Batch Start:  163000\n",
      "Batch Start:  164000\n",
      "Batch Start:  165000\n",
      "Batch Start:  166000\n",
      "Batch Start:  167000\n",
      "Batch Start:  168000\n",
      "Batch Start:  169000\n",
      "Batch Start:  170000\n",
      "Batch Start:  171000\n",
      "Batch Start:  172000\n",
      "Batch Start:  173000\n",
      "Batch Start:  174000\n",
      "Batch Start:  175000\n",
      "Batch Start:  176000\n",
      "Batch Start:  177000\n",
      "Batch Start:  178000\n",
      "Batch Start:  179000\n",
      "Batch Start:  180000\n",
      "Batch Start:  181000\n",
      "Batch Start:  182000\n",
      "Batch Start:  183000\n",
      "Batch Start:  184000\n",
      "Batch Start:  185000\n",
      "Batch Start:  186000\n",
      "Batch Start:  187000\n",
      "Batch Start:  188000\n",
      "Batch Start:  189000\n",
      "             TCR_name      TRBV     TRBJ           TRB_CDR3   TRBC  \\\n",
      "0  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "1  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "2  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "3  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "4  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "\n",
      "      Epitope          MHC Binding task  \n",
      "0   VTEHDTLLY  HLA-A*01:01       0  nan  \n",
      "1   KTWGQYWQV  HLA-A*02:01       0  nan  \n",
      "2  ELAGIGILTV  HLA-A*02:01       0  nan  \n",
      "3  CLLWSFQTSA  HLA-A*02:01       0  nan  \n",
      "4   IMDQVPFSV  HLA-A*02:01       0  nan  \n"
     ]
    }
   ],
   "source": [
    "#Dieser Code für ganzen Datensatz laufen lassen\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Festlegen der Batch-Größe für die Verarbeitung\n",
    "batch_size = 1000  # Passe diese Zahl je nach Speicherressourcen an\n",
    "\n",
    "# Identifizieren von Epitope-Spalten, aber ohne \"NR(B0801)_AAKGRGAAL_NC_binder\"\n",
    "epitope_columns = [col for col in all_donors_consensus.columns if '_binder' in col and col != \"NR(B0801)_AAKGRGAAL_NC_binder\"]\n",
    "\n",
    "# Liste für alle Batch-Ergebnisse\n",
    "all_batches = []\n",
    "\n",
    "# Verarbeite `all_donors_consensus` in Batches\n",
    "for batch_start in range(0, len(all_donors_consensus), batch_size):\n",
    "    print(\"Batch Start: \", batch_start)\n",
    "    # Batch definieren\n",
    "    batch = all_donors_consensus.iloc[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Filtern auf Zeilen, die 'TRB:' enthalten\n",
    "    batch_trb = batch[batch['cell_clono_cdr3_aa'].str.contains(\"TRB:\", na=False)]\n",
    "\n",
    "    # Liste, um Zeilen für diesen Batch zu speichern\n",
    "    expanded_rows = []\n",
    "    \n",
    "    # Iteriere durch jede Zeile im Batch\n",
    "    for _, row in batch_trb.iterrows():\n",
    "        for col in epitope_columns:\n",
    "            # Extrahiere MHC und Epitope\n",
    "            match = re.match(r'([A-Z0-9]+)_([A-Z]+)_.*_binder', col)\n",
    "            if match:\n",
    "                mhc_raw, epitope = match.groups()\n",
    "                mhc_formatted = f'HLA-{mhc_raw[0]}*{mhc_raw[1:3]}:{mhc_raw[3:]}'\n",
    "\n",
    "                # Füge `Epitope` und `MHC` zur Zeile hinzu\n",
    "                new_row = row.copy()\n",
    "                new_row['Epitope'] = epitope\n",
    "                new_row['MHC'] = mhc_formatted\n",
    "\n",
    "                # Füge neue Zeile zur Batch-Liste hinzu\n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    # Erstelle einen DataFrame aus dem Batch\n",
    "    batch_df = pd.DataFrame(expanded_rows)\n",
    "    all_batches.append(batch_df)  # Speichere den Batch in der Liste\n",
    "\n",
    "# Kombiniere alle Batch-Ergebnisse zu einem DataFrame\n",
    "expanded_df = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "# Nur die TRB-Chain-Einträge in `all_donors_meta` beibehalten\n",
    "all_donors_meta_trb = all_donors_meta[all_donors_meta['chain'] == 'TRB']\n",
    "\n",
    "# Zusammenführen der beiden DataFrames basierend auf der 'barcode' Spalte\n",
    "merged_df = pd.merge(all_donors_meta_trb, expanded_df[['barcode', 'Epitope', 'MHC']], on='barcode', how='inner')\n",
    "\n",
    "# Spalten umbenennen und Format anpassen\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'barcode': 'TCR_name',\n",
    "    'v_gene': 'TRBV',\n",
    "    'j_gene': 'TRBJ',\n",
    "    'c_gene': 'TRBC',\n",
    "    'cdr3': 'TRB_CDR3'\n",
    "})\n",
    "\n",
    "# Fehlende Spalten auffüllen\n",
    "desired_columns = ['TCR_name', 'TRBV', 'TRBJ', 'TRB_CDR3', 'TRBC', 'Epitope', 'MHC', 'Binding', 'task']\n",
    "for col in desired_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = 'nan' if col == 'task' else '0'\n",
    "\n",
    "# Nur die gewünschten Spalten beibehalten und Zeilen mit `None` in `TRB_CDR3` entfernen\n",
    "final_df = merged_df[desired_columns]\n",
    "final_df = final_df[final_df['TRB_CDR3'] != 'None']\n",
    "\n",
    "final_df = final_df[final_df['TRB_CDR3'].notna() & (final_df['TRB_CDR3'] != '')]\n",
    "\n",
    "# Ausgabe des ersten Teils des Ergebnisses zur Überprüfung\n",
    "print(final_df.head())\n",
    "\n",
    "# Speichern des kombinierten DataFrames\n",
    "output_path = f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv'\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Samples generieren für Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/340240480.py:28: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  beta_train_df = pd.read_csv(read_path_train, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for beta dataset\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "read_path_train = f\"{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv\" #fürs erste mal durchführen train_prenegsamples und new weg\n",
    "read_path_test = f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv' #fürs erste mal durchführen test_prenegsamples und new weg\n",
    "read_path_validation = f\"{pipeline_data_splitted}/{precision}/beta/validation_prenegsamples.tsv\" #fürs erste mal durchführen validation_prenegsamples und new weg\n",
    "temp_path = f'{pipeline_data_temp_bucket}/negative_samples/beta/'\n",
    "output_path = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives\"\n",
    "train_output_name = \"train_neg5.tsv\"\n",
    "validation_output_name = \"val_neg5.tsv\"\n",
    "test_output_name = \"test_neg5.tsv\"\n",
    "\n",
    "create_folders_if_not_exists([temp_path])\n",
    "\n",
    "%run ../negative_samples/negative_samples_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spalte 'source' auf 'generated' gesetzt und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Dateipfade ---\n",
    "read_path_train = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{train_output_name}\"\n",
    "read_path_val = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{validation_output_name}\"\n",
    "read_path_test = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{test_output_name}\"\n",
    "\n",
    "# --- Dateien laden ---\n",
    "train_neg = pd.read_csv(read_path_train, sep='\\t')\n",
    "val_neg = pd.read_csv(read_path_val, sep='\\t')\n",
    "test_neg = pd.read_csv(read_path_test, sep='\\t')\n",
    "\n",
    "# --- Spalte \"source\" setzen ---\n",
    "for df in [train_neg, val_neg, test_neg]:\n",
    "    df[\"source\"] = \"generated\"\n",
    "\n",
    "# --- Zurückspeichern ---\n",
    "train_neg.to_csv(read_path_train, sep='\\t', index=False)\n",
    "val_neg.to_csv(read_path_val, sep='\\t', index=False)\n",
    "test_neg.to_csv(read_path_test, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Spalte 'source' auf 'generated' gesetzt und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ TRAIN: Entferne 14469 Duplikate mit Konflikt.\n",
      "✅ TRAIN: Übrig nach Cleaning: 125409\n",
      "❌ VALIDATION: Entferne 4459 Duplikate mit Konflikt.\n",
      "✅ VALIDATION: Übrig nach Cleaning: 30401\n",
      "❌ TEST: Entferne 12 Duplikate mit Konflikt.\n",
      "✅ TEST: Übrig nach Cleaning: 9055\n"
     ]
    }
   ],
   "source": [
    "# === Load negative datasets ===\n",
    "train_neg = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/train_neg5.tsv\", sep='\\t')\n",
    "val_neg = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/val_neg5.tsv\", sep='\\t')\n",
    "test_neg = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/test_neg5.tsv\", sep='\\t')\n",
    "\n",
    "# === Load positive datasets ===\n",
    "train_pos = pd.read_csv(output_train_path, sep='\\t')\n",
    "val_pos = pd.read_csv(output_val_path, sep='\\t')\n",
    "test_pos = pd.read_csv(test_output_path, sep='\\t')\n",
    "\n",
    "# 🔁 Wiederverwendbare Funktion zum Entfernen\n",
    "def remove_conflicting_negatives(pos_df, neg_df, name):\n",
    "    dupe_cols = [\"Epitope\", \"TRB_CDR3\"]\n",
    "    merged = pd.merge(neg_df, pos_df[dupe_cols], on=dupe_cols, how=\"inner\")\n",
    "    print(f\"❌ {name.upper()}: Entferne {len(merged)} Duplikate mit Konflikt.\")\n",
    "    \n",
    "    # Nur die, die NICHT im positiven vorkommen → behalten\n",
    "    cleaned_neg = neg_df.merge(merged[dupe_cols], on=dupe_cols, how=\"left\", indicator=True)\n",
    "    cleaned_neg = cleaned_neg[cleaned_neg[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n",
    "    print(f\"✅ {name.upper()}: Übrig nach Cleaning: {len(cleaned_neg)}\")\n",
    "    return cleaned_neg\n",
    "# Neue Clean-Dateipfade\n",
    "neg_base_path = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives\"\n",
    "\n",
    "train_neg_clean = remove_conflicting_negatives(train_pos, train_neg, \"train\")\n",
    "train_neg_clean.to_csv(f\"{neg_base_path}/train_neg_clean5.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "val_neg_clean = remove_conflicting_negatives(val_pos, val_neg, \"validation\")\n",
    "val_neg_clean.to_csv(f\"{neg_base_path}/val_neg_clean5.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "test_neg_clean = remove_conflicting_negatives(test_pos, test_neg, \"test\")\n",
    "test_neg_clean.to_csv(f\"{neg_base_path}/test_neg_clean5.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ===\n",
      "📄 Gelesen: train_neg_clean.tsv → 134756 Einträge\n",
      "📄 Gelesen: train_neg_clean2.tsv → 131262 Einträge\n",
      "📄 Gelesen: train_neg_clean3.tsv → 125544 Einträge\n",
      "📄 Gelesen: train_neg_clean4.tsv → 125795 Einträge\n",
      "📄 Gelesen: train_neg_clean5.tsv → 125409 Einträge\n",
      "🔢 Gesamt vor Deduplikation: 642766\n",
      "✅ Nach Deduplikation: 597003\n",
      "🚨 Überschneidungen mit Positiven: 139737\n",
      "           Epitope        TRB_CDR3\n",
      "0  VLPFNDGVYFASTEK  CASIWAGLGQPQHF\n",
      "1        GLCTLVAML  CASSVQGLGQPQHF\n",
      "2        GLCTLVAML   CASTPPGSETQYF\n",
      "\n",
      "=== VALIDATION ===\n",
      "📄 Gelesen: val_neg_clean.tsv → 32805 Einträge\n",
      "📄 Gelesen: val_neg_clean2.tsv → 32349 Einträge\n",
      "📄 Gelesen: val_neg_clean3.tsv → 30375 Einträge\n",
      "📄 Gelesen: val_neg_clean4.tsv → 30329 Einträge\n",
      "📄 Gelesen: val_neg_clean5.tsv → 30401 Einträge\n",
      "🔢 Gesamt vor Deduplikation: 156259\n",
      "✅ Nach Deduplikation: 130637\n",
      "🚨 Überschneidungen mit Positiven: 0\n",
      "\n",
      "=== TEST ===\n",
      "📄 Gelesen: test_neg_clean.tsv → 9059 Einträge\n",
      "📄 Gelesen: test_neg_clean2.tsv → 9048 Einträge\n",
      "📄 Gelesen: test_neg_clean3.tsv → 9041 Einträge\n",
      "📄 Gelesen: test_neg_clean4.tsv → 9059 Einträge\n",
      "📄 Gelesen: test_neg_clean5.tsv → 9055 Einträge\n",
      "🔢 Gesamt vor Deduplikation: 45262\n",
      "✅ Nach Deduplikation: 35950\n",
      "🚨 Überschneidungen mit Positiven: 9055\n",
      "      Epitope            TRB_CDR3\n",
      "0  GTSGSPIINR   CASSEAWGATNTGELFF\n",
      "1  ELAGIGILTV  CASSSWTSGGRSYNEQFF\n",
      "2  ELAGIGILTV     CASSLVGTGYDEQYF\n",
      "✅ Kombinierte Negative wurden gespeichert ohne Duplikate.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dupe_cols = [\"Epitope\", \"TRB_CDR3\"]\n",
    "\n",
    "# === Negativ-Dateipfade pro Split ===\n",
    "paths = {\n",
    "    \"train\": [f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/train_neg_clean{i}.tsv\" for i in [\"\", \"2\", \"3\", \"4\", \"5\"]],\n",
    "    \"validation\": [f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/val_neg_clean{i}.tsv\" for i in [\"\", \"2\", \"3\", \"4\", \"5\"]],\n",
    "    \"test\": [f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/test_neg_clean{i}.tsv\" for i in [\"\", \"2\", \"3\", \"4\", \"5\"]],\n",
    "}\n",
    "\n",
    "# === Positivdaten laden ===\n",
    "train_pos = pd.read_csv(read_path_train, sep=\"\\t\")\n",
    "val_pos   = pd.read_csv(read_path_validation, sep=\"\\t\")\n",
    "test_pos  = pd.read_csv(read_path_test,  sep=\"\\t\")\n",
    "\n",
    "positives = {\n",
    "    \"train\": train_pos,\n",
    "    \"validation\": val_pos,\n",
    "    \"test\": test_pos\n",
    "}\n",
    "\n",
    "# === Helper-Funktion: alle N Files kombinieren, deduplizieren, auf Leaks prüfen ===\n",
    "def check_negatives_multiple(split_name, file_list, pos_df):\n",
    "    print(f\"\\n=== {split_name.upper()} ===\")\n",
    "    \n",
    "    neg_list = []\n",
    "    for path in file_list:\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path, sep=\"\\t\")\n",
    "            print(f\"📄 Gelesen: {os.path.basename(path)} → {len(df)} Einträge\")\n",
    "            neg_list.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ Datei nicht gefunden: {path}\")\n",
    "\n",
    "    # Alle kombinieren\n",
    "    combined = pd.concat(neg_list, ignore_index=True)\n",
    "    print(f\"🔢 Gesamt vor Deduplikation: {len(combined)}\")\n",
    "\n",
    "    # Deduplikation\n",
    "    combined.drop_duplicates(subset=dupe_cols, inplace=True)\n",
    "    print(f\"✅ Nach Deduplikation: {len(combined)}\")\n",
    "\n",
    "    # Leak-Check gegen Positives\n",
    "    merged = pd.merge(combined, pos_df, on=dupe_cols)\n",
    "    print(f\"🚨 Überschneidungen mit Positiven: {len(merged)}\")\n",
    "    if not merged.empty:\n",
    "        print(merged[dupe_cols].head(3))\n",
    "\n",
    "    return combined\n",
    "\n",
    "# === Ausführen für alle Splits ===\n",
    "combined_negatives = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    combined_negatives[split] = check_negatives_multiple(\n",
    "        split,\n",
    "        paths[split],\n",
    "        positives[split]\n",
    "    )\n",
    "\n",
    "# Speicherpfade definieren\n",
    "output_dir = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/combined_no_duplicates\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Exportieren pro Split\n",
    "combined_negatives[\"train\"].to_csv(f\"{output_dir}/train_neg_combined.tsv\", sep=\"\\t\", index=False)\n",
    "combined_negatives[\"validation\"].to_csv(f\"{output_dir}/val_neg_combined.tsv\", sep=\"\\t\", index=False)\n",
    "combined_negatives[\"test\"].to_csv(f\"{output_dir}/test_neg_combined.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"✅ Kombinierte Negative wurden gespeichert ohne Duplikate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_balanced_negatives_with_all_epitopes(\n",
    "    neg_source_1,\n",
    "    neg_source_2,\n",
    "    target_neg_count,\n",
    "    used_pairs=set(),\n",
    "    vdjdb_tcrs=set(),\n",
    "    vdjdb_epitopes=set(),\n",
    "    ensure_all_neg_epitopes=True,\n",
    "    tpp_target_distribution=None\n",
    "):\n",
    "    neg_source_1 = neg_source_1.copy()\n",
    "    neg_source_2 = neg_source_2.copy()\n",
    "    neg_source_1['source'] = '10X'\n",
    "    neg_source_2['source'] = 'generated'\n",
    "\n",
    "    # --- VDJdb & verwendete Paare filtern ---\n",
    "    def filter_df(df, remove_epitopes=False):\n",
    "        df = df[~df['TRB_CDR3'].isin(vdjdb_tcrs)].copy()\n",
    "        if remove_epitopes:\n",
    "            df = df[~df['Epitope'].isin(vdjdb_epitopes)]\n",
    "    \n",
    "        df['Pair'] = list(map(tuple, df[['Epitope', 'TRB_CDR3']].values))\n",
    "    \n",
    "        # Entferne negative Paare, die in positiven Beispielen (used_pairs) vorkommen\n",
    "        df = df[~df['Pair'].isin(used_pairs)]\n",
    "    \n",
    "        # Entferne Duplikate innerhalb der negativen Quellen\n",
    "        df = df.drop_duplicates(subset=[\"Epitope\", \"TRB_CDR3\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    neg_source_1 = filter_df(neg_source_1)\n",
    "    neg_source_2 = filter_df(neg_source_2)\n",
    "\n",
    "    # --- Vorverarbeitung für TPP-Filterung ---\n",
    "    trainval_tcrs = set(pd.concat([train_final, val_final])['TRB_CDR3'])\n",
    "    trainval_epitopes = set(pd.concat([train_final, val_final])['Epitope'])\n",
    "\n",
    "    # Mapping beider Quellen nach TPP-Typ\n",
    "    combined_sources = pd.concat([neg_source_1, neg_source_2], ignore_index=True)\n",
    "    \n",
    "    task_map = {\n",
    "        \"TPP1\": [],\n",
    "        \"TPP2\": [],\n",
    "        \"TPP3\": [],\n",
    "        \"TPP4\": []\n",
    "    }\n",
    "    \n",
    "    for _, row in combined_sources.iterrows():\n",
    "        task = classify_task(row[\"TRB_CDR3\"], row[\"Epitope\"], trainval_tcrs, trainval_epitopes)\n",
    "        task_map[task].append(row)\n",
    "\n",
    "    # Konvertieren zu DataFrames\n",
    "    task_dfs = {task: pd.DataFrame(rows) for task, rows in task_map.items()}\n",
    "\n",
    "    neg_selected = []\n",
    "    for task, ratio in tpp_target_distribution.items():\n",
    "        count = int(target_neg_count * ratio)\n",
    "        df_pool = task_dfs.get(task, pd.DataFrame())\n",
    "        if not df_pool.empty:\n",
    "            count = min(count, len(df_pool))\n",
    "            neg_selected.append(df_pool.sample(n=count, random_state=42))\n",
    "\n",
    "    neg_source_1 = pd.concat(neg_selected, ignore_index=True)\n",
    "    print(\"✅ Neue Verteilung:\")\n",
    "    for task, df in task_dfs.items():\n",
    "        print(f\"{task}: {len(df)} → verwendet: {len([d for d in neg_selected if d.shape[0] and classify_task(d.iloc[0]['TRB_CDR3'], d.iloc[0]['Epitope'], trainval_tcrs, trainval_epitopes) == task])}\")\n",
    "\n",
    "\n",
    "    # --- Mindestens 1x alle Epitope aus beiden Quellen übernehmen ---\n",
    "    def ensure_epitope_coverage(df):\n",
    "        guaranteed = []\n",
    "        for epitope in df['Epitope'].unique():\n",
    "            group = df[df['Epitope'] == epitope]\n",
    "            if not group.empty:\n",
    "                guaranteed.append(group.sample(1, random_state=42))\n",
    "        return pd.concat(guaranteed, ignore_index=True)\n",
    "\n",
    "    guaranteed_1 = ensure_epitope_coverage(neg_source_1)\n",
    "    guaranteed_2 = ensure_epitope_coverage(neg_source_2)\n",
    "    guaranteed_df = pd.concat([guaranteed_1, guaranteed_2], ignore_index=True)\n",
    "\n",
    "    # Begrenze garantierte, falls sie zu groß geworden sind\n",
    "    if len(guaranteed_df) > target_neg_count:\n",
    "        print(f\"Zu viele garantierte Negative ({len(guaranteed_df)}), trimme auf Zielmenge {target_neg_count}\")\n",
    "        guaranteed_df = guaranteed_df.sample(n=target_neg_count, random_state=42)\n",
    "\n",
    "    # --- Stratified Sampling für Restauffüllung ---\n",
    "    def stratified_sample(df, n):\n",
    "        epitope_groups = df.groupby('Epitope')\n",
    "        unique_epitopes = list(epitope_groups.groups.keys())\n",
    "        print(f\"→ Stratified sampling from {len(df)} rows | {len(unique_epitopes)} unique epitopes | need {n} samples\")\n",
    "    \n",
    "        # Schritt 1: Garantiert 1 Sample pro Epitope\n",
    "        guaranteed = [group.sample(1, random_state=42) for _, group in epitope_groups]\n",
    "        guaranteed_df = pd.concat(guaranteed, ignore_index=True)\n",
    "    \n",
    "        remaining_n = n - len(guaranteed_df)\n",
    "        if remaining_n <= 0:\n",
    "            return guaranteed_df.sample(n=n, random_state=42)\n",
    "    \n",
    "        # Schritt 2: Aufstocken durch gewichtetes Sampling\n",
    "        remaining_pool = df.drop(index=guaranteed_df.index, errors='ignore')\n",
    "    \n",
    "        # Gewichte: Häufigkeit pro Epitope → normalize\n",
    "        epitope_counts = remaining_pool['Epitope'].value_counts()\n",
    "        remaining_pool = remaining_pool.copy()\n",
    "        remaining_pool['weight'] = remaining_pool['Epitope'].map(epitope_counts)\n",
    "        total = remaining_pool['weight'].sum()\n",
    "        remaining_pool['weight'] = remaining_pool['weight'] / total\n",
    "    \n",
    "        print(f\"→ Stratified fill-in: drawing {remaining_n} samples weighted by epitope frequency\")\n",
    "    \n",
    "        replace = len(remaining_pool) < remaining_n\n",
    "        if replace:\n",
    "            print(f\"Achtung: Sampling mit Replacement (n={remaining_n}, pool={len(remaining_pool)})\")\n",
    "        \n",
    "        sampled_rest = remaining_pool.sample(\n",
    "            n=remaining_n,\n",
    "            weights='weight',\n",
    "            replace=replace,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        final_df = pd.concat([guaranteed_df, sampled_rest], ignore_index=True)\n",
    "        return final_df\n",
    "\n",
    "    remaining_needed = target_neg_count - len(guaranteed_df)\n",
    "    if remaining_needed <= 0:\n",
    "        print(f\"Es wurden bereits {len(guaranteed_df)} garantierte Samples übernommen (mehr als benötigt).\")\n",
    "        final_df = guaranteed_df.sample(n=target_neg_count, random_state=42)\n",
    "    else:\n",
    "        # Garantierte rausnehmen\n",
    "        used_idx_1 = guaranteed_1.index if not guaranteed_1.empty else []\n",
    "        used_idx_2 = guaranteed_2.index if not guaranteed_2.empty else []\n",
    "\n",
    "        remaining_1 = neg_source_1\n",
    "        remaining_2 = neg_source_2\n",
    "\n",
    "        half = remaining_needed // 2\n",
    "        rest = remaining_needed - half\n",
    "\n",
    "        sample_1 = stratified_sample(remaining_1, half)\n",
    "        sample_2 = stratified_sample(remaining_2, rest)\n",
    "\n",
    "        final_df = pd.concat([guaranteed_df, sample_1, sample_2], ignore_index=True)\n",
    "\n",
    "    return final_df.drop(columns=['Pair'])\n",
    "\n",
    "def classify_task(tcr, epitope, train_tcrs, train_epitopes):\n",
    "    seen_tcr = tcr in train_tcrs\n",
    "    seen_epi = epitope in train_epitopes\n",
    "    if seen_tcr and seen_epi:\n",
    "        return 'TPP1'\n",
    "    elif not seen_tcr and seen_epi:\n",
    "        return 'TPP2'\n",
    "    elif not seen_tcr and not seen_epi:\n",
    "        return 'TPP3'\n",
    "    elif seen_tcr and not seen_epi:\n",
    "        return 'TPP4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/968474884.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# === Datei- und Pfadangaben ===\n",
    "\n",
    "# Originaldaten (10X)\n",
    "beta = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv', sep=',')\n",
    "\n",
    "# Generierte Negativdaten\n",
    "neg_ba_train = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/combined_no_duplicates/train_neg_combined.tsv\", sep='\\t')\n",
    "neg_ba_val   = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/combined_no_duplicates/val_neg_combined.tsv\", sep='\\t')\n",
    "neg_ba_test  = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/combined_no_duplicates/test_neg_combined.tsv\", sep='\\t')\n",
    "\n",
    "# Positive Beispiele\n",
    "train_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n",
    "val_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/validation_prenegsamples.tsv', sep='\\t')\n",
    "test_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv', sep='\\t')\n",
    "\n",
    "# Output-Ziele\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "# VDJdb zum Herausfiltern\n",
    "vdjdb_df = pd.read_csv(VDJdb_cleaned_beta_output, sep='\\t')\n",
    "vdjdb_tcrs = set(vdjdb_df['TRB_CDR3'])\n",
    "vdjdb_epitopes = set(vdjdb_df['Epitope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Neue Verteilung:\n",
      "TPP1: 2710167 → verwendet: 1\n",
      "TPP2: 5684 → verwendet: 1\n",
      "TPP3: 0 → verwendet: 0\n",
      "TPP4: 0 → verwendet: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 23\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Zielverteilung Test\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tpp_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPP1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.4\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPP2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPP3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPP4\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     21\u001b[0m }\n\u001b[0;32m---> 23\u001b[0m test_neg \u001b[38;5;241m=\u001b[39m create_balanced_negatives_with_all_epitopes(\n\u001b[1;32m     24\u001b[0m     neg_source_1\u001b[38;5;241m=\u001b[39mneg_10x,\n\u001b[1;32m     25\u001b[0m     neg_source_2\u001b[38;5;241m=\u001b[39mneg_ba_test,\n\u001b[1;32m     26\u001b[0m     target_neg_count\u001b[38;5;241m=\u001b[39mtest_neg_target,\n\u001b[1;32m     27\u001b[0m     used_pairs\u001b[38;5;241m=\u001b[39mused_trainval_pairs,\n\u001b[1;32m     28\u001b[0m     vdjdb_tcrs\u001b[38;5;241m=\u001b[39mvdjdb_tcrs,\n\u001b[1;32m     29\u001b[0m     vdjdb_epitopes\u001b[38;5;241m=\u001b[39mvdjdb_epitopes,\n\u001b[1;32m     30\u001b[0m     tpp_target_distribution\u001b[38;5;241m=\u001b[39mtpp_dist  \n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Combine & save\u001b[39;00m\n\u001b[1;32m     34\u001b[0m test_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([test_preneg, test_neg], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 82\u001b[0m, in \u001b[0;36mcreate_balanced_negatives_with_all_epitopes\u001b[0;34m(neg_source_1, neg_source_2, target_neg_count, used_pairs, vdjdb_tcrs, vdjdb_epitopes, ensure_all_neg_epitopes, tpp_target_distribution)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(guaranteed, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m guaranteed_1 \u001b[38;5;241m=\u001b[39m ensure_epitope_coverage(neg_source_1)\n\u001b[0;32m---> 82\u001b[0m guaranteed_2 \u001b[38;5;241m=\u001b[39m ensure_epitope_coverage(neg_source_2)\n\u001b[1;32m     83\u001b[0m guaranteed_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([guaranteed_1, guaranteed_2], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Begrenze garantierte, falls sie zu groß geworden sind\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[68], line 79\u001b[0m, in \u001b[0;36mcreate_balanced_negatives_with_all_epitopes.<locals>.ensure_epitope_coverage\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     78\u001b[0m         guaranteed\u001b[38;5;241m.\u001b[39mappend(group\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(guaranteed, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# === TESTDATEN VERARBEITUNG ===\n",
    "# Zielmenge: 1:5 Verhältnis\n",
    "num_test_pos = len(test_preneg)\n",
    "test_neg_target = num_test_pos * 5\n",
    "\n",
    "neg_10x = beta[beta['Binding'] == 0]\n",
    "train_final = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\", sep='\\t')\n",
    "val_final = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\", sep='\\t')\n",
    "\n",
    "# Paare aus Train/Val ausschließen\n",
    "used_trainval_pairs = set(\n",
    "    map(tuple, pd.concat([train_final, val_final])[['Epitope', 'TRB_CDR3']].values)\n",
    ")\n",
    "\n",
    "# Zielverteilung Test\n",
    "tpp_dist = {\n",
    "    \"TPP1\": 0.4,\n",
    "    \"TPP2\": 0.3,\n",
    "    \"TPP3\": 0.2,\n",
    "    \"TPP4\": 0.1\n",
    "}\n",
    "\n",
    "test_neg = create_balanced_negatives_with_all_epitopes(\n",
    "    neg_source_1=neg_10x,\n",
    "    neg_source_2=neg_ba_test,\n",
    "    target_neg_count=test_neg_target,\n",
    "    used_pairs=used_trainval_pairs,\n",
    "    vdjdb_tcrs=vdjdb_tcrs,\n",
    "    vdjdb_epitopes=vdjdb_epitopes,\n",
    "    tpp_target_distribution=tpp_dist  \n",
    ")\n",
    "\n",
    "# Combine & save\n",
    "test_final = pd.concat([test_preneg, test_neg], ignore_index=True).sample(frac=1, random_state=42)\n",
    "test_final.to_csv(test_output_path, sep='\\t', index=False)\n",
    "\n",
    "df_check = pd.read_csv(test_output_path, sep='\\t')\n",
    "print(df_check['Binding'].value_counts())\n",
    "\n",
    "# Ausgabe\n",
    "print(\"✅ Testset erfolgreich erstellt & gespeichert.\")\n",
    "print(f\"Test: {len(test_final)} Beispiele\")\n",
    "print(f\"- Binding=1: {test_final['Binding'].value_counts().get(1, 0)}\")\n",
    "print(f\"- Binding=0: {test_final['Binding'].value_counts().get(0, 0)}\")\n",
    "print(f\"- Unique Epitope: {test_final['Epitope'].nunique()}\")\n",
    "print(f\"- Unique TCRs: {test_final['TRB_CDR3'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gewünschte Änderungen in den finalen Splits wurden vorgenommen und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Pfade zu finalen Splits ---\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path   = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path  = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "\n",
    "# --- Dateien einlesen ---\n",
    "train_df = pd.read_csv(output_train_path, sep='\\t')\n",
    "val_df   = pd.read_csv(output_val_path, sep='\\t')\n",
    "test_df  = pd.read_csv(test_output_path, sep='\\t')\n",
    "\n",
    "# --- Funktion zur Bearbeitung ---\n",
    "def clean_and_update(df):\n",
    "    if 'weight' in df.columns:\n",
    "        df = df.drop(columns=['weight'])\n",
    "    if 'Epitope MHC MHC class' in df.columns:\n",
    "        df = df.drop(columns=['Epitope MHC MHC class'])\n",
    "    if 'pair_count' in df.columns:\n",
    "        df = df.drop(columns=['pair_count'])\n",
    "    if 'epi_count' in df.columns:\n",
    "        df = df.drop(columns=['epi_count'])\n",
    "    if 'pair' in df.columns:\n",
    "        df = df.drop(columns=['pair'])\n",
    "    if 'source' not in df.columns:\n",
    "        df['source'] = ''\n",
    "    df.loc[df['Binding'] == 1, 'source'] = 'datasets'\n",
    "    return df\n",
    "\n",
    "# --- Anwenden ---\n",
    "train_df = clean_and_update(train_df)\n",
    "val_df   = clean_and_update(val_df)\n",
    "test_df  = clean_and_update(test_df)\n",
    "\n",
    "# --- Zurückschreiben (überschreibt die Dateien direkt) ---\n",
    "train_df.to_csv(output_train_path, sep='\\t', index=False)\n",
    "val_df.to_csv(output_val_path, sep='\\t', index=False)\n",
    "test_df.to_csv(test_output_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Gewünschte Änderungen in den finalen Splits wurden vorgenommen und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exakte Duplikate im Testset entfernt. Verbleibend: 97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pfad zur Test-Datei\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "\n",
    "# Laden\n",
    "df_test = pd.read_csv(test_path, sep=\"\\t\")\n",
    "\n",
    "# Separiere Binding = 0 und = 1\n",
    "df_neg = df_test[df_test[\"Binding\"] == 0]\n",
    "df_pos = df_test[df_test[\"Binding\"] == 1]\n",
    "\n",
    "# Entferne exakte Duplikate unter den negativen Beispielen\n",
    "df_neg_cleaned = df_neg.drop_duplicates(subset=[\"Epitope\", \"TRB_CDR3\", \"Binding\"])\n",
    "\n",
    "# Kombiniere wieder\n",
    "df_test_cleaned = pd.concat([df_pos, df_neg_cleaned], ignore_index=True)\n",
    "\n",
    "# Speichern\n",
    "df_test_cleaned.to_csv(test_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Check zur Sicherheit\n",
    "remaining_dups = df_test_cleaned.duplicated(subset=[\"Epitope\", \"TRB_CDR3\", \"Binding\"]).sum()\n",
    "print(f\"✅ Exakte Duplikate im Testset entfernt. Verbleibend: {remaining_dups}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hinzugefügte neue TPP3-Beispiele: 53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Negativdatenquellen\n",
    "beta = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv', sep=',')\n",
    "neg_ba_train = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/train_neg_clean.tsv\", sep='\\t')\n",
    "neg_ba_val = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/val_neg_clean.tsv\", sep='\\t')\n",
    "neg_ba_test = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/test_neg_clean.tsv\", sep='\\t')\n",
    "\n",
    "\n",
    "train_file = f'{pipeline_data_splitted}/{precision}/beta/new/train.tsv'\n",
    "validation_file = f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv'\n",
    "test_file = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "# Lade die bestehenden train/val/test-Sets\n",
    "train_df = pd.read_csv(train_file, sep=\"\\t\")\n",
    "val_df = pd.read_csv(validation_file, sep=\"\\t\")\n",
    "test_df = pd.read_csv(test_file, sep=\"\\t\")\n",
    "\n",
    "# Already used (TRB_CDR3, Epitope) pairs\n",
    "used_pairs = set(pd.concat([train_df, val_df, test_df])[[\"TRB_CDR3\", \"Epitope\"]].apply(tuple, axis=1))\n",
    "\n",
    "# Klassifizierungsfunktion\n",
    "def classify_task(tcr, epitope, train_tcrs, train_epitopes):\n",
    "    seen_tcr = tcr in train_tcrs\n",
    "    seen_epi = epitope in train_epitopes\n",
    "    if seen_tcr and seen_epi:\n",
    "        return 'TPP1'\n",
    "    elif not seen_tcr and seen_epi:\n",
    "        return 'TPP2'\n",
    "    elif not seen_tcr and not seen_epi:\n",
    "        return 'TPP3'\n",
    "    elif seen_tcr and not seen_epi:\n",
    "        return 'TPP4'\n",
    "\n",
    "# Kontext für Klassifizierung\n",
    "trainval_tcrs = set(pd.concat([train_df, val_df])[\"TRB_CDR3\"])\n",
    "trainval_epitopes = set(pd.concat([train_df, val_df])[\"Epitope\"])\n",
    "\n",
    "# Kombinierte Quellen\n",
    "all_neg_sources = pd.concat([beta, neg_ba_train, neg_ba_val, neg_ba_test], ignore_index=True)\n",
    "\n",
    "# Finde TPP3 inline (ohne neue Spalte)\n",
    "def is_tpp3_not_used(row):\n",
    "    return (\n",
    "        classify_task(row[\"TRB_CDR3\"], row[\"Epitope\"], trainval_tcrs, trainval_epitopes) == \"TPP3\" and\n",
    "        (row[\"TRB_CDR3\"], row[\"Epitope\"]) not in used_pairs\n",
    "    )\n",
    "\n",
    "tpp3_new = all_neg_sources[all_neg_sources.apply(is_tpp3_not_used, axis=1)]\n",
    "\n",
    "# Füge zu Test hinzu\n",
    "test_df_extended = pd.concat([test_df, tpp3_new], ignore_index=True)\n",
    "\n",
    "# Speichern\n",
    "test_df_extended.to_csv(test_file, sep=\"\\t\", index=False)\n",
    "\n",
    "# Ausgabe: Anzahl neuer TPP3\n",
    "print(f\"✅ Hinzugefügte neue TPP3-Beispiele: {len(tpp3_new)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Klassifikation an Validation Angleichung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Für Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for beta dataset\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "read_path_train = f\"{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv\" #fürs erste mal durchführen train_prenegsamples und new weg\n",
    "read_path_test = f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv' #fürs erste mal durchführen test_prenegsamples und new weg\n",
    "read_path_validation = f\"{pipeline_data_splitted}/{precision}/beta/validation_prenegsamples.tsv\" #fürs erste mal durchführen validation_prenegsamples und new weg\n",
    "temp_path = f'{pipeline_data_temp_bucket}/negative_samples/beta/'\n",
    "output_path = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives\"\n",
    "train_output_name = \"train_neg_tpp3.tsv\"\n",
    "validation_output_name = \"val_neg_tpp3.tsv\"\n",
    "test_output_name = \"test_neg_tpp3.tsv\"\n",
    "\n",
    "create_folders_if_not_exists([temp_path])\n",
    "\n",
    "%run ../negative_samples/negative_samples_beta_tpp_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spalte 'source' auf 'generated' gesetzt und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Dateipfade ---\n",
    "read_path_val = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{validation_output_name}\"\n",
    "read_path_test = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{test_output_name}\"\n",
    "\n",
    "# --- Dateien laden ---\n",
    "val_neg = pd.read_csv(read_path_val, sep='\\t')\n",
    "test_neg = pd.read_csv(read_path_test, sep='\\t')\n",
    "\n",
    "# --- Spalte \"source\" setzen ---\n",
    "for df in [val_neg, test_neg]:\n",
    "    df[\"source\"] = \"generated\"\n",
    "\n",
    "# --- Zurückspeichern ---\n",
    "val_neg.to_csv(read_path_val, sep='\\t', index=False)\n",
    "test_neg.to_csv(read_path_test, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Spalte 'source' auf 'generated' gesetzt und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/402861573.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(val_path, sep='\\t')\n",
      "/tmp/ipykernel_164984/402861573.py:14: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(test_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Predicted Tasks Verteilung für neue Validation-Negative:\n",
      "task_predicted\n",
      "TPP1    2426\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📊 Predicted Tasks Verteilung für neue Test-Negative:\n",
      "task_predicted\n",
      "TPP3    1030\n",
      "Name: count, dtype: int64\n",
      "\\n🔎 Validation: 122 Duplikate gefunden.\n",
      "🔎 Test: 645 Duplikate gefunden.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Pfade ---\n",
    "train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "\n",
    "neg_val_path = f\"{output_path}/{validation_output_name}\"\n",
    "neg_test_path = f\"{output_path}/{test_output_name}\"\n",
    "\n",
    "# --- Daten laden ---\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "val_df = pd.read_csv(val_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "neg_val_df = pd.read_csv(neg_val_path, sep='\\t')\n",
    "neg_test_df = pd.read_csv(neg_test_path, sep='\\t')\n",
    "\n",
    "# --- Existing train/val/test Kontext für Klassifizierung ---\n",
    "trainval_tcrs = set(pd.concat([train_df, val_df])['TRB_CDR3'])\n",
    "trainval_epitopes = set(pd.concat([train_df, val_df])['Epitope'])\n",
    "\n",
    "# --- Klassifikationsfunktion ---\n",
    "def classify_task(tcr, epitope):\n",
    "    seen_tcr = tcr in trainval_tcrs\n",
    "    seen_epi = epitope in trainval_epitopes\n",
    "    if seen_tcr and seen_epi:\n",
    "        return 'TPP1'\n",
    "    elif not seen_tcr and seen_epi:\n",
    "        return 'TPP2'\n",
    "    elif not seen_tcr and not seen_epi:\n",
    "        return 'TPP3'\n",
    "    elif seen_tcr and not seen_epi:\n",
    "        return 'TPP4'\n",
    "\n",
    "# --- Tasks für neue Negatives zuweisen ---\n",
    "neg_val_df['task_predicted'] = neg_val_df.apply(lambda row: classify_task(row['TRB_CDR3'], row['Epitope']), axis=1)\n",
    "neg_test_df['task_predicted'] = neg_test_df.apply(lambda row: classify_task(row['TRB_CDR3'], row['Epitope']), axis=1)\n",
    "\n",
    "# --- Prüfen wie viele wirklich TPP3 sind ---\n",
    "print(\"\\n📊 Predicted Tasks Verteilung für neue Validation-Negative:\")\n",
    "print(neg_val_df['task_predicted'].value_counts())\n",
    "\n",
    "print(\"\\n📊 Predicted Tasks Verteilung für neue Test-Negative:\")\n",
    "print(neg_test_df['task_predicted'].value_counts())\n",
    "\n",
    "# --- Prüfen auf Duplikate (Epitope, TCR) gegen bestehende Beispiele ---\n",
    "used_pairs = set(pd.concat([train_df, val_df, test_df])[['TRB_CDR3', 'Epitope']].apply(tuple, axis=1))\n",
    "\n",
    "neg_val_pairs = set(neg_val_df[['TRB_CDR3', 'Epitope']].apply(tuple, axis=1))\n",
    "neg_test_pairs = set(neg_test_df[['TRB_CDR3', 'Epitope']].apply(tuple, axis=1))\n",
    "\n",
    "dups_val = neg_val_pairs.intersection(used_pairs)\n",
    "dups_test = neg_test_pairs.intersection(used_pairs)\n",
    "\n",
    "print(f\"\\\\n🔎 Validation: {len(dups_val)} Duplikate gefunden.\")\n",
    "print(f\"🔎 Test: {len(dups_test)} Duplikate gefunden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/3785446809.py:3: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(test_path, sep='\\t')\n",
      "/tmp/ipykernel_164984/3785446809.py:12: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\", sep='\\t'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 375 saubere neue TPP2-Negative im Test übrig.\n",
      "✅ Testset erfolgreich aktualisiert.\n"
     ]
    }
   ],
   "source": [
    "# --- Lade bestehende Testdaten ---\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# --- Lade neu generierte Negative ---\n",
    "neg_test_path = f\"{output_path}/{test_output_name}\"\n",
    "neg_test_df = pd.read_csv(neg_test_path, sep='\\t')\n",
    "\n",
    "# --- Positive Paare (TRB_CDR3, Epitope) aus bestehenden Daten ---\n",
    "existing_pairs = set(pd.concat([\n",
    "    pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\", sep='\\t'),\n",
    "    pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\", sep='\\t'),\n",
    "    test_df\n",
    "])[['TRB_CDR3', 'Epitope']].apply(tuple, axis=1))\n",
    "\n",
    "# --- Prüfe neue Negative auf Duplikate ---\n",
    "neg_test_df['pair'] = list(zip(neg_test_df['TRB_CDR3'], neg_test_df['Epitope']))\n",
    "\n",
    "# --- Nur behalten, was kein Duplikat ist ---\n",
    "neg_test_df_clean = neg_test_df[~neg_test_df['pair'].isin(existing_pairs)].drop(columns=['pair'])\n",
    "\n",
    "print(f\"✅ {len(neg_test_df_clean)} saubere neue TPP2-Negative im Test übrig.\")\n",
    "\n",
    "# --- Test aktualisieren ---\n",
    "test_df_final = pd.concat([test_df, neg_test_df_clean], ignore_index=True)\n",
    "test_df_final.to_csv(test_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Testset erfolgreich aktualisiert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/1846581037.py:3: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(test_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Aktuell TPP1 Test: 1129 Binder und 15981 Non-Binder\n",
      "🎯 Ziel: 1129 Binder und 5143 Non-Binder\n",
      "✅ Neue TPP1 Größe: 6272 Beispiele (Binder: 1129, Non-Binder: 5143)\n",
      "✅ Test-Set erfolgreich aktualisiert mit neuem TPP1-Verhältnis.\n"
     ]
    }
   ],
   "source": [
    "# --- Test laden ---\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# --- TPP1-Daten herausfiltern ---\n",
    "tpp1_df = test_df[test_df['task'] == 'TPP1']\n",
    "\n",
    "# --- Non-Binder und Binder trennen ---\n",
    "tpp1_nonbinder = tpp1_df[tpp1_df['Binding'] == 0]\n",
    "tpp1_binder = tpp1_df[tpp1_df['Binding'] == 1]\n",
    "\n",
    "print(f\"🔎 Aktuell TPP1 Test: {len(tpp1_binder)} Binder und {len(tpp1_nonbinder)} Non-Binder\")\n",
    "\n",
    "# --- Zielverhältnis ---\n",
    "target_binder_percentage = 18  # 18% Binder\n",
    "\n",
    "# --- Berechne gewünschte Gesamtanzahl an Samples für TPP1 ---\n",
    "target_total = int(len(tpp1_binder) / (target_binder_percentage / 100))\n",
    "target_nonbinder = target_total - len(tpp1_binder)\n",
    "\n",
    "print(f\"🎯 Ziel: {len(tpp1_binder)} Binder und {target_nonbinder} Non-Binder\")\n",
    "\n",
    "# --- Reduziere Non-Binder auf Zielmenge ---\n",
    "tpp1_nonbinder_reduced = tpp1_nonbinder.sample(n=target_nonbinder, random_state=42)\n",
    "\n",
    "# --- Neues TPP1-Set bauen ---\n",
    "tpp1_final = pd.concat([tpp1_binder, tpp1_nonbinder_reduced], ignore_index=True)\n",
    "\n",
    "print(f\"✅ Neue TPP1 Größe: {len(tpp1_final)} Beispiele (Binder: {tpp1_final['Binding'].sum()}, Non-Binder: {len(tpp1_final) - tpp1_final['Binding'].sum()})\")\n",
    "\n",
    "# --- Den Rest der Test behalten ---\n",
    "test_df_rest = test_df[test_df['task'] != 'TPP1']\n",
    "\n",
    "# --- Neues Test-Set bauen ---\n",
    "test_df_new = pd.concat([test_df_rest, tpp1_final], ignore_index=True)\n",
    "\n",
    "# --- Speichern ---\n",
    "test_df_new.to_csv(test_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Test-Set erfolgreich aktualisiert mit neuem TPP1-Verhältnis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Für Validation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/1496975124.py:28: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  beta_train_df = pd.read_csv(read_path_train, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for beta dataset\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "read_path_train = f\"{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv\" #fürs erste mal durchführen train_prenegsamples und new weg\n",
    "read_path_test = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv' \n",
    "read_path_validation = f\"{pipeline_data_splitted}/{precision}/beta/validation_prenegsamples.tsv\" #fürs erste mal durchführen validation_prenegsamples und new weg\n",
    "temp_path = f'{pipeline_data_temp_bucket}/negative_samples/beta/'\n",
    "output_path = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives\"\n",
    "train_output_name = \"train_neg_tpp2.tsv\"\n",
    "validation_output_name = \"val_neg_tpp2.tsv\"\n",
    "test_output_name = \"test_neg_tpp2.tsv\"\n",
    "\n",
    "create_folders_if_not_exists([temp_path])\n",
    "\n",
    "%run ../negative_samples/negative_samples_beta_task_val.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spalte 'source' auf 'generated' gesetzt und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Dateipfade ---\n",
    "read_path_val = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{validation_output_name}\"\n",
    "read_path_test = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{test_output_name}\"\n",
    "\n",
    "# --- Dateien laden ---\n",
    "val_neg = pd.read_csv(read_path_val, sep='\\t')\n",
    "test_neg = pd.read_csv(read_path_test, sep='\\t')\n",
    "\n",
    "# --- Spalte \"source\" setzen ---\n",
    "for df in [val_neg, test_neg]:\n",
    "    df[\"source\"] = \"generated\"\n",
    "\n",
    "# --- Zurückspeichern ---\n",
    "val_neg.to_csv(read_path_val, sep='\\t', index=False)\n",
    "test_neg.to_csv(read_path_test, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Spalte 'source' auf 'generated' gesetzt und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/1502064681.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(val_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Predicted Tasks Verteilung für neue Validation-Negative:\n",
      "task_predicted\n",
      "TPP2    8986\n",
      "TPP1    1854\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📊 Predicted Tasks Verteilung für neue Test-Negative:\n",
      "task_predicted\n",
      "TPP1    7128\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Pfade ---\n",
    "train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "\n",
    "neg_val_path = f\"{output_path}/{validation_output_name}\"\n",
    "neg_test_path = f\"{output_path}/{test_output_name}\"\n",
    "\n",
    "# --- Daten laden ---\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "val_df = pd.read_csv(val_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "neg_val_df = pd.read_csv(neg_val_path, sep='\\t')\n",
    "neg_test_df = pd.read_csv(neg_test_path, sep='\\t')\n",
    "\n",
    "# --- Paare erstellen ---\n",
    "used_pairs = set(pd.concat([train_df, val_df, test_df])[['TRB_CDR3', 'Epitope']].apply(tuple, axis=1))\n",
    "\n",
    "neg_val_df['pair'] = list(zip(neg_val_df['TRB_CDR3'], neg_val_df['Epitope']))\n",
    "neg_test_df['pair'] = list(zip(neg_test_df['TRB_CDR3'], neg_test_df['Epitope']))\n",
    "\n",
    "# --- Nur behalten, was kein Duplikat ist ---\n",
    "neg_val_df = neg_val_df[~neg_val_df['pair'].isin(used_pairs)].drop(columns=['pair'])\n",
    "neg_test_df = neg_test_df[~neg_test_df['pair'].isin(used_pairs)].drop(columns=['pair'])\n",
    "\n",
    "# --- Existing train/val Kontext für Klassifizierung \n",
    "trainval_tcrs = set(pd.concat([train_df, test_df])['TRB_CDR3'])\n",
    "trainval_epitopes = set(pd.concat([train_df, test_df])['Epitope'])\n",
    "\n",
    "# --- Klassifikationsfunktion ---\n",
    "def classify_task(tcr, epitope):\n",
    "    seen_tcr = tcr in trainval_tcrs\n",
    "    seen_epi = epitope in trainval_epitopes\n",
    "    if seen_tcr and seen_epi:\n",
    "        return 'TPP1'\n",
    "    elif not seen_tcr and seen_epi:\n",
    "        return 'TPP2'\n",
    "    elif not seen_tcr and not seen_epi:\n",
    "        return 'TPP3'\n",
    "    elif seen_tcr and not seen_epi:\n",
    "        return 'TPP4'\n",
    "\n",
    "# --- Tasks für neue saubere Negatives zuweisen ---\n",
    "neg_val_df['task_predicted'] = neg_val_df.apply(lambda row: classify_task(row['TRB_CDR3'], row['Epitope']), axis=1)\n",
    "neg_test_df['task_predicted'] = neg_test_df.apply(lambda row: classify_task(row['TRB_CDR3'], row['Epitope']), axis=1)\n",
    "\n",
    "# --- Prüfen wie viele wirklich TPP3 sind ---\n",
    "print(\"\\n📊 Predicted Tasks Verteilung für neue Validation-Negative:\")\n",
    "print(neg_val_df['task_predicted'].value_counts())\n",
    "\n",
    "print(\"\\n📊 Predicted Tasks Verteilung für neue Test-Negative:\")\n",
    "print(neg_test_df['task_predicted'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/1967676510.py:9: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(val_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 8986 saubere neue TPP2-Negative für Validation übrig.\n",
      "✅ Validation-Set erfolgreich aktualisiert nur mit TPP3-Negatives.\n"
     ]
    }
   ],
   "source": [
    "# --- Pfade ---\n",
    "train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_path = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "neg_test_path = f\"{output_path}/{test_output_name}\"\n",
    "\n",
    "# --- Daten laden ---\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "val_df = pd.read_csv(val_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "neg_test_df = pd.read_csv(neg_test_path, sep='\\t')\n",
    "\n",
    "# --- Paare erstellen ---\n",
    "existing_trainval_pairs = set(pd.concat([train_df, val_df])[['TRB_CDR3', 'Epitope']].apply(tuple, axis=1))\n",
    "existing_test_pairs = set(test_df[['TRB_CDR3', 'Epitope']].apply(tuple, axis=1))\n",
    "\n",
    "neg_val_df['pair'] = list(zip(neg_val_df['TRB_CDR3'], neg_val_df['Epitope']))\n",
    "\n",
    "# --- Step 1: Nur neue Paare, die noch nicht in Train+Val existieren\n",
    "neg_not_in_trainval = neg_val_df[~neg_val_df['pair'].isin(existing_trainval_pairs)]\n",
    "\n",
    "# --- Step 2: Von denen nur die behalten, die auch NICHT im Test existieren\n",
    "neg_safe_for_val = neg_not_in_trainval[~neg_not_in_trainval['pair'].isin(existing_test_pairs)].drop(columns=['pair'])\n",
    "\n",
    "# --- Step 3: Nur TPP3 Negatives auswählen ---\n",
    "neg_safe_for_val_tpp3 = neg_safe_for_val[neg_safe_for_val['task_predicted'] == 'TPP2']\n",
    "\n",
    "print(f\"✅ {len(neg_safe_for_val_tpp3)} saubere neue TPP2-Negative für Validation übrig.\")\n",
    "\n",
    "# --- Validation aktualisieren ---\n",
    "val_df_final = pd.concat([val_df, neg_safe_for_val_tpp3], ignore_index=True)\n",
    "val_df_final.to_csv(val_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Validation-Set erfolgreich aktualisiert nur mit TPP3-Negatives.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/352721293.py:3: DtypeWarning: Columns (1,7,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(val_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Aktuell TPP4 Validation: 16 Binder und 753 Non-Binder\n",
      "🎯 Ziel: 16 Binder und 84 Non-Binder\n",
      "✅ Neue TPP4 Größe: 100 Beispiele (Binder: 16, Non-Binder: 84)\n",
      "✅ Validation-Set erfolgreich aktualisiert mit neuem TPP4-Verhältnis.\n"
     ]
    }
   ],
   "source": [
    "# --- Validation laden ---\n",
    "val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "val_df = pd.read_csv(val_path, sep='\\t')\n",
    "\n",
    "# --- TPP4-Daten herausfiltern ---\n",
    "tpp4_df = val_df[val_df['task'] == 'TPP4']\n",
    "\n",
    "# --- Non-Binder und Binder trennen ---\n",
    "tpp4_nonbinder = tpp4_df[tpp4_df['Binding'] == 0]\n",
    "tpp4_binder = tpp4_df[tpp4_df['Binding'] == 1]\n",
    "\n",
    "print(f\"🔎 Aktuell TPP4 Validation: {len(tpp4_binder)} Binder und {len(tpp4_nonbinder)} Non-Binder\")\n",
    "\n",
    "# --- Zielverhältnis ---\n",
    "target_binder_percentage = 16  # 16% Binder, 84% Non-Binder\n",
    "\n",
    "# --- Berechne gewünschte Gesamtanzahl an Samples für TPP4 ---\n",
    "target_total = int(len(tpp4_binder) / (target_binder_percentage / 100))\n",
    "target_nonbinder = target_total - len(tpp4_binder)\n",
    "\n",
    "print(f\"🎯 Ziel: {len(tpp4_binder)} Binder und {target_nonbinder} Non-Binder\")\n",
    "\n",
    "# --- Reduziere Non-Binder auf Zielmenge ---\n",
    "tpp4_nonbinder_reduced = tpp4_nonbinder.sample(n=target_nonbinder, random_state=42)\n",
    "\n",
    "# --- Neues TPP4-Set bauen ---\n",
    "tpp4_final = pd.concat([tpp4_binder, tpp4_nonbinder_reduced], ignore_index=True)\n",
    "\n",
    "print(f\"✅ Neue TPP4 Größe: {len(tpp4_final)} Beispiele (Binder: {tpp4_final['Binding'].sum()}, Non-Binder: {len(tpp4_final) - tpp4_final['Binding'].sum()})\")\n",
    "\n",
    "# --- Den Rest der Validation behalten ---\n",
    "val_df_rest = val_df[val_df['task'] != 'TPP4']\n",
    "\n",
    "# --- Neues Validation-Set bauen ---\n",
    "val_df_new = pd.concat([val_df_rest, tpp4_final], ignore_index=True)\n",
    "\n",
    "# --- Speichern ---\n",
    "val_df_new.to_csv(val_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Validation-Set erfolgreich aktualisiert mit neuem TPP4-Verhältnis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/2670492909.py:3: DtypeWarning: Columns (1,7,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(val_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Aktuell TPP3 Validation: 12792 Binder und 96877 Non-Binder\n",
      "🎯 Ziel: 12792 Binder und 67158 Non-Binder\n",
      "✅ Neue TPP1 Größe: 79950 Beispiele (Binder: 12792, Non-Binder: 67158)\n",
      "✅ Validation-Set erfolgreich aktualisiert mit neuem TPP3-Verhältnis.\n"
     ]
    }
   ],
   "source": [
    "# --- Validation laden ---\n",
    "val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "val_df = pd.read_csv(val_path, sep='\\t')\n",
    "\n",
    "# --- TPP3-Daten herausfiltern ---\n",
    "tpp3_df = val_df[val_df['task'] == 'TPP1']\n",
    "\n",
    "# --- Non-Binder und Binder trennen ---\n",
    "tpp3_nonbinder = tpp3_df[tpp3_df['Binding'] == 0]\n",
    "tpp3_binder = tpp3_df[tpp3_df['Binding'] == 1]\n",
    "\n",
    "print(f\"🔎 Aktuell TPP3 Validation: {len(tpp3_binder)} Binder und {len(tpp3_nonbinder)} Non-Binder\")\n",
    "\n",
    "# --- Zielverhältnis ---\n",
    "target_binder_percentage = 16  # 20% Binder, 80% Non-Binder\n",
    "\n",
    "# --- Berechne gewünschte Gesamtanzahl an Samples für TPP3 ---\n",
    "target_total = int(len(tpp3_binder) / (target_binder_percentage / 100))\n",
    "target_nonbinder = target_total - len(tpp3_binder)\n",
    "\n",
    "print(f\"🎯 Ziel: {len(tpp3_binder)} Binder und {target_nonbinder} Non-Binder\")\n",
    "\n",
    "# --- Reduziere Non-Binder auf Zielmenge ---\n",
    "tpp3_nonbinder_reduced = tpp3_nonbinder.sample(n=target_nonbinder, random_state=42)\n",
    "\n",
    "# --- Neues TPP3-Set bauen ---\n",
    "tpp3_final = pd.concat([tpp3_binder, tpp3_nonbinder_reduced], ignore_index=True)\n",
    "\n",
    "print(f\"✅ Neue TPP1 Größe: {len(tpp3_final)} Beispiele (Binder: {tpp3_final['Binding'].sum()}, Non-Binder: {len(tpp3_final) - tpp3_final['Binding'].sum()})\")\n",
    "\n",
    "# --- Den Rest der Validation behalten ---\n",
    "val_df_rest = val_df[val_df['task'] != 'TPP1']\n",
    "\n",
    "# --- Neues Validation-Set bauen ---\n",
    "val_df_new = pd.concat([val_df_rest, tpp3_final], ignore_index=True)\n",
    "\n",
    "# --- Speichern ---\n",
    "val_df_new.to_csv(val_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Validation-Set erfolgreich aktualisiert mit neuem TPP3-Verhältnis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'train' aktualisiert: 'source' für 126463 positive Einträge auf 'dataset' gesetzt.\n",
      "✅ 'test' aktualisiert: 'source' für 9105 positive Einträge auf 'dataset' gesetzt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/2056519087.py:17: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'validation' aktualisiert: 'source' für 29559 positive Einträge auf 'dataset' gesetzt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Pfade ---\n",
    "datasets = {\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f'{pipeline_data_splitted}/{precision}/beta/new/train.tsv',\n",
    "        \"test\": f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv',\n",
    "        \"validation\": f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Verarbeitung: positive Bindings markieren ===\n",
    "for name, paths in datasets.items():\n",
    "    for split, path in paths.items():\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path, sep=\"\\t\")\n",
    "            \n",
    "            # Nur Binding==1 filtern\n",
    "            mask = df[\"Binding\"] == 1\n",
    "            df.loc[mask, \"source\"] = \"dataset\"\n",
    "            \n",
    "            # Datei überschreiben\n",
    "            df.to_csv(path, sep=\"\\t\", index=False)\n",
    "            print(f\"✅ '{split}' aktualisiert: 'source' für {mask.sum()} positive Einträge auf 'dataset' gesetzt.\")\n",
    "        else:\n",
    "            print(f\"⚠️ Datei nicht gefunden: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRAIN ---\n",
      "source\n",
      "10X          315652\n",
      "generated    315617\n",
      "dataset      126463\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- VALIDATION ---\n",
      "source\n",
      "generated    85279\n",
      "10X          54753\n",
      "dataset      29559\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- TEST ---\n",
      "source\n",
      "generated    31728\n",
      "dataset       9105\n",
      "10X           4279\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/1299836558.py:14: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Pfade\n",
    "datasets = {\n",
    "    \"train\": f'{pipeline_data_splitted}/{precision}/beta/new/train.tsv',\n",
    "    \"validation\": f'{pipeline_data_splitted}/{precision}/beta/new/validation.tsv',\n",
    "    \"test\": f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "}\n",
    "\n",
    "# Check der source-Verteilung\n",
    "for split, path in datasets.items():\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        print(f\"\\n--- {split.upper()} ---\")\n",
    "        if \"source\" in df.columns:\n",
    "            print(df[\"source\"].value_counts(dropna=False))\n",
    "        else:\n",
    "            print(\"⚠️ Keine 'source'-Spalte vorhanden.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Datei nicht gefunden: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Classification \n",
    "The classification in the split notebook correct for positive only data. After adding negative data, some classifications might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta/new'\n",
    "test_file_name = 'test.tsv'\n",
    "validation_file_name = 'validation.tsv'\n",
    "train_file_name = 'train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/1815672389.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_data_path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 82726 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 76994 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 600 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 42 TPP4 tasks (seen tcr & unseen epitope).\n"
     ]
    }
   ],
   "source": [
    "# do the classification for paired data\n",
    "paired = True\n",
    "test_data_path = f'{paired_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "train_data_path = f'{paired_output_folder}/{train_file_name}'\n",
    "\n",
    "%run ../data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allele\n",
      "../../data/splitted_datasets/allele/paired/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/1788404633.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_path, sep=\"\\t\", index_col=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train+validate data has 72656 entries\n",
      "test data has 160362 entries\n",
      "test data has 128885 TPP1 tasks (old value: 82726) (seen tcr & seen epitopes).\n",
      "test data has 30835 TPP2 tasks (old value: 76994) (unseen tcr & seen epitopes).\n",
      "test data has 508 TPP3 tasks (old value: 600) (unseen tcr & unseen epitope).\n",
      "test data has 134 TPP4 tasks (old value: 42) (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.31180423829918724/0.6881957617008128\n",
      "../../data/splitted_datasets/allele/paired/test_reclassified_paired_specific.tsv\n",
      "/home/ubuntu/arina/BA-Cancer-Immunotherapy\n",
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250310_171148-c8scp4az</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">kind-sun-24</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/allele/paired)... Done. 0.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1d4067a88546039870f6c520cee5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-sun-24</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250310_171148-c8scp4az/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extended classification for paired data\n",
    "train_path = f'{paired_output_folder}/{train_file_name}'\n",
    "validation_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "test_path = f'{paired_output_folder}/{test_file_name}'\n",
    "output_path = f'{paired_output_folder}/test_reclassified_paired_specific.tsv'\n",
    "paired_data_path = paired_output_folder\n",
    "alpha_cdr3_name = 'TRA_CDR3'\n",
    "beta_cdr3_name = 'TRB_CDR3'\n",
    "epitope_name = 'Epitope'\n",
    "task_name = 'task'\n",
    "\n",
    "%run ../data_preparation/paired_reclassification_testonly.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/1815672389.py:3: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_validation = pd.read_csv(validation_data_path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 6272 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 32753 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 5416 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 671 TPP4 tasks (seen tcr & unseen epitope).\n"
     ]
    }
   ],
   "source": [
    "# do the classification for beta data\n",
    "paired = False\n",
    "train_data_path = f'{beta_output_folder}/{train_file_name}'\n",
    "validation_data_path = f'{beta_output_folder}/{validation_file_name}'\n",
    "test_data_path = f'{beta_output_folder}/{test_file_name}'\n",
    "\n",
    "%run ../data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells the classification is checked. If the output says \"Classification is correct\", everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/788315683.py:21: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_file, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train+validate data has 72656 entries\n",
      "test data has 160362 entries\n",
      "test data has 82726 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 76994 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 600 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 42 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.31180423829918724/0.6881957617008128\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    160362\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification paired\n",
    "splitted_data_path = paired_output_folder\n",
    "\n",
    "%run ../data_preparation/check_task_classification_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/1838351243.py:20: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_validate = pd.read_csv(f\"{splitted_data_path}/{validation_file_name}\", sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 757732 entries\n",
      "test data has 45112 entries\n",
      "test data has 6272 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 32753 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 5416 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 671 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.9536092386637667/0.04639076133623327\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    45112\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification beta\n",
    "splitted_data_path = beta_output_folder\n",
    "\n",
    "%run ../data_preparation/check_task_classification_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'validation_prenegsamples.tsv', 'backup_res13042025', 'test.tsv', 'archiv', 'new', 'train.tsv', 'TCRPeg_data', 'validation.tsv', 'train_prenegsamples.tsv', 'test_prenegsamples.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(path_to_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/data_scripts/datapipeline/wandb/run-20250510_154922-dfmgu9y9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/dfmgu9y9' target=\"_blank\">valiant-deluge-648</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/dfmgu9y9' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/dfmgu9y9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../../../data/splitted_datasets/allele/paired)... Done. 0.3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">valiant-deluge-648</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/dfmgu9y9' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/dfmgu9y9</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250510_154922-dfmgu9y9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# upload paired data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "dataset_name = f'paired_{precision}'\n",
    "#main_project_name = os.getenv(\"MAIN_PROJECT_NAME\")\n",
    "main_project_name = f\"dataset-{precision}\"\n",
    "\n",
    "%run ../upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/data_scripts/datapipeline/wandb/run-20250510_155713-ate7wnih</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/ate7wnih' target=\"_blank\">decent-blaze-650</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/ate7wnih' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/ate7wnih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../../../data/splitted_datasets/allele/beta)... Done. 2.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619ea118967e41949593989936e4f191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.245 MB of 177.852 MB uploaded\\r'), FloatProgress(value=0.006998664355910054, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-blaze-650</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/ate7wnih' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/ate7wnih</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250510_155713-ate7wnih/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload beta data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "dataset_name = f'beta_{precision}'\n",
    "\n",
    "%run ../upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings >> ProtBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Sollte True zurückgeben\n",
    "print(torch.version.cuda)  # Sollte die richtige CUDA-Version anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164984/2599291069.py:11: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_paired_test = pd.read_csv(path_paired_test, sep=\"\\t\", index_col=False)\n",
      "/tmp/ipykernel_164984/2599291069.py:25: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_beta_validation = pd.read_csv(path_beta_validation, sep=\"\\t\", index_col=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n",
      "Model is on device: cuda:0\n",
      "Processing Batch:  0 64\n",
      "Processing Batch:  64 128\n",
      "Processing Batch:  128 192\n",
      "Processing Batch:  192 256\n",
      "Processing Batch:  256 320\n",
      "Processing Batch:  320 384\n",
      "Processing Batch:  384 448\n",
      "Processing Batch:  448 512\n",
      "Processing Batch:  512 576\n",
      "Processing Batch:  576 640\n",
      "Processing Batch:  640 704\n",
      "Processing Batch:  704 768\n",
      "Processing Batch:  768 832\n",
      "Processing Batch:  832 896\n",
      "Processing Batch:  896 960\n",
      "Processing Batch:  960 1024\n",
      "Processing Batch:  1024 1088\n",
      "Processing Batch:  1088 1152\n",
      "Processing Batch:  1152 1216\n",
      "Processing Batch:  1216 1280\n",
      "Processing Batch:  1280 1344\n",
      "Processing Batch:  1344 1408\n",
      "Processing Batch:  1408 1472\n",
      "Processing Batch:  1472 1536\n",
      "Processing Batch:  1536 1600\n",
      "Processing Batch:  1600 1664\n",
      "Processing Batch:  1664 1728\n",
      "Processing Batch:  1728 1792\n",
      "Processing Batch:  1792 1856\n",
      "Processing Batch:  1856 1920\n",
      "Processing Batch:  1920 1984\n",
      "Processing Batch:  1984 2048\n",
      "Processing Batch:  2048 2112\n",
      "Processing Batch:  2112 2176\n",
      "Processing Batch:  2176 2240\n",
      "Processing Batch:  2240 2304\n",
      "Processing Batch:  2304 2368\n",
      "Processing Batch:  2368 2432\n",
      "Processing Batch:  2432 2496\n",
      "Processing Batch:  2496 2560\n",
      "Processing Batch:  2560 2624\n",
      "Processing Batch:  2624 2688\n",
      "Processing Batch:  2688 2752\n",
      "Processing Batch:  2752 2816\n",
      "Processing Batch:  2816 2880\n",
      "Processing Batch:  2880 2944\n",
      "Processing Batch:  2944 3008\n",
      "Processing Batch:  3008 3072\n",
      "Processing Batch:  3072 3136\n",
      "Processing Batch:  3136 3200\n",
      "Processing Batch:  3200 3264\n",
      "Processing Batch:  3264 3328\n",
      "Processing Batch:  3328 3392\n",
      "Processing Batch:  3392 3456\n",
      "Processing Batch:  3456 3520\n",
      "Processing Batch:  3520 3584\n",
      "Processing Batch:  3584 3648\n",
      "Processing Batch:  3648 3712\n",
      "Processing Batch:  3712 3776\n",
      "Processing Batch:  3776 3840\n",
      "Processing Batch:  3840 3904\n",
      "Processing Batch:  3904 3968\n",
      "Processing Batch:  3968 4032\n",
      "Processing Batch:  4032 4096\n",
      "Processing Batch:  4096 4160\n",
      "Processing Batch:  4160 4224\n",
      "Processing Batch:  4224 4288\n",
      "Processing Batch:  4288 4352\n",
      "Processing Batch:  4352 4416\n",
      "Processing Batch:  4416 4480\n",
      "Processing Batch:  4480 4544\n",
      "Processing Batch:  4544 4608\n",
      "Processing Batch:  4608 4672\n",
      "Processing Batch:  4672 4736\n",
      "Processing Batch:  4736 4800\n",
      "Processing Batch:  4800 4864\n",
      "Processing Batch:  4864 4928\n",
      "Processing Batch:  4928 4992\n",
      "Processing Batch:  4992 5056\n",
      "Processing Batch:  5056 5120\n",
      "Processing Batch:  5120 5184\n",
      "Processing Batch:  5184 5248\n",
      "Processing Batch:  5248 5312\n",
      "Processing Batch:  5312 5376\n",
      "Processing Batch:  5376 5440\n",
      "Processing Batch:  5440 5504\n",
      "Processing Batch:  5504 5568\n",
      "Processing Batch:  5568 5632\n",
      "Processing Batch:  5632 5696\n",
      "Processing Batch:  5696 5760\n",
      "Processing Batch:  5760 5824\n",
      "Processing Batch:  5824 5888\n",
      "Processing Batch:  5888 5952\n",
      "Processing Batch:  5952 6016\n",
      "Processing Batch:  6016 6080\n",
      "Processing Batch:  6080 6144\n",
      "Processing Batch:  6144 6208\n",
      "Processing Batch:  6208 6272\n",
      "Processing Batch:  6272 6336\n",
      "Processing Batch:  6336 6400\n",
      "Processing Batch:  6400 6464\n",
      "Processing Batch:  6464 6528\n",
      "Processing Batch:  6528 6592\n",
      "Processing Batch:  6592 6656\n",
      "Processing Batch:  6656 6720\n",
      "Processing Batch:  6720 6784\n",
      "Processing Batch:  6784 6848\n",
      "Processing Batch:  6848 6912\n",
      "Processing Batch:  6912 6976\n",
      "Processing Batch:  6976 7040\n",
      "Processing Batch:  7040 7104\n",
      "Processing Batch:  7104 7168\n",
      "Processing Batch:  7168 7232\n",
      "Processing Batch:  7232 7296\n",
      "Processing Batch:  7296 7360\n",
      "Processing Batch:  7360 7424\n",
      "Processing Batch:  7424 7488\n",
      "Processing Batch:  7488 7552\n",
      "Processing Batch:  7552 7616\n",
      "Processing Batch:  7616 7680\n",
      "Processing Batch:  7680 7744\n",
      "Processing Batch:  7744 7808\n",
      "Processing Batch:  7808 7872\n",
      "Processing Batch:  7872 7936\n",
      "Processing Batch:  7936 8000\n",
      "Processing Batch:  8000 8064\n",
      "Processing Batch:  8064 8128\n",
      "Processing Batch:  8128 8192\n",
      "Processing Batch:  8192 8256\n",
      "Processing Batch:  8256 8320\n",
      "Processing Batch:  8320 8384\n",
      "Processing Batch:  8384 8448\n",
      "Processing Batch:  8448 8512\n",
      "Processing Batch:  8512 8576\n",
      "Processing Batch:  8576 8640\n",
      "Processing Batch:  8640 8704\n",
      "Processing Batch:  8704 8768\n",
      "Processing Batch:  8768 8832\n",
      "Processing Batch:  8832 8896\n",
      "Processing Batch:  8896 8960\n",
      "Processing Batch:  8960 9024\n",
      "Processing Batch:  9024 9088\n",
      "Processing Batch:  9088 9152\n",
      "Processing Batch:  9152 9216\n",
      "Processing Batch:  9216 9280\n",
      "Processing Batch:  9280 9344\n",
      "Processing Batch:  9344 9408\n",
      "Processing Batch:  9408 9472\n",
      "Processing Batch:  9472 9536\n",
      "Processing Batch:  9536 9600\n",
      "Processing Batch:  9600 9664\n",
      "Processing Batch:  9664 9728\n",
      "Processing Batch:  9728 9792\n",
      "Processing Batch:  9792 9856\n",
      "Processing Batch:  9856 9920\n",
      "Processing Batch:  9920 9984\n",
      "Processing Batch:  9984 10048\n",
      "Processing Batch:  10048 10112\n",
      "Processing Batch:  10112 10176\n",
      "Processing Batch:  10176 10240\n",
      "Processing Batch:  10240 10304\n",
      "Processing Batch:  10304 10368\n",
      "Processing Batch:  10368 10432\n",
      "Processing Batch:  10432 10496\n",
      "Processing Batch:  10496 10560\n",
      "Processing Batch:  10560 10624\n",
      "Processing Batch:  10624 10688\n",
      "Processing Batch:  10688 10752\n",
      "Processing Batch:  10752 10816\n",
      "Processing Batch:  10816 10880\n",
      "Processing Batch:  10880 10944\n",
      "Processing Batch:  10944 11008\n",
      "Processing Batch:  11008 11072\n",
      "Processing Batch:  11072 11136\n",
      "Processing Batch:  11136 11200\n",
      "Processing Batch:  11200 11264\n",
      "Processing Batch:  11264 11328\n",
      "Processing Batch:  11328 11392\n",
      "Processing Batch:  11392 11456\n",
      "Processing Batch:  11456 11520\n",
      "Processing Batch:  11520 11584\n",
      "Processing Batch:  11584 11648\n",
      "Processing Batch:  11648 11712\n",
      "Processing Batch:  11712 11776\n",
      "Processing Batch:  11776 11840\n",
      "Processing Batch:  11840 11904\n",
      "Processing Batch:  11904 11968\n",
      "Processing Batch:  11968 12032\n",
      "Processing Batch:  12032 12096\n",
      "Processing Batch:  12096 12160\n",
      "Processing Batch:  12160 12224\n",
      "Processing Batch:  12224 12288\n",
      "Processing Batch:  12288 12352\n",
      "Processing Batch:  12352 12416\n",
      "Processing Batch:  12416 12480\n",
      "Processing Batch:  12480 12544\n",
      "Processing Batch:  12544 12608\n",
      "Processing Batch:  12608 12672\n",
      "Processing Batch:  12672 12736\n",
      "Processing Batch:  12736 12800\n",
      "Processing Batch:  12800 12864\n",
      "Processing Batch:  12864 12928\n",
      "Processing Batch:  12928 12992\n",
      "Processing Batch:  12992 13056\n",
      "Processing Batch:  13056 13120\n",
      "Processing Batch:  13120 13184\n",
      "Processing Batch:  13184 13248\n",
      "Processing Batch:  13248 13312\n",
      "Processing Batch:  13312 13376\n",
      "Processing Batch:  13376 13440\n",
      "Processing Batch:  13440 13504\n",
      "Processing Batch:  13504 13568\n",
      "Processing Batch:  13568 13632\n",
      "Processing Batch:  13632 13696\n",
      "Processing Batch:  13696 13760\n",
      "Processing Batch:  13760 13824\n",
      "Processing Batch:  13824 13888\n",
      "Processing Batch:  13888 13952\n",
      "Processing Batch:  13952 14016\n",
      "Processing Batch:  14016 14080\n",
      "Processing Batch:  14080 14144\n",
      "Processing Batch:  14144 14208\n",
      "Processing Batch:  14208 14272\n",
      "Processing Batch:  14272 14336\n",
      "Processing Batch:  14336 14400\n",
      "Processing Batch:  14400 14464\n",
      "Processing Batch:  14464 14528\n",
      "Processing Batch:  14528 14592\n",
      "Processing Batch:  14592 14656\n",
      "Processing Batch:  14656 14720\n",
      "Processing Batch:  14720 14784\n",
      "Processing Batch:  14784 14848\n",
      "Processing Batch:  14848 14912\n",
      "Processing Batch:  14912 14976\n",
      "Processing Batch:  14976 15040\n",
      "Processing Batch:  15040 15104\n",
      "Processing Batch:  15104 15168\n",
      "Processing Batch:  15168 15232\n",
      "Processing Batch:  15232 15296\n",
      "Processing Batch:  15296 15360\n",
      "Processing Batch:  15360 15424\n",
      "Processing Batch:  15424 15488\n",
      "Processing Batch:  15488 15552\n",
      "Processing Batch:  15552 15616\n",
      "Processing Batch:  15616 15680\n",
      "Processing Batch:  15680 15744\n",
      "Processing Batch:  15744 15808\n",
      "Processing Batch:  15808 15872\n",
      "Processing Batch:  15872 15936\n",
      "Processing Batch:  15936 16000\n",
      "Processing Batch:  16000 16064\n",
      "Processing Batch:  16064 16128\n",
      "Processing Batch:  16128 16192\n",
      "Processing Batch:  16192 16256\n",
      "Processing Batch:  16256 16320\n",
      "Processing Batch:  16320 16384\n",
      "Processing Batch:  16384 16448\n",
      "Processing Batch:  16448 16512\n",
      "Processing Batch:  16512 16576\n",
      "Processing Batch:  16576 16640\n",
      "Processing Batch:  16640 16704\n",
      "Processing Batch:  16704 16768\n",
      "Processing Batch:  16768 16832\n",
      "Processing Batch:  16832 16896\n",
      "Processing Batch:  16896 16960\n",
      "Processing Batch:  16960 17024\n",
      "Processing Batch:  17024 17088\n",
      "Processing Batch:  17088 17152\n",
      "Processing Batch:  17152 17216\n",
      "Processing Batch:  17216 17280\n",
      "Processing Batch:  17280 17344\n",
      "Processing Batch:  17344 17408\n",
      "Processing Batch:  17408 17472\n",
      "Processing Batch:  17472 17536\n",
      "Processing Batch:  17536 17600\n",
      "Processing Batch:  17600 17664\n",
      "Processing Batch:  17664 17728\n",
      "Processing Batch:  17728 17792\n",
      "Processing Batch:  17792 17856\n",
      "Processing Batch:  17856 17920\n",
      "Processing Batch:  17920 17984\n",
      "Processing Batch:  17984 18048\n",
      "Processing Batch:  18048 18112\n",
      "Processing Batch:  18112 18176\n",
      "Processing Batch:  18176 18240\n",
      "Processing Batch:  18240 18304\n",
      "Processing Batch:  18304 18368\n",
      "Processing Batch:  18368 18432\n",
      "Processing Batch:  18432 18496\n",
      "Processing Batch:  18496 18560\n",
      "Processing Batch:  18560 18624\n",
      "Processing Batch:  18624 18688\n",
      "Processing Batch:  18688 18752\n",
      "Processing Batch:  18752 18816\n",
      "Processing Batch:  18816 18880\n",
      "Processing Batch:  18880 18944\n",
      "Processing Batch:  18944 19008\n",
      "Processing Batch:  19008 19072\n",
      "Processing Batch:  19072 19136\n",
      "Processing Batch:  19136 19200\n",
      "Processing Batch:  19200 19264\n",
      "Processing Batch:  19264 19328\n",
      "Processing Batch:  19328 19392\n",
      "Processing Batch:  19392 19456\n",
      "Processing Batch:  19456 19520\n",
      "Processing Batch:  19520 19584\n",
      "Processing Batch:  19584 19648\n",
      "Processing Batch:  19648 19712\n",
      "Processing Batch:  19712 19776\n",
      "Processing Batch:  19776 19840\n",
      "Processing Batch:  19840 19904\n",
      "Processing Batch:  19904 19968\n",
      "Processing Batch:  19968 20032\n",
      "Processing Batch:  20032 20096\n",
      "Processing Batch:  20096 20160\n",
      "Processing Batch:  20160 20224\n",
      "Processing Batch:  20224 20288\n",
      "Processing Batch:  20288 20352\n",
      "Processing Batch:  20352 20416\n",
      "Processing Batch:  20416 20480\n",
      "Processing Batch:  20480 20544\n",
      "Processing Batch:  20544 20608\n",
      "Processing Batch:  20608 20672\n",
      "Processing Batch:  20672 20736\n",
      "Processing Batch:  20736 20800\n",
      "Processing Batch:  20800 20864\n",
      "Processing Batch:  20864 20928\n",
      "Processing Batch:  20928 20992\n",
      "Processing Batch:  20992 21056\n",
      "Processing Batch:  21056 21120\n",
      "Processing Batch:  21120 21184\n",
      "Processing Batch:  21184 21248\n",
      "Processing Batch:  21248 21312\n",
      "Processing Batch:  21312 21376\n",
      "Processing Batch:  21376 21440\n",
      "Processing Batch:  21440 21504\n",
      "Processing Batch:  21504 21568\n",
      "Processing Batch:  21568 21632\n",
      "Processing Batch:  21632 21696\n",
      "Processing Batch:  21696 21760\n",
      "Processing Batch:  21760 21824\n",
      "Processing Batch:  21824 21888\n",
      "Processing Batch:  21888 21952\n",
      "Processing Batch:  21952 22016\n",
      "Processing Batch:  22016 22080\n",
      "Processing Batch:  22080 22144\n",
      "Processing Batch:  22144 22208\n",
      "Processing Batch:  22208 22272\n",
      "Processing Batch:  22272 22336\n",
      "Processing Batch:  22336 22400\n",
      "Processing Batch:  22400 22464\n",
      "Processing Batch:  22464 22528\n",
      "Processing Batch:  22528 22592\n",
      "Processing Batch:  22592 22656\n",
      "Processing Batch:  22656 22720\n",
      "Processing Batch:  22720 22784\n",
      "Processing Batch:  22784 22848\n",
      "Processing Batch:  22848 22912\n",
      "Processing Batch:  22912 22976\n",
      "Processing Batch:  22976 23040\n",
      "Processing Batch:  23040 23104\n",
      "Processing Batch:  23104 23168\n",
      "Processing Batch:  23168 23232\n",
      "Processing Batch:  23232 23296\n",
      "Processing Batch:  23296 23360\n",
      "Processing Batch:  23360 23424\n",
      "Processing Batch:  23424 23488\n",
      "Processing Batch:  23488 23552\n",
      "Processing Batch:  23552 23616\n",
      "Processing Batch:  23616 23680\n",
      "Processing Batch:  23680 23744\n",
      "Processing Batch:  23744 23808\n",
      "Processing Batch:  23808 23872\n",
      "Processing Batch:  23872 23936\n",
      "Processing Batch:  23936 24000\n",
      "Processing Batch:  24000 24064\n",
      "Processing Batch:  24064 24128\n",
      "Processing Batch:  24128 24192\n",
      "Processing Batch:  24192 24256\n",
      "Processing Batch:  24256 24320\n",
      "Processing Batch:  24320 24384\n",
      "Processing Batch:  24384 24448\n",
      "Processing Batch:  24448 24512\n",
      "Processing Batch:  24512 24576\n",
      "Processing Batch:  24576 24640\n",
      "Processing Batch:  24640 24704\n",
      "Processing Batch:  24704 24768\n",
      "Processing Batch:  24768 24832\n",
      "Processing Batch:  24832 24896\n",
      "Processing Batch:  24896 24960\n",
      "Processing Batch:  24960 25024\n",
      "Processing Batch:  25024 25088\n",
      "Processing Batch:  25088 25152\n",
      "Processing Batch:  25152 25216\n",
      "Processing Batch:  25216 25280\n",
      "Processing Batch:  25280 25344\n",
      "Processing Batch:  25344 25408\n",
      "Processing Batch:  25408 25472\n",
      "Processing Batch:  25472 25536\n",
      "Processing Batch:  25536 25600\n",
      "Processing Batch:  25600 25664\n",
      "Processing Batch:  25664 25728\n",
      "Processing Batch:  25728 25792\n",
      "Processing Batch:  25792 25856\n",
      "Processing Batch:  25856 25920\n",
      "Processing Batch:  25920 25984\n",
      "Processing Batch:  25984 26048\n",
      "Processing Batch:  26048 26112\n",
      "Processing Batch:  26112 26176\n",
      "Processing Batch:  26176 26240\n",
      "Processing Batch:  26240 26304\n",
      "Processing Batch:  26304 26368\n",
      "Processing Batch:  26368 26432\n",
      "Processing Batch:  26432 26496\n",
      "Processing Batch:  26496 26560\n",
      "Processing Batch:  26560 26624\n",
      "Processing Batch:  26624 26688\n",
      "Processing Batch:  26688 26752\n",
      "Processing Batch:  26752 26816\n",
      "Processing Batch:  26816 26880\n",
      "Processing Batch:  26880 26944\n",
      "Processing Batch:  26944 27008\n",
      "Processing Batch:  27008 27072\n",
      "Processing Batch:  27072 27136\n",
      "Processing Batch:  27136 27200\n",
      "Processing Batch:  27200 27264\n",
      "Processing Batch:  27264 27328\n",
      "Processing Batch:  27328 27392\n",
      "Processing Batch:  27392 27456\n",
      "Processing Batch:  27456 27520\n",
      "Processing Batch:  27520 27584\n",
      "Processing Batch:  27584 27648\n",
      "Processing Batch:  27648 27712\n",
      "Processing Batch:  27712 27776\n",
      "Processing Batch:  27776 27840\n",
      "Processing Batch:  27840 27904\n",
      "Processing Batch:  27904 27968\n",
      "Processing Batch:  27968 28032\n",
      "Processing Batch:  28032 28096\n",
      "Processing Batch:  28096 28160\n",
      "Processing Batch:  28160 28224\n",
      "Processing Batch:  28224 28288\n",
      "Processing Batch:  28288 28352\n",
      "Processing Batch:  28352 28416\n",
      "Processing Batch:  28416 28480\n",
      "Processing Batch:  28480 28544\n",
      "Processing Batch:  28544 28608\n",
      "Processing Batch:  28608 28672\n",
      "Processing Batch:  28672 28736\n",
      "Processing Batch:  28736 28800\n",
      "Processing Batch:  28800 28864\n",
      "Processing Batch:  28864 28928\n",
      "Processing Batch:  28928 28992\n",
      "Processing Batch:  28992 29056\n",
      "Processing Batch:  29056 29120\n",
      "Processing Batch:  29120 29184\n",
      "Processing Batch:  29184 29248\n",
      "Processing Batch:  29248 29312\n",
      "Processing Batch:  29312 29376\n",
      "Processing Batch:  29376 29440\n",
      "Processing Batch:  29440 29504\n",
      "Processing Batch:  29504 29568\n",
      "Processing Batch:  29568 29632\n",
      "Processing Batch:  29632 29696\n",
      "Processing Batch:  29696 29760\n",
      "Processing Batch:  29760 29824\n",
      "Processing Batch:  29824 29888\n",
      "Processing Batch:  29888 29952\n",
      "Processing Batch:  29952 30016\n",
      "Processing Batch:  30016 30080\n",
      "Processing Batch:  30080 30144\n",
      "Processing Batch:  30144 30208\n",
      "Processing Batch:  30208 30272\n",
      "Processing Batch:  30272 30336\n",
      "Processing Batch:  30336 30400\n",
      "Processing Batch:  30400 30464\n",
      "Processing Batch:  30464 30528\n",
      "Processing Batch:  30528 30592\n",
      "Processing Batch:  30592 30656\n",
      "Processing Batch:  30656 30720\n",
      "Processing Batch:  30720 30784\n",
      "Processing Batch:  30784 30848\n",
      "Processing Batch:  30848 30912\n",
      "Processing Batch:  30912 30976\n",
      "Processing Batch:  30976 31040\n",
      "Processing Batch:  31040 31104\n",
      "Processing Batch:  31104 31168\n",
      "Processing Batch:  31168 31232\n",
      "Processing Batch:  31232 31296\n",
      "Processing Batch:  31296 31360\n",
      "Processing Batch:  31360 31424\n",
      "Processing Batch:  31424 31488\n",
      "Processing Batch:  31488 31552\n",
      "Processing Batch:  31552 31616\n",
      "Processing Batch:  31616 31680\n",
      "Processing Batch:  31680 31744\n",
      "Processing Batch:  31744 31808\n",
      "Processing Batch:  31808 31872\n",
      "Processing Batch:  31872 31936\n",
      "Processing Batch:  31936 32000\n",
      "Processing Batch:  32000 32064\n",
      "Processing Batch:  32064 32128\n",
      "Processing Batch:  32128 32192\n",
      "Processing Batch:  32192 32256\n",
      "Processing Batch:  32256 32320\n",
      "Processing Batch:  32320 32384\n",
      "Processing Batch:  32384 32448\n",
      "Processing Batch:  32448 32512\n",
      "Processing Batch:  32512 32576\n",
      "Processing Batch:  32576 32640\n",
      "Processing Batch:  32640 32704\n",
      "Processing Batch:  32704 32768\n",
      "Processing Batch:  32768 32832\n",
      "Processing Batch:  32832 32896\n",
      "Processing Batch:  32896 32960\n",
      "Processing Batch:  32960 33024\n",
      "Processing Batch:  33024 33088\n",
      "Processing Batch:  33088 33152\n",
      "Processing Batch:  33152 33216\n",
      "Processing Batch:  33216 33280\n",
      "Processing Batch:  33280 33344\n",
      "Processing Batch:  33344 33408\n",
      "Processing Batch:  33408 33472\n",
      "Processing Batch:  33472 33536\n",
      "Processing Batch:  33536 33600\n",
      "Processing Batch:  33600 33664\n",
      "Processing Batch:  33664 33728\n",
      "Processing Batch:  33728 33792\n",
      "Processing Batch:  33792 33856\n",
      "Processing Batch:  33856 33920\n",
      "Processing Batch:  33920 33984\n",
      "Processing Batch:  33984 34048\n",
      "Processing Batch:  34048 34112\n",
      "Processing Batch:  34112 34176\n",
      "Processing Batch:  34176 34240\n",
      "Processing Batch:  34240 34304\n",
      "Processing Batch:  34304 34368\n",
      "Processing Batch:  34368 34432\n",
      "Processing Batch:  34432 34496\n",
      "Processing Batch:  34496 34560\n",
      "Processing Batch:  34560 34624\n",
      "Processing Batch:  34624 34688\n",
      "Processing Batch:  34688 34752\n",
      "Processing Batch:  34752 34816\n",
      "Processing Batch:  34816 34880\n",
      "Processing Batch:  34880 34944\n",
      "Processing Batch:  34944 35008\n",
      "Processing Batch:  35008 35072\n",
      "Processing Batch:  35072 35136\n",
      "Processing Batch:  35136 35200\n",
      "Processing Batch:  35200 35264\n",
      "Processing Batch:  35264 35328\n",
      "Processing Batch:  35328 35392\n",
      "Processing Batch:  35392 35456\n",
      "Processing Batch:  35456 35520\n",
      "Processing Batch:  35520 35584\n",
      "Processing Batch:  35584 35648\n",
      "Processing Batch:  35648 35712\n",
      "Processing Batch:  35712 35776\n",
      "Processing Batch:  35776 35840\n",
      "Processing Batch:  35840 35904\n",
      "Processing Batch:  35904 35968\n",
      "Processing Batch:  35968 36032\n",
      "Processing Batch:  36032 36096\n",
      "Processing Batch:  36096 36160\n",
      "Processing Batch:  36160 36224\n",
      "Processing Batch:  36224 36288\n",
      "Processing Batch:  36288 36352\n",
      "Processing Batch:  36352 36416\n",
      "Processing Batch:  36416 36480\n",
      "Processing Batch:  36480 36544\n",
      "Processing Batch:  36544 36608\n",
      "Processing Batch:  36608 36672\n",
      "Processing Batch:  36672 36736\n",
      "Processing Batch:  36736 36800\n",
      "Processing Batch:  36800 36864\n",
      "Processing Batch:  36864 36928\n",
      "Processing Batch:  36928 36992\n",
      "Processing Batch:  36992 37056\n",
      "Processing Batch:  37056 37120\n",
      "Processing Batch:  37120 37184\n",
      "Processing Batch:  37184 37248\n",
      "Processing Batch:  37248 37312\n",
      "Processing Batch:  37312 37376\n",
      "Processing Batch:  37376 37440\n",
      "Processing Batch:  37440 37504\n",
      "Processing Batch:  37504 37568\n",
      "Processing Batch:  37568 37632\n",
      "Processing Batch:  37632 37696\n",
      "Processing Batch:  37696 37760\n",
      "Processing Batch:  37760 37824\n",
      "Processing Batch:  37824 37888\n",
      "Processing Batch:  37888 37952\n",
      "Processing Batch:  37952 38016\n",
      "Processing Batch:  38016 38080\n",
      "Processing Batch:  38080 38144\n",
      "Processing Batch:  38144 38208\n",
      "Processing Batch:  38208 38272\n",
      "Processing Batch:  38272 38336\n",
      "Processing Batch:  38336 38400\n",
      "Processing Batch:  38400 38464\n",
      "Processing Batch:  38464 38528\n",
      "Processing Batch:  38528 38592\n",
      "Processing Batch:  38592 38656\n",
      "Processing Batch:  38656 38720\n",
      "Processing Batch:  38720 38784\n",
      "Processing Batch:  38784 38848\n",
      "Processing Batch:  38848 38912\n",
      "Processing Batch:  38912 38976\n",
      "Processing Batch:  38976 39040\n",
      "Processing Batch:  39040 39104\n",
      "Processing Batch:  39104 39168\n",
      "Processing Batch:  39168 39232\n",
      "Processing Batch:  39232 39296\n",
      "Processing Batch:  39296 39360\n",
      "Processing Batch:  39360 39424\n",
      "Processing Batch:  39424 39488\n",
      "Processing Batch:  39488 39552\n",
      "Processing Batch:  39552 39616\n",
      "Processing Batch:  39616 39680\n",
      "Processing Batch:  39680 39744\n",
      "Processing Batch:  39744 39808\n",
      "Processing Batch:  39808 39872\n",
      "Processing Batch:  39872 39936\n",
      "Processing Batch:  39936 40000\n",
      "Processing Batch:  40000 40064\n",
      "Processing Batch:  40064 40128\n",
      "Processing Batch:  40128 40192\n",
      "Processing Batch:  40192 40256\n",
      "Processing Batch:  40256 40320\n",
      "Processing Batch:  40320 40384\n",
      "Processing Batch:  40384 40448\n",
      "Processing Batch:  40448 40512\n",
      "Processing Batch:  40512 40576\n",
      "Processing Batch:  40576 40640\n",
      "Processing Batch:  40640 40704\n",
      "Processing Batch:  40704 40768\n",
      "Processing Batch:  40768 40832\n",
      "Processing Batch:  40832 40896\n",
      "Processing Batch:  40896 40960\n",
      "Processing Batch:  40960 41024\n",
      "Processing Batch:  41024 41088\n",
      "Processing Batch:  41088 41152\n",
      "Processing Batch:  41152 41216\n",
      "Processing Batch:  41216 41280\n",
      "Processing Batch:  41280 41344\n",
      "Processing Batch:  41344 41408\n",
      "Processing Batch:  41408 41472\n",
      "Processing Batch:  41472 41536\n",
      "Processing Batch:  41536 41600\n",
      "Processing Batch:  41600 41664\n",
      "Processing Batch:  41664 41728\n",
      "Processing Batch:  41728 41792\n",
      "Processing Batch:  41792 41856\n",
      "Processing Batch:  41856 41920\n",
      "Processing Batch:  41920 41984\n",
      "Processing Batch:  41984 42048\n",
      "Processing Batch:  42048 42112\n",
      "Processing Batch:  42112 42176\n",
      "Processing Batch:  42176 42240\n",
      "Processing Batch:  42240 42304\n",
      "Processing Batch:  42304 42368\n",
      "Processing Batch:  42368 42432\n",
      "Processing Batch:  42432 42496\n",
      "Processing Batch:  42496 42560\n",
      "Processing Batch:  42560 42624\n",
      "Processing Batch:  42624 42688\n",
      "Processing Batch:  42688 42752\n",
      "Processing Batch:  42752 42816\n",
      "Processing Batch:  42816 42880\n",
      "Processing Batch:  42880 42944\n",
      "Processing Batch:  42944 43008\n",
      "Processing Batch:  43008 43072\n",
      "Processing Batch:  43072 43136\n",
      "Processing Batch:  43136 43200\n",
      "Processing Batch:  43200 43264\n",
      "Processing Batch:  43264 43328\n",
      "Processing Batch:  43328 43392\n",
      "Processing Batch:  43392 43456\n",
      "Processing Batch:  43456 43520\n",
      "Processing Batch:  43520 43584\n",
      "Processing Batch:  43584 43648\n",
      "Processing Batch:  43648 43712\n",
      "Processing Batch:  43712 43776\n",
      "Processing Batch:  43776 43840\n",
      "Processing Batch:  43840 43904\n",
      "Processing Batch:  43904 43968\n",
      "Processing Batch:  43968 44032\n",
      "Processing Batch:  44032 44096\n",
      "Processing Batch:  44096 44160\n",
      "Processing Batch:  44160 44224\n",
      "Processing Batch:  44224 44288\n",
      "Processing Batch:  44288 44352\n",
      "Processing Batch:  44352 44416\n",
      "Processing Batch:  44416 44480\n",
      "Processing Batch:  44480 44544\n",
      "Processing Batch:  44544 44608\n",
      "Processing Batch:  44608 44672\n",
      "Processing Batch:  44672 44736\n",
      "Processing Batch:  44736 44800\n",
      "Processing Batch:  44800 44864\n",
      "Processing Batch:  44864 44928\n",
      "Processing Batch:  44928 44992\n",
      "Processing Batch:  44992 45056\n",
      "Processing Batch:  45056 45120\n",
      "Processing Batch:  45120 45184\n",
      "Processing Batch:  45184 45248\n",
      "Processing Batch:  45248 45312\n",
      "Processing Batch:  45312 45376\n",
      "Processing Batch:  45376 45440\n",
      "Processing Batch:  45440 45504\n",
      "Processing Batch:  45504 45568\n",
      "Processing Batch:  45568 45632\n",
      "Processing Batch:  45632 45696\n",
      "Processing Batch:  45696 45760\n",
      "Processing Batch:  45760 45824\n",
      "Processing Batch:  45824 45888\n",
      "Processing Batch:  45888 45952\n",
      "Processing Batch:  45952 46016\n",
      "Processing Batch:  46016 46080\n",
      "Processing Batch:  46080 46144\n",
      "Processing Batch:  46144 46208\n",
      "Processing Batch:  46208 46272\n",
      "Processing Batch:  46272 46336\n",
      "Processing Batch:  46336 46400\n",
      "Processing Batch:  46400 46464\n",
      "Processing Batch:  46464 46528\n",
      "Processing Batch:  46528 46592\n",
      "Processing Batch:  46592 46656\n",
      "Processing Batch:  46656 46720\n",
      "Processing Batch:  46720 46784\n",
      "Processing Batch:  46784 46848\n",
      "Processing Batch:  46848 46912\n",
      "Processing Batch:  46912 46976\n",
      "Processing Batch:  46976 47040\n",
      "Processing Batch:  47040 47104\n",
      "Processing Batch:  47104 47168\n",
      "Processing Batch:  47168 47232\n",
      "Processing Batch:  47232 47296\n",
      "Processing Batch:  47296 47360\n",
      "Processing Batch:  47360 47424\n",
      "Processing Batch:  47424 47488\n",
      "Processing Batch:  47488 47552\n",
      "Processing Batch:  47552 47616\n",
      "Processing Batch:  47616 47680\n",
      "Processing Batch:  47680 47744\n",
      "Processing Batch:  47744 47808\n",
      "Processing Batch:  47808 47872\n",
      "Processing Batch:  47872 47936\n",
      "Processing Batch:  47936 48000\n",
      "Processing Batch:  48000 48064\n",
      "Processing Batch:  48064 48128\n",
      "Processing Batch:  48128 48192\n",
      "Processing Batch:  48192 48256\n",
      "Processing Batch:  48256 48320\n",
      "Processing Batch:  48320 48384\n",
      "Processing Batch:  48384 48448\n",
      "Processing Batch:  48448 48512\n",
      "Processing Batch:  48512 48576\n",
      "Processing Batch:  48576 48640\n",
      "Processing Batch:  48640 48704\n",
      "Processing Batch:  48704 48768\n",
      "Processing Batch:  48768 48832\n",
      "Processing Batch:  48832 48896\n",
      "Processing Batch:  48896 48960\n",
      "Processing Batch:  48960 49024\n",
      "Processing Batch:  49024 49088\n",
      "Processing Batch:  49088 49152\n",
      "Processing Batch:  49152 49216\n",
      "Processing Batch:  49216 49280\n",
      "Processing Batch:  49280 49344\n",
      "Processing Batch:  49344 49408\n",
      "Processing Batch:  49408 49472\n",
      "Processing Batch:  49472 49536\n",
      "Processing Batch:  49536 49600\n",
      "Processing Batch:  49600 49664\n",
      "Processing Batch:  49664 49728\n",
      "Processing Batch:  49728 49792\n",
      "Processing Batch:  49792 49856\n",
      "Processing Batch:  49856 49920\n",
      "Processing Batch:  49920 49984\n",
      "Processing Batch:  49984 50048\n",
      "Processing Batch:  50048 50112\n",
      "Processing Batch:  50112 50176\n",
      "Processing Batch:  50176 50240\n",
      "Processing Batch:  50240 50304\n",
      "Processing Batch:  50304 50368\n",
      "Processing Batch:  50368 50432\n",
      "Processing Batch:  50432 50496\n",
      "Processing Batch:  50496 50560\n",
      "Processing Batch:  50560 50624\n",
      "Processing Batch:  50624 50688\n",
      "Processing Batch:  50688 50752\n",
      "Processing Batch:  50752 50816\n",
      "Processing Batch:  50816 50880\n",
      "Processing Batch:  50880 50944\n",
      "Processing Batch:  50944 51008\n",
      "Processing Batch:  51008 51072\n",
      "Processing Batch:  51072 51136\n",
      "Processing Batch:  51136 51200\n",
      "Processing Batch:  51200 51264\n",
      "Processing Batch:  51264 51328\n",
      "Processing Batch:  51328 51392\n",
      "Processing Batch:  51392 51456\n",
      "Processing Batch:  51456 51520\n",
      "Processing Batch:  51520 51584\n",
      "Processing Batch:  51584 51648\n",
      "Processing Batch:  51648 51712\n",
      "Processing Batch:  51712 51776\n",
      "Processing Batch:  51776 51840\n",
      "Processing Batch:  51840 51904\n",
      "Processing Batch:  51904 51968\n",
      "Processing Batch:  51968 52032\n",
      "Processing Batch:  52032 52096\n",
      "Processing Batch:  52096 52160\n",
      "Processing Batch:  52160 52224\n",
      "Processing Batch:  52224 52288\n",
      "Processing Batch:  52288 52352\n",
      "Processing Batch:  52352 52416\n",
      "Processing Batch:  52416 52480\n",
      "Processing Batch:  52480 52544\n",
      "Processing Batch:  52544 52608\n",
      "Processing Batch:  52608 52672\n",
      "Processing Batch:  52672 52736\n",
      "Processing Batch:  52736 52800\n",
      "Processing Batch:  52800 52864\n",
      "Processing Batch:  52864 52928\n",
      "Processing Batch:  52928 52992\n",
      "Processing Batch:  52992 53056\n",
      "Processing Batch:  53056 53120\n",
      "Processing Batch:  53120 53184\n",
      "Processing Batch:  53184 53248\n",
      "Processing Batch:  53248 53312\n",
      "Processing Batch:  53312 53376\n",
      "Processing Batch:  53376 53440\n",
      "Processing Batch:  53440 53504\n",
      "Processing Batch:  53504 53568\n",
      "Processing Batch:  53568 53632\n",
      "Processing Batch:  53632 53696\n",
      "Processing Batch:  53696 53760\n",
      "Processing Batch:  53760 53824\n",
      "Processing Batch:  53824 53888\n",
      "Processing Batch:  53888 53952\n",
      "Processing Batch:  53952 54016\n",
      "Processing Batch:  54016 54080\n",
      "Processing Batch:  54080 54144\n",
      "Processing Batch:  54144 54208\n",
      "Processing Batch:  54208 54272\n",
      "Processing Batch:  54272 54336\n",
      "Processing Batch:  54336 54400\n",
      "Processing Batch:  54400 54464\n",
      "Processing Batch:  54464 54528\n",
      "Processing Batch:  54528 54592\n",
      "Processing Batch:  54592 54656\n",
      "Processing Batch:  54656 54720\n",
      "Processing Batch:  54720 54784\n",
      "Processing Batch:  54784 54848\n",
      "Processing Batch:  54848 54912\n",
      "Processing Batch:  54912 54976\n",
      "Processing Batch:  54976 55040\n",
      "Processing Batch:  55040 55104\n",
      "Processing Batch:  55104 55168\n",
      "Processing Batch:  55168 55232\n",
      "Processing Batch:  55232 55296\n",
      "Processing Batch:  55296 55360\n",
      "Processing Batch:  55360 55424\n",
      "Processing Batch:  55424 55488\n",
      "Processing Batch:  55488 55552\n",
      "Processing Batch:  55552 55616\n",
      "Processing Batch:  55616 55680\n",
      "Processing Batch:  55680 55744\n",
      "Processing Batch:  55744 55808\n",
      "Processing Batch:  55808 55872\n",
      "Processing Batch:  55872 55936\n",
      "Processing Batch:  55936 56000\n",
      "Processing Batch:  56000 56064\n",
      "Processing Batch:  56064 56128\n",
      "Processing Batch:  56128 56192\n",
      "Processing Batch:  56192 56256\n",
      "Processing Batch:  56256 56320\n",
      "Processing Batch:  56320 56384\n",
      "Processing Batch:  56384 56448\n",
      "Processing Batch:  56448 56512\n",
      "Processing Batch:  56512 56576\n",
      "Processing Batch:  56576 56640\n",
      "Processing Batch:  56640 56704\n",
      "Processing Batch:  56704 56768\n",
      "Processing Batch:  56768 56832\n",
      "Processing Batch:  56832 56896\n",
      "Processing Batch:  56896 56960\n",
      "Processing Batch:  56960 57024\n",
      "Processing Batch:  57024 57088\n",
      "Processing Batch:  57088 57152\n",
      "Processing Batch:  57152 57216\n",
      "Processing Batch:  57216 57280\n",
      "Processing Batch:  57280 57344\n",
      "Processing Batch:  57344 57408\n",
      "Processing Batch:  57408 57472\n",
      "Processing Batch:  57472 57536\n",
      "Processing Batch:  57536 57600\n",
      "Processing Batch:  57600 57664\n",
      "Processing Batch:  57664 57728\n",
      "Processing Batch:  57728 57792\n",
      "Processing Batch:  57792 57856\n",
      "Processing Batch:  57856 57920\n",
      "Processing Batch:  57920 57984\n",
      "Processing Batch:  57984 58048\n",
      "Processing Batch:  58048 58112\n",
      "Processing Batch:  58112 58176\n",
      "Processing Batch:  58176 58240\n",
      "Processing Batch:  58240 58304\n",
      "Processing Batch:  58304 58368\n",
      "Processing Batch:  58368 58432\n",
      "Processing Batch:  58432 58496\n",
      "Processing Batch:  58496 58560\n",
      "Processing Batch:  58560 58624\n",
      "Processing Batch:  58624 58688\n",
      "Processing Batch:  58688 58752\n",
      "Processing Batch:  58752 58816\n",
      "Processing Batch:  58816 58880\n",
      "Processing Batch:  58880 58944\n",
      "Processing Batch:  58944 59008\n",
      "Processing Batch:  59008 59072\n",
      "Processing Batch:  59072 59136\n",
      "Processing Batch:  59136 59200\n",
      "Processing Batch:  59200 59264\n",
      "Processing Batch:  59264 59328\n",
      "Processing Batch:  59328 59392\n",
      "Processing Batch:  59392 59456\n",
      "Processing Batch:  59456 59520\n",
      "Processing Batch:  59520 59584\n",
      "Processing Batch:  59584 59648\n",
      "Processing Batch:  59648 59712\n",
      "Processing Batch:  59712 59776\n",
      "Processing Batch:  59776 59840\n",
      "Processing Batch:  59840 59904\n",
      "Processing Batch:  59904 59968\n",
      "Processing Batch:  59968 60032\n",
      "Processing Batch:  60032 60096\n",
      "Processing Batch:  60096 60160\n",
      "Processing Batch:  60160 60224\n",
      "Processing Batch:  60224 60288\n",
      "Processing Batch:  60288 60352\n",
      "Processing Batch:  60352 60416\n",
      "Processing Batch:  60416 60480\n",
      "Processing Batch:  60480 60544\n",
      "Processing Batch:  60544 60608\n",
      "Processing Batch:  60608 60672\n",
      "Processing Batch:  60672 60736\n",
      "Processing Batch:  60736 60800\n",
      "Processing Batch:  60800 60864\n",
      "Processing Batch:  60864 60928\n",
      "Processing Batch:  60928 60992\n",
      "Processing Batch:  60992 61056\n",
      "Processing Batch:  61056 61120\n",
      "Processing Batch:  61120 61184\n",
      "Processing Batch:  61184 61248\n",
      "Processing Batch:  61248 61312\n",
      "Processing Batch:  61312 61376\n",
      "Processing Batch:  61376 61440\n",
      "Processing Batch:  61440 61504\n",
      "Processing Batch:  61504 61568\n",
      "Processing Batch:  61568 61632\n",
      "Processing Batch:  61632 61696\n",
      "Processing Batch:  61696 61760\n",
      "Processing Batch:  61760 61824\n",
      "Processing Batch:  61824 61888\n",
      "Processing Batch:  61888 61952\n",
      "Processing Batch:  61952 62016\n",
      "Processing Batch:  62016 62080\n",
      "Processing Batch:  62080 62144\n",
      "Processing Batch:  62144 62208\n",
      "Processing Batch:  62208 62272\n",
      "Processing Batch:  62272 62336\n",
      "Processing Batch:  62336 62400\n",
      "Processing Batch:  62400 62464\n",
      "Processing Batch:  62464 62528\n",
      "Processing Batch:  62528 62592\n",
      "Processing Batch:  62592 62656\n",
      "Processing Batch:  62656 62720\n",
      "Processing Batch:  62720 62784\n",
      "Processing Batch:  62784 62848\n",
      "Processing Batch:  62848 62912\n",
      "Processing Batch:  62912 62976\n",
      "Processing Batch:  62976 63040\n",
      "Processing Batch:  63040 63104\n",
      "Processing Batch:  63104 63168\n",
      "Processing Batch:  63168 63232\n",
      "Processing Batch:  63232 63296\n",
      "Processing Batch:  63296 63360\n",
      "Processing Batch:  63360 63424\n",
      "Processing Batch:  63424 63488\n",
      "Processing Batch:  63488 63552\n",
      "Processing Batch:  63552 63616\n",
      "Processing Batch:  63616 63680\n",
      "Processing Batch:  63680 63744\n",
      "Processing Batch:  63744 63808\n",
      "Processing Batch:  63808 63872\n",
      "Processing Batch:  63872 63936\n",
      "Processing Batch:  63936 64000\n",
      "Processing Batch:  64000 64064\n",
      "Processing Batch:  64064 64128\n",
      "Processing Batch:  64128 64192\n",
      "Processing Batch:  64192 64256\n",
      "Processing Batch:  64256 64320\n",
      "Processing Batch:  64320 64384\n",
      "Processing Batch:  64384 64448\n",
      "Processing Batch:  64448 64512\n",
      "Processing Batch:  64512 64576\n",
      "Processing Batch:  64576 64640\n",
      "Processing Batch:  64640 64704\n",
      "Processing Batch:  64704 64768\n",
      "Processing Batch:  64768 64832\n",
      "Processing Batch:  64832 64896\n",
      "Processing Batch:  64896 64960\n",
      "Processing Batch:  64960 65024\n",
      "Processing Batch:  65024 65088\n",
      "Processing Batch:  65088 65152\n",
      "Processing Batch:  65152 65216\n",
      "Processing Batch:  65216 65280\n",
      "Processing Batch:  65280 65344\n",
      "Processing Batch:  65344 65408\n",
      "Processing Batch:  65408 65472\n",
      "Processing Batch:  65472 65536\n",
      "Processing Batch:  65536 65600\n",
      "Processing Batch:  65600 65664\n",
      "Processing Batch:  65664 65728\n",
      "Processing Batch:  65728 65792\n",
      "Processing Batch:  65792 65856\n",
      "Processing Batch:  65856 65920\n",
      "Processing Batch:  65920 65984\n",
      "Processing Batch:  65984 66048\n",
      "Processing Batch:  66048 66112\n",
      "Processing Batch:  66112 66176\n",
      "Processing Batch:  66176 66240\n",
      "Processing Batch:  66240 66304\n",
      "Processing Batch:  66304 66368\n",
      "Processing Batch:  66368 66432\n",
      "Processing Batch:  66432 66496\n",
      "Processing Batch:  66496 66560\n",
      "Processing Batch:  66560 66624\n",
      "Processing Batch:  66624 66688\n",
      "Processing Batch:  66688 66752\n",
      "Processing Batch:  66752 66816\n",
      "Processing Batch:  66816 66880\n",
      "Processing Batch:  66880 66944\n",
      "Processing Batch:  66944 67008\n",
      "Processing Batch:  67008 67072\n",
      "Processing Batch:  67072 67136\n",
      "Processing Batch:  67136 67200\n",
      "Processing Batch:  67200 67264\n"
     ]
    }
   ],
   "source": [
    "path_paired_test = f\"{pipeline_data_splitted}/{precision}/paired/test.tsv\"\n",
    "path_paired_validation = f\"{pipeline_data_splitted}/{precision}/paired/validation.tsv\"\n",
    "path_paired_train = f\"{pipeline_data_splitted}/{precision}/paired/train.tsv\"\n",
    "path_beta_test = f\"{pipeline_data_splitted}/{precision}/beta/test.tsv\"\n",
    "path_beta_validation = f\"{pipeline_data_splitted}/{precision}/beta/validation.tsv\"\n",
    "path_beta_train = f\"{pipeline_data_splitted}/{precision}/beta/train.tsv\"\n",
    "\n",
    "\n",
    "path_paired = f\"{pipeline_data}/embeddings/temp/{precision}/paired_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_paired)])\n",
    "df_paired_test = pd.read_csv(path_paired_test, sep=\"\\t\", index_col=False)\n",
    "df_paired_validation = pd.read_csv(path_paired_validation, sep=\"\\t\", index_col=False)\n",
    "df_paired_train = pd.read_csv(path_paired_train, sep=\"\\t\", index_col=False)\n",
    "df_paired = pd.concat([df_paired_test, df_paired_validation, df_paired_train])\n",
    "df_paired.to_csv(path_paired, sep=\"\\t\", index=False)\n",
    "\n",
    "# paired\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz TRA_CDR3\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz TRB_CDR3\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz Epitope\n",
    "\n",
    "path_beta = f\"{pipeline_data}/embeddings/temp/{precision}/beta_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_beta)])\n",
    "df_beta_test = pd.read_csv(path_beta_test, sep=\"\\t\", index_col=False)\n",
    "df_beta_validation = pd.read_csv(path_beta_validation, sep=\"\\t\", index_col=False)\n",
    "df_beta_train = pd.read_csv(path_beta_train, sep=\"\\t\", index_col=False)\n",
    "df_beta = pd.concat([df_beta_test, df_beta_validation, df_beta_train])\n",
    "df_beta.to_csv(path_beta, sep=\"\\t\", index=False)\n",
    "\n",
    "# beta\n",
    "%run ../generateEmbeddings.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/TRB_beta_embeddings.npz TRB_CDR3\n",
    "%run ../generateEmbeddings.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/Epitope_beta_embeddings.npz Epitope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
