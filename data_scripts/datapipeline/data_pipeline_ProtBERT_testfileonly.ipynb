{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tidytcells in /home/ubuntu/anaconda3/lib/python3.12/site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install tidytcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision of mhc and V/J values (gene or allele)\n",
    "precision = 'allele'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is not thread safe\n",
    "def create_folders_if_not_exists(folders):\n",
    "  for path in folders:\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = '../../../../data'\n",
    "pipeline_data_plain = f'{pipeline_data}/plain_datasets'\n",
    "pipeline_data_cleaned = f'{pipeline_data}/cleaned_datasets'\n",
    "pipeline_data_concatenated = f'{pipeline_data}/concatenated_datasets'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "pipeline_data_temp_bucket = f'{pipeline_data}/temp'\n",
    "\n",
    "pipeline_folders = [pipeline_data, pipeline_data_plain, pipeline_data_cleaned, pipeline_data_concatenated, pipeline_data_splitted, pipeline_data_temp_bucket]\n",
    "\n",
    "create_folders_if_not_exists(pipeline_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VDJdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "VDJdb_data_plain = f'{pipeline_data_plain}/VDJdb'\n",
    "VDJdb_data_cleaned = f'{pipeline_data_cleaned}/VDJdb'\n",
    "VDJdb_data_fitted = f'{pipeline_data_temp_bucket}/VDJdb'\n",
    "\n",
    "VDJdb_folders = [VDJdb_data_plain, VDJdb_data_cleaned, VDJdb_data_fitted]\n",
    "create_folders_if_not_exists(VDJdb_folders)\n",
    "\n",
    "fitted_beta_file = 'VDJdb_beta_fitted.tsv'\n",
    "fitted_paired_file = 'VDJdb_paired_fitted.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data paired\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_paired_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_paired_file\n",
    "\n",
    "# fit paired VDJdb data\n",
    "%run ../VDJdb/fit_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data beta\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_beta_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_beta_file\n",
    "\n",
    "# fit beta VDJdb data\n",
    "%run ../VDJdb/fit_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 27414 entries\n",
      "whole dataframe has 28119 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 27414\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data paired\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_paired_file}'\n",
    "cleaned_file_paired = 'VDJdb_cleaned_data_paired.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'\n",
    "\n",
    "# clean paired VDJdb data\n",
    "%run ../VDJdb/clean_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 46507 entries\n",
      "whole dataframe has 49042 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 46507\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data beta\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_beta_file}'\n",
    "cleaned_file_beta = 'VDJdb_cleaned_data_beta.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "\n",
    "# clean beta VDJdb data\n",
    "%run ../VDJdb/clean_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDJdb_cleaned_beta_output = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "VDJdb_cleaned_paired_output = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of beta_df: 46507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 7188 entries removed.\n",
      "removed all duplicates (CDR3, Epitope) from distinct values (most_important_columns, keep=False). 1435 entries removed.\n",
      "beta removed entries df length: 1435\n",
      "\n",
      "\n",
      "Number of groups formed: 655\n",
      "1435 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 46507 entries, 7188 entries have been removed.\n",
      "for beta dataset :\n",
      "size difference is: 7188\n",
      "  39319 information score cleaned: 6.0\n",
      "  46507 information score dropout: 6.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_88007/4134180033.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ==================== NEU: Duplikate gegenüber Train/Val entfernen ====================\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_data_splitted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/beta/train.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m val_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_data_splitted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/beta/validation.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m trainval_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([train_df, val_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m output_file_paired \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaired_concatenated_test.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m create_folders_if_not_exists([custom_dataset_path])\n\u001b[0;32m---> 12\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../concatDatasets_onlytest.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/IPython/core/magics/execution.py:737\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 737\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39msafe_execfile_ipy(filename, raise_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3005\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[0;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[1;32m   3003\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[0;32m-> 3005\u001b[0m     result\u001b[38;5;241m.\u001b[39mraise_error()\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m   3007\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/tmp/ipykernel_88007/4134180033.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ==================== NEU: Duplikate gegenüber Train/Val entfernen ====================\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_data_splitted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/beta/train.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m val_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_data_splitted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/beta/validation.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m trainval_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([train_df, val_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "vdjdb_beta_read_path = VDJdb_cleaned_beta_output\n",
    "vdjdb_paired_read_path = VDJdb_cleaned_paired_output\n",
    "\n",
    "output_file_beta = 'beta_concatenated_test.tsv'\n",
    "output_file_paired = 'paired_concatenated_test.tsv'\n",
    "\n",
    "create_folders_if_not_exists([custom_dataset_path])\n",
    "\n",
    "%run ../concatDatasets_onlytest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta file copied successfully to ../../../../data/splitted_datasets/allele/beta/test_prenegsamples.tsv\n",
      "Paired file copied successfully to ../../../../data/splitted_datasets/allele/paired/test_prenegsamples.tsv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define source folder where the files are currently stored\n",
    "source_folder = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "# Define file names\n",
    "output_file_beta = 'beta_concatenated_test.tsv'\n",
    "output_file_paired = 'paired_concatenated_test.tsv'\n",
    "\n",
    "# Define destination folders\n",
    "destination_beta_folder = f'{pipeline_data_splitted}/{precision}/beta/'\n",
    "destination_paired_folder = f'{pipeline_data_splitted}/{precision}/paired/'\n",
    "\n",
    "# Ensure destination folders exist\n",
    "os.makedirs(destination_beta_folder, exist_ok=True)\n",
    "os.makedirs(destination_paired_folder, exist_ok=True)\n",
    "\n",
    "# Copy files\n",
    "shutil.copy(os.path.join(source_folder, output_file_beta), os.path.join(destination_beta_folder, 'test_prenegsamples.tsv'))\n",
    "shutil.copy(os.path.join(source_folder, output_file_paired), os.path.join(destination_paired_folder, 'test_prenegsamples.tsv'))\n",
    "\n",
    "print(f'Beta file copied successfully to {destination_beta_folder}test_prenegsamples.tsv')\n",
    "print(f'Paired file copied successfully to {destination_paired_folder}test_prenegsamples.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beta Dataset:\n",
      "- Unique TCRs: 8464\n",
      "- Unique Epitope: 293\n",
      "\n",
      "Paired Dataset:\n",
      "- Unique TCRs: 21101\n",
      "- Unique Epitope: 825\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "beta_file_path = f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv'\n",
    "paired_file_path = f'{pipeline_data_splitted}/{precision}/paired/test_prenegsamples.tsv'\n",
    "\n",
    "# Load beta dataset\n",
    "beta_df = pd.read_csv(beta_file_path, sep='\\t')\n",
    "\n",
    "# Load paired dataset\n",
    "paired_df = pd.read_csv(paired_file_path, sep='\\t')\n",
    "\n",
    "# Calculate unique values for beta dataset\n",
    "unique_tcr_beta = beta_df['TRB_CDR3'].nunique()\n",
    "unique_epitope_beta = beta_df['Epitope'].nunique()\n",
    "\n",
    "# Calculate unique values for paired dataset\n",
    "unique_tcr_paired = paired_df['TRB_CDR3'].nunique()\n",
    "unique_epitope_paired = paired_df['Epitope'].nunique()\n",
    "\n",
    "# Print results for beta dataset\n",
    "print(\"\\nBeta Dataset:\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_beta}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_beta}\")\n",
    "\n",
    "# Print results for paired dataset\n",
    "print(\"\\nPaired Dataset:\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_paired}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_paired}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus:                 barcode   donor  \\\n",
      "0   AAACCTGAGACAAAGG-4  donor1   \n",
      "1  AAACCTGAGACTGTAA-34  donor1   \n",
      "2   AAACCTGAGAGCCCAA-5  donor1   \n",
      "3  AAACCTGAGAGCTGCA-24  donor1   \n",
      "4   AAACCTGAGAGGGATA-8  donor1   \n",
      "\n",
      "                                  cell_clono_cdr3_aa  \\\n",
      "0  TRA:CAASVSIWTGTASKLTF;TRA:CAAWDMEYGNKLVF;TRB:C...   \n",
      "1                                    TRB:CASDTPVGQFF   \n",
      "2                 TRA:CASYTDKLIF;TRB:CASSGGSISTDTQYF   \n",
      "3                                 TRB:CASSGGQSSYEQYF   \n",
      "4          TRA:CAASGYGNTGRRALTF;TRB:CASSQDPAGGYNEQFF   \n",
      "\n",
      "                                  cell_clono_cdr3_nt     CD3  CD19  CD45RA  \\\n",
      "0  TRA:TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAA...  2125.0   0.0   912.0   \n",
      "1              TRB:TGTGCCAGCGATACCCCGGTTGGGCAGTTCTTC  1023.0   0.0  2028.0   \n",
      "2  TRA:TGTGCTTCCTACACCGACAAGCTCATCTTT;TRB:TGCGCCA...  1598.0   3.0  3454.0   \n",
      "3     TRB:TGCGCCAGCAGTGGCGGACAGAGCTCCTACGAGCAGTACTTC   298.0   1.0   880.0   \n",
      "4  TRA:TGTGCAGCAAGCGGGTATGGAAACACGGGCAGGAGAGCACTT...  1036.0   0.0  2457.0   \n",
      "\n",
      "   CD4    CD8a  CD14  ...  B0702_RPHERNGFTVL_pp65_CMV_binder  \\\n",
      "0  1.0  2223.0   4.0  ...                              False   \n",
      "1  2.0  3485.0   1.0  ...                              False   \n",
      "2  4.0  3383.0   1.0  ...                              False   \n",
      "3  1.0  2389.0   1.0  ...                              False   \n",
      "4  2.0  3427.0   3.0  ...                              False   \n",
      "\n",
      "   B0801_RAKFKQLL_BZLF1_EBV_binder  B0801_ELRRKMMYM_IE-1_CMV_binder  \\\n",
      "0                            False                            False   \n",
      "1                            False                            False   \n",
      "2                            False                            False   \n",
      "3                            False                            False   \n",
      "4                            False                            False   \n",
      "\n",
      "   B0801_FLRGRAYGL_EBNA-3A_EBV_binder  A0101_SLEGGGLGY_NC_binder  \\\n",
      "0                               False                      False   \n",
      "1                               False                      False   \n",
      "2                               False                      False   \n",
      "3                               False                      False   \n",
      "4                               False                      False   \n",
      "\n",
      "   A0101_STEGGGLAY_NC_binder  A0201_ALIAPVHAV_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   A2402_AYSSAGASI_NC_binder  B0702_GPAESAAGL_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   NR(B0801)_AAKGRGAAL_NC_binder  \n",
      "0                          False  \n",
      "1                          False  \n",
      "2                          False  \n",
      "3                          False  \n",
      "4                          False  \n",
      "\n",
      "[5 rows x 118 columns]\n",
      "Meta:                barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_1             True   \n",
      "1  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_2             True   \n",
      "2  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_3             True   \n",
      "3  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_4            False   \n",
      "4  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_5            False   \n",
      "\n",
      "   length chain     v_gene d_gene   j_gene c_gene  full_length productive  \\\n",
      "0     722   TRB   TRBV10-3  TRBD2  TRBJ2-1  TRBC2         True       True   \n",
      "1     605   TRA  TRAV29DV5    NaN   TRAJ44   TRAC         True       True   \n",
      "2     738   TRA    TRAV8-6    NaN   TRAJ47   TRAC         True       True   \n",
      "3     468   TRB        NaN    NaN  TRBJ2-3  TRBC2        False        NaN   \n",
      "4     488   TRB        NaN    NaN  TRBJ2-6  TRBC2        False        NaN   \n",
      "\n",
      "                cdr3                                            cdr3_nt  \\\n",
      "0  CAISDPGLAGGGGEQFF  TGTGCCATCAGTGACCCCGGACTAGCGGGAGGCGGGGGGGAGCAGT...   \n",
      "1  CAASVSIWTGTASKLTF  TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAACTCA...   \n",
      "2     CAAWDMEYGNKLVF         TGTGCCGCCTGGGACATGGAATATGGAAACAAGCTGGTCTTT   \n",
      "3                NaN                                                NaN   \n",
      "4                NaN                                                NaN   \n",
      "\n",
      "   reads  umis raw_clonotype_id         raw_consensus_id  \n",
      "0  32237    18      clonotype19  clonotype19_consensus_1  \n",
      "1   6088     3      clonotype19  clonotype19_consensus_2  \n",
      "2   5358     3      clonotype19  clonotype19_consensus_3  \n",
      "3   2517     1      clonotype19                      NaN  \n",
      "4   2468     1      clonotype19                      NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231714/3850524540.py:9: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n"
     ]
    }
   ],
   "source": [
    "#Daten einlesen\n",
    "\n",
    "combined_donors_path = f'{pipeline_data_plain}/10x/combined_donors_consensus_annotations.csv'\n",
    "all_donors_consensus = pd.read_csv(combined_donors_path, sep=',')\n",
    "\n",
    "print(\"Consensus: \", all_donors_consensus.head())\n",
    "\n",
    "all_donors_meta_path = f'{pipeline_data_plain}/10x/meta.csv'\n",
    "all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n",
    "\n",
    "print(\"Meta: \", all_donors_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Start:  0\n",
      "Batch Start:  1000\n",
      "Batch Start:  2000\n",
      "Batch Start:  3000\n",
      "Batch Start:  4000\n",
      "Batch Start:  5000\n",
      "Batch Start:  6000\n",
      "Batch Start:  7000\n",
      "Batch Start:  8000\n",
      "Batch Start:  9000\n",
      "Batch Start:  10000\n",
      "Batch Start:  11000\n",
      "Batch Start:  12000\n",
      "Batch Start:  13000\n",
      "Batch Start:  14000\n",
      "Batch Start:  15000\n",
      "Batch Start:  16000\n",
      "Batch Start:  17000\n",
      "Batch Start:  18000\n",
      "Batch Start:  19000\n",
      "Batch Start:  20000\n",
      "Batch Start:  21000\n",
      "Batch Start:  22000\n",
      "Batch Start:  23000\n",
      "Batch Start:  24000\n",
      "Batch Start:  25000\n",
      "Batch Start:  26000\n",
      "Batch Start:  27000\n",
      "Batch Start:  28000\n",
      "Batch Start:  29000\n",
      "Batch Start:  30000\n",
      "Batch Start:  31000\n",
      "Batch Start:  32000\n",
      "Batch Start:  33000\n",
      "Batch Start:  34000\n",
      "Batch Start:  35000\n",
      "Batch Start:  36000\n",
      "Batch Start:  37000\n",
      "Batch Start:  38000\n",
      "Batch Start:  39000\n",
      "Batch Start:  40000\n",
      "Batch Start:  41000\n",
      "Batch Start:  42000\n",
      "Batch Start:  43000\n",
      "Batch Start:  44000\n",
      "Batch Start:  45000\n",
      "Batch Start:  46000\n",
      "Batch Start:  47000\n",
      "Batch Start:  48000\n",
      "Batch Start:  49000\n",
      "Batch Start:  50000\n",
      "Batch Start:  51000\n",
      "Batch Start:  52000\n",
      "Batch Start:  53000\n",
      "Batch Start:  54000\n",
      "Batch Start:  55000\n",
      "Batch Start:  56000\n",
      "Batch Start:  57000\n",
      "Batch Start:  58000\n",
      "Batch Start:  59000\n",
      "Batch Start:  60000\n",
      "Batch Start:  61000\n",
      "Batch Start:  62000\n",
      "Batch Start:  63000\n",
      "Batch Start:  64000\n",
      "Batch Start:  65000\n",
      "Batch Start:  66000\n",
      "Batch Start:  67000\n",
      "Batch Start:  68000\n",
      "Batch Start:  69000\n",
      "Batch Start:  70000\n",
      "Batch Start:  71000\n",
      "Batch Start:  72000\n",
      "Batch Start:  73000\n",
      "Batch Start:  74000\n",
      "Batch Start:  75000\n",
      "Batch Start:  76000\n",
      "Batch Start:  77000\n",
      "Batch Start:  78000\n",
      "Batch Start:  79000\n",
      "Batch Start:  80000\n",
      "Batch Start:  81000\n",
      "Batch Start:  82000\n",
      "Batch Start:  83000\n",
      "Batch Start:  84000\n",
      "Batch Start:  85000\n",
      "Batch Start:  86000\n",
      "Batch Start:  87000\n",
      "Batch Start:  88000\n",
      "Batch Start:  89000\n",
      "Batch Start:  90000\n",
      "Batch Start:  91000\n",
      "Batch Start:  92000\n",
      "Batch Start:  93000\n",
      "Batch Start:  94000\n",
      "Batch Start:  95000\n",
      "Batch Start:  96000\n",
      "Batch Start:  97000\n",
      "Batch Start:  98000\n",
      "Batch Start:  99000\n",
      "Batch Start:  100000\n",
      "Batch Start:  101000\n",
      "Batch Start:  102000\n",
      "Batch Start:  103000\n",
      "Batch Start:  104000\n",
      "Batch Start:  105000\n",
      "Batch Start:  106000\n",
      "Batch Start:  107000\n",
      "Batch Start:  108000\n",
      "Batch Start:  109000\n",
      "Batch Start:  110000\n",
      "Batch Start:  111000\n",
      "Batch Start:  112000\n",
      "Batch Start:  113000\n",
      "Batch Start:  114000\n",
      "Batch Start:  115000\n",
      "Batch Start:  116000\n",
      "Batch Start:  117000\n",
      "Batch Start:  118000\n",
      "Batch Start:  119000\n",
      "Batch Start:  120000\n",
      "Batch Start:  121000\n",
      "Batch Start:  122000\n",
      "Batch Start:  123000\n",
      "Batch Start:  124000\n",
      "Batch Start:  125000\n",
      "Batch Start:  126000\n",
      "Batch Start:  127000\n",
      "Batch Start:  128000\n",
      "Batch Start:  129000\n",
      "Batch Start:  130000\n",
      "Batch Start:  131000\n",
      "Batch Start:  132000\n",
      "Batch Start:  133000\n",
      "Batch Start:  134000\n",
      "Batch Start:  135000\n",
      "Batch Start:  136000\n",
      "Batch Start:  137000\n",
      "Batch Start:  138000\n",
      "Batch Start:  139000\n",
      "Batch Start:  140000\n",
      "Batch Start:  141000\n",
      "Batch Start:  142000\n",
      "Batch Start:  143000\n",
      "Batch Start:  144000\n",
      "Batch Start:  145000\n",
      "Batch Start:  146000\n",
      "Batch Start:  147000\n",
      "Batch Start:  148000\n",
      "Batch Start:  149000\n",
      "Batch Start:  150000\n",
      "Batch Start:  151000\n",
      "Batch Start:  152000\n",
      "Batch Start:  153000\n",
      "Batch Start:  154000\n",
      "Batch Start:  155000\n",
      "Batch Start:  156000\n",
      "Batch Start:  157000\n",
      "Batch Start:  158000\n",
      "Batch Start:  159000\n",
      "Batch Start:  160000\n",
      "Batch Start:  161000\n",
      "Batch Start:  162000\n",
      "Batch Start:  163000\n",
      "Batch Start:  164000\n",
      "Batch Start:  165000\n",
      "Batch Start:  166000\n",
      "Batch Start:  167000\n",
      "Batch Start:  168000\n",
      "Batch Start:  169000\n",
      "Batch Start:  170000\n",
      "Batch Start:  171000\n",
      "Batch Start:  172000\n",
      "Batch Start:  173000\n",
      "Batch Start:  174000\n",
      "Batch Start:  175000\n",
      "Batch Start:  176000\n",
      "Batch Start:  177000\n",
      "Batch Start:  178000\n",
      "Batch Start:  179000\n",
      "Batch Start:  180000\n",
      "Batch Start:  181000\n",
      "Batch Start:  182000\n",
      "Batch Start:  183000\n",
      "Batch Start:  184000\n",
      "Batch Start:  185000\n",
      "Batch Start:  186000\n",
      "Batch Start:  187000\n",
      "Batch Start:  188000\n",
      "Batch Start:  189000\n",
      "             TCR_name      TRBV     TRBJ           TRB_CDR3   TRBC  \\\n",
      "0  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "1  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "2  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "3  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "4  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "\n",
      "      Epitope          MHC Binding task  \n",
      "0   VTEHDTLLY  HLA-A*01:01       0  nan  \n",
      "1   KTWGQYWQV  HLA-A*02:01       0  nan  \n",
      "2  ELAGIGILTV  HLA-A*02:01       0  nan  \n",
      "3  CLLWSFQTSA  HLA-A*02:01       0  nan  \n",
      "4   IMDQVPFSV  HLA-A*02:01       0  nan  \n"
     ]
    }
   ],
   "source": [
    "#Dieser Code für ganzen Datensatz laufen lassen\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Festlegen der Batch-Größe für die Verarbeitung\n",
    "batch_size = 1000  # Passe diese Zahl je nach Speicherressourcen an\n",
    "\n",
    "# Identifizieren von Epitope-Spalten, aber ohne \"NR(B0801)_AAKGRGAAL_NC_binder\"\n",
    "epitope_columns = [col for col in all_donors_consensus.columns if '_binder' in col and col != \"NR(B0801)_AAKGRGAAL_NC_binder\"]\n",
    "\n",
    "# Liste für alle Batch-Ergebnisse\n",
    "all_batches = []\n",
    "\n",
    "# Verarbeite `all_donors_consensus` in Batches\n",
    "for batch_start in range(0, len(all_donors_consensus), batch_size):\n",
    "    print(\"Batch Start: \", batch_start)\n",
    "    # Batch definieren\n",
    "    batch = all_donors_consensus.iloc[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Filtern auf Zeilen, die 'TRB:' enthalten\n",
    "    batch_trb = batch[batch['cell_clono_cdr3_aa'].str.contains(\"TRB:\", na=False)]\n",
    "\n",
    "    # Liste, um Zeilen für diesen Batch zu speichern\n",
    "    expanded_rows = []\n",
    "    \n",
    "    # Iteriere durch jede Zeile im Batch\n",
    "    for _, row in batch_trb.iterrows():\n",
    "        for col in epitope_columns:\n",
    "            # Extrahiere MHC und Epitope\n",
    "            match = re.match(r'([A-Z0-9]+)_([A-Z]+)_.*_binder', col)\n",
    "            if match:\n",
    "                mhc_raw, epitope = match.groups()\n",
    "                mhc_formatted = f'HLA-{mhc_raw[0]}*{mhc_raw[1:3]}:{mhc_raw[3:]}'\n",
    "\n",
    "                # Füge `Epitope` und `MHC` zur Zeile hinzu\n",
    "                new_row = row.copy()\n",
    "                new_row['Epitope'] = epitope\n",
    "                new_row['MHC'] = mhc_formatted\n",
    "\n",
    "                # Füge neue Zeile zur Batch-Liste hinzu\n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    # Erstelle einen DataFrame aus dem Batch\n",
    "    batch_df = pd.DataFrame(expanded_rows)\n",
    "    all_batches.append(batch_df)  # Speichere den Batch in der Liste\n",
    "\n",
    "# Kombiniere alle Batch-Ergebnisse zu einem DataFrame\n",
    "expanded_df = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "# Nur die TRB-Chain-Einträge in `all_donors_meta` beibehalten\n",
    "all_donors_meta_trb = all_donors_meta[all_donors_meta['chain'] == 'TRB']\n",
    "\n",
    "# Zusammenführen der beiden DataFrames basierend auf der 'barcode' Spalte\n",
    "merged_df = pd.merge(all_donors_meta_trb, expanded_df[['barcode', 'Epitope', 'MHC']], on='barcode', how='inner')\n",
    "\n",
    "# Spalten umbenennen und Format anpassen\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'barcode': 'TCR_name',\n",
    "    'v_gene': 'TRBV',\n",
    "    'j_gene': 'TRBJ',\n",
    "    'c_gene': 'TRBC',\n",
    "    'cdr3': 'TRB_CDR3'\n",
    "})\n",
    "\n",
    "# Fehlende Spalten auffüllen\n",
    "desired_columns = ['TCR_name', 'TRBV', 'TRBJ', 'TRB_CDR3', 'TRBC', 'Epitope', 'MHC', 'Binding', 'task']\n",
    "for col in desired_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = 'nan' if col == 'task' else '0'\n",
    "\n",
    "# Nur die gewünschten Spalten beibehalten und Zeilen mit `None` in `TRB_CDR3` entfernen\n",
    "final_df = merged_df[desired_columns]\n",
    "final_df = final_df[final_df['TRB_CDR3'] != 'None']\n",
    "\n",
    "final_df = final_df[final_df['TRB_CDR3'].notna() & (final_df['TRB_CDR3'] != '')]\n",
    "\n",
    "# Ausgabe des ersten Teils des Ergebnisses zur Überprüfung\n",
    "print(final_df.head())\n",
    "\n",
    "# Speichern des kombinierten DataFrames\n",
    "output_path = f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv'\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Samples generieren für Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_balanced_negatives_with_all_epitopes(\n",
    "    neg_source_1,\n",
    "    neg_source_2,\n",
    "    target_neg_count,\n",
    "    used_pairs=set(),\n",
    "    vdjdb_tcrs=set(),\n",
    "    vdjdb_epitopes=set(),\n",
    "    ensure_all_neg_epitopes=True\n",
    "):\n",
    "    neg_source_1 = neg_source_1.copy()\n",
    "    neg_source_2 = neg_source_2.copy()\n",
    "    neg_source_1['source'] = '10X'\n",
    "    neg_source_2['source'] = 'generated'\n",
    "\n",
    "    # --- VDJdb & verwendete Paare filtern ---\n",
    "    def filter_df(df, remove_epitopes=False):\n",
    "        df = df[~df['TRB_CDR3'].isin(vdjdb_tcrs)].copy()\n",
    "        if remove_epitopes:\n",
    "            df = df[~df['Epitope'].isin(vdjdb_epitopes)]\n",
    "        df['Pair'] = list(map(tuple, df[['Epitope', 'TRB_CDR3']].values))\n",
    "        df = df[~df['Pair'].isin(used_pairs)]\n",
    "        return df\n",
    "\n",
    "    neg_source_1 = filter_df(neg_source_1)\n",
    "    neg_source_2 = filter_df(neg_source_2)\n",
    "\n",
    "    # --- Mindestens 1x alle Epitope aus beiden Quellen übernehmen ---\n",
    "    def ensure_epitope_coverage(df):\n",
    "        guaranteed = []\n",
    "        for epitope in df['Epitope'].unique():\n",
    "            group = df[df['Epitope'] == epitope]\n",
    "            if not group.empty:\n",
    "                guaranteed.append(group.sample(1, random_state=42))\n",
    "        return pd.concat(guaranteed, ignore_index=True)\n",
    "\n",
    "    guaranteed_1 = ensure_epitope_coverage(neg_source_1)\n",
    "    guaranteed_2 = ensure_epitope_coverage(neg_source_2)\n",
    "    guaranteed_df = pd.concat([guaranteed_1, guaranteed_2], ignore_index=True)\n",
    "\n",
    "    # Begrenze garantierte, falls sie zu groß geworden sind\n",
    "    if len(guaranteed_df) > target_neg_count:\n",
    "        print(f\"Zu viele garantierte Negative ({len(guaranteed_df)}), trimme auf Zielmenge {target_neg_count}\")\n",
    "        guaranteed_df = guaranteed_df.sample(n=target_neg_count, random_state=42)\n",
    "\n",
    "    # --- Stratified Sampling für Restauffüllung ---\n",
    "    def stratified_sample(df, n):\n",
    "        epitope_groups = df.groupby('Epitope')\n",
    "        unique_epitopes = list(epitope_groups.groups.keys())\n",
    "        print(f\"→ Stratified sampling from {len(df)} rows | {len(unique_epitopes)} unique epitopes | need {n} samples\")\n",
    "    \n",
    "        # Schritt 1: Garantiert 1 Sample pro Epitope\n",
    "        guaranteed = [group.sample(1, random_state=42) for _, group in epitope_groups]\n",
    "        guaranteed_df = pd.concat(guaranteed, ignore_index=True)\n",
    "    \n",
    "        remaining_n = n - len(guaranteed_df)\n",
    "        if remaining_n <= 0:\n",
    "            return guaranteed_df.sample(n=n, random_state=42)\n",
    "    \n",
    "        # Schritt 2: Aufstocken durch gewichtetes Sampling\n",
    "        remaining_pool = df.drop(index=guaranteed_df.index, errors='ignore')\n",
    "    \n",
    "        # Gewichte: Häufigkeit pro Epitope → normalize\n",
    "        epitope_counts = remaining_pool['Epitope'].value_counts()\n",
    "        remaining_pool = remaining_pool.copy()\n",
    "        remaining_pool['weight'] = remaining_pool['Epitope'].map(epitope_counts)\n",
    "        total = remaining_pool['weight'].sum()\n",
    "        remaining_pool['weight'] = remaining_pool['weight'] / total\n",
    "    \n",
    "        print(f\"→ Stratified fill-in: drawing {remaining_n} samples weighted by epitope frequency\")\n",
    "    \n",
    "        replace = len(remaining_pool) < remaining_n\n",
    "        if replace:\n",
    "            print(f\"Achtung: Sampling mit Replacement (n={remaining_n}, pool={len(remaining_pool)})\")\n",
    "        \n",
    "        sampled_rest = remaining_pool.sample(\n",
    "            n=remaining_n,\n",
    "            weights='weight',\n",
    "            replace=replace,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        final_df = pd.concat([guaranteed_df, sampled_rest], ignore_index=True)\n",
    "        return final_df\n",
    "\n",
    "    remaining_needed = target_neg_count - len(guaranteed_df)\n",
    "    if remaining_needed <= 0:\n",
    "        print(f\"Es wurden bereits {len(guaranteed_df)} garantierte Samples übernommen (mehr als benötigt).\")\n",
    "        final_df = guaranteed_df.sample(n=target_neg_count, random_state=42)\n",
    "    else:\n",
    "        # Garantierte rausnehmen\n",
    "        used_idx_1 = guaranteed_1.index if not guaranteed_1.empty else []\n",
    "        used_idx_2 = guaranteed_2.index if not guaranteed_2.empty else []\n",
    "\n",
    "        remaining_1 = neg_source_1.drop(index=used_idx_1, errors='ignore')\n",
    "        remaining_2 = neg_source_2.drop(index=used_idx_2, errors='ignore')\n",
    "\n",
    "        half = remaining_needed // 2\n",
    "        rest = remaining_needed - half\n",
    "\n",
    "        sample_1 = stratified_sample(remaining_1, half)\n",
    "        sample_2 = stratified_sample(remaining_2, rest)\n",
    "\n",
    "        final_df = pd.concat([guaranteed_df, sample_1, sample_2], ignore_index=True)\n",
    "\n",
    "    return final_df.drop(columns=['Pair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88007/3486599779.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# === Datei- und Pfadangaben ===\n",
    "\n",
    "# Originaldaten (10X)\n",
    "beta = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv', sep=',')\n",
    "\n",
    "# Generierte Negativdaten\n",
    "neg_ba_train = pd.read_csv(f'{pipeline_data}/ba_splitted/train.tsv', sep='\\t')\n",
    "neg_ba_val = pd.read_csv(f'{pipeline_data}/ba_splitted/validation.tsv', sep='\\t')\n",
    "neg_ba_test = pd.read_csv(f'{pipeline_data}/ba_splitted/test.tsv', sep='\\t')\n",
    "neg_ba_test = neg_ba_test[(neg_ba_test['source'] == 'BA') & (neg_ba_test['Binding'] == 0)]\n",
    "\n",
    "# Positive Beispiele\n",
    "train_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n",
    "val_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/validation_prenegsamples.tsv', sep='\\t')\n",
    "test_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv', sep='\\t')\n",
    "\n",
    "# Output-Ziele\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "# VDJdb zum Herausfiltern\n",
    "vdjdb_df = pd.read_csv(VDJdb_cleaned_beta_output, sep='\\t')\n",
    "vdjdb_tcrs = set(vdjdb_df['TRB_CDR3'])\n",
    "vdjdb_epitopes = set(vdjdb_df['Epitope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Stratified sampling from 3096223 rows | 49 unique epitopes | need 21916 samples\n",
      "→ Stratified fill-in: drawing 21867 samples weighted by epitope frequency\n",
      "→ Stratified sampling from 36822 rows | 617 unique epitopes | need 21916 samples\n",
      "→ Stratified fill-in: drawing 21299 samples weighted by epitope frequency\n",
      "✅ Testset erfolgreich erstellt & gespeichert.\n",
      "Test: 53400 Beispiele\n",
      "- Binding=1: 8900\n",
      "- Binding=0: 44500\n",
      "- Unique Epitope: 786\n",
      "- Unique TCRs: 41975\n"
     ]
    }
   ],
   "source": [
    "# === TESTDATEN VERARBEITUNG ===\n",
    "# Zielmenge: 1:5 Verhältnis\n",
    "num_test_pos = len(test_preneg)\n",
    "test_neg_target = num_test_pos * 5\n",
    "\n",
    "neg_10x = beta[beta['Binding'] == 0]\n",
    "train_final = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\", sep='\\t')\n",
    "val_final = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\", sep='\\t')\n",
    "\n",
    "# Paare aus Train/Val ausschließen\n",
    "used_trainval_pairs = set(\n",
    "    map(tuple, pd.concat([train_final, val_final])[['Epitope', 'TRB_CDR3']].values)\n",
    ")\n",
    "\n",
    "# Negative Samples für Test generieren\n",
    "test_neg = create_balanced_negatives_with_all_epitopes(\n",
    "    neg_source_1=neg_10x,\n",
    "    neg_source_2=neg_ba_test,\n",
    "    target_neg_count=test_neg_target,\n",
    "    used_pairs=used_trainval_pairs,\n",
    "    vdjdb_tcrs=vdjdb_tcrs,\n",
    "    vdjdb_epitopes=vdjdb_epitopes\n",
    ")\n",
    "\n",
    "# Combine & save\n",
    "test_final = pd.concat([test_preneg, test_neg], ignore_index=True).sample(frac=1, random_state=42)\n",
    "test_final.to_csv(test_output_path, sep='\\t', index=False)\n",
    "\n",
    "# Ausgabe\n",
    "print(\"✅ Testset erfolgreich erstellt & gespeichert.\")\n",
    "print(f\"Test: {len(test_final)} Beispiele\")\n",
    "print(f\"- Binding=1: {test_final['Binding'].value_counts().get(1, 0)}\")\n",
    "print(f\"- Binding=0: {test_final['Binding'].value_counts().get(0, 0)}\")\n",
    "print(f\"- Unique Epitope: {test_final['Epitope'].nunique()}\")\n",
    "print(f\"- Unique TCRs: {test_final['TRB_CDR3'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gewünschte Änderungen in den finalen Splits wurden vorgenommen und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Pfade zu finalen Splits ---\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path   = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path  = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "\n",
    "# --- Dateien einlesen ---\n",
    "train_df = pd.read_csv(output_train_path, sep='\\t')\n",
    "val_df   = pd.read_csv(output_val_path, sep='\\t')\n",
    "test_df  = pd.read_csv(test_output_path, sep='\\t')\n",
    "\n",
    "# --- Funktion zur Bearbeitung ---\n",
    "def clean_and_update(df):\n",
    "    if 'weight' in df.columns:\n",
    "        df = df.drop(columns=['weight'])\n",
    "    if 'source' not in df.columns:\n",
    "        df['source'] = ''\n",
    "    df.loc[df['Binding'] == 1, 'source'] = 'datasets'\n",
    "    return df\n",
    "\n",
    "# --- Anwenden ---\n",
    "train_df = clean_and_update(train_df)\n",
    "val_df   = clean_and_update(val_df)\n",
    "test_df  = clean_and_update(test_df)\n",
    "\n",
    "# --- Zurückschreiben (überschreibt die Dateien direkt) ---\n",
    "train_df.to_csv(output_train_path, sep='\\t', index=False)\n",
    "val_df.to_csv(output_val_path, sep='\\t', index=False)\n",
    "test_df.to_csv(test_output_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Gewünschte Änderungen in den finalen Splits wurden vorgenommen und gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Start: 0\n",
      "Batch 0 gespeichert.\n",
      "Batch Start: 1000\n",
      "Batch 1000 gespeichert.\n",
      "Batch Start: 2000\n",
      "Batch 2000 gespeichert.\n",
      "Batch Start: 3000\n",
      "Batch 3000 gespeichert.\n",
      "Batch Start: 4000\n",
      "Batch 4000 gespeichert.\n",
      "Batch Start: 5000\n",
      "Batch 5000 gespeichert.\n",
      "Batch Start: 6000\n",
      "Batch 6000 gespeichert.\n",
      "Batch Start: 7000\n",
      "Batch 7000 gespeichert.\n",
      "Batch Start: 8000\n",
      "Batch 8000 gespeichert.\n",
      "Batch Start: 9000\n",
      "Batch 9000 gespeichert.\n",
      "Batch Start: 10000\n",
      "Batch 10000 gespeichert.\n",
      "Batch Start: 11000\n",
      "Batch 11000 gespeichert.\n",
      "Batch Start: 12000\n",
      "Batch 12000 gespeichert.\n",
      "Batch Start: 13000\n",
      "Batch 13000 gespeichert.\n",
      "Batch Start: 14000\n",
      "Batch 14000 gespeichert.\n",
      "Batch Start: 15000\n",
      "Batch 15000 gespeichert.\n",
      "Batch Start: 16000\n",
      "Batch 16000 gespeichert.\n",
      "Batch Start: 17000\n",
      "Batch 17000 gespeichert.\n",
      "Batch Start: 18000\n",
      "Batch 18000 gespeichert.\n",
      "Batch Start: 19000\n",
      "Batch 19000 gespeichert.\n",
      "Batch Start: 20000\n",
      "Batch 20000 gespeichert.\n",
      "Batch Start: 21000\n",
      "Batch 21000 gespeichert.\n",
      "Batch Start: 22000\n",
      "Batch 22000 gespeichert.\n",
      "Batch Start: 23000\n",
      "Batch 23000 gespeichert.\n",
      "Batch Start: 24000\n",
      "Batch 24000 gespeichert.\n",
      "Batch Start: 25000\n",
      "Batch 25000 gespeichert.\n",
      "Batch Start: 26000\n",
      "Batch 26000 gespeichert.\n",
      "Batch Start: 27000\n",
      "Batch 27000 gespeichert.\n",
      "Batch Start: 28000\n",
      "Batch 28000 gespeichert.\n",
      "Batch Start: 29000\n",
      "Batch 29000 gespeichert.\n",
      "Batch Start: 30000\n",
      "Batch 30000 gespeichert.\n",
      "Batch Start: 31000\n",
      "Batch 31000 gespeichert.\n",
      "Batch Start: 32000\n",
      "Batch 32000 gespeichert.\n",
      "Batch Start: 33000\n",
      "Batch 33000 gespeichert.\n",
      "Batch Start: 34000\n",
      "Batch 34000 gespeichert.\n",
      "Batch Start: 35000\n",
      "Batch 35000 gespeichert.\n",
      "Batch Start: 36000\n",
      "Batch 36000 gespeichert.\n",
      "Batch Start: 37000\n",
      "Batch 37000 gespeichert.\n",
      "Batch Start: 38000\n",
      "Batch 38000 gespeichert.\n",
      "Batch Start: 39000\n",
      "Batch 39000 gespeichert.\n",
      "Batch Start: 40000\n",
      "Batch 40000 gespeichert.\n",
      "Batch Start: 41000\n",
      "Batch 41000 gespeichert.\n",
      "Batch Start: 42000\n",
      "Batch 42000 gespeichert.\n",
      "Batch Start: 43000\n",
      "Batch 43000 gespeichert.\n",
      "Batch Start: 44000\n",
      "Batch 44000 gespeichert.\n",
      "Batch Start: 45000\n",
      "Batch 45000 gespeichert.\n",
      "Batch Start: 46000\n",
      "Batch 46000 gespeichert.\n",
      "Batch Start: 47000\n",
      "Batch 47000 gespeichert.\n",
      "Batch Start: 48000\n",
      "Batch 48000 gespeichert.\n",
      "Batch Start: 49000\n",
      "Batch 49000 gespeichert.\n",
      "Batch Start: 50000\n",
      "Batch 50000 gespeichert.\n",
      "Batch Start: 51000\n",
      "Batch 51000 gespeichert.\n",
      "Batch Start: 52000\n",
      "Batch 52000 gespeichert.\n",
      "Batch Start: 53000\n",
      "Batch 53000 gespeichert.\n",
      "Batch Start: 54000\n",
      "Batch 54000 gespeichert.\n",
      "Batch Start: 55000\n",
      "Batch 55000 gespeichert.\n",
      "Batch Start: 56000\n",
      "Batch 56000 gespeichert.\n",
      "Batch Start: 57000\n",
      "Batch 57000 gespeichert.\n",
      "Batch Start: 58000\n",
      "Batch 58000 gespeichert.\n",
      "Batch Start: 59000\n",
      "Batch 59000 gespeichert.\n",
      "Batch Start: 60000\n",
      "Batch 60000 gespeichert.\n",
      "Batch Start: 61000\n",
      "Batch 61000 gespeichert.\n",
      "Batch Start: 62000\n",
      "Batch 62000 gespeichert.\n",
      "Batch Start: 63000\n",
      "Batch 63000 gespeichert.\n",
      "Batch Start: 64000\n",
      "Batch 64000 gespeichert.\n",
      "Batch Start: 65000\n",
      "Batch 65000 gespeichert.\n",
      "Batch Start: 66000\n",
      "Batch 66000 gespeichert.\n",
      "Batch Start: 67000\n",
      "Batch 67000 gespeichert.\n",
      "Batch Start: 68000\n",
      "Batch 68000 gespeichert.\n",
      "Batch Start: 69000\n",
      "Batch 69000 gespeichert.\n",
      "Batch Start: 70000\n",
      "Batch 70000 gespeichert.\n",
      "Batch Start: 71000\n",
      "Batch 71000 gespeichert.\n",
      "Batch Start: 72000\n",
      "Batch 72000 gespeichert.\n",
      "Batch Start: 73000\n",
      "Batch 73000 gespeichert.\n",
      "Batch Start: 74000\n",
      "Batch 74000 gespeichert.\n",
      "Batch Start: 75000\n",
      "Batch 75000 gespeichert.\n",
      "Batch Start: 76000\n",
      "Batch 76000 gespeichert.\n",
      "Batch Start: 77000\n",
      "Batch 77000 gespeichert.\n",
      "Batch Start: 78000\n",
      "Batch 78000 gespeichert.\n",
      "Batch Start: 79000\n",
      "Batch 79000 gespeichert.\n",
      "Batch Start: 80000\n",
      "Batch 80000 gespeichert.\n",
      "Batch Start: 81000\n",
      "Batch 81000 gespeichert.\n",
      "Batch Start: 82000\n",
      "Batch 82000 gespeichert.\n",
      "Batch Start: 83000\n",
      "Batch 83000 gespeichert.\n",
      "Batch Start: 84000\n",
      "Batch 84000 gespeichert.\n",
      "Batch Start: 85000\n",
      "Batch 85000 gespeichert.\n",
      "Batch Start: 86000\n",
      "Batch 86000 gespeichert.\n",
      "Batch Start: 87000\n",
      "Batch 87000 gespeichert.\n",
      "Batch Start: 88000\n",
      "Batch 88000 gespeichert.\n",
      "Batch Start: 89000\n",
      "Batch 89000 gespeichert.\n",
      "Batch Start: 90000\n",
      "Batch 90000 gespeichert.\n",
      "Batch Start: 91000\n",
      "Batch 91000 gespeichert.\n",
      "Batch Start: 92000\n",
      "Batch 92000 gespeichert.\n",
      "Batch Start: 93000\n",
      "Batch 93000 gespeichert.\n",
      "Batch Start: 94000\n",
      "Batch 94000 gespeichert.\n",
      "Batch Start: 95000\n",
      "Batch 95000 gespeichert.\n",
      "Batch Start: 96000\n",
      "Batch 96000 gespeichert.\n",
      "Batch Start: 97000\n",
      "Batch 97000 gespeichert.\n",
      "Batch Start: 98000\n",
      "Batch 98000 gespeichert.\n",
      "Batch Start: 99000\n",
      "Batch 99000 gespeichert.\n",
      "Batch Start: 100000\n",
      "Batch 100000 gespeichert.\n",
      "Batch Start: 101000\n",
      "Batch 101000 gespeichert.\n",
      "Batch Start: 102000\n",
      "Batch 102000 gespeichert.\n",
      "Batch Start: 103000\n",
      "Batch 103000 gespeichert.\n",
      "Batch Start: 104000\n",
      "Batch 104000 gespeichert.\n",
      "Batch Start: 105000\n",
      "Batch 105000 gespeichert.\n",
      "Batch Start: 106000\n",
      "Batch 106000 gespeichert.\n",
      "Batch Start: 107000\n",
      "Batch 107000 gespeichert.\n",
      "Batch Start: 108000\n",
      "Batch 108000 gespeichert.\n",
      "Batch Start: 109000\n",
      "Batch 109000 gespeichert.\n",
      "Batch Start: 110000\n",
      "Batch 110000 gespeichert.\n",
      "Batch Start: 111000\n",
      "Batch 111000 gespeichert.\n",
      "Batch Start: 112000\n",
      "Batch 112000 gespeichert.\n",
      "Batch Start: 113000\n",
      "Batch 113000 gespeichert.\n",
      "Batch Start: 114000\n",
      "Batch 114000 gespeichert.\n",
      "Batch Start: 115000\n",
      "Batch 115000 gespeichert.\n",
      "Batch Start: 116000\n",
      "Batch 116000 gespeichert.\n",
      "Batch Start: 117000\n",
      "Batch 117000 gespeichert.\n",
      "Batch Start: 118000\n",
      "Batch 118000 gespeichert.\n",
      "Batch Start: 119000\n",
      "Batch 119000 gespeichert.\n",
      "Batch Start: 120000\n",
      "Batch 120000 gespeichert.\n",
      "Batch Start: 121000\n",
      "Batch 121000 gespeichert.\n",
      "Batch Start: 122000\n",
      "Batch 122000 gespeichert.\n",
      "Batch Start: 123000\n",
      "Batch 123000 gespeichert.\n",
      "Batch Start: 124000\n",
      "Batch 124000 gespeichert.\n",
      "Batch Start: 125000\n",
      "Batch 125000 gespeichert.\n",
      "Batch Start: 126000\n",
      "Batch 126000 gespeichert.\n",
      "Batch Start: 127000\n",
      "Batch 127000 gespeichert.\n",
      "Batch Start: 128000\n",
      "Batch 128000 gespeichert.\n",
      "Batch Start: 129000\n",
      "Batch 129000 gespeichert.\n",
      "Batch Start: 130000\n",
      "Batch 130000 gespeichert.\n",
      "Batch Start: 131000\n",
      "Batch 131000 gespeichert.\n",
      "Batch Start: 132000\n",
      "Batch 132000 gespeichert.\n",
      "Batch Start: 133000\n",
      "Batch 133000 gespeichert.\n",
      "Batch Start: 134000\n",
      "Batch 134000 gespeichert.\n",
      "Batch Start: 135000\n",
      "Batch 135000 gespeichert.\n",
      "Batch Start: 136000\n",
      "Batch 136000 gespeichert.\n",
      "Batch Start: 137000\n",
      "Batch 137000 gespeichert.\n",
      "Batch Start: 138000\n",
      "Batch 138000 gespeichert.\n",
      "Batch Start: 139000\n",
      "Batch 139000 gespeichert.\n",
      "Batch Start: 140000\n",
      "Batch 140000 gespeichert.\n",
      "Batch Start: 141000\n",
      "Batch 141000 gespeichert.\n",
      "Batch Start: 142000\n",
      "Batch 142000 gespeichert.\n",
      "Batch Start: 143000\n",
      "Batch 143000 gespeichert.\n",
      "Batch Start: 144000\n",
      "Batch 144000 gespeichert.\n",
      "Batch Start: 145000\n",
      "Batch 145000 gespeichert.\n",
      "Batch Start: 146000\n",
      "Batch 146000 gespeichert.\n",
      "Batch Start: 147000\n",
      "Batch 147000 gespeichert.\n",
      "Batch Start: 148000\n",
      "Batch 148000 gespeichert.\n",
      "Batch Start: 149000\n",
      "Batch 149000 gespeichert.\n",
      "Batch Start: 150000\n",
      "Batch 150000 gespeichert.\n",
      "Batch Start: 151000\n",
      "Batch 151000 gespeichert.\n",
      "Batch Start: 152000\n",
      "Batch 152000 gespeichert.\n",
      "Batch Start: 153000\n",
      "Batch 153000 gespeichert.\n",
      "Batch Start: 154000\n",
      "Batch 154000 gespeichert.\n",
      "Batch Start: 155000\n",
      "Batch 155000 gespeichert.\n",
      "Batch Start: 156000\n",
      "Batch 156000 gespeichert.\n",
      "Batch Start: 157000\n",
      "Batch 157000 gespeichert.\n",
      "Batch Start: 158000\n",
      "Batch 158000 gespeichert.\n",
      "Batch Start: 159000\n",
      "Batch 159000 gespeichert.\n",
      "Batch Start: 160000\n",
      "Batch 160000 gespeichert.\n",
      "Batch Start: 161000\n",
      "Batch 161000 gespeichert.\n",
      "Batch Start: 162000\n",
      "Batch 162000 gespeichert.\n",
      "Batch Start: 163000\n",
      "Batch 163000 gespeichert.\n",
      "Batch Start: 164000\n",
      "Batch 164000 gespeichert.\n",
      "Batch Start: 165000\n",
      "Batch 165000 gespeichert.\n",
      "Batch Start: 166000\n",
      "Batch 166000 gespeichert.\n",
      "Batch Start: 167000\n",
      "Batch 167000 gespeichert.\n",
      "Batch Start: 168000\n",
      "Batch 168000 gespeichert.\n",
      "Batch Start: 169000\n",
      "Batch 169000 gespeichert.\n",
      "Batch Start: 170000\n",
      "Batch 170000 gespeichert.\n",
      "Batch Start: 171000\n",
      "Batch 171000 gespeichert.\n",
      "Batch Start: 172000\n",
      "Batch 172000 gespeichert.\n",
      "Batch Start: 173000\n",
      "Batch 173000 gespeichert.\n",
      "Batch Start: 174000\n",
      "Batch 174000 gespeichert.\n",
      "Batch Start: 175000\n",
      "Batch 175000 gespeichert.\n",
      "Batch Start: 176000\n",
      "Batch 176000 gespeichert.\n",
      "Batch Start: 177000\n",
      "Batch 177000 gespeichert.\n",
      "Batch Start: 178000\n",
      "Batch 178000 gespeichert.\n",
      "Batch Start: 179000\n",
      "Batch 179000 gespeichert.\n",
      "Batch Start: 180000\n",
      "Batch 180000 gespeichert.\n",
      "Batch Start: 181000\n",
      "Batch 181000 gespeichert.\n",
      "Batch Start: 182000\n",
      "Batch 182000 gespeichert.\n",
      "Batch Start: 183000\n",
      "Batch 183000 gespeichert.\n",
      "Batch Start: 184000\n",
      "Batch 184000 gespeichert.\n",
      "Batch Start: 185000\n",
      "Batch 185000 gespeichert.\n",
      "Batch Start: 186000\n",
      "Batch 186000 gespeichert.\n",
      "Batch Start: 187000\n",
      "Batch 187000 gespeichert.\n",
      "Batch Start: 188000\n",
      "Batch 188000 gespeichert.\n",
      "Batch Start: 189000\n",
      "Batch 189000 gespeichert.\n",
      "             TCR_name       TRAV    TRAJ           TRA_CDR3      TRBV  \\\n",
      "0  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "1  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "2  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "3  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "4  AAACCTGAGACAAAGG-4  TRAV29DV5  TRAJ44  CAASVSIWTGTASKLTF  TRBV10-3   \n",
      "\n",
      "      TRBJ           TRB_CDR3  TRAC   TRBC     Epitope          MHC Binding  \\\n",
      "0  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   VTEHDTLLY  HLA-A*01:01       0   \n",
      "1  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   KTWGQYWQV  HLA-A*02:01       0   \n",
      "2  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2  ELAGIGILTV  HLA-A*02:01       0   \n",
      "3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2  CLLWSFQTSA  HLA-A*02:01       0   \n",
      "4  TRBJ2-1  CAISDPGLAGGGGEQFF  TRAC  TRBC2   IMDQVPFSV  HLA-A*02:01       0   \n",
      "\n",
      "  task  \n",
      "0  nan  \n",
      "1  nan  \n",
      "2  nan  \n",
      "3  nan  \n",
      "4  nan  \n",
      "Datei erfolgreich gespeichert!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Annahme: all_donors_consensus und all_donors_meta sind bereits geladen und gefiltert\n",
    "\n",
    "# Festlegen der Batch-Größe für die Verarbeitung\n",
    "batch_size = 1000\n",
    "\n",
    "# Identifizieren von Epitope-Spalten, aber ohne \"NR(B0801)_AAKGRGAAL_NC_binder\"\n",
    "epitope_columns = [col for col in all_donors_consensus.columns if '_binder' in col and col != \"NR(B0801)_AAKGRGAAL_NC_binder\"]\n",
    "\n",
    "# Ausgabe-Datei für Batch-Ergebnisse\n",
    "output_batch_file = f'{pipeline_data_plain}/10x/expanded_batches.csv'\n",
    "\n",
    "# Stelle sicher, dass die Ausgabedatei leer ist\n",
    "with open(output_batch_file, 'w') as f:\n",
    "    f.write('')  # Leere Datei erstellen\n",
    "\n",
    "# Verarbeite `all_donors_consensus` in Batches\n",
    "for batch_start in range(0, len(all_donors_consensus), batch_size):\n",
    "    print(f\"Batch Start: {batch_start}\")\n",
    "    # Definiere Batch\n",
    "    batch = all_donors_consensus.iloc[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Filtern auf Zeilen, die sowohl 'TRA:' als auch 'TRB:' in 'cell_clono_cdr3_aa' enthalten\n",
    "    batch_paired = batch[\n",
    "        batch['cell_clono_cdr3_aa'].str.contains(\"TRA:\", na=False) &\n",
    "        batch['cell_clono_cdr3_aa'].str.contains(\"TRB:\", na=False)\n",
    "    ]\n",
    "\n",
    "    # Liste, um Zeilen für diesen Batch zu speichern\n",
    "    expanded_rows = []\n",
    "    \n",
    "    # Iteriere durch jede Zeile im Batch\n",
    "    for _, row in batch_paired.iterrows():\n",
    "        for col in epitope_columns:\n",
    "            # Extrahiere MHC und Epitope\n",
    "            match = re.match(r'([A-Z0-9]+)_([A-Z]+)_.*_binder', col)\n",
    "            if match:\n",
    "                mhc_raw, epitope = match.groups()\n",
    "                mhc_formatted = f'HLA-{mhc_raw[0]}*{mhc_raw[1:3]}:{mhc_raw[3:]}'\n",
    "\n",
    "                # Füge `Epitope` und `MHC` zur Zeile hinzu\n",
    "                new_row = row.copy()\n",
    "                new_row['Epitope'] = epitope\n",
    "                new_row['MHC'] = mhc_formatted\n",
    "\n",
    "                # Neue Zeile zur Batch-Liste hinzufügen\n",
    "                expanded_rows.append(new_row)\n",
    "\n",
    "    # Erstelle einen DataFrame aus dem Batch\n",
    "    batch_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    # Füge den Batch direkt in die Datei ein\n",
    "    batch_df.to_csv(output_batch_file, mode='a', index=False, header=not batch_start, sep=',')\n",
    "    print(f\"Batch {batch_start} gespeichert.\")\n",
    "\n",
    "# Laden der gespeicherten Batch-Ergebnisse\n",
    "expanded_df = pd.read_csv(output_batch_file)\n",
    "\n",
    "# Nur die Paired-Einträge in `all_donors_meta` beibehalten\n",
    "# Filtern auf Barcodes, die sowohl eine TRA- als auch eine TRB-Kette haben\n",
    "paired_barcodes = all_donors_meta.groupby('barcode').filter(\n",
    "    lambda x: set(x['chain']) == {'TRA', 'TRB'}\n",
    ")['barcode'].unique()\n",
    "all_donors_meta_paired = all_donors_meta[all_donors_meta['barcode'].isin(paired_barcodes)]\n",
    "\n",
    "# Split `all_donors_meta_paired` nach `chain` in separate DataFrames für TRA und TRB\n",
    "alpha_chain = all_donors_meta_paired[all_donors_meta_paired['chain'] == 'TRA'].rename(\n",
    "    columns={'v_gene': 'TRAV', 'j_gene': 'TRAJ', 'cdr3': 'TRA_CDR3', 'c_gene': 'TRAC'}\n",
    ")\n",
    "beta_chain = all_donors_meta_paired[all_donors_meta_paired['chain'] == 'TRB'].rename(\n",
    "    columns={'v_gene': 'TRBV', 'j_gene': 'TRBJ', 'cdr3': 'TRB_CDR3', 'c_gene': 'TRBC'}\n",
    ")\n",
    "\n",
    "# Zusammenführen von alpha_chain und beta_chain anhand der gemeinsamen 'barcode'-Spalte\n",
    "paired_meta = pd.merge(alpha_chain, beta_chain, on='barcode', suffixes=('_alpha', '_beta'))\n",
    "\n",
    "# Zusammenführen von `paired_meta` mit `expanded_df` anhand der 'barcode'-Spalte\n",
    "merged_df = pd.merge(paired_meta, expanded_df[['barcode', 'Epitope', 'MHC']], on='barcode', how='inner')\n",
    "\n",
    "# Spalten umbenennen und Format anpassen\n",
    "merged_df = merged_df.rename(columns={'barcode': 'TCR_name'})\n",
    "\n",
    "# Fehlende Spalten auffüllen\n",
    "desired_columns = [\n",
    "    'TCR_name', 'TRAV', 'TRAJ', 'TRA_CDR3', 'TRBV', 'TRBJ', 'TRB_CDR3', 'TRAC', 'TRBC', \n",
    "    'Epitope', 'MHC', 'Binding', 'task'\n",
    "]\n",
    "for col in desired_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = 'nan' if col == 'task' else '0'\n",
    "\n",
    "# Nur die gewünschten Spalten beibehalten und Zeilen mit `None` in `TRB_CDR3` entfernen\n",
    "final_df = merged_df[desired_columns]\n",
    "final_df = final_df[final_df['TRB_CDR3'] != 'None']\n",
    "\n",
    "final_df = final_df[\n",
    "    final_df['TRB_CDR3'].notna() & (final_df['TRB_CDR3'] != '') &\n",
    "    final_df['TRA_CDR3'].notna() & (final_df['TRA_CDR3'] != '')\n",
    "]\n",
    "\n",
    "# Ausgabe des ersten Teils des Ergebnisses zur Überprüfung\n",
    "print(final_df.head())\n",
    "\n",
    "# Optional: Speichern des kombinierten DataFrames\n",
    "output_path = f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_paired_only_expanded-all.csv'\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(\"Datei erfolgreich gespeichert!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15960/1931590610.py:7: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(train_file, sep='\\t'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alle Datensätze wurden erfolgreich gespeichert.\n",
      "Test: 160362 Einträge\n",
      "- Unique TCRs: 52994\n",
      "- Unique Epitope: 831\n",
      "- Positive (binding == 1): 26727\n",
      "- Negative (binding == 0): 133635\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_file = f'{pipeline_data_splitted}/{precision}/paired/train.tsv'\n",
    "validation_file = f'{pipeline_data_splitted}/{precision}/paired/validation.tsv'\n",
    "\n",
    "trainval = pd.concat([\n",
    "    pd.read_csv(train_file, sep='\\t'),\n",
    "    pd.read_csv(validation_file, sep='\\t')\n",
    "], ignore_index=True)\n",
    "\n",
    "trainval_negatives = set(zip(trainval.loc[trainval['Binding'] == 0, 'TRA_CDR3'], \n",
    "                             trainval.loc[trainval['Binding'] == 0, 'TRB_CDR3'], \n",
    "                             trainval.loc[trainval['Binding'] == 0, 'Epitope']))\n",
    "\n",
    "# Negative Daten laden\n",
    "paired = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_paired_only_expanded-all.csv', sep=',')\n",
    "\n",
    "# Positive Samples laden\n",
    "test_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/paired/test_prenegsamples.tsv', sep='\\t')\n",
    "\n",
    "# Anzahl positiver Samples\n",
    "num_test_pos = len(test_preneg)\n",
    "\n",
    "# Zielgrößen für negative Samples\n",
    "test_neg_needed = num_test_pos * 5\n",
    "\n",
    "# Stelle sicher, dass ALLE positiven Epitope enthalten sind\n",
    "positive_epitopes = set(test_preneg[\"Epitope\"])\n",
    "paired_with_epitopes = paired[paired[\"Epitope\"].isin(positive_epitopes)]\n",
    "\n",
    "# Nur negative Samples, die NICHT in trainval existieren\n",
    "paired_negatives = paired[paired['Binding'] == 0].copy()\n",
    "paired_negatives_filtered = paired_negatives[~paired_negatives[['TRA_CDR3', 'TRB_CDR3', 'Epitope']].apply(tuple, axis=1).isin(trainval_negatives)]\n",
    "\n",
    "# Falls zu wenige negative übrig sind, mit Replacement auffüllen\n",
    "if len(paired_negatives_filtered) < test_neg_needed:\n",
    "    test_negatives = paired_negatives_filtered.sample(test_neg_needed, replace=True, random_state=42)\n",
    "else:\n",
    "    test_negatives = paired_negatives_filtered.sample(test_neg_needed, random_state=42)\n",
    "\n",
    "# Positive und negative Samples kombinieren\n",
    "test_combined = pd.concat([test_preneg, test_negatives], ignore_index=True)\n",
    "\n",
    "# Speichern der kombinierten Datensätze\n",
    "output_dir = f'{pipeline_data_splitted}/{precision}/paired/'\n",
    "test_combined.to_csv(output_dir + \"test.tsv\", sep='\\t', index=False)\n",
    "\n",
    "# Berechne Unique Werte\n",
    "unique_tcr_test = test_combined['TRB_CDR3'].nunique()\n",
    "unique_epitope_test = test_combined['Epitope'].nunique()\n",
    "\n",
    "# Berechne Anzahl positiver und negativer Einträge\n",
    "test_binding_counts = test_combined['Binding'].value_counts()\n",
    "\n",
    "# Finale Ausgabe\n",
    "print(\"\\nAlle Datensätze wurden erfolgreich gespeichert.\")\n",
    "print(f\"Test: {len(test_combined)} Einträge\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_test}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_test}\")\n",
    "print(f\"- Positive (binding == 1): {test_binding_counts.get(1, 0)}\")\n",
    "print(f\"- Negative (binding == 0): {test_binding_counts.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Classification \n",
    "The classification in the split notebook correct for positive only data. After adding negative data, some classifications might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta/new'\n",
    "test_file_name = 'test.tsv'\n",
    "validation_file_name = 'validation.tsv'\n",
    "train_file_name = 'train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/1815672389.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_data_path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 82726 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 76994 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 600 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 42 TPP4 tasks (seen tcr & unseen epitope).\n"
     ]
    }
   ],
   "source": [
    "# do the classification for paired data\n",
    "paired = True\n",
    "test_data_path = f'{paired_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "train_data_path = f'{paired_output_folder}/{train_file_name}'\n",
    "\n",
    "%run ../data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allele\n",
      "../../data/splitted_datasets/allele/paired/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/1788404633.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_path, sep=\"\\t\", index_col=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train+validate data has 72656 entries\n",
      "test data has 160362 entries\n",
      "test data has 128885 TPP1 tasks (old value: 82726) (seen tcr & seen epitopes).\n",
      "test data has 30835 TPP2 tasks (old value: 76994) (unseen tcr & seen epitopes).\n",
      "test data has 508 TPP3 tasks (old value: 600) (unseen tcr & unseen epitope).\n",
      "test data has 134 TPP4 tasks (old value: 42) (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.31180423829918724/0.6881957617008128\n",
      "../../data/splitted_datasets/allele/paired/test_reclassified_paired_specific.tsv\n",
      "/home/ubuntu/arina/BA-Cancer-Immunotherapy\n",
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250310_171148-c8scp4az</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">kind-sun-24</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/allele/paired)... Done. 0.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1d4067a88546039870f6c520cee5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-sun-24</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250310_171148-c8scp4az/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extended classification for paired data\n",
    "train_path = f'{paired_output_folder}/{train_file_name}'\n",
    "validation_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "test_path = f'{paired_output_folder}/{test_file_name}'\n",
    "output_path = f'{paired_output_folder}/test_reclassified_paired_specific.tsv'\n",
    "paired_data_path = paired_output_folder\n",
    "alpha_cdr3_name = 'TRA_CDR3'\n",
    "beta_cdr3_name = 'TRB_CDR3'\n",
    "epitope_name = 'Epitope'\n",
    "task_name = 'task'\n",
    "\n",
    "%run ../data_preparation/paired_reclassification_testonly.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 45481 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 7796 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 88 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 35 TPP4 tasks (seen tcr & unseen epitope).\n"
     ]
    }
   ],
   "source": [
    "# do the classification for beta data\n",
    "paired = False\n",
    "train_data_path = f'{beta_output_folder}/{train_file_name}'\n",
    "validation_data_path = f'{beta_output_folder}/{validation_file_name}'\n",
    "test_data_path = f'{beta_output_folder}/{test_file_name}'\n",
    "\n",
    "%run ../data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells the classification is checked. If the output says \"Classification is correct\", everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/788315683.py:21: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_file, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train+validate data has 72656 entries\n",
      "test data has 160362 entries\n",
      "test data has 82726 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 76994 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 600 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 42 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.31180423829918724/0.6881957617008128\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    160362\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification paired\n",
    "splitted_data_path = paired_output_folder\n",
    "\n",
    "%run ../data_preparation/check_task_classification_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 840756 entries\n",
      "test data has 53400 entries\n",
      "test data has 45481 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 7796 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 88 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 35 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.9516456769061926/0.04835432309380739\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    53400\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification beta\n",
    "splitted_data_path = beta_output_folder\n",
    "\n",
    "%run ../data_preparation/check_task_classification_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'validation_prenegsamples.tsv', 'test.tsv', 'train.tsv', 'test_reclassified_paired_specific.tsv', 'validation.tsv', 'train_prenegsamples.tsv', 'validate_reclassified_paired_specific.tsv', 'test_prenegsamples.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(path_to_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/data_scripts/datapipeline/wandb/run-20250404_111745-xgr5xfpq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xgr5xfpq' target=\"_blank\">daily-glade-105</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xgr5xfpq' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xgr5xfpq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../../../data/splitted_datasets/allele/paired)... Done. 0.3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-glade-105</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xgr5xfpq' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xgr5xfpq</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250404_111745-xgr5xfpq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# upload paired data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "dataset_name = f'paired_{precision}'\n",
    "#main_project_name = os.getenv(\"MAIN_PROJECT_NAME\")\n",
    "main_project_name = f\"dataset-{precision}\"\n",
    "\n",
    "%run ../upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/data_scripts/datapipeline/wandb/run-20250404_111748-zmgq2j1d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/zmgq2j1d' target=\"_blank\">royal-pyramid-106</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/zmgq2j1d' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/zmgq2j1d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../../../data/splitted_datasets/allele/beta)... Done. 0.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24354de586664950b4e882dcd739666a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.229 MB of 174.826 MB uploaded\\r'), FloatProgress(value=0.007027826128148868, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-pyramid-106</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/zmgq2j1d' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/zmgq2j1d</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250404_111748-zmgq2j1d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload beta data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "dataset_name = f'beta_{precision}'\n",
    "\n",
    "%run ../upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings >> ProtBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Sollte True zurückgeben\n",
    "print(torch.version.cuda)  # Sollte die richtige CUDA-Version anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88007/1782043182.py:11: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_paired_test = pd.read_csv(path_paired_test, sep=\"\\t\", index_col=False)\n",
      "2025-04-04 11:18:19.859011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743765500.399692   88007 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743765500.543912   88007 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743765501.789001   88007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743765501.789067   88007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743765501.789074   88007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743765501.789080   88007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-04 11:18:21.932782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Loading: Rostlab/prot_bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c4ddc1a3dd468d9ed5eee8fbe96a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/361 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c642b032eb9c43fb9030414be016a378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faf7235aedb4385a9bdb736d3237545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24bfe7783bc421696099f66876e2e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4053e71e49499daa51153121026bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch:  0 64\n",
      "Processing Batch:  64 128\n",
      "Processing Batch:  128 192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cb5e40e33c49649f0483ed945cc52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch:  192 256\n",
      "Processing Batch:  256 320\n",
      "Processing Batch:  320 384\n",
      "Processing Batch:  384 448\n",
      "Processing Batch:  448 512\n",
      "Processing Batch:  512 576\n",
      "Processing Batch:  576 640\n",
      "Processing Batch:  640 704\n",
      "Processing Batch:  704 768\n",
      "Processing Batch:  768 832\n",
      "Processing Batch:  832 896\n",
      "Processing Batch:  896 960\n",
      "Processing Batch:  960 1024\n",
      "Processing Batch:  1024 1088\n",
      "Processing Batch:  1088 1152\n",
      "Processing Batch:  1152 1216\n",
      "Processing Batch:  1216 1280\n",
      "Processing Batch:  1280 1344\n",
      "Processing Batch:  1344 1408\n",
      "Processing Batch:  1408 1472\n",
      "Processing Batch:  1472 1536\n",
      "Processing Batch:  1536 1600\n",
      "Processing Batch:  1600 1664\n",
      "Processing Batch:  1664 1728\n",
      "Processing Batch:  1728 1792\n",
      "Processing Batch:  1792 1856\n",
      "Processing Batch:  1856 1920\n",
      "Processing Batch:  1920 1984\n",
      "Processing Batch:  1984 2048\n",
      "Processing Batch:  2048 2112\n",
      "Processing Batch:  2112 2176\n",
      "Processing Batch:  2176 2240\n",
      "Processing Batch:  2240 2304\n",
      "Processing Batch:  2304 2368\n",
      "Processing Batch:  2368 2432\n",
      "Processing Batch:  2432 2496\n",
      "Processing Batch:  2496 2560\n",
      "Processing Batch:  2560 2624\n",
      "Processing Batch:  2624 2688\n",
      "Processing Batch:  2688 2752\n",
      "Processing Batch:  2752 2816\n",
      "Processing Batch:  2816 2880\n",
      "Processing Batch:  2880 2944\n",
      "Processing Batch:  2944 3008\n",
      "Processing Batch:  3008 3072\n",
      "Processing Batch:  3072 3136\n",
      "Processing Batch:  3136 3200\n",
      "Processing Batch:  3200 3264\n",
      "Processing Batch:  3264 3328\n",
      "Processing Batch:  3328 3392\n",
      "Processing Batch:  3392 3456\n",
      "Processing Batch:  3456 3520\n",
      "Processing Batch:  3520 3584\n",
      "Processing Batch:  3584 3648\n",
      "Processing Batch:  3648 3712\n",
      "Processing Batch:  3712 3776\n",
      "Processing Batch:  3776 3840\n",
      "Processing Batch:  3840 3904\n",
      "Processing Batch:  3904 3968\n",
      "Processing Batch:  3968 4032\n",
      "Processing Batch:  4032 4096\n",
      "Processing Batch:  4096 4160\n",
      "Processing Batch:  4160 4224\n",
      "Processing Batch:  4224 4288\n",
      "Processing Batch:  4288 4352\n",
      "Processing Batch:  4352 4416\n",
      "Processing Batch:  4416 4480\n",
      "Processing Batch:  4480 4544\n",
      "Processing Batch:  4544 4608\n",
      "Processing Batch:  4608 4672\n",
      "Processing Batch:  4672 4736\n",
      "Processing Batch:  4736 4800\n",
      "Processing Batch:  4800 4864\n",
      "Processing Batch:  4864 4928\n",
      "Processing Batch:  4928 4992\n",
      "Processing Batch:  4992 5056\n",
      "Processing Batch:  5056 5120\n",
      "Processing Batch:  5120 5184\n",
      "Processing Batch:  5184 5248\n",
      "Processing Batch:  5248 5312\n",
      "Processing Batch:  5312 5376\n",
      "Processing Batch:  5376 5440\n",
      "Processing Batch:  5440 5504\n",
      "Processing Batch:  5504 5568\n",
      "Processing Batch:  5568 5632\n",
      "Processing Batch:  5632 5696\n",
      "Processing Batch:  5696 5760\n",
      "Processing Batch:  5760 5824\n",
      "Processing Batch:  5824 5888\n",
      "Processing Batch:  5888 5952\n",
      "Processing Batch:  5952 6016\n",
      "Processing Batch:  6016 6080\n",
      "Processing Batch:  6080 6144\n",
      "Processing Batch:  6144 6208\n",
      "Processing Batch:  6208 6272\n",
      "Processing Batch:  6272 6336\n",
      "Processing Batch:  6336 6400\n",
      "Processing Batch:  6400 6464\n",
      "Processing Batch:  6464 6528\n",
      "Processing Batch:  6528 6592\n",
      "Processing Batch:  6592 6656\n",
      "Processing Batch:  6656 6720\n",
      "Processing Batch:  6720 6784\n",
      "Processing Batch:  6784 6848\n",
      "Processing Batch:  6848 6912\n",
      "Processing Batch:  6912 6976\n",
      "Processing Batch:  6976 7040\n",
      "Processing Batch:  7040 7104\n",
      "Processing Batch:  7104 7168\n",
      "Processing Batch:  7168 7232\n",
      "Processing Batch:  7232 7296\n",
      "Processing Batch:  7296 7360\n",
      "Processing Batch:  7360 7424\n",
      "Processing Batch:  7424 7488\n",
      "Processing Batch:  7488 7552\n",
      "Processing Batch:  7552 7616\n",
      "Processing Batch:  7616 7680\n",
      "Processing Batch:  7680 7744\n",
      "Processing Batch:  7744 7808\n",
      "Processing Batch:  7808 7872\n",
      "Processing Batch:  7872 7936\n",
      "Processing Batch:  7936 8000\n",
      "Processing Batch:  8000 8064\n",
      "Processing Batch:  8064 8128\n",
      "Processing Batch:  8128 8192\n",
      "Processing Batch:  8192 8256\n",
      "Processing Batch:  8256 8320\n",
      "Processing Batch:  8320 8384\n",
      "Processing Batch:  8384 8448\n",
      "Processing Batch:  8448 8512\n",
      "Processing Batch:  8512 8576\n",
      "Processing Batch:  8576 8640\n",
      "Processing Batch:  8640 8704\n",
      "Processing Batch:  8704 8768\n",
      "Processing Batch:  8768 8832\n",
      "Processing Batch:  8832 8896\n",
      "Processing Batch:  8896 8960\n",
      "Processing Batch:  8960 9024\n",
      "Processing Batch:  9024 9088\n",
      "Processing Batch:  9088 9152\n",
      "Processing Batch:  9152 9216\n",
      "Processing Batch:  9216 9280\n",
      "Processing Batch:  9280 9344\n",
      "Processing Batch:  9344 9408\n",
      "Processing Batch:  9408 9472\n",
      "Processing Batch:  9472 9536\n",
      "Processing Batch:  9536 9600\n",
      "Processing Batch:  9600 9664\n",
      "Processing Batch:  9664 9728\n",
      "Processing Batch:  9728 9792\n",
      "Processing Batch:  9792 9856\n",
      "Processing Batch:  9856 9920\n",
      "Processing Batch:  9920 9984\n",
      "Processing Batch:  9984 10048\n",
      "Processing Batch:  10048 10112\n",
      "Processing Batch:  10112 10176\n",
      "Processing Batch:  10176 10240\n",
      "Processing Batch:  10240 10304\n",
      "Processing Batch:  10304 10368\n",
      "Processing Batch:  10368 10432\n",
      "Processing Batch:  10432 10496\n",
      "Processing Batch:  10496 10560\n",
      "Processing Batch:  10560 10624\n",
      "Processing Batch:  10624 10688\n",
      "Processing Batch:  10688 10752\n",
      "Processing Batch:  10752 10816\n",
      "Processing Batch:  10816 10880\n",
      "Processing Batch:  10880 10944\n",
      "Processing Batch:  10944 11008\n",
      "Processing Batch:  11008 11072\n",
      "Processing Batch:  11072 11136\n",
      "Processing Batch:  11136 11200\n",
      "Processing Batch:  11200 11264\n",
      "Processing Batch:  11264 11328\n",
      "Processing Batch:  11328 11392\n",
      "Processing Batch:  11392 11456\n",
      "Processing Batch:  11456 11520\n",
      "Processing Batch:  11520 11584\n",
      "Processing Batch:  11584 11648\n",
      "Processing Batch:  11648 11712\n",
      "Processing Batch:  11712 11776\n",
      "Processing Batch:  11776 11840\n",
      "Processing Batch:  11840 11904\n",
      "Processing Batch:  11904 11968\n",
      "Processing Batch:  11968 12032\n",
      "Processing Batch:  12032 12096\n",
      "Processing Batch:  12096 12160\n",
      "Processing Batch:  12160 12224\n",
      "Processing Batch:  12224 12288\n",
      "Processing Batch:  12288 12352\n",
      "Processing Batch:  12352 12416\n",
      "Processing Batch:  12416 12480\n",
      "Processing Batch:  12480 12544\n",
      "Processing Batch:  12544 12608\n",
      "Processing Batch:  12608 12672\n",
      "Processing Batch:  12672 12736\n",
      "Processing Batch:  12736 12800\n",
      "Processing Batch:  12800 12864\n",
      "Processing Batch:  12864 12928\n",
      "Processing Batch:  12928 12992\n",
      "Processing Batch:  12992 13056\n",
      "Processing Batch:  13056 13120\n",
      "Processing Batch:  13120 13184\n",
      "Processing Batch:  13184 13248\n",
      "Processing Batch:  13248 13312\n",
      "Processing Batch:  13312 13376\n",
      "Processing Batch:  13376 13440\n",
      "Processing Batch:  13440 13504\n",
      "Processing Batch:  13504 13568\n",
      "Processing Batch:  13568 13632\n",
      "Processing Batch:  13632 13696\n",
      "Processing Batch:  13696 13760\n",
      "Processing Batch:  13760 13824\n",
      "Processing Batch:  13824 13888\n",
      "Processing Batch:  13888 13952\n",
      "Processing Batch:  13952 14016\n",
      "Processing Batch:  14016 14080\n",
      "Processing Batch:  14080 14144\n",
      "Processing Batch:  14144 14208\n",
      "Processing Batch:  14208 14272\n",
      "Processing Batch:  14272 14336\n",
      "Processing Batch:  14336 14400\n",
      "Processing Batch:  14400 14464\n",
      "Processing Batch:  14464 14528\n",
      "Processing Batch:  14528 14592\n",
      "Processing Batch:  14592 14656\n",
      "Processing Batch:  14656 14720\n",
      "Processing Batch:  14720 14784\n",
      "Processing Batch:  14784 14848\n",
      "Processing Batch:  14848 14912\n",
      "Processing Batch:  14912 14976\n",
      "Processing Batch:  14976 15040\n",
      "Processing Batch:  15040 15104\n",
      "Processing Batch:  15104 15168\n",
      "Processing Batch:  15168 15232\n",
      "Processing Batch:  15232 15296\n",
      "Processing Batch:  15296 15360\n",
      "Processing Batch:  15360 15424\n",
      "Processing Batch:  15424 15488\n",
      "Processing Batch:  15488 15552\n",
      "Processing Batch:  15552 15616\n",
      "Processing Batch:  15616 15680\n",
      "Processing Batch:  15680 15744\n",
      "Processing Batch:  15744 15808\n",
      "Processing Batch:  15808 15872\n",
      "Processing Batch:  15872 15936\n",
      "Processing Batch:  15936 16000\n",
      "Processing Batch:  16000 16064\n",
      "Processing Batch:  16064 16128\n",
      "Processing Batch:  16128 16192\n",
      "Processing Batch:  16192 16256\n",
      "Processing Batch:  16256 16320\n",
      "Processing Batch:  16320 16384\n",
      "Processing Batch:  16384 16448\n",
      "Processing Batch:  16448 16512\n",
      "Processing Batch:  16512 16576\n",
      "Processing Batch:  16576 16640\n",
      "Processing Batch:  16640 16704\n",
      "Processing Batch:  16704 16768\n",
      "Processing Batch:  16768 16832\n",
      "Processing Batch:  16832 16896\n",
      "Processing Batch:  16896 16960\n",
      "Processing Batch:  16960 17024\n",
      "Processing Batch:  17024 17088\n",
      "Processing Batch:  17088 17152\n",
      "Processing Batch:  17152 17216\n",
      "Processing Batch:  17216 17280\n",
      "Processing Batch:  17280 17344\n",
      "Processing Batch:  17344 17408\n",
      "Processing Batch:  17408 17472\n",
      "Processing Batch:  17472 17536\n",
      "Processing Batch:  17536 17600\n",
      "Processing Batch:  17600 17664\n",
      "Processing Batch:  17664 17728\n",
      "Processing Batch:  17728 17792\n",
      "Processing Batch:  17792 17856\n",
      "Processing Batch:  17856 17920\n",
      "Processing Batch:  17920 17984\n",
      "Processing Batch:  17984 18048\n",
      "Processing Batch:  18048 18112\n",
      "Processing Batch:  18112 18176\n",
      "Processing Batch:  18176 18240\n",
      "Processing Batch:  18240 18304\n",
      "Processing Batch:  18304 18368\n",
      "Processing Batch:  18368 18432\n",
      "Processing Batch:  18432 18496\n",
      "Processing Batch:  18496 18560\n",
      "Processing Batch:  18560 18624\n",
      "Processing Batch:  18624 18688\n",
      "Processing Batch:  18688 18752\n",
      "Processing Batch:  18752 18816\n",
      "Processing Batch:  18816 18880\n",
      "Processing Batch:  18880 18944\n",
      "Processing Batch:  18944 19008\n",
      "Processing Batch:  19008 19072\n",
      "Processing Batch:  19072 19136\n",
      "Processing Batch:  19136 19200\n",
      "Processing Batch:  19200 19264\n",
      "Processing Batch:  19264 19328\n",
      "Processing Batch:  19328 19392\n",
      "Processing Batch:  19392 19456\n",
      "Processing Batch:  19456 19520\n",
      "Processing Batch:  19520 19584\n",
      "Processing Batch:  19584 19648\n",
      "Processing Batch:  19648 19712\n",
      "Processing Batch:  19712 19776\n",
      "Processing Batch:  19776 19840\n",
      "Processing Batch:  19840 19904\n",
      "Processing Batch:  19904 19968\n",
      "Processing Batch:  19968 20032\n",
      "Processing Batch:  20032 20096\n",
      "Processing Batch:  20096 20160\n",
      "Processing Batch:  20160 20224\n",
      "Processing Batch:  20224 20288\n",
      "Processing Batch:  20288 20352\n",
      "Processing Batch:  20352 20416\n",
      "Processing Batch:  20416 20480\n",
      "Processing Batch:  20480 20544\n",
      "Processing Batch:  20544 20608\n",
      "Processing Batch:  20608 20672\n",
      "Processing Batch:  20672 20736\n",
      "Processing Batch:  20736 20800\n",
      "Processing Batch:  20800 20864\n",
      "Processing Batch:  20864 20928\n",
      "Processing Batch:  20928 20992\n",
      "Processing Batch:  20992 21056\n",
      "Processing Batch:  21056 21120\n",
      "Processing Batch:  21120 21184\n",
      "Processing Batch:  21184 21248\n",
      "Processing Batch:  21248 21312\n",
      "Processing Batch:  21312 21376\n",
      "Processing Batch:  21376 21440\n",
      "Processing Batch:  21440 21504\n",
      "Processing Batch:  21504 21568\n",
      "Processing Batch:  21568 21632\n",
      "Processing Batch:  21632 21696\n",
      "Processing Batch:  21696 21760\n",
      "Processing Batch:  21760 21824\n",
      "Processing Batch:  21824 21888\n",
      "Processing Batch:  21888 21952\n",
      "Processing Batch:  21952 22016\n",
      "Processing Batch:  22016 22080\n",
      "Processing Batch:  22080 22144\n",
      "Processing Batch:  22144 22208\n",
      "Processing Batch:  22208 22272\n",
      "Processing Batch:  22272 22336\n",
      "Processing Batch:  22336 22400\n",
      "Processing Batch:  22400 22464\n",
      "Processing Batch:  22464 22528\n",
      "Processing Batch:  22528 22592\n",
      "Processing Batch:  22592 22656\n",
      "Processing Batch:  22656 22720\n",
      "Processing Batch:  22720 22784\n",
      "Processing Batch:  22784 22848\n",
      "Processing Batch:  22848 22912\n",
      "Processing Batch:  22912 22976\n",
      "Processing Batch:  22976 23040\n",
      "Processing Batch:  23040 23104\n",
      "Processing Batch:  23104 23168\n",
      "Processing Batch:  23168 23232\n",
      "Processing Batch:  23232 23296\n",
      "Processing Batch:  23296 23360\n",
      "Processing Batch:  23360 23424\n",
      "Processing Batch:  23424 23488\n",
      "Processing Batch:  23488 23552\n",
      "Processing Batch:  23552 23616\n",
      "Processing Batch:  23616 23680\n",
      "Processing Batch:  23680 23744\n",
      "Processing Batch:  23744 23808\n",
      "Processing Batch:  23808 23872\n",
      "Processing Batch:  23872 23936\n",
      "Processing Batch:  23936 24000\n",
      "Processing Batch:  24000 24064\n",
      "Processing Batch:  24064 24128\n",
      "Processing Batch:  24128 24192\n",
      "Processing Batch:  24192 24256\n",
      "Processing Batch:  24256 24320\n",
      "Processing Batch:  24320 24384\n",
      "Processing Batch:  24384 24448\n",
      "Processing Batch:  24448 24512\n",
      "Processing Batch:  24512 24576\n",
      "Processing Batch:  24576 24640\n",
      "Processing Batch:  24640 24704\n",
      "Processing Batch:  24704 24768\n",
      "Processing Batch:  24768 24832\n",
      "Processing Batch:  24832 24896\n",
      "Processing Batch:  24896 24960\n",
      "Processing Batch:  24960 25024\n",
      "Processing Batch:  25024 25088\n",
      "Processing Batch:  25088 25152\n",
      "Processing Batch:  25152 25216\n",
      "Processing Batch:  25216 25280\n",
      "Processing Batch:  25280 25344\n",
      "Processing Batch:  25344 25408\n",
      "Processing Batch:  25408 25472\n",
      "Processing Batch:  25472 25536\n",
      "Processing Batch:  25536 25600\n",
      "Processing Batch:  25600 25664\n",
      "Processing Batch:  25664 25728\n",
      "Processing Batch:  25728 25792\n",
      "Processing Batch:  25792 25856\n",
      "Processing Batch:  25856 25920\n",
      "Processing Batch:  25920 25984\n",
      "Processing Batch:  25984 26048\n",
      "Processing Batch:  26048 26112\n",
      "Processing Batch:  26112 26176\n",
      "Processing Batch:  26176 26240\n",
      "Processing Batch:  26240 26304\n",
      "Processing Batch:  26304 26368\n",
      "Processing Batch:  26368 26432\n",
      "Processing Batch:  26432 26496\n",
      "Processing Batch:  26496 26560\n",
      "Processing Batch:  26560 26624\n",
      "Processing Batch:  26624 26688\n",
      "Processing Batch:  26688 26752\n",
      "Processing Batch:  26752 26816\n",
      "Processing Batch:  26816 26880\n",
      "Processing Batch:  26880 26944\n",
      "Processing Batch:  26944 27008\n",
      "Processing Batch:  27008 27072\n",
      "Processing Batch:  27072 27136\n",
      "Processing Batch:  27136 27200\n",
      "Processing Batch:  27200 27264\n",
      "Processing Batch:  27264 27328\n",
      "Processing Batch:  27328 27392\n",
      "Processing Batch:  27392 27456\n",
      "Processing Batch:  27456 27520\n",
      "Processing Batch:  27520 27584\n",
      "Processing Batch:  27584 27648\n",
      "Processing Batch:  27648 27712\n",
      "Processing Batch:  27712 27776\n",
      "Processing Batch:  27776 27840\n",
      "Processing Batch:  27840 27904\n",
      "Processing Batch:  27904 27968\n",
      "Processing Batch:  27968 28032\n",
      "Processing Batch:  28032 28096\n",
      "Processing Batch:  28096 28160\n",
      "Processing Batch:  28160 28224\n",
      "Processing Batch:  28224 28288\n",
      "Processing Batch:  28288 28352\n",
      "Processing Batch:  28352 28416\n",
      "Processing Batch:  28416 28480\n",
      "Processing Batch:  28480 28544\n",
      "Processing Batch:  28544 28608\n",
      "Processing Batch:  28608 28672\n",
      "Processing Batch:  28672 28736\n",
      "Processing Batch:  28736 28800\n",
      "Processing Batch:  28800 28864\n",
      "Processing Batch:  28864 28928\n",
      "Processing Batch:  28928 28992\n",
      "Processing Batch:  28992 29056\n",
      "Processing Batch:  29056 29120\n",
      "Processing Batch:  29120 29184\n",
      "Processing Batch:  29184 29248\n",
      "Processing Batch:  29248 29312\n",
      "Processing Batch:  29312 29376\n",
      "Processing Batch:  29376 29440\n",
      "Processing Batch:  29440 29504\n",
      "Processing Batch:  29504 29568\n",
      "Processing Batch:  29568 29632\n",
      "Processing Batch:  29632 29696\n",
      "Processing Batch:  29696 29760\n",
      "Processing Batch:  29760 29824\n",
      "Processing Batch:  29824 29888\n",
      "Processing Batch:  29888 29952\n",
      "Processing Batch:  29952 30016\n",
      "Processing Batch:  30016 30080\n",
      "Processing Batch:  30080 30144\n",
      "Processing Batch:  30144 30208\n",
      "Processing Batch:  30208 30272\n",
      "Processing Batch:  30272 30336\n",
      "Processing Batch:  30336 30400\n",
      "Processing Batch:  30400 30464\n",
      "Processing Batch:  30464 30528\n",
      "Processing Batch:  30528 30592\n",
      "Processing Batch:  30592 30656\n",
      "Processing Batch:  30656 30720\n",
      "Processing Batch:  30720 30784\n",
      "Processing Batch:  30784 30848\n",
      "Processing Batch:  30848 30912\n",
      "Processing Batch:  30912 30976\n",
      "Processing Batch:  30976 31040\n",
      "Processing Batch:  31040 31104\n",
      "Processing Batch:  31104 31168\n",
      "Processing Batch:  31168 31232\n",
      "Processing Batch:  31232 31296\n",
      "Processing Batch:  31296 31360\n",
      "Processing Batch:  31360 31424\n",
      "Processing Batch:  31424 31488\n",
      "Processing Batch:  31488 31552\n",
      "Processing Batch:  31552 31616\n",
      "Processing Batch:  31616 31680\n",
      "Processing Batch:  31680 31744\n",
      "Processing Batch:  31744 31808\n",
      "Processing Batch:  31808 31872\n",
      "Processing Batch:  31872 31936\n",
      "Processing Batch:  31936 32000\n",
      "Processing Batch:  32000 32064\n",
      "Processing Batch:  32064 32128\n",
      "Processing Batch:  32128 32192\n",
      "Processing Batch:  32192 32256\n",
      "Processing Batch:  32256 32320\n",
      "Processing Batch:  32320 32384\n",
      "Processing Batch:  32384 32448\n",
      "Processing Batch:  32448 32512\n",
      "Processing Batch:  32512 32576\n",
      "Processing Batch:  32576 32640\n",
      "Processing Batch:  32640 32704\n",
      "Processing Batch:  32704 32768\n",
      "Processing Batch:  32768 32832\n",
      "Processing Batch:  32832 32896\n",
      "Processing Batch:  32896 32960\n",
      "Processing Batch:  32960 33024\n",
      "Processing Batch:  33024 33088\n",
      "Processing Batch:  33088 33152\n",
      "Processing Batch:  33152 33216\n",
      "Processing Batch:  33216 33280\n",
      "Processing Batch:  33280 33344\n",
      "Processing Batch:  33344 33408\n",
      "Processing Batch:  33408 33472\n",
      "Processing Batch:  33472 33536\n",
      "Processing Batch:  33536 33600\n",
      "Processing Batch:  33600 33664\n",
      "Processing Batch:  33664 33728\n",
      "Processing Batch:  33728 33792\n",
      "Processing Batch:  33792 33856\n",
      "Processing Batch:  33856 33920\n",
      "Processing Batch:  33920 33984\n",
      "Processing Batch:  33984 34048\n",
      "Processing Batch:  34048 34112\n",
      "Processing Batch:  34112 34176\n",
      "Processing Batch:  34176 34240\n",
      "Processing Batch:  34240 34304\n",
      "Processing Batch:  34304 34368\n",
      "Processing Batch:  34368 34432\n",
      "Processing Batch:  34432 34496\n",
      "Processing Batch:  34496 34560\n",
      "Processing Batch:  34560 34624\n",
      "Processing Batch:  34624 34688\n",
      "Processing Batch:  34688 34752\n",
      "Processing Batch:  34752 34816\n",
      "Processing Batch:  34816 34880\n",
      "Processing Batch:  34880 34944\n",
      "Processing Batch:  34944 35008\n",
      "Processing Batch:  35008 35072\n",
      "Processing Batch:  35072 35136\n",
      "Processing Batch:  35136 35200\n",
      "Processing Batch:  35200 35264\n",
      "Processing Batch:  35264 35328\n",
      "Processing Batch:  35328 35392\n",
      "Processing Batch:  35392 35456\n",
      "Processing Batch:  35456 35520\n",
      "Processing Batch:  35520 35584\n",
      "Processing Batch:  35584 35648\n",
      "Processing Batch:  35648 35712\n",
      "Processing Batch:  35712 35776\n",
      "Processing Batch:  35776 35840\n",
      "Processing Batch:  35840 35904\n",
      "Processing Batch:  35904 35968\n",
      "Processing Batch:  35968 36032\n",
      "Processing Batch:  36032 36096\n",
      "Processing Batch:  36096 36160\n",
      "Processing Batch:  36160 36224\n",
      "Processing Batch:  36224 36288\n",
      "Processing Batch:  36288 36352\n",
      "Processing Batch:  36352 36416\n",
      "Processing Batch:  36416 36480\n",
      "Processing Batch:  36480 36544\n",
      "Processing Batch:  36544 36608\n",
      "Processing Batch:  36608 36672\n",
      "Processing Batch:  36672 36736\n",
      "Processing Batch:  36736 36800\n",
      "Processing Batch:  36800 36864\n",
      "Processing Batch:  36864 36928\n",
      "Processing Batch:  36928 36992\n",
      "Processing Batch:  36992 37056\n",
      "Processing Batch:  37056 37120\n",
      "Processing Batch:  37120 37184\n",
      "Processing Batch:  37184 37248\n",
      "Processing Batch:  37248 37312\n",
      "Processing Batch:  37312 37376\n",
      "Processing Batch:  37376 37440\n",
      "Processing Batch:  37440 37504\n",
      "Processing Batch:  37504 37568\n",
      "Processing Batch:  37568 37632\n",
      "Processing Batch:  37632 37696\n",
      "Processing Batch:  37696 37760\n",
      "Processing Batch:  37760 37824\n",
      "Processing Batch:  37824 37888\n",
      "Processing Batch:  37888 37952\n",
      "Processing Batch:  37952 38016\n",
      "Processing Batch:  38016 38080\n",
      "Processing Batch:  38080 38144\n",
      "Processing Batch:  38144 38208\n",
      "Processing Batch:  38208 38272\n",
      "Processing Batch:  38272 38336\n",
      "Processing Batch:  38336 38400\n",
      "Processing Batch:  38400 38464\n",
      "Processing Batch:  38464 38528\n",
      "Processing Batch:  38528 38592\n",
      "Processing Batch:  38592 38656\n",
      "Processing Batch:  38656 38720\n",
      "Processing Batch:  38720 38784\n",
      "Processing Batch:  38784 38848\n",
      "Processing Batch:  38848 38912\n",
      "Processing Batch:  38912 38976\n",
      "Processing Batch:  38976 39040\n",
      "Processing Batch:  39040 39104\n",
      "Processing Batch:  39104 39168\n",
      "Processing Batch:  39168 39232\n",
      "Processing Batch:  39232 39296\n",
      "Processing Batch:  39296 39360\n",
      "Processing Batch:  39360 39424\n",
      "Processing Batch:  39424 39488\n",
      "Processing Batch:  39488 39552\n",
      "Processing Batch:  39552 39616\n",
      "Processing Batch:  39616 39680\n",
      "Processing Batch:  39680 39744\n",
      "Processing Batch:  39744 39808\n",
      "Processing Batch:  39808 39872\n",
      "Processing Batch:  39872 39936\n",
      "Processing Batch:  39936 40000\n",
      "Processing Batch:  40000 40064\n",
      "Processing Batch:  40064 40128\n",
      "Processing Batch:  40128 40192\n",
      "Processing Batch:  40192 40256\n",
      "Processing Batch:  40256 40320\n",
      "Processing Batch:  40320 40384\n",
      "Processing Batch:  40384 40448\n",
      "Processing Batch:  40448 40512\n",
      "Processing Batch:  40512 40576\n",
      "Processing Batch:  40576 40640\n",
      "Processing Batch:  40640 40704\n",
      "Processing Batch:  40704 40768\n",
      "Processing Batch:  40768 40832\n",
      "Processing Batch:  40832 40896\n",
      "Processing Batch:  40896 40960\n",
      "Processing Batch:  40960 41024\n",
      "Processing Batch:  41024 41088\n",
      "Processing Batch:  41088 41152\n",
      "Processing Batch:  41152 41216\n",
      "Processing Batch:  41216 41280\n",
      "Processing Batch:  41280 41344\n",
      "Processing Batch:  41344 41408\n",
      "Processing Batch:  41408 41472\n",
      "Processing Batch:  41472 41536\n",
      "Processing Batch:  41536 41600\n",
      "Processing Batch:  41600 41664\n",
      "Processing Batch:  41664 41728\n",
      "Processing Batch:  41728 41792\n",
      "Processing Batch:  41792 41856\n",
      "Processing Batch:  41856 41920\n",
      "Processing Batch:  41920 41984\n",
      "Processing Batch:  41984 42048\n",
      "Processing Batch:  42048 42112\n",
      "Processing Batch:  42112 42176\n",
      "Processing Batch:  42176 42240\n",
      "Processing Batch:  42240 42304\n",
      "Processing Batch:  42304 42368\n",
      "Processing Batch:  42368 42432\n",
      "Processing Batch:  42432 42496\n",
      "Processing Batch:  42496 42560\n",
      "Processing Batch:  42560 42624\n",
      "Processing Batch:  42624 42688\n",
      "Processing Batch:  42688 42752\n",
      "Processing Batch:  42752 42816\n",
      "Processing Batch:  42816 42880\n",
      "Processing Batch:  42880 42944\n",
      "Processing Batch:  42944 43008\n",
      "Processing Batch:  43008 43072\n",
      "Processing Batch:  43072 43136\n",
      "Processing Batch:  43136 43200\n",
      "Processing Batch:  43200 43264\n",
      "Processing Batch:  43264 43328\n",
      "Processing Batch:  43328 43392\n",
      "Processing Batch:  43392 43456\n",
      "Processing Batch:  43456 43520\n",
      "Processing Batch:  43520 43584\n",
      "Processing Batch:  43584 43648\n",
      "Processing Batch:  43648 43712\n",
      "Processing Batch:  43712 43776\n",
      "Processing Batch:  43776 43840\n",
      "Processing Batch:  43840 43904\n",
      "Processing Batch:  43904 43968\n",
      "Processing Batch:  43968 44032\n",
      "Processing Batch:  44032 44096\n",
      "Processing Batch:  44096 44160\n",
      "Processing Batch:  44160 44224\n",
      "Processing Batch:  44224 44288\n",
      "Processing Batch:  44288 44352\n",
      "Processing Batch:  44352 44416\n",
      "Processing Batch:  44416 44480\n",
      "Processing Batch:  44480 44544\n",
      "Processing Batch:  44544 44608\n",
      "Processing Batch:  44608 44672\n",
      "Processing Batch:  44672 44736\n",
      "Processing Batch:  44736 44800\n",
      "Processing Batch:  44800 44864\n",
      "Processing Batch:  44864 44928\n",
      "Processing Batch:  44928 44992\n",
      "Processing Batch:  44992 45056\n",
      "Processing Batch:  45056 45120\n",
      "Processing Batch:  45120 45184\n",
      "Processing Batch:  45184 45248\n",
      "Processing Batch:  45248 45312\n",
      "Processing Batch:  45312 45376\n",
      "Processing Batch:  45376 45440\n",
      "Processing Batch:  45440 45504\n",
      "Processing Batch:  45504 45568\n",
      "Processing Batch:  45568 45632\n",
      "Processing Batch:  45632 45696\n",
      "Processing Batch:  45696 45760\n",
      "Processing Batch:  45760 45824\n",
      "Processing Batch:  45824 45888\n",
      "Processing Batch:  45888 45952\n",
      "Processing Batch:  45952 46016\n",
      "Processing Batch:  46016 46080\n",
      "Processing Batch:  46080 46144\n",
      "Processing Batch:  46144 46208\n",
      "Processing Batch:  46208 46272\n",
      "Processing Batch:  46272 46336\n",
      "Processing Batch:  46336 46400\n",
      "Processing Batch:  46400 46464\n",
      "Processing Batch:  46464 46528\n",
      "Processing Batch:  46528 46592\n",
      "Processing Batch:  46592 46656\n",
      "Processing Batch:  46656 46720\n",
      "Processing Batch:  46720 46784\n",
      "Processing Batch:  46784 46848\n",
      "Processing Batch:  46848 46912\n",
      "Processing Batch:  46912 46976\n",
      "Processing Batch:  46976 47040\n",
      "Processing Batch:  47040 47104\n",
      "Processing Batch:  47104 47168\n",
      "Processing Batch:  47168 47232\n",
      "Processing Batch:  47232 47296\n",
      "Processing Batch:  47296 47360\n",
      "Processing Batch:  47360 47424\n",
      "Processing Batch:  47424 47488\n",
      "Processing Batch:  47488 47552\n",
      "Processing Batch:  47552 47616\n",
      "Processing Batch:  47616 47680\n",
      "Processing Batch:  47680 47744\n",
      "Processing Batch:  47744 47808\n",
      "Processing Batch:  47808 47872\n",
      "Processing Batch:  47872 47936\n",
      "Processing Batch:  47936 48000\n",
      "Processing Batch:  48000 48064\n",
      "Processing Batch:  48064 48128\n",
      "Processing Batch:  48128 48192\n",
      "Processing Batch:  48192 48256\n",
      "Processing Batch:  48256 48320\n",
      "Processing Batch:  48320 48384\n",
      "Processing Batch:  48384 48448\n",
      "Processing Batch:  48448 48512\n",
      "Processing Batch:  48512 48576\n",
      "Processing Batch:  48576 48640\n",
      "Processing Batch:  48640 48704\n",
      "Processing Batch:  48704 48768\n",
      "Processing Batch:  48768 48832\n",
      "Processing Batch:  48832 48896\n",
      "Processing Batch:  48896 48960\n",
      "Processing Batch:  48960 49024\n",
      "Processing Batch:  49024 49088\n",
      "Processing Batch:  49088 49152\n",
      "Processing Batch:  49152 49216\n",
      "Processing Batch:  49216 49280\n",
      "Processing Batch:  49280 49344\n",
      "Processing Batch:  49344 49408\n",
      "Processing Batch:  49408 49472\n",
      "Processing Batch:  49472 49536\n",
      "Processing Batch:  49536 49600\n",
      "Processing Batch:  49600 49664\n",
      "Processing Batch:  49664 49728\n",
      "Processing Batch:  49728 49792\n",
      "Processing Batch:  49792 49856\n",
      "Processing Batch:  49856 49920\n",
      "Processing Batch:  49920 49984\n",
      "Processing Batch:  49984 50048\n",
      "Processing Batch:  50048 50112\n",
      "Processing Batch:  50112 50176\n",
      "Processing Batch:  50176 50240\n",
      "Processing Batch:  50240 50304\n",
      "Processing Batch:  50304 50368\n",
      "Processing Batch:  50368 50432\n",
      "Processing Batch:  50432 50496\n",
      "Processing Batch:  50496 50560\n",
      "Processing Batch:  50560 50624\n",
      "Processing Batch:  50624 50688\n",
      "Processing Batch:  50688 50752\n",
      "Processing Batch:  50752 50816\n",
      "Processing Batch:  50816 50880\n",
      "Processing Batch:  50880 50944\n",
      "Processing Batch:  50944 51008\n",
      "Processing Batch:  51008 51072\n",
      "Processing Batch:  51072 51136\n",
      "Processing Batch:  51136 51200\n",
      "Processing Batch:  51200 51264\n",
      "Processing Batch:  51264 51328\n",
      "Processing Batch:  51328 51392\n",
      "Processing Batch:  51392 51456\n",
      "Processing Batch:  51456 51520\n",
      "Processing Batch:  51520 51584\n",
      "Processing Batch:  51584 51648\n",
      "Processing Batch:  51648 51712\n",
      "Processing Batch:  51712 51776\n",
      "Processing Batch:  51776 51840\n",
      "Processing Batch:  51840 51904\n",
      "Processing Batch:  51904 51968\n",
      "Processing Batch:  51968 52032\n",
      "Processing Batch:  52032 52096\n",
      "Processing Batch:  52096 52160\n",
      "Processing Batch:  52160 52224\n",
      "Processing Batch:  52224 52288\n",
      "Processing Batch:  52288 52352\n",
      "Processing Batch:  52352 52416\n",
      "Processing Batch:  52416 52480\n",
      "Processing Batch:  52480 52544\n",
      "Processing Batch:  52544 52608\n",
      "Processing Batch:  52608 52672\n",
      "Processing Batch:  52672 52736\n",
      "Processing Batch:  52736 52800\n",
      "Processing Batch:  52800 52864\n",
      "Processing Batch:  52864 52928\n",
      "Processing Batch:  52928 52992\n",
      "Processing Batch:  52992 53056\n",
      "Processing Batch:  53056 53120\n",
      "Processing Batch:  53120 53184\n",
      "Processing Batch:  53184 53248\n",
      "Processing Batch:  53248 53312\n",
      "Processing Batch:  53312 53376\n",
      "Processing Batch:  53376 53440\n",
      "Processing Batch:  53440 53504\n",
      "Processing Batch:  53504 53568\n",
      "Processing Batch:  53568 53632\n",
      "Processing Batch:  53632 53696\n",
      "Processing Batch:  53696 53760\n",
      "Processing Batch:  53760 53824\n",
      "Processing Batch:  53824 53888\n",
      "Processing Batch:  53888 53952\n",
      "Processing Batch:  53952 54016\n",
      "Processing Batch:  54016 54080\n",
      "Processing Batch:  54080 54144\n",
      "Processing Batch:  54144 54208\n",
      "Processing Batch:  54208 54272\n",
      "Processing Batch:  54272 54336\n",
      "Processing Batch:  54336 54400\n",
      "Processing Batch:  54400 54464\n",
      "Processing Batch:  54464 54528\n",
      "Processing Batch:  54528 54592\n",
      "Processing Batch:  54592 54656\n",
      "Processing Batch:  54656 54720\n",
      "Processing Batch:  54720 54784\n",
      "Processing Batch:  54784 54848\n",
      "Processing Batch:  54848 54912\n",
      "Processing Batch:  54912 54976\n",
      "Processing Batch:  54976 55040\n",
      "Processing Batch:  55040 55104\n",
      "Processing Batch:  55104 55168\n",
      "Processing Batch:  55168 55232\n",
      "Processing Batch:  55232 55296\n",
      "Processing Batch:  55296 55360\n",
      "Processing Batch:  55360 55424\n",
      "Processing Batch:  55424 55488\n",
      "Processing Batch:  55488 55552\n",
      "Processing Batch:  55552 55616\n",
      "Processing Batch:  55616 55680\n",
      "Processing Batch:  55680 55744\n",
      "Processing Batch:  55744 55808\n",
      "Processing Batch:  55808 55872\n",
      "Processing Batch:  55872 55936\n",
      "Processing Batch:  55936 56000\n",
      "Processing Batch:  56000 56064\n",
      "Processing Batch:  56064 56128\n",
      "Processing Batch:  56128 56192\n",
      "Processing Batch:  56192 56256\n",
      "Processing Batch:  56256 56320\n",
      "Processing Batch:  56320 56384\n",
      "Processing Batch:  56384 56448\n",
      "Processing Batch:  56448 56512\n",
      "Processing Batch:  56512 56576\n",
      "Processing Batch:  56576 56640\n",
      "Processing Batch:  56640 56704\n",
      "Processing Batch:  56704 56768\n",
      "Processing Batch:  56768 56832\n",
      "Processing Batch:  56832 56896\n",
      "Processing Batch:  56896 56960\n",
      "Processing Batch:  56960 57024\n",
      "Processing Batch:  57024 57088\n",
      "Processing Batch:  57088 57152\n",
      "Processing Batch:  57152 57216\n",
      "Processing Batch:  57216 57280\n",
      "Processing Batch:  57280 57344\n",
      "Processing Batch:  57344 57408\n",
      "Processing Batch:  57408 57472\n",
      "Processing Batch:  57472 57536\n",
      "Processing Batch:  57536 57600\n",
      "Processing Batch:  57600 57664\n",
      "Processing Batch:  57664 57728\n",
      "Processing Batch:  57728 57792\n",
      "Processing Batch:  57792 57856\n",
      "Processing Batch:  57856 57920\n",
      "Processing Batch:  57920 57984\n",
      "Processing Batch:  57984 58048\n",
      "Processing Batch:  58048 58112\n",
      "Processing Batch:  58112 58176\n",
      "Processing Batch:  58176 58240\n",
      "Processing Batch:  58240 58304\n",
      "Processing Batch:  58304 58368\n",
      "Processing Batch:  58368 58432\n",
      "Processing Batch:  58432 58496\n",
      "Processing Batch:  58496 58560\n",
      "Processing Batch:  58560 58624\n",
      "Processing Batch:  58624 58688\n",
      "Processing Batch:  58688 58752\n",
      "Processing Batch:  58752 58816\n",
      "Processing Batch:  58816 58880\n",
      "Processing Batch:  58880 58944\n",
      "Processing Batch:  58944 59008\n",
      "Processing Batch:  59008 59072\n",
      "Processing Batch:  59072 59136\n",
      "Processing Batch:  59136 59200\n",
      "Processing Batch:  59200 59264\n",
      "Processing Batch:  59264 59328\n",
      "Processing Batch:  59328 59392\n",
      "Processing Batch:  59392 59456\n",
      "Processing Batch:  59456 59520\n",
      "Processing Batch:  59520 59584\n",
      "Processing Batch:  59584 59648\n",
      "Processing Batch:  59648 59712\n",
      "Processing Batch:  59712 59776\n",
      "Processing Batch:  59776 59840\n",
      "Processing Batch:  59840 59904\n",
      "Processing Batch:  59904 59968\n",
      "Processing Batch:  59968 60032\n",
      "Processing Batch:  60032 60096\n",
      "Processing Batch:  60096 60160\n",
      "Processing Batch:  60160 60224\n",
      "Processing Batch:  60224 60288\n",
      "Processing Batch:  60288 60352\n",
      "Processing Batch:  60352 60416\n",
      "Processing Batch:  60416 60480\n",
      "Processing Batch:  60480 60544\n",
      "Processing Batch:  60544 60608\n",
      "Processing Batch:  60608 60672\n",
      "Processing Batch:  60672 60736\n",
      "Processing Batch:  60736 60800\n",
      "Processing Batch:  60800 60864\n",
      "Processing Batch:  60864 60928\n",
      "Processing Batch:  60928 60992\n",
      "Processing Batch:  60992 61056\n",
      "Processing Batch:  61056 61120\n",
      "Processing Batch:  61120 61184\n",
      "Processing Batch:  61184 61248\n",
      "Processing Batch:  61248 61312\n",
      "Processing Batch:  61312 61376\n",
      "Processing Batch:  61376 61440\n",
      "Processing Batch:  61440 61504\n",
      "Processing Batch:  61504 61568\n",
      "Processing Batch:  61568 61632\n",
      "Processing Batch:  61632 61696\n",
      "Processing Batch:  61696 61760\n",
      "Processing Batch:  61760 61824\n",
      "Processing Batch:  61824 61888\n",
      "Processing Batch:  61888 61952\n",
      "Processing Batch:  61952 62016\n",
      "Processing Batch:  62016 62080\n",
      "Processing Batch:  62080 62144\n",
      "Processing Batch:  62144 62208\n",
      "Processing Batch:  62208 62272\n",
      "Processing Batch:  62272 62336\n",
      "Processing Batch:  62336 62400\n",
      "Processing Batch:  62400 62464\n",
      "Processing Batch:  62464 62528\n",
      "Processing Batch:  62528 62592\n",
      "Processing Batch:  62592 62656\n",
      "Processing Batch:  62656 62720\n",
      "Processing Batch:  62720 62784\n",
      "Processing Batch:  62784 62848\n",
      "Processing Batch:  62848 62912\n",
      "Processing Batch:  62912 62976\n",
      "Processing Batch:  62976 63040\n",
      "Processing Batch:  63040 63104\n",
      "Processing Batch:  63104 63168\n",
      "Processing Batch:  63168 63232\n",
      "Processing Batch:  63232 63296\n",
      "Processing Batch:  63296 63360\n",
      "Processing Batch:  63360 63424\n",
      "Processing Batch:  63424 63488\n",
      "Processing Batch:  63488 63552\n",
      "Processing Batch:  63552 63616\n",
      "Processing Batch:  63616 63680\n",
      "Processing Batch:  63680 63744\n",
      "Processing Batch:  63744 63808\n",
      "Processing Batch:  63808 63872\n",
      "Processing Batch:  63872 63936\n",
      "Processing Batch:  63936 64000\n",
      "Processing Batch:  64000 64064\n",
      "Processing Batch:  64064 64128\n",
      "Processing Batch:  64128 64192\n",
      "Processing Batch:  64192 64256\n",
      "Processing Batch:  64256 64320\n",
      "Processing Batch:  64320 64384\n",
      "Processing Batch:  64384 64448\n",
      "Processing Batch:  64448 64512\n",
      "Processing Batch:  64512 64576\n",
      "Processing Batch:  64576 64640\n",
      "Processing Batch:  64640 64704\n",
      "Processing Batch:  64704 64768\n",
      "Processing Batch:  64768 64832\n",
      "Processing Batch:  64832 64896\n",
      "Processing Batch:  64896 64960\n",
      "Processing Batch:  64960 65024\n",
      "Processing Batch:  65024 65088\n",
      "Processing Batch:  65088 65152\n",
      "Processing Batch:  65152 65216\n",
      "Processing Batch:  65216 65280\n",
      "Processing Batch:  65280 65344\n",
      "Processing Batch:  65344 65408\n",
      "Processing Batch:  65408 65472\n",
      "Processing Batch:  65472 65536\n",
      "Processing Batch:  65536 65600\n",
      "Processing Batch:  65600 65664\n",
      "Processing Batch:  65664 65728\n",
      "Processing Batch:  65728 65792\n",
      "Processing Batch:  65792 65856\n",
      "Processing Batch:  65856 65920\n",
      "Processing Batch:  65920 65984\n",
      "Processing Batch:  65984 66048\n",
      "Processing Batch:  66048 66112\n",
      "Processing Batch:  66112 66176\n",
      "Processing Batch:  66176 66240\n",
      "Processing Batch:  66240 66304\n",
      "Processing Batch:  66304 66368\n",
      "Processing Batch:  66368 66432\n",
      "Processing Batch:  66432 66496\n",
      "Processing Batch:  66496 66560\n",
      "Processing Batch:  66560 66624\n",
      "Processing Batch:  66624 66688\n",
      "Processing Batch:  66688 66752\n",
      "Processing Batch:  66752 66816\n",
      "Processing Batch:  66816 66880\n",
      "Processing Batch:  66880 66944\n",
      "Processing Batch:  66944 67008\n",
      "Processing Batch:  67008 67072\n",
      "Processing Batch:  67072 67136\n",
      "Processing Batch:  67136 67200\n",
      "Processing Batch:  67200 67264\n",
      "Processing Batch:  67264 67328\n",
      "Processing Batch:  67328 67392\n",
      "Processing Batch:  67392 67456\n",
      "Processing Batch:  67456 67520\n",
      "Processing Batch:  67520 67584\n",
      "Processing Batch:  67584 67648\n",
      "Processing Batch:  67648 67712\n",
      "Processing Batch:  67712 67776\n",
      "Processing Batch:  67776 67840\n",
      "Processing Batch:  67840 67904\n",
      "Processing Batch:  67904 67968\n",
      "Processing Batch:  67968 68032\n",
      "Processing Batch:  68032 68096\n",
      "Processing Batch:  68096 68160\n",
      "Processing Batch:  68160 68224\n",
      "Processing Batch:  68224 68288\n",
      "Processing Batch:  68288 68352\n",
      "Processing Batch:  68352 68416\n",
      "Processing Batch:  68416 68480\n",
      "Processing Batch:  68480 68544\n",
      "Processing Batch:  68544 68608\n",
      "Processing Batch:  68608 68672\n",
      "Processing Batch:  68672 68736\n",
      "Processing Batch:  68736 68800\n",
      "Processing Batch:  68800 68864\n",
      "Processing Batch:  68864 68928\n",
      "Processing Batch:  68928 68992\n",
      "Processing Batch:  68992 69056\n",
      "Processing Batch:  69056 69120\n",
      "Processing Batch:  69120 69184\n",
      "Processing Batch:  69184 69248\n",
      "Processing Batch:  69248 69312\n",
      "Processing Batch:  69312 69376\n",
      "Processing Batch:  69376 69440\n",
      "Processing Batch:  69440 69504\n",
      "Processing Batch:  69504 69568\n",
      "Processing Batch:  69568 69632\n",
      "Processing Batch:  69632 69696\n",
      "Processing Batch:  69696 69760\n",
      "Processing Batch:  69760 69824\n",
      "Processing Batch:  69824 69888\n",
      "Processing Batch:  69888 69952\n",
      "Processing Batch:  69952 70016\n",
      "Processing Batch:  70016 70080\n",
      "Processing Batch:  70080 70144\n",
      "Processing Batch:  70144 70208\n",
      "Processing Batch:  70208 70272\n",
      "Processing Batch:  70272 70336\n",
      "Processing Batch:  70336 70400\n",
      "Processing Batch:  70400 70464\n",
      "Processing Batch:  70464 70528\n",
      "Processing Batch:  70528 70592\n",
      "Processing Batch:  70592 70656\n",
      "Processing Batch:  70656 70720\n",
      "Processing Batch:  70720 70784\n",
      "Processing Batch:  70784 70848\n",
      "Processing Batch:  70848 70912\n",
      "Processing Batch:  70912 70976\n",
      "Processing Batch:  70976 71040\n",
      "Processing Batch:  71040 71104\n",
      "Processing Batch:  71104 71168\n",
      "Processing Batch:  71168 71232\n",
      "Processing Batch:  71232 71296\n",
      "Processing Batch:  71296 71360\n",
      "Processing Batch:  71360 71424\n",
      "Processing Batch:  71424 71488\n",
      "Processing Batch:  71488 71552\n",
      "Processing Batch:  71552 71616\n",
      "Processing Batch:  71616 71680\n",
      "Processing Batch:  71680 71744\n",
      "Processing Batch:  71744 71808\n",
      "Processing Batch:  71808 71872\n",
      "Processing Batch:  71872 71936\n",
      "Processing Batch:  71936 72000\n",
      "Processing Batch:  72000 72064\n",
      "Processing Batch:  72064 72128\n",
      "Processing Batch:  72128 72192\n",
      "Processing Batch:  72192 72256\n",
      "Processing Batch:  72256 72320\n",
      "Processing Batch:  72320 72384\n",
      "Processing Batch:  72384 72448\n",
      "Processing Batch:  72448 72512\n",
      "Processing Batch:  72512 72576\n",
      "Processing Batch:  72576 72640\n",
      "Processing Batch:  72640 72704\n",
      "Processing Batch:  72704 72768\n",
      "Processing Batch:  72768 72832\n",
      "Processing Batch:  72832 72896\n",
      "Processing Batch:  72896 72960\n",
      "Processing Batch:  72960 73024\n"
     ]
    }
   ],
   "source": [
    "path_paired_test = f\"{pipeline_data_splitted}/{precision}/paired/test.tsv\"\n",
    "path_paired_validation = f\"{pipeline_data_splitted}/{precision}/paired/validation.tsv\"\n",
    "path_paired_train = f\"{pipeline_data_splitted}/{precision}/paired/train.tsv\"\n",
    "path_beta_test = f\"{pipeline_data_splitted}/{precision}/beta/test.tsv\"\n",
    "path_beta_validation = f\"{pipeline_data_splitted}/{precision}/beta/validation.tsv\"\n",
    "path_beta_train = f\"{pipeline_data_splitted}/{precision}/beta/train.tsv\"\n",
    "\n",
    "\n",
    "path_paired = f\"{pipeline_data}/embeddings/temp/{precision}/paired_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_paired)])\n",
    "df_paired_test = pd.read_csv(path_paired_test, sep=\"\\t\", index_col=False)\n",
    "df_paired_validation = pd.read_csv(path_paired_validation, sep=\"\\t\", index_col=False)\n",
    "df_paired_train = pd.read_csv(path_paired_train, sep=\"\\t\", index_col=False)\n",
    "df_paired = pd.concat([df_paired_test, df_paired_validation, df_paired_train])\n",
    "df_paired.to_csv(path_paired, sep=\"\\t\", index=False)\n",
    "\n",
    "# paired\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz TRA_CDR3\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz TRB_CDR3\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz Epitope\n",
    "\n",
    "path_beta = f\"{pipeline_data}/embeddings/temp/{precision}/beta_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_beta)])\n",
    "df_beta_test = pd.read_csv(path_beta_test, sep=\"\\t\", index_col=False)\n",
    "df_beta_validation = pd.read_csv(path_beta_validation, sep=\"\\t\", index_col=False)\n",
    "df_beta_train = pd.read_csv(path_beta_train, sep=\"\\t\", index_col=False)\n",
    "df_beta = pd.concat([df_beta_test, df_beta_validation, df_beta_train])\n",
    "df_beta.to_csv(path_beta, sep=\"\\t\", index=False)\n",
    "\n",
    "# beta\n",
    "%run ../generateEmbeddingsProtBERT.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/TRB_beta_embeddings.npz TRB_CDR3\n",
    "%run ../generateEmbeddingsProtBERT.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/Epitope_beta_embeddings.npz Epitope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 TRA Embedding Shape: (596417, 1024)\n",
      "📌 TRB Embedding Shape: (694427, 1024)\n",
      "📌 Epitope Embedding Shape: (12870, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Funktion, um Embeddings korrekt zu laden\n",
    "def load_embeddings(file_path):\n",
    "    npz_data = np.load(file_path)\n",
    "    all_keys = list(npz_data.keys())\n",
    "\n",
    "    # Falls Embeddings als einzelne Sequenzen gespeichert sind\n",
    "    if len(all_keys) > 1:\n",
    "        all_values = [npz_data[k] for k in all_keys]\n",
    "        return np.vstack(all_values)  # Alles zusammenfügen\n",
    "    else:\n",
    "        return npz_data[all_keys[0]]\n",
    "\n",
    "# Embeddings für TRA, TRB und Epitope laden\n",
    "tra_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz\")\n",
    "trb_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz\")\n",
    "epitope_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz\")\n",
    "\n",
    "# Ausgabe der finalen Shapes\n",
    "print(f\"📌 TRA Embedding Shape: {tra_embeddings.shape}\")\n",
    "print(f\"📌 TRB Embedding Shape: {trb_embeddings.shape}\")\n",
    "print(f\"📌 Epitope Embedding Shape: {epitope_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93065/3207364397.py:24: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 67422 (Binding=1: 33711, Binding=0: 33711, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 43356 (Binding=1: 7226, Binding=0: 36130, TPP1: 27972, TPP2: 15095, TPP3: 289)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 43344 (Binding=1: 7224, Binding=0: 36120, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 154122\n",
      "\n",
      "--- Beta Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 251750 (Binding=1: 125875, Binding=0: 125875, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 161844 (Binding=1: 26974, Binding=0: 134870, TPP1: 140896, TPP2: 20645, TPP3: 299)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 161838 (Binding=1: 26973, Binding=0: 134865, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 575432\n",
      "\n",
      "       Dataset   Train  Train_Binding_1  Train_Binding_0  Train_TPP1  \\\n",
      "0  Paired Gene   67422            33711            33711           0   \n",
      "1    Beta Gene  251750           125875           125875           0   \n",
      "\n",
      "   Train_TPP2  Train_TPP3  Train_TPP4    Test  Test_Binding_1  ...  Test_TPP3  \\\n",
      "0           0           0           0   43356            7226  ...        289   \n",
      "1           0           0           0  161844           26974  ...        299   \n",
      "\n",
      "   Test_TPP4  Validation  Validation_Binding_1  Validation_Binding_0  \\\n",
      "0          0       43344                  7224                 36120   \n",
      "1          4      161838                 26973                134865   \n",
      "\n",
      "   Validation_TPP1  Validation_TPP2  Validation_TPP3  Validation_TPP4   Total  \n",
      "0                0                0                0                0  154122  \n",
      "1                0                0                0                0  575432  \n",
      "\n",
      "[2 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = pipeline_data_splitted\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen für jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "    \n",
    "    # Zähle die Anzahl der Bindings 1 und 0 in jedem Datensatz\n",
    "    train_binding_counts = train_df['Binding'].value_counts()\n",
    "    test_binding_counts = test_df['Binding'].value_counts()\n",
    "    validation_binding_counts = validation_df['Binding'].value_counts()\n",
    "    \n",
    "    # Zähle die Anzahl der TPP1, TPP2, TPP3 Einträge in jedem Datensatz\n",
    "    train_task_counts = train_df['task'].value_counts()\n",
    "    test_task_counts = test_df['task'].value_counts()\n",
    "    validation_task_counts = validation_df['task'].value_counts()\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Train_Binding_1\": train_binding_counts.get(1, 0),\n",
    "        \"Train_Binding_0\": train_binding_counts.get(0, 0),\n",
    "        \"Train_TPP1\": train_task_counts.get(\"TPP1\", 0),\n",
    "        \"Train_TPP2\": train_task_counts.get(\"TPP2\", 0),\n",
    "        \"Train_TPP3\": train_task_counts.get(\"TPP3\", 0),\n",
    "        \"Train_TPP4\": train_task_counts.get(\"TPP4\", 0),\n",
    "        \"Test\": test_length,\n",
    "        \"Test_Binding_1\": test_binding_counts.get(1, 0),\n",
    "        \"Test_Binding_0\": test_binding_counts.get(0, 0),\n",
    "        \"Test_TPP1\": test_task_counts.get(\"TPP1\", 0),\n",
    "        \"Test_TPP2\": test_task_counts.get(\"TPP2\", 0),\n",
    "        \"Test_TPP3\": test_task_counts.get(\"TPP3\", 0),\n",
    "        \"Test_TPP4\": test_task_counts.get(\"TPP4\", 0),\n",
    "        \"Validation\": validation_length,\n",
    "        \"Validation_Binding_1\": validation_binding_counts.get(1, 0),\n",
    "        \"Validation_Binding_0\": validation_binding_counts.get(0, 0),\n",
    "        \"Validation_TPP1\": validation_task_counts.get(\"TPP1\", 0),\n",
    "        \"Validation_TPP2\": validation_task_counts.get(\"TPP2\", 0),\n",
    "        \"Validation_TPP3\": validation_task_counts.get(\"TPP3\", 0),\n",
    "        \"Validation_TPP4\": validation_task_counts.get(\"TPP4\", 0),\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]} (Binding=1: {lengths[\"Train_Binding_1\"]}, Binding=0: {lengths[\"Train_Binding_0\"]}, TPP1: {lengths[\"Train_TPP1\"]}, TPP2: {lengths[\"Train_TPP2\"]}, TPP3: {lengths[\"Train_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]} (Binding=1: {lengths[\"Test_Binding_1\"]}, Binding=0: {lengths[\"Test_Binding_0\"]}, TPP1: {lengths[\"Test_TPP1\"]}, TPP2: {lengths[\"Test_TPP2\"]}, TPP3: {lengths[\"Test_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]} (Binding=1: {lengths[\"Validation_Binding_1\"]}, Binding=0: {lengths[\"Validation_Binding_0\"]}, TPP1: {lengths[\"Validation_TPP1\"]}, TPP2: {lengths[\"Validation_TPP2\"]}, TPP3: {lengths[\"Validation_TPP3\"]})')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')\n",
    "\n",
    "# Optional: Ergebnisse in einer Übersichtstabelle darstellen\n",
    "summary_data = []\n",
    "for dataset, lengths in results.items():\n",
    "    summary_data.append({\n",
    "        \"Dataset\": dataset.replace(\"_\", \" \").title(),\n",
    "        \"Train\": lengths[\"Train\"],\n",
    "        \"Train_Binding_1\": lengths[\"Train_Binding_1\"],\n",
    "        \"Train_Binding_0\": lengths[\"Train_Binding_0\"],\n",
    "        \"Train_TPP1\": lengths[\"Train_TPP1\"],\n",
    "        \"Train_TPP2\": lengths[\"Train_TPP2\"],\n",
    "        \"Train_TPP3\": lengths[\"Train_TPP3\"],\n",
    "        \"Train_TPP4\": lengths[\"Train_TPP4\"],\n",
    "        \"Test\": lengths[\"Test\"],\n",
    "        \"Test_Binding_1\": lengths[\"Test_Binding_1\"],\n",
    "        \"Test_Binding_0\": lengths[\"Test_Binding_0\"],\n",
    "        \"Test_TPP1\": lengths[\"Test_TPP1\"],\n",
    "        \"Test_TPP2\": lengths[\"Test_TPP2\"],\n",
    "        \"Test_TPP3\": lengths[\"Test_TPP3\"],\n",
    "        \"Test_TPP4\": lengths[\"Test_TPP4\"],\n",
    "        \"Validation\": lengths[\"Validation\"],\n",
    "        \"Validation_Binding_1\": lengths[\"Validation_Binding_1\"],\n",
    "        \"Validation_Binding_0\": lengths[\"Validation_Binding_0\"],\n",
    "        \"Validation_TPP1\": lengths[\"Validation_TPP1\"],\n",
    "        \"Validation_TPP2\": lengths[\"Validation_TPP2\"],\n",
    "        \"Validation_TPP3\": lengths[\"Validation_TPP3\"],\n",
    "        \"Validation_TPP4\": lengths[\"Validation_TPP4\"],\n",
    "        \"Total\": lengths[\"Total\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/splitted_datasets/allele/paired/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, paths \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Daten laden\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m     validation_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/splitted_datasets/allele/paired/train.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = pipeline_data_splitted\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"paired_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    },\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen für jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "    \n",
    "    # Zähle die Anzahl der Bindings 1 und 0 in jedem Datensatz\n",
    "    train_binding_counts = train_df['Binding'].value_counts()\n",
    "    test_binding_counts = test_df['Binding'].value_counts()\n",
    "    validation_binding_counts = validation_df['Binding'].value_counts()\n",
    "    \n",
    "    # Zähle die Anzahl der TPP1, TPP2, TPP3 Einträge in jedem Datensatz\n",
    "    train_task_counts = train_df['task'].value_counts()\n",
    "    test_task_counts = test_df['task'].value_counts()\n",
    "    validation_task_counts = validation_df['task'].value_counts()\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Train_Binding_1\": train_binding_counts.get(1, 0),\n",
    "        \"Train_Binding_0\": train_binding_counts.get(0, 0),\n",
    "        \"Train_TPP1\": train_task_counts.get(\"TPP1\", 0),\n",
    "        \"Train_TPP2\": train_task_counts.get(\"TPP2\", 0),\n",
    "        \"Train_TPP3\": train_task_counts.get(\"TPP3\", 0),\n",
    "        \"Train_TPP4\": train_task_counts.get(\"TPP4\", 0),\n",
    "        \"Test\": test_length,\n",
    "        \"Test_Binding_1\": test_binding_counts.get(1, 0),\n",
    "        \"Test_Binding_0\": test_binding_counts.get(0, 0),\n",
    "        \"Test_TPP1\": test_task_counts.get(\"TPP1\", 0),\n",
    "        \"Test_TPP2\": test_task_counts.get(\"TPP2\", 0),\n",
    "        \"Test_TPP3\": test_task_counts.get(\"TPP3\", 0),\n",
    "        \"Test_TPP4\": test_task_counts.get(\"TPP4\", 0),\n",
    "        \"Validation\": validation_length,\n",
    "        \"Validation_Binding_1\": validation_binding_counts.get(1, 0),\n",
    "        \"Validation_Binding_0\": validation_binding_counts.get(0, 0),\n",
    "        \"Validation_TPP1\": validation_task_counts.get(\"TPP1\", 0),\n",
    "        \"Validation_TPP2\": validation_task_counts.get(\"TPP2\", 0),\n",
    "        \"Validation_TPP3\": validation_task_counts.get(\"TPP3\", 0),\n",
    "        \"Validation_TPP4\": validation_task_counts.get(\"TPP4\", 0),\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]} (Binding=1: {lengths[\"Train_Binding_1\"]}, Binding=0: {lengths[\"Train_Binding_0\"]}, TPP1: {lengths[\"Train_TPP1\"]}, TPP2: {lengths[\"Train_TPP2\"]}, TPP3: {lengths[\"Train_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]} (Binding=1: {lengths[\"Test_Binding_1\"]}, Binding=0: {lengths[\"Test_Binding_0\"]}, TPP1: {lengths[\"Test_TPP1\"]}, TPP2: {lengths[\"Test_TPP2\"]}, TPP3: {lengths[\"Test_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]} (Binding=1: {lengths[\"Validation_Binding_1\"]}, Binding=0: {lengths[\"Validation_Binding_0\"]}, TPP1: {lengths[\"Validation_TPP1\"]}, TPP2: {lengths[\"Validation_TPP2\"]}, TPP3: {lengths[\"Validation_TPP3\"]})')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')\n",
    "\n",
    "# Optional: Ergebnisse in einer Übersichtstabelle darstellen\n",
    "summary_data = []\n",
    "for dataset, lengths in results.items():\n",
    "    summary_data.append({\n",
    "        \"Dataset\": dataset.replace(\"_\", \" \").title(),\n",
    "        \"Train\": lengths[\"Train\"],\n",
    "        \"Train_Binding_1\": lengths[\"Train_Binding_1\"],\n",
    "        \"Train_Binding_0\": lengths[\"Train_Binding_0\"],\n",
    "        \"Train_TPP1\": lengths[\"Train_TPP1\"],\n",
    "        \"Train_TPP2\": lengths[\"Train_TPP2\"],\n",
    "        \"Train_TPP3\": lengths[\"Train_TPP3\"],\n",
    "        \"Train_TPP4\": lengths[\"Train_TPP4\"],\n",
    "        \"Test\": lengths[\"Test\"],\n",
    "        \"Test_Binding_1\": lengths[\"Test_Binding_1\"],\n",
    "        \"Test_Binding_0\": lengths[\"Test_Binding_0\"],\n",
    "        \"Test_TPP1\": lengths[\"Test_TPP1\"],\n",
    "        \"Test_TPP2\": lengths[\"Test_TPP2\"],\n",
    "        \"Test_TPP3\": lengths[\"Test_TPP3\"],\n",
    "        \"Test_TPP4\": lengths[\"Test_TPP4\"],\n",
    "        \"Validation\": lengths[\"Validation\"],\n",
    "        \"Validation_Binding_1\": lengths[\"Validation_Binding_1\"],\n",
    "        \"Validation_Binding_0\": lengths[\"Validation_Binding_0\"],\n",
    "        \"Validation_TPP1\": lengths[\"Validation_TPP1\"],\n",
    "        \"Validation_TPP2\": lengths[\"Validation_TPP2\"],\n",
    "        \"Validation_TPP3\": lengths[\"Validation_TPP3\"],\n",
    "        \"Validation_TPP4\": lengths[\"Validation_TPP4\"],\n",
    "        \"Total\": lengths[\"Total\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
