{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tidytcells in /home/ubuntu/anaconda3/lib/python3.12/site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install tidytcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision of mhc and V/J values (gene or allele)\n",
    "precision = 'allele'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is not thread safe\n",
    "def create_folders_if_not_exists(folders):\n",
    "  for path in folders:\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = '../../../../data'\n",
    "pipeline_data_plain = f'{pipeline_data}/plain_datasets'\n",
    "pipeline_data_cleaned = f'{pipeline_data}/cleaned_datasets'\n",
    "pipeline_data_concatenated = f'{pipeline_data}/concatenated_datasets'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "pipeline_data_temp_bucket = f'{pipeline_data}/temp'\n",
    "\n",
    "pipeline_folders = [pipeline_data, pipeline_data_plain, pipeline_data_cleaned, pipeline_data_concatenated, pipeline_data_splitted, pipeline_data_temp_bucket]\n",
    "\n",
    "create_folders_if_not_exists(pipeline_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VDJdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "VDJdb_data_plain = f'{pipeline_data_plain}/VDJdb'\n",
    "VDJdb_data_cleaned = f'{pipeline_data_cleaned}/VDJdb'\n",
    "VDJdb_data_fitted = f'{pipeline_data_temp_bucket}/VDJdb'\n",
    "\n",
    "VDJdb_folders = [VDJdb_data_plain, VDJdb_data_cleaned, VDJdb_data_fitted]\n",
    "create_folders_if_not_exists(VDJdb_folders)\n",
    "\n",
    "fitted_beta_file = 'VDJdb_beta_fitted.tsv'\n",
    "fitted_paired_file = 'VDJdb_paired_fitted.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data paired\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_paired_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_paired_file\n",
    "\n",
    "# fit paired VDJdb data\n",
    "%run ../VDJdb/fit_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data beta\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_beta_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_beta_file\n",
    "\n",
    "# fit beta VDJdb data\n",
    "%run ../VDJdb/fit_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 27414 entries\n",
      "whole dataframe has 28119 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 27414\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data paired\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_paired_file}'\n",
    "cleaned_file_paired = 'VDJdb_cleaned_data_paired.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'\n",
    "\n",
    "# clean paired VDJdb data\n",
    "%run ../VDJdb/clean_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 46507 entries\n",
      "whole dataframe has 49042 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 46507\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data beta\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_beta_file}'\n",
    "cleaned_file_beta = 'VDJdb_cleaned_data_beta.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "\n",
    "# clean beta VDJdb data\n",
    "%run ../VDJdb/clean_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDJdb_cleaned_beta_output = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "VDJdb_cleaned_paired_output = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of beta_df: 46507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 7188 entries removed.\n",
      "removed all duplicates (CDR3, Epitope) from distinct values (most_important_columns, keep=False). 1435 entries removed.\n",
      "beta removed entries df length: 1435\n",
      "\n",
      "\n",
      "Number of groups formed: 655\n",
      "1435 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 46507 entries, 7188 entries have been removed.\n",
      "for beta dataset :\n",
      "size difference is: 7188\n",
      "  39319 information score cleaned: 6.0\n",
      "  46507 information score dropout: 6.0\n",
      "✅ Nach Duplikat-Filter (Train/Val): final_beta_df enthält 9105 Einträge.\n",
      "final_beta_df length = 9105\n",
      "length of paired_df: 27414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 687 entries removed.\n",
      "removed all duplicates from distinct values (cultivated columns, keep=False). 246 entries removed.\n",
      "paired removed entries df length: 246\n",
      "\n",
      "\n",
      "246 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 27414 entries, 687 entries have been removed.\n",
      "for paired dataset:\n",
      "size difference is: 687\n",
      "  26727 information score cleaned: 8.976241254162458\n",
      "  27414 information score dropout: 8.975888232290071\n",
      "final_paired_df length: 26727\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "vdjdb_beta_read_path = VDJdb_cleaned_beta_output\n",
    "vdjdb_paired_read_path = VDJdb_cleaned_paired_output\n",
    "\n",
    "output_file_beta = 'beta_concatenated_test.tsv'\n",
    "output_file_paired = 'paired_concatenated_test.tsv'\n",
    "\n",
    "create_folders_if_not_exists([custom_dataset_path])\n",
    "\n",
    "%run ../concatDatasets_onlytest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta file copied successfully to ../../../../data/splitted_datasets/allele/beta/test_prenegsamples.tsv\n",
      "Paired file copied successfully to ../../../../data/splitted_datasets/allele/paired/test_prenegsamples.tsv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define source folder where the files are currently stored\n",
    "source_folder = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "# Define file names\n",
    "output_file_beta = 'beta_concatenated_test.tsv'\n",
    "output_file_paired = 'paired_concatenated_test.tsv'\n",
    "\n",
    "# Define destination folders\n",
    "destination_beta_folder = f'{pipeline_data_splitted}/{precision}/beta/'\n",
    "destination_paired_folder = f'{pipeline_data_splitted}/{precision}/paired/'\n",
    "\n",
    "# Ensure destination folders exist\n",
    "os.makedirs(destination_beta_folder, exist_ok=True)\n",
    "os.makedirs(destination_paired_folder, exist_ok=True)\n",
    "\n",
    "# Copy files\n",
    "shutil.copy(os.path.join(source_folder, output_file_beta), os.path.join(destination_beta_folder, 'test_prenegsamples.tsv'))\n",
    "shutil.copy(os.path.join(source_folder, output_file_paired), os.path.join(destination_paired_folder, 'test_prenegsamples.tsv'))\n",
    "\n",
    "print(f'Beta file copied successfully to {destination_beta_folder}test_prenegsamples.tsv')\n",
    "print(f'Paired file copied successfully to {destination_paired_folder}test_prenegsamples.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beta Dataset:\n",
      "- Unique TCRs: 8464\n",
      "- Unique Epitope: 293\n",
      "\n",
      "Paired Dataset:\n",
      "- Unique TCRs: 21101\n",
      "- Unique Epitope: 825\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "beta_file_path = f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv'\n",
    "paired_file_path = f'{pipeline_data_splitted}/{precision}/paired/test_prenegsamples.tsv'\n",
    "\n",
    "# Load beta dataset\n",
    "beta_df = pd.read_csv(beta_file_path, sep='\\t')\n",
    "\n",
    "# Load paired dataset\n",
    "paired_df = pd.read_csv(paired_file_path, sep='\\t')\n",
    "\n",
    "# Calculate unique values for beta dataset\n",
    "unique_tcr_beta = beta_df['TRB_CDR3'].nunique()\n",
    "unique_epitope_beta = beta_df['Epitope'].nunique()\n",
    "\n",
    "# Calculate unique values for paired dataset\n",
    "unique_tcr_paired = paired_df['TRB_CDR3'].nunique()\n",
    "unique_epitope_paired = paired_df['Epitope'].nunique()\n",
    "\n",
    "# Print results for beta dataset\n",
    "print(\"\\nBeta Dataset:\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_beta}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_beta}\")\n",
    "\n",
    "# Print results for paired dataset\n",
    "print(\"\\nPaired Dataset:\")\n",
    "print(f\"- Unique TCRs: {unique_tcr_paired}\")\n",
    "print(f\"- Unique Epitope: {unique_epitope_paired}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus:                 barcode   donor  \\\n",
      "0   AAACCTGAGACAAAGG-4  donor1   \n",
      "1  AAACCTGAGACTGTAA-34  donor1   \n",
      "2   AAACCTGAGAGCCCAA-5  donor1   \n",
      "3  AAACCTGAGAGCTGCA-24  donor1   \n",
      "4   AAACCTGAGAGGGATA-8  donor1   \n",
      "\n",
      "                                  cell_clono_cdr3_aa  \\\n",
      "0  TRA:CAASVSIWTGTASKLTF;TRA:CAAWDMEYGNKLVF;TRB:C...   \n",
      "1                                    TRB:CASDTPVGQFF   \n",
      "2                 TRA:CASYTDKLIF;TRB:CASSGGSISTDTQYF   \n",
      "3                                 TRB:CASSGGQSSYEQYF   \n",
      "4          TRA:CAASGYGNTGRRALTF;TRB:CASSQDPAGGYNEQFF   \n",
      "\n",
      "                                  cell_clono_cdr3_nt     CD3  CD19  CD45RA  \\\n",
      "0  TRA:TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAA...  2125.0   0.0   912.0   \n",
      "1              TRB:TGTGCCAGCGATACCCCGGTTGGGCAGTTCTTC  1023.0   0.0  2028.0   \n",
      "2  TRA:TGTGCTTCCTACACCGACAAGCTCATCTTT;TRB:TGCGCCA...  1598.0   3.0  3454.0   \n",
      "3     TRB:TGCGCCAGCAGTGGCGGACAGAGCTCCTACGAGCAGTACTTC   298.0   1.0   880.0   \n",
      "4  TRA:TGTGCAGCAAGCGGGTATGGAAACACGGGCAGGAGAGCACTT...  1036.0   0.0  2457.0   \n",
      "\n",
      "   CD4    CD8a  CD14  ...  B0702_RPHERNGFTVL_pp65_CMV_binder  \\\n",
      "0  1.0  2223.0   4.0  ...                              False   \n",
      "1  2.0  3485.0   1.0  ...                              False   \n",
      "2  4.0  3383.0   1.0  ...                              False   \n",
      "3  1.0  2389.0   1.0  ...                              False   \n",
      "4  2.0  3427.0   3.0  ...                              False   \n",
      "\n",
      "   B0801_RAKFKQLL_BZLF1_EBV_binder  B0801_ELRRKMMYM_IE-1_CMV_binder  \\\n",
      "0                            False                            False   \n",
      "1                            False                            False   \n",
      "2                            False                            False   \n",
      "3                            False                            False   \n",
      "4                            False                            False   \n",
      "\n",
      "   B0801_FLRGRAYGL_EBNA-3A_EBV_binder  A0101_SLEGGGLGY_NC_binder  \\\n",
      "0                               False                      False   \n",
      "1                               False                      False   \n",
      "2                               False                      False   \n",
      "3                               False                      False   \n",
      "4                               False                      False   \n",
      "\n",
      "   A0101_STEGGGLAY_NC_binder  A0201_ALIAPVHAV_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   A2402_AYSSAGASI_NC_binder  B0702_GPAESAAGL_NC_binder  \\\n",
      "0                      False                      False   \n",
      "1                      False                      False   \n",
      "2                      False                      False   \n",
      "3                      False                      False   \n",
      "4                      False                      False   \n",
      "\n",
      "   NR(B0801)_AAKGRGAAL_NC_binder  \n",
      "0                          False  \n",
      "1                          False  \n",
      "2                          False  \n",
      "3                          False  \n",
      "4                          False  \n",
      "\n",
      "[5 rows x 118 columns]\n",
      "Meta:                barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_1             True   \n",
      "1  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_2             True   \n",
      "2  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_3             True   \n",
      "3  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_4            False   \n",
      "4  AAACCTGAGACAAAGG-4     True  AAACCTGAGACAAAGG-4_contig_5            False   \n",
      "\n",
      "   length chain     v_gene d_gene   j_gene c_gene  full_length productive  \\\n",
      "0     722   TRB   TRBV10-3  TRBD2  TRBJ2-1  TRBC2         True       True   \n",
      "1     605   TRA  TRAV29DV5    NaN   TRAJ44   TRAC         True       True   \n",
      "2     738   TRA    TRAV8-6    NaN   TRAJ47   TRAC         True       True   \n",
      "3     468   TRB        NaN    NaN  TRBJ2-3  TRBC2        False        NaN   \n",
      "4     488   TRB        NaN    NaN  TRBJ2-6  TRBC2        False        NaN   \n",
      "\n",
      "                cdr3                                            cdr3_nt  \\\n",
      "0  CAISDPGLAGGGGEQFF  TGTGCCATCAGTGACCCCGGACTAGCGGGAGGCGGGGGGGAGCAGT...   \n",
      "1  CAASVSIWTGTASKLTF  TGTGCAGCAAGCGTTAGTATTTGGACCGGCACTGCCAGTAAACTCA...   \n",
      "2     CAAWDMEYGNKLVF         TGTGCCGCCTGGGACATGGAATATGGAAACAAGCTGGTCTTT   \n",
      "3                NaN                                                NaN   \n",
      "4                NaN                                                NaN   \n",
      "\n",
      "   reads  umis raw_clonotype_id         raw_consensus_id  \n",
      "0  32237    18      clonotype19  clonotype19_consensus_1  \n",
      "1   6088     3      clonotype19  clonotype19_consensus_2  \n",
      "2   5358     3      clonotype19  clonotype19_consensus_3  \n",
      "3   2517     1      clonotype19                      NaN  \n",
      "4   2468     1      clonotype19                      NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231714/3850524540.py:9: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n"
     ]
    }
   ],
   "source": [
    "#Daten einlesen\n",
    "\n",
    "combined_donors_path = f'{pipeline_data_plain}/10x/combined_donors_consensus_annotations.csv'\n",
    "all_donors_consensus = pd.read_csv(combined_donors_path, sep=',')\n",
    "\n",
    "print(\"Consensus: \", all_donors_consensus.head())\n",
    "\n",
    "all_donors_meta_path = f'{pipeline_data_plain}/10x/meta.csv'\n",
    "all_donors_meta = pd.read_csv(all_donors_meta_path, sep=',')\n",
    "\n",
    "print(\"Meta: \", all_donors_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Start:  0\n",
      "Batch Start:  1000\n",
      "Batch Start:  2000\n",
      "Batch Start:  3000\n",
      "Batch Start:  4000\n",
      "Batch Start:  5000\n",
      "Batch Start:  6000\n",
      "Batch Start:  7000\n",
      "Batch Start:  8000\n",
      "Batch Start:  9000\n",
      "Batch Start:  10000\n",
      "Batch Start:  11000\n",
      "Batch Start:  12000\n",
      "Batch Start:  13000\n",
      "Batch Start:  14000\n",
      "Batch Start:  15000\n",
      "Batch Start:  16000\n",
      "Batch Start:  17000\n",
      "Batch Start:  18000\n",
      "Batch Start:  19000\n",
      "Batch Start:  20000\n",
      "Batch Start:  21000\n",
      "Batch Start:  22000\n",
      "Batch Start:  23000\n",
      "Batch Start:  24000\n",
      "Batch Start:  25000\n",
      "Batch Start:  26000\n",
      "Batch Start:  27000\n",
      "Batch Start:  28000\n",
      "Batch Start:  29000\n",
      "Batch Start:  30000\n",
      "Batch Start:  31000\n",
      "Batch Start:  32000\n",
      "Batch Start:  33000\n",
      "Batch Start:  34000\n",
      "Batch Start:  35000\n",
      "Batch Start:  36000\n",
      "Batch Start:  37000\n",
      "Batch Start:  38000\n",
      "Batch Start:  39000\n",
      "Batch Start:  40000\n",
      "Batch Start:  41000\n",
      "Batch Start:  42000\n",
      "Batch Start:  43000\n",
      "Batch Start:  44000\n",
      "Batch Start:  45000\n",
      "Batch Start:  46000\n",
      "Batch Start:  47000\n",
      "Batch Start:  48000\n",
      "Batch Start:  49000\n",
      "Batch Start:  50000\n",
      "Batch Start:  51000\n",
      "Batch Start:  52000\n",
      "Batch Start:  53000\n",
      "Batch Start:  54000\n",
      "Batch Start:  55000\n",
      "Batch Start:  56000\n",
      "Batch Start:  57000\n",
      "Batch Start:  58000\n",
      "Batch Start:  59000\n",
      "Batch Start:  60000\n",
      "Batch Start:  61000\n",
      "Batch Start:  62000\n",
      "Batch Start:  63000\n",
      "Batch Start:  64000\n",
      "Batch Start:  65000\n",
      "Batch Start:  66000\n",
      "Batch Start:  67000\n",
      "Batch Start:  68000\n",
      "Batch Start:  69000\n",
      "Batch Start:  70000\n",
      "Batch Start:  71000\n",
      "Batch Start:  72000\n",
      "Batch Start:  73000\n",
      "Batch Start:  74000\n",
      "Batch Start:  75000\n",
      "Batch Start:  76000\n",
      "Batch Start:  77000\n",
      "Batch Start:  78000\n",
      "Batch Start:  79000\n",
      "Batch Start:  80000\n",
      "Batch Start:  81000\n",
      "Batch Start:  82000\n",
      "Batch Start:  83000\n",
      "Batch Start:  84000\n",
      "Batch Start:  85000\n",
      "Batch Start:  86000\n",
      "Batch Start:  87000\n",
      "Batch Start:  88000\n",
      "Batch Start:  89000\n",
      "Batch Start:  90000\n",
      "Batch Start:  91000\n",
      "Batch Start:  92000\n",
      "Batch Start:  93000\n",
      "Batch Start:  94000\n",
      "Batch Start:  95000\n",
      "Batch Start:  96000\n",
      "Batch Start:  97000\n",
      "Batch Start:  98000\n",
      "Batch Start:  99000\n",
      "Batch Start:  100000\n",
      "Batch Start:  101000\n",
      "Batch Start:  102000\n",
      "Batch Start:  103000\n",
      "Batch Start:  104000\n",
      "Batch Start:  105000\n",
      "Batch Start:  106000\n",
      "Batch Start:  107000\n",
      "Batch Start:  108000\n",
      "Batch Start:  109000\n",
      "Batch Start:  110000\n",
      "Batch Start:  111000\n",
      "Batch Start:  112000\n",
      "Batch Start:  113000\n",
      "Batch Start:  114000\n",
      "Batch Start:  115000\n",
      "Batch Start:  116000\n",
      "Batch Start:  117000\n",
      "Batch Start:  118000\n",
      "Batch Start:  119000\n",
      "Batch Start:  120000\n",
      "Batch Start:  121000\n",
      "Batch Start:  122000\n",
      "Batch Start:  123000\n",
      "Batch Start:  124000\n",
      "Batch Start:  125000\n",
      "Batch Start:  126000\n",
      "Batch Start:  127000\n",
      "Batch Start:  128000\n",
      "Batch Start:  129000\n",
      "Batch Start:  130000\n",
      "Batch Start:  131000\n",
      "Batch Start:  132000\n",
      "Batch Start:  133000\n",
      "Batch Start:  134000\n",
      "Batch Start:  135000\n",
      "Batch Start:  136000\n",
      "Batch Start:  137000\n",
      "Batch Start:  138000\n",
      "Batch Start:  139000\n",
      "Batch Start:  140000\n",
      "Batch Start:  141000\n",
      "Batch Start:  142000\n",
      "Batch Start:  143000\n",
      "Batch Start:  144000\n",
      "Batch Start:  145000\n",
      "Batch Start:  146000\n",
      "Batch Start:  147000\n",
      "Batch Start:  148000\n",
      "Batch Start:  149000\n",
      "Batch Start:  150000\n",
      "Batch Start:  151000\n",
      "Batch Start:  152000\n",
      "Batch Start:  153000\n",
      "Batch Start:  154000\n",
      "Batch Start:  155000\n",
      "Batch Start:  156000\n",
      "Batch Start:  157000\n",
      "Batch Start:  158000\n",
      "Batch Start:  159000\n",
      "Batch Start:  160000\n",
      "Batch Start:  161000\n",
      "Batch Start:  162000\n",
      "Batch Start:  163000\n",
      "Batch Start:  164000\n",
      "Batch Start:  165000\n",
      "Batch Start:  166000\n",
      "Batch Start:  167000\n",
      "Batch Start:  168000\n",
      "Batch Start:  169000\n",
      "Batch Start:  170000\n",
      "Batch Start:  171000\n",
      "Batch Start:  172000\n",
      "Batch Start:  173000\n",
      "Batch Start:  174000\n",
      "Batch Start:  175000\n",
      "Batch Start:  176000\n",
      "Batch Start:  177000\n",
      "Batch Start:  178000\n",
      "Batch Start:  179000\n",
      "Batch Start:  180000\n",
      "Batch Start:  181000\n",
      "Batch Start:  182000\n",
      "Batch Start:  183000\n",
      "Batch Start:  184000\n",
      "Batch Start:  185000\n",
      "Batch Start:  186000\n",
      "Batch Start:  187000\n",
      "Batch Start:  188000\n",
      "Batch Start:  189000\n",
      "             TCR_name      TRBV     TRBJ           TRB_CDR3   TRBC  \\\n",
      "0  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "1  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "2  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "3  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "4  AAACCTGAGACAAAGG-4  TRBV10-3  TRBJ2-1  CAISDPGLAGGGGEQFF  TRBC2   \n",
      "\n",
      "      Epitope          MHC Binding task  \n",
      "0   VTEHDTLLY  HLA-A*01:01       0  nan  \n",
      "1   KTWGQYWQV  HLA-A*02:01       0  nan  \n",
      "2  ELAGIGILTV  HLA-A*02:01       0  nan  \n",
      "3  CLLWSFQTSA  HLA-A*02:01       0  nan  \n",
      "4   IMDQVPFSV  HLA-A*02:01       0  nan  \n"
     ]
    }
   ],
   "source": [
    "#Dieser Code für ganzen Datensatz laufen lassen\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Festlegen der Batch-Größe für die Verarbeitung\n",
    "batch_size = 1000  # Passe diese Zahl je nach Speicherressourcen an\n",
    "\n",
    "# Identifizieren von Epitope-Spalten, aber ohne \"NR(B0801)_AAKGRGAAL_NC_binder\"\n",
    "epitope_columns = [col for col in all_donors_consensus.columns if '_binder' in col and col != \"NR(B0801)_AAKGRGAAL_NC_binder\"]\n",
    "\n",
    "# Liste für alle Batch-Ergebnisse\n",
    "all_batches = []\n",
    "\n",
    "# Verarbeite `all_donors_consensus` in Batches\n",
    "for batch_start in range(0, len(all_donors_consensus), batch_size):\n",
    "    print(\"Batch Start: \", batch_start)\n",
    "    # Batch definieren\n",
    "    batch = all_donors_consensus.iloc[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Filtern auf Zeilen, die 'TRB:' enthalten\n",
    "    batch_trb = batch[batch['cell_clono_cdr3_aa'].str.contains(\"TRB:\", na=False)]\n",
    "\n",
    "    # Liste, um Zeilen für diesen Batch zu speichern\n",
    "    expanded_rows = []\n",
    "    \n",
    "    # Iteriere durch jede Zeile im Batch\n",
    "    for _, row in batch_trb.iterrows():\n",
    "        for col in epitope_columns:\n",
    "            # Extrahiere MHC und Epitope\n",
    "            match = re.match(r'([A-Z0-9]+)_([A-Z]+)_.*_binder', col)\n",
    "            if match:\n",
    "                mhc_raw, epitope = match.groups()\n",
    "                mhc_formatted = f'HLA-{mhc_raw[0]}*{mhc_raw[1:3]}:{mhc_raw[3:]}'\n",
    "\n",
    "                # Füge `Epitope` und `MHC` zur Zeile hinzu\n",
    "                new_row = row.copy()\n",
    "                new_row['Epitope'] = epitope\n",
    "                new_row['MHC'] = mhc_formatted\n",
    "\n",
    "                # Füge neue Zeile zur Batch-Liste hinzu\n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    # Erstelle einen DataFrame aus dem Batch\n",
    "    batch_df = pd.DataFrame(expanded_rows)\n",
    "    all_batches.append(batch_df)  # Speichere den Batch in der Liste\n",
    "\n",
    "# Kombiniere alle Batch-Ergebnisse zu einem DataFrame\n",
    "expanded_df = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "# Nur die TRB-Chain-Einträge in `all_donors_meta` beibehalten\n",
    "all_donors_meta_trb = all_donors_meta[all_donors_meta['chain'] == 'TRB']\n",
    "\n",
    "# Zusammenführen der beiden DataFrames basierend auf der 'barcode' Spalte\n",
    "merged_df = pd.merge(all_donors_meta_trb, expanded_df[['barcode', 'Epitope', 'MHC']], on='barcode', how='inner')\n",
    "\n",
    "# Spalten umbenennen und Format anpassen\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'barcode': 'TCR_name',\n",
    "    'v_gene': 'TRBV',\n",
    "    'j_gene': 'TRBJ',\n",
    "    'c_gene': 'TRBC',\n",
    "    'cdr3': 'TRB_CDR3'\n",
    "})\n",
    "\n",
    "# Fehlende Spalten auffüllen\n",
    "desired_columns = ['TCR_name', 'TRBV', 'TRBJ', 'TRB_CDR3', 'TRBC', 'Epitope', 'MHC', 'Binding', 'task']\n",
    "for col in desired_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = 'nan' if col == 'task' else '0'\n",
    "\n",
    "# Nur die gewünschten Spalten beibehalten und Zeilen mit `None` in `TRB_CDR3` entfernen\n",
    "final_df = merged_df[desired_columns]\n",
    "final_df = final_df[final_df['TRB_CDR3'] != 'None']\n",
    "\n",
    "final_df = final_df[final_df['TRB_CDR3'].notna() & (final_df['TRB_CDR3'] != '')]\n",
    "\n",
    "# Ausgabe des ersten Teils des Ergebnisses zur Überprüfung\n",
    "print(final_df.head())\n",
    "\n",
    "# Speichern des kombinierten DataFrames\n",
    "output_path = f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv'\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Samples generieren für Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for beta dataset\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "read_path_train = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "read_path_test = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "read_path_validation = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "temp_path = f'{pipeline_data_temp_bucket}/negative_samples/beta/'\n",
    "output_path = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives\"\n",
    "train_output_name = \"train_neg.tsv\"\n",
    "validation_output_name = \"val_neg.tsv\"\n",
    "test_output_name = \"test_neg.tsv\"\n",
    "\n",
    "create_folders_if_not_exists([temp_path])\n",
    "\n",
    "%run ../negative_samples/negative_samples_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spalte 'source' auf 'generated' gesetzt und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Dateipfade ---\n",
    "read_path_train = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{train_output_name}\"\n",
    "read_path_val = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{validation_output_name}\"\n",
    "read_path_test = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/{test_output_name}\"\n",
    "\n",
    "# --- Dateien laden ---\n",
    "train_neg = pd.read_csv(read_path_train, sep='\\t')\n",
    "val_neg = pd.read_csv(read_path_val, sep='\\t')\n",
    "test_neg = pd.read_csv(read_path_test, sep='\\t')\n",
    "\n",
    "# --- Spalte \"source\" setzen ---\n",
    "for df in [train_neg, val_neg, test_neg]:\n",
    "    df[\"source\"] = \"generated\"\n",
    "\n",
    "# --- Zurückspeichern ---\n",
    "train_neg.to_csv(read_path_train, sep='\\t', index=False)\n",
    "val_neg.to_csv(read_path_val, sep='\\t', index=False)\n",
    "test_neg.to_csv(read_path_test, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Spalte 'source' auf 'generated' gesetzt und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ TRAIN: Entferne 157965 Duplikate mit Konflikt.\n",
      "✅ TRAIN: Übrig nach Cleaning: 624111\n",
      "❌ VALIDATION: Entferne 31437 Duplikate mit Konflikt.\n",
      "✅ VALIDATION: Übrig nach Cleaning: 189566\n",
      "❌ TEST: Entferne 1085 Duplikate mit Konflikt.\n",
      "✅ TEST: Übrig nach Cleaning: 51705\n"
     ]
    }
   ],
   "source": [
    "# 🔁 Wiederverwendbare Funktion zum Entfernen\n",
    "def remove_conflicting_negatives(pos_df, neg_df, name):\n",
    "    dupe_cols = [\"Epitope\", \"TRB_CDR3\"]\n",
    "    merged = pd.merge(neg_df, pos_df[dupe_cols], on=dupe_cols, how=\"inner\")\n",
    "    print(f\"❌ {name.upper()}: Entferne {len(merged)} Duplikate mit Konflikt.\")\n",
    "    \n",
    "    # Nur die, die NICHT im positiven vorkommen → behalten\n",
    "    cleaned_neg = neg_df.merge(merged[dupe_cols], on=dupe_cols, how=\"left\", indicator=True)\n",
    "    cleaned_neg = cleaned_neg[cleaned_neg[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n",
    "    print(f\"✅ {name.upper()}: Übrig nach Cleaning: {len(cleaned_neg)}\")\n",
    "    return cleaned_neg\n",
    "# Neue Clean-Dateipfade\n",
    "neg_base_path = f\"{pipeline_data_splitted}/{precision}/beta/new/negatives\"\n",
    "\n",
    "train_neg_clean = remove_conflicting_negatives(train_pos, train_neg, \"train\")\n",
    "train_neg_clean.to_csv(f\"{neg_base_path}/train_neg_clean.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "val_neg_clean = remove_conflicting_negatives(val_pos, val_neg, \"validation\")\n",
    "val_neg_clean.to_csv(f\"{neg_base_path}/val_neg_clean.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "test_neg_clean = remove_conflicting_negatives(test_pos, test_neg, \"test\")\n",
    "test_neg_clean.to_csv(f\"{neg_base_path}/test_neg_clean.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 TRAIN:\n",
      "→ Duplikate (gesamt): 0\n",
      "\n",
      "🔎 VALIDATION:\n",
      "→ Duplikate (gesamt): 0\n",
      "\n",
      "🔎 TEST:\n",
      "→ Duplikate (gesamt): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Load negative datasets ===\n",
    "train_neg = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/train_neg_clean.tsv\", sep='\\t')\n",
    "val_neg = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/val_neg_clean.tsv\", sep='\\t')\n",
    "test_neg = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/test_neg_clean.tsv\", sep='\\t')\n",
    "\n",
    "# === Load positive datasets ===\n",
    "train_pos = pd.read_csv(output_train_path, sep='\\t')\n",
    "val_pos = pd.read_csv(output_val_path, sep='\\t')\n",
    "test_pos = pd.read_csv(test_output_path, sep='\\t')\n",
    "\n",
    "# === Spalten zur Duplikatprüfung ===\n",
    "dupe_cols = [\"Epitope\", \"TRB_CDR3\"]\n",
    "\n",
    "# === Funktion zum Vergleich ===\n",
    "def check_duplicates(pos_df, neg_df, name):\n",
    "    merged = pd.merge(pos_df, neg_df, on=dupe_cols, suffixes=('_pos', '_neg'))\n",
    "    print(f\"\\n🔎 {name.upper()}:\")\n",
    "    print(f\"→ Duplikate (gesamt): {len(merged)}\")\n",
    "    if not merged.empty:\n",
    "        print(\"→ Aufschlüsselung nach Binding (pos/neg):\")\n",
    "        print(merged[[\"Binding_pos\", \"Binding_neg\"]].value_counts().sort_index())\n",
    "\n",
    "# === Checks durchführen ===\n",
    "check_duplicates(train_pos, train_neg, \"train\")\n",
    "check_duplicates(val_pos, val_neg, \"validation\")\n",
    "check_duplicates(test_pos, test_neg, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_balanced_negatives_with_all_epitopes(\n",
    "    neg_source_1,\n",
    "    neg_source_2,\n",
    "    target_neg_count,\n",
    "    used_pairs=set(),\n",
    "    vdjdb_tcrs=set(),\n",
    "    vdjdb_epitopes=set(),\n",
    "    ensure_all_neg_epitopes=True\n",
    "):\n",
    "    neg_source_1 = neg_source_1.copy()\n",
    "    neg_source_2 = neg_source_2.copy()\n",
    "    neg_source_1['source'] = '10X'\n",
    "    neg_source_2['source'] = 'generated'\n",
    "\n",
    "    # --- VDJdb & verwendete Paare filtern ---\n",
    "    def filter_df(df, remove_epitopes=False):\n",
    "        df = df[~df['TRB_CDR3'].isin(vdjdb_tcrs)].copy()\n",
    "        if remove_epitopes:\n",
    "            df = df[~df['Epitope'].isin(vdjdb_epitopes)]\n",
    "        df['Pair'] = list(map(tuple, df[['Epitope', 'TRB_CDR3']].values))\n",
    "        df = df[~df['Pair'].isin(used_pairs)]\n",
    "        return df\n",
    "\n",
    "    neg_source_1 = filter_df(neg_source_1)\n",
    "    neg_source_2 = filter_df(neg_source_2)\n",
    "\n",
    "    # --- Vorverarbeitung für TPP-Filterung ---\n",
    "    trainval_tcrs = set(pd.concat([train_final, val_final])['TRB_CDR3'])\n",
    "    trainval_epitopes = set(pd.concat([train_final, val_final])['Epitope'])\n",
    "\n",
    "    def is_task(row, task):\n",
    "        return classify_task(row['TRB_CDR3'], row['Epitope'], trainval_tcrs, trainval_epitopes) == task\n",
    "    \n",
    "    # TPP3 & TPP4 vollständig übernehmen\n",
    "    tpp3_10x = neg_source_1[neg_source_1.apply(lambda r: is_task(r, 'TPP3'), axis=1)]\n",
    "    tpp4_10x = neg_source_1[neg_source_1.apply(lambda r: is_task(r, 'TPP4'), axis=1)]\n",
    "    \n",
    "    # TPP2 begrenzen auf max. 7000\n",
    "    tpp2_full = neg_source_1[neg_source_1.apply(lambda r: is_task(r, 'TPP2'), axis=1)]\n",
    "    tpp2_10x = tpp2_full.sample(n=min(7000, len(tpp2_full)), random_state=42)\n",
    "    \n",
    "    # TPP1 auffüllen mit Rest\n",
    "    used_pairs_in_10x = set(pd.concat([tpp3_10x, tpp4_10x, tpp2_10x])[['Epitope', 'TRB_CDR3']].apply(tuple, axis=1))\n",
    "    \n",
    "    tpp1_pool = neg_source_1[\n",
    "        neg_source_1.apply(lambda r: is_task(r, 'TPP1'), axis=1) &\n",
    "        ~neg_source_1[['Epitope', 'TRB_CDR3']].apply(tuple, axis=1).isin(used_pairs_in_10x)\n",
    "    ]\n",
    "    \n",
    "    # Wieviele TPP1 wir brauchen, um Gesamtziel zu erreichen:\n",
    "    already_selected = len(tpp3_10x) + len(tpp4_10x) + len(tpp2_10x)\n",
    "    tpp1_needed = max(target_neg_count - already_selected, 0)\n",
    "    \n",
    "    tpp1_10x = tpp1_pool.sample(n=min(tpp1_needed, len(tpp1_pool)), random_state=42)\n",
    "    \n",
    "    # Kombinieren zu neuer neg_source_1 (10X)\n",
    "    neg_source_1 = pd.concat([tpp3_10x, tpp4_10x, tpp2_10x, tpp1_10x], ignore_index=True)\n",
    "    print(f\"✅ Neue neg_source_1 aus 10X enthält {len(neg_source_1)} Beispiele (TPP3: {len(tpp3_10x)}, TPP4: {len(tpp4_10x)}, TPP2: {len(tpp2_10x)}, TPP1: {len(tpp1_10x)})\")\n",
    "\n",
    "\n",
    "    # --- Mindestens 1x alle Epitope aus beiden Quellen übernehmen ---\n",
    "    def ensure_epitope_coverage(df):\n",
    "        guaranteed = []\n",
    "        for epitope in df['Epitope'].unique():\n",
    "            group = df[df['Epitope'] == epitope]\n",
    "            if not group.empty:\n",
    "                guaranteed.append(group.sample(1, random_state=42))\n",
    "        return pd.concat(guaranteed, ignore_index=True)\n",
    "\n",
    "    guaranteed_1 = ensure_epitope_coverage(neg_source_1)\n",
    "    guaranteed_2 = ensure_epitope_coverage(neg_source_2)\n",
    "    guaranteed_df = pd.concat([guaranteed_1, guaranteed_2], ignore_index=True)\n",
    "\n",
    "    # Begrenze garantierte, falls sie zu groß geworden sind\n",
    "    if len(guaranteed_df) > target_neg_count:\n",
    "        print(f\"Zu viele garantierte Negative ({len(guaranteed_df)}), trimme auf Zielmenge {target_neg_count}\")\n",
    "        guaranteed_df = guaranteed_df.sample(n=target_neg_count, random_state=42)\n",
    "\n",
    "    # --- Stratified Sampling für Restauffüllung ---\n",
    "    def stratified_sample(df, n):\n",
    "        epitope_groups = df.groupby('Epitope')\n",
    "        unique_epitopes = list(epitope_groups.groups.keys())\n",
    "        print(f\"→ Stratified sampling from {len(df)} rows | {len(unique_epitopes)} unique epitopes | need {n} samples\")\n",
    "    \n",
    "        # Schritt 1: Garantiert 1 Sample pro Epitope\n",
    "        guaranteed = [group.sample(1, random_state=42) for _, group in epitope_groups]\n",
    "        guaranteed_df = pd.concat(guaranteed, ignore_index=True)\n",
    "    \n",
    "        remaining_n = n - len(guaranteed_df)\n",
    "        if remaining_n <= 0:\n",
    "            return guaranteed_df.sample(n=n, random_state=42)\n",
    "    \n",
    "        # Schritt 2: Aufstocken durch gewichtetes Sampling\n",
    "        remaining_pool = df.drop(index=guaranteed_df.index, errors='ignore')\n",
    "    \n",
    "        # Gewichte: Häufigkeit pro Epitope → normalize\n",
    "        epitope_counts = remaining_pool['Epitope'].value_counts()\n",
    "        remaining_pool = remaining_pool.copy()\n",
    "        remaining_pool['weight'] = remaining_pool['Epitope'].map(epitope_counts)\n",
    "        total = remaining_pool['weight'].sum()\n",
    "        remaining_pool['weight'] = remaining_pool['weight'] / total\n",
    "    \n",
    "        print(f\"→ Stratified fill-in: drawing {remaining_n} samples weighted by epitope frequency\")\n",
    "    \n",
    "        replace = len(remaining_pool) < remaining_n\n",
    "        if replace:\n",
    "            print(f\"Achtung: Sampling mit Replacement (n={remaining_n}, pool={len(remaining_pool)})\")\n",
    "        \n",
    "        sampled_rest = remaining_pool.sample(\n",
    "            n=remaining_n,\n",
    "            weights='weight',\n",
    "            replace=replace,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        final_df = pd.concat([guaranteed_df, sampled_rest], ignore_index=True)\n",
    "        return final_df\n",
    "\n",
    "    remaining_needed = target_neg_count - len(guaranteed_df)\n",
    "    if remaining_needed <= 0:\n",
    "        print(f\"Es wurden bereits {len(guaranteed_df)} garantierte Samples übernommen (mehr als benötigt).\")\n",
    "        final_df = guaranteed_df.sample(n=target_neg_count, random_state=42)\n",
    "    else:\n",
    "        # Garantierte rausnehmen\n",
    "        used_idx_1 = guaranteed_1.index if not guaranteed_1.empty else []\n",
    "        used_idx_2 = guaranteed_2.index if not guaranteed_2.empty else []\n",
    "\n",
    "        remaining_1 = neg_source_1\n",
    "        remaining_2 = neg_source_2\n",
    "\n",
    "        half = remaining_needed // 2\n",
    "        rest = remaining_needed - half\n",
    "\n",
    "        sample_1 = stratified_sample(remaining_1, half)\n",
    "        sample_2 = stratified_sample(remaining_2, rest)\n",
    "\n",
    "        final_df = pd.concat([guaranteed_df, sample_1, sample_2], ignore_index=True)\n",
    "\n",
    "    return final_df.drop(columns=['Pair'])\n",
    "\n",
    "def classify_task(tcr, epitope, train_tcrs, train_epitopes):\n",
    "    seen_tcr = tcr in train_tcrs\n",
    "    seen_epi = epitope in train_epitopes\n",
    "    if seen_tcr and seen_epi:\n",
    "        return 'TPP1'\n",
    "    elif not seen_tcr and seen_epi:\n",
    "        return 'TPP2'\n",
    "    elif not seen_tcr and not seen_epi:\n",
    "        return 'TPP3'\n",
    "    elif seen_tcr and not seen_epi:\n",
    "        return 'TPP4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102220/4182396430.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# === Datei- und Pfadangaben ===\n",
    "\n",
    "# Originaldaten (10X)\n",
    "beta = pd.read_csv(f'{pipeline_data_plain}/10x/combined_output_with_epitope_mhc_TRB_only_expanded-all.csv', sep=',')\n",
    "\n",
    "# Generierte Negativdaten\n",
    "neg_ba_train = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/train_neg_clean.tsv\", sep='\\t')\n",
    "neg_ba_val = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/val_neg_clean.tsv\", sep='\\t')\n",
    "neg_ba_test = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/negatives/test_neg_clean.tsv\", sep='\\t')\n",
    "\n",
    "# Positive Beispiele\n",
    "train_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/train_prenegsamples.tsv', sep='\\t')\n",
    "val_pos = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/validation_prenegsamples.tsv', sep='\\t')\n",
    "test_preneg = pd.read_csv(f'{pipeline_data_splitted}/{precision}/beta/test_prenegsamples.tsv', sep='\\t')\n",
    "\n",
    "# Output-Ziele\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path = f'{pipeline_data_splitted}/{precision}/beta/new/test.tsv'\n",
    "\n",
    "# VDJdb zum Herausfiltern\n",
    "vdjdb_df = pd.read_csv(VDJdb_cleaned_beta_output, sep='\\t')\n",
    "vdjdb_tcrs = set(vdjdb_df['TRB_CDR3'])\n",
    "vdjdb_epitopes = set(vdjdb_df['Epitope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Neue neg_source_1 aus 10X enthält 44500 Beispiele (TPP3: 0, TPP4: 0, TPP2: 5047, TPP1: 39453)\n",
      "→ Stratified sampling from 44500 rows | 49 unique epitopes | need 21900 samples\n",
      "→ Stratified fill-in: drawing 21851 samples weighted by epitope frequency\n",
      "→ Stratified sampling from 37996 rows | 650 unique epitopes | need 21901 samples\n",
      "→ Stratified fill-in: drawing 21251 samples weighted by epitope frequency\n",
      "Binding\n",
      "0    44500\n",
      "1     8900\n",
      "Name: count, dtype: int64\n",
      "✅ Testset erfolgreich erstellt & gespeichert.\n",
      "Test: 53400 Beispiele\n",
      "- Binding=1: 8900\n",
      "- Binding=0: 44500\n",
      "- Unique Epitope: 724\n",
      "- Unique TCRs: 34479\n"
     ]
    }
   ],
   "source": [
    "# === TESTDATEN VERARBEITUNG ===\n",
    "# Zielmenge: 1:5 Verhältnis\n",
    "num_test_pos = len(test_preneg)\n",
    "test_neg_target = num_test_pos * 5\n",
    "\n",
    "neg_10x = beta[beta['Binding'] == 0]\n",
    "train_final = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\", sep='\\t')\n",
    "val_final = pd.read_csv(f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\", sep='\\t')\n",
    "\n",
    "# Paare aus Train/Val ausschließen\n",
    "used_trainval_pairs = set(\n",
    "    map(tuple, pd.concat([train_final, val_final])[['Epitope', 'TRB_CDR3']].values)\n",
    ")\n",
    "\n",
    "# Negative Samples für Test generieren\n",
    "test_neg = create_balanced_negatives_with_all_epitopes(\n",
    "    neg_source_1=neg_10x,\n",
    "    neg_source_2=neg_ba_test,\n",
    "    target_neg_count=test_neg_target,\n",
    "    used_pairs=used_trainval_pairs,\n",
    "    vdjdb_tcrs=vdjdb_tcrs,\n",
    "    vdjdb_epitopes=vdjdb_epitopes\n",
    ")\n",
    "\n",
    "# Combine & save\n",
    "test_final = pd.concat([test_preneg, test_neg], ignore_index=True).sample(frac=1, random_state=42)\n",
    "test_final.to_csv(test_output_path, sep='\\t', index=False)\n",
    "\n",
    "df_check = pd.read_csv(test_output_path, sep='\\t')\n",
    "print(df_check['Binding'].value_counts())\n",
    "\n",
    "# Ausgabe\n",
    "print(\"✅ Testset erfolgreich erstellt & gespeichert.\")\n",
    "print(f\"Test: {len(test_final)} Beispiele\")\n",
    "print(f\"- Binding=1: {test_final['Binding'].value_counts().get(1, 0)}\")\n",
    "print(f\"- Binding=0: {test_final['Binding'].value_counts().get(0, 0)}\")\n",
    "print(f\"- Unique Epitope: {test_final['Epitope'].nunique()}\")\n",
    "print(f\"- Unique TCRs: {test_final['TRB_CDR3'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gewünschte Änderungen in den finalen Splits wurden vorgenommen und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Pfade zu finalen Splits ---\n",
    "output_train_path = f\"{pipeline_data_splitted}/{precision}/beta/new/train.tsv\"\n",
    "output_val_path   = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path  = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "\n",
    "# --- Dateien einlesen ---\n",
    "train_df = pd.read_csv(output_train_path, sep='\\t')\n",
    "val_df   = pd.read_csv(output_val_path, sep='\\t')\n",
    "test_df  = pd.read_csv(test_output_path, sep='\\t')\n",
    "\n",
    "# --- Funktion zur Bearbeitung ---\n",
    "def clean_and_update(df):\n",
    "    if 'weight' in df.columns:\n",
    "        df = df.drop(columns=['weight'])\n",
    "    if 'Epitope MHC MHC class' in df.columns:\n",
    "        df = df.drop(columns=['Epitope MHC MHC class'])\n",
    "    if 'pair_count' in df.columns:\n",
    "        df = df.drop(columns=['pair_count'])\n",
    "    if 'epi_count' in df.columns:\n",
    "        df = df.drop(columns=['epi_count'])\n",
    "    if 'pair' in df.columns:\n",
    "        df = df.drop(columns=['pair'])\n",
    "    if 'source' not in df.columns:\n",
    "        df['source'] = ''\n",
    "    df.loc[df['Binding'] == 1, 'source'] = 'datasets'\n",
    "    return df\n",
    "\n",
    "# --- Anwenden ---\n",
    "train_df = clean_and_update(train_df)\n",
    "val_df   = clean_and_update(val_df)\n",
    "test_df  = clean_and_update(test_df)\n",
    "\n",
    "# --- Zurückschreiben (überschreibt die Dateien direkt) ---\n",
    "train_df.to_csv(output_train_path, sep='\\t', index=False)\n",
    "val_df.to_csv(output_val_path, sep='\\t', index=False)\n",
    "test_df.to_csv(test_output_path, sep='\\t', index=False)\n",
    "\n",
    "print(\"✅ Gewünschte Änderungen in den finalen Splits wurden vorgenommen und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# === Shift von TPP3 und TPP4 aus Val zu Test, weil dort zu wenig\\noutput_val_path   = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\\ntest_output_path  = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\\n\\nval_final   = pd.read_csv(output_val_path, sep=\\'\\t\\')\\ntest_final  = pd.read_csv(test_output_path, sep=\\'\\t\\')\\n\\nprint(\"\\n📊 Alte Testverteilung:\")\\nprint(test_final.groupby([\\'task\\', \\'Binding\\']).size().unstack(fill_value=0))\\n\\nprint(\"\\n📊 Alte Validationverteilung:\")\\nprint(val_final.groupby([\\'task\\', \\'Binding\\']).size().unstack(fill_value=0))\\n\\n# === 1. Finde TPP3/TPP4 NON-BINDER in val_final\\nval_tpp3_neg = val_final[\\n    (val_final[\\'Binding\\'] == 0) & (val_final[\\'task\\'] == \\'TPP3\\')\\n]\\n\\nval_tpp4_neg = val_final[\\n    (val_final[\\'Binding\\'] == 0) & (val_final[\\'task\\'] == \\'TPP4\\')\\n]\\n\\n# === 2. Ziehe jeweils 50%\\n# === Für TPP3: (Epitope, TRB_CDR3) einmalig\\nval_tpp3_neg[\"pair\"] = val_tpp3_neg[[\\'Epitope\\', \\'TRB_CDR3\\']].apply(tuple, axis=1)\\npair_counts = val_tpp3_neg[\"pair\"].value_counts()\\nval_tpp3_neg[\"pair_count\"] = val_tpp3_neg[\"pair\"].map(pair_counts)\\n\\nunique_pairs = val_tpp3_neg[val_tpp3_neg[\"pair_count\"] == 1]\\ndupe_pairs   = val_tpp3_neg[val_tpp3_neg[\"pair_count\"] > 1]\\n\\ntarget_tpp3 = int(len(val_tpp3_neg) * 0.5)\\n\\nval_tpp3_half = pd.concat([\\n    unique_pairs,\\n    dupe_pairs.sample(n=max(0, target_tpp3 - len(unique_pairs)), random_state=42)\\n], ignore_index=True)\\n\\n# === Für TPP4: Epitope einmalig\\nepi_counts = val_tpp4_neg[\"Epitope\"].value_counts()\\nval_tpp4_neg[\"epi_count\"] = val_tpp4_neg[\"Epitope\"].map(epi_counts)\\n\\nunique_epi = val_tpp4_neg[val_tpp4_neg[\"epi_count\"] == 1]\\ndupe_epi   = val_tpp4_neg[val_tpp4_neg[\"epi_count\"] > 1]\\n\\ntarget_tpp4 = int(len(val_tpp4_neg) * 0.5)\\n\\nval_tpp4_half = pd.concat([\\n    unique_epi,\\n    dupe_epi.sample(n=max(0, target_tpp4 - len(unique_epi)), random_state=42)\\n], ignore_index=True)\\n\\n\\ntpp3_4_half = pd.concat([val_tpp3_half, val_tpp4_half])  # kein ignore_index\\n\\nprint(f\"✅ Verschiebe {len(tpp3_4_half)} TPP3/4-Non-Binder von Validation → Test\")\\n\\n# === 3. In Testset einfügen\\ntest_final_updated = pd.concat([test_final, tpp3_4_half], ignore_index=True)\\n\\n# === 4. Entferne aus val_final alle Zeilen mit denselben (Epitope, TRB_CDR3)\\nmoved_pairs = set(tpp3_4_half[[\\'Epitope\\', \\'TRB_CDR3\\']].apply(tuple, axis=1))\\n\\nval_final_updated = val_final[\\n    ~val_final[[\\'Epitope\\', \\'TRB_CDR3\\']].apply(tuple, axis=1).isin(moved_pairs)\\n]\\n\\n# === 5. Speichern\\ntest_final_updated.to_csv(test_output_path, sep=\\'\\t\\', index=False)\\nval_final_updated.to_csv(output_val_path, sep=\\'\\t\\', index=False)\\n\\n# === 6. Kontrolle\\ndf_test_check = pd.read_csv(test_output_path, sep=\\'\\t\\')\\ndf_val_check = pd.read_csv(output_val_path, sep=\\'\\t\\')\\n\\nprint(\"\\n📊 Neue Testverteilung:\")\\nprint(df_test_check.groupby([\\'task\\', \\'Binding\\']).size().unstack(fill_value=0))\\n\\nprint(\"\\n📊 Neue Validationverteilung:\")\\nprint(df_val_check.groupby([\\'task\\', \\'Binding\\']).size().unstack(fill_value=0))\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# === Shift von TPP3 und TPP4 aus Val zu Test, weil dort zu wenig\n",
    "output_val_path   = f\"{pipeline_data_splitted}/{precision}/beta/new/validation.tsv\"\n",
    "test_output_path  = f\"{pipeline_data_splitted}/{precision}/beta/new/test.tsv\"\n",
    "\n",
    "val_final   = pd.read_csv(output_val_path, sep='\\t')\n",
    "test_final  = pd.read_csv(test_output_path, sep='\\t')\n",
    "\n",
    "print(\"\\n📊 Alte Testverteilung:\")\n",
    "print(test_final.groupby(['task', 'Binding']).size().unstack(fill_value=0))\n",
    "\n",
    "print(\"\\n📊 Alte Validationverteilung:\")\n",
    "print(val_final.groupby(['task', 'Binding']).size().unstack(fill_value=0))\n",
    "\n",
    "# === 1. Finde TPP3/TPP4 NON-BINDER in val_final\n",
    "val_tpp3_neg = val_final[\n",
    "    (val_final['Binding'] == 0) & (val_final['task'] == 'TPP3')\n",
    "]\n",
    "\n",
    "val_tpp4_neg = val_final[\n",
    "    (val_final['Binding'] == 0) & (val_final['task'] == 'TPP4')\n",
    "]\n",
    "\n",
    "# === 2. Ziehe jeweils 50%\n",
    "# === Für TPP3: (Epitope, TRB_CDR3) einmalig\n",
    "val_tpp3_neg[\"pair\"] = val_tpp3_neg[['Epitope', 'TRB_CDR3']].apply(tuple, axis=1)\n",
    "pair_counts = val_tpp3_neg[\"pair\"].value_counts()\n",
    "val_tpp3_neg[\"pair_count\"] = val_tpp3_neg[\"pair\"].map(pair_counts)\n",
    "\n",
    "unique_pairs = val_tpp3_neg[val_tpp3_neg[\"pair_count\"] == 1]\n",
    "dupe_pairs   = val_tpp3_neg[val_tpp3_neg[\"pair_count\"] > 1]\n",
    "\n",
    "target_tpp3 = int(len(val_tpp3_neg) * 0.5)\n",
    "\n",
    "val_tpp3_half = pd.concat([\n",
    "    unique_pairs,\n",
    "    dupe_pairs.sample(n=max(0, target_tpp3 - len(unique_pairs)), random_state=42)\n",
    "], ignore_index=True)\n",
    "\n",
    "# === Für TPP4: Epitope einmalig\n",
    "epi_counts = val_tpp4_neg[\"Epitope\"].value_counts()\n",
    "val_tpp4_neg[\"epi_count\"] = val_tpp4_neg[\"Epitope\"].map(epi_counts)\n",
    "\n",
    "unique_epi = val_tpp4_neg[val_tpp4_neg[\"epi_count\"] == 1]\n",
    "dupe_epi   = val_tpp4_neg[val_tpp4_neg[\"epi_count\"] > 1]\n",
    "\n",
    "target_tpp4 = int(len(val_tpp4_neg) * 0.5)\n",
    "\n",
    "val_tpp4_half = pd.concat([\n",
    "    unique_epi,\n",
    "    dupe_epi.sample(n=max(0, target_tpp4 - len(unique_epi)), random_state=42)\n",
    "], ignore_index=True)\n",
    "\n",
    "\n",
    "tpp3_4_half = pd.concat([val_tpp3_half, val_tpp4_half])  # kein ignore_index\n",
    "\n",
    "print(f\"✅ Verschiebe {len(tpp3_4_half)} TPP3/4-Non-Binder von Validation → Test\")\n",
    "\n",
    "# === 3. In Testset einfügen\n",
    "test_final_updated = pd.concat([test_final, tpp3_4_half], ignore_index=True)\n",
    "\n",
    "# === 4. Entferne aus val_final alle Zeilen mit denselben (Epitope, TRB_CDR3)\n",
    "moved_pairs = set(tpp3_4_half[['Epitope', 'TRB_CDR3']].apply(tuple, axis=1))\n",
    "\n",
    "val_final_updated = val_final[\n",
    "    ~val_final[['Epitope', 'TRB_CDR3']].apply(tuple, axis=1).isin(moved_pairs)\n",
    "]\n",
    "\n",
    "# === 5. Speichern\n",
    "test_final_updated.to_csv(test_output_path, sep='\\t', index=False)\n",
    "val_final_updated.to_csv(output_val_path, sep='\\t', index=False)\n",
    "\n",
    "# === 6. Kontrolle\n",
    "df_test_check = pd.read_csv(test_output_path, sep='\\t')\n",
    "df_val_check = pd.read_csv(output_val_path, sep='\\t')\n",
    "\n",
    "print(\"\\n📊 Neue Testverteilung:\")\n",
    "print(df_test_check.groupby(['task', 'Binding']).size().unstack(fill_value=0))\n",
    "\n",
    "print(\"\\n📊 Neue Validationverteilung:\")\n",
    "print(df_val_check.groupby(['task', 'Binding']).size().unstack(fill_value=0))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Classification \n",
    "The classification in the split notebook correct for positive only data. After adding negative data, some classifications might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta/new'\n",
    "test_file_name = 'test.tsv'\n",
    "validation_file_name = 'validation.tsv'\n",
    "train_file_name = 'train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/1815672389.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_data_path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 82726 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 76994 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 600 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 42 TPP4 tasks (seen tcr & unseen epitope).\n"
     ]
    }
   ],
   "source": [
    "# do the classification for paired data\n",
    "paired = True\n",
    "test_data_path = f'{paired_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "train_data_path = f'{paired_output_folder}/{train_file_name}'\n",
    "\n",
    "%run ../data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allele\n",
      "../../data/splitted_datasets/allele/paired/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/1788404633.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_path, sep=\"\\t\", index_col=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train+validate data has 72656 entries\n",
      "test data has 160362 entries\n",
      "test data has 128885 TPP1 tasks (old value: 82726) (seen tcr & seen epitopes).\n",
      "test data has 30835 TPP2 tasks (old value: 76994) (unseen tcr & seen epitopes).\n",
      "test data has 508 TPP3 tasks (old value: 600) (unseen tcr & unseen epitope).\n",
      "test data has 134 TPP4 tasks (old value: 42) (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.31180423829918724/0.6881957617008128\n",
      "../../data/splitted_datasets/allele/paired/test_reclassified_paired_specific.tsv\n",
      "/home/ubuntu/arina/BA-Cancer-Immunotherapy\n",
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/wandb/run-20250310_171148-c8scp4az</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">kind-sun-24</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../data/splitted_datasets/allele/paired)... Done. 0.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1d4067a88546039870f6c520cee5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-sun-24</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/c8scp4az</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250310_171148-c8scp4az/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extended classification for paired data\n",
    "train_path = f'{paired_output_folder}/{train_file_name}'\n",
    "validation_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "test_path = f'{paired_output_folder}/{test_file_name}'\n",
    "output_path = f'{paired_output_folder}/test_reclassified_paired_specific.tsv'\n",
    "paired_data_path = paired_output_folder\n",
    "alpha_cdr3_name = 'TRA_CDR3'\n",
    "beta_cdr3_name = 'TRB_CDR3'\n",
    "epitope_name = 'Epitope'\n",
    "task_name = 'task'\n",
    "\n",
    "%run ../data_preparation/paired_reclassification_testonly.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Bevor Testverteilung:\n",
      "Binding      0\n",
      "task          \n",
      "TPP1     21859\n",
      "TPP2       665\n",
      "TPP4        27\n",
      "TPP1  TPP4 0 IVTDFSVIK CASSVDGNTEAFF\n",
      "TPP1  TPP4 0 FLYALALLL CASRSLAPARPSEQFF\n",
      "TPP1  TPP4 0 ELAGIGILTV CASSLLAGGPNEQFF\n",
      "TPP1  TPP4 0 IYSKHTPINL CASSYRTGSSYNEQFF\n",
      "TPP1  TPP4 0 AVFDRKSDAK CASSYSKAGGPGEDTQYF\n",
      "TPP1  TPP4 0 MIELSLIDFYLCFLAFLLFLVLIML CASSEGTGLYEQYF\n",
      "TPP1  TPP4 0 ELAGIGILTV CASRGNSYEQYF\n",
      "TPP1  TPP4 0 ELAGIGILTV CASRGNSYEQYF\n",
      "TPP1  TPP4 0 QYDPVAALF CASSSQEGIEAFF\n",
      "TPP1  TPP4 0 YLNDHLEPWI CASSLPRAGGTYEQYF\n",
      "TPP1  TPP4 0 AVFDRKSDAK CASSLDTLSYNEQFF\n",
      "TPP1  TPP4 0 QPRAPIRPI CASSLGGLAKQETQYF\n",
      "TPP1  TPP4 0 RIAAWMATY CASSLASGGEQFF\n",
      "TPP1  TPP4 0 ELAGIGILTV CASSLPWAGVLNTEAFF\n",
      "TPP1  TPP4 0 ELAGIGILTV CASSLLAGAGETQYF\n",
      "TPP1  TPP4 0 SLEGGGLGY CASSLLLANSYNEQFF\n",
      "TPP1  TPP4 0 QPRAPIRPI CASSLPWAGVLNTEAFF\n",
      "TPP1  TPP4 0 KLGGALQAK CAFQEASYGYTF\n",
      "TPP1  TPP4 0 RIAAWMATY CASSEMAGGLEAFF\n",
      "TPP1  TPP4 0 ELAGIGILTV CASSEMTGNTEAFF\n",
      "TPP1  TPP4 0 IYSKHTPINL CASSYRTGSSYNEQFF\n",
      "TPP1  TPP4 0 KVLEYVIKV CASSEMTGNTEAFF\n",
      "TPP1  TPP4 0 GILGFVFTL CASSFLGTGLNEQYF\n",
      "TPP1  TPP4 0 GILGFVFTL CASSAGGNTIYF\n",
      "TPP1  TPP4 0 GLCTLVAML CASRWTGKDSNQPQHF\n",
      "TPP1  TPP4 0 GILGFVFTL CASSLGRGSTDTQYF\n",
      "TPP1  TPP4 0 AYAQKIFKI CASSLGAGGPYTEAFF\n",
      "\n",
      "📊 Danach Testverteilung:\n",
      "Binding      0     1\n",
      "task                \n",
      "TPP1     41276  1059\n",
      "TPP2      3135  7728\n",
      "TPP3        10    88\n",
      "TPP4        79    25\n",
      "test data has 42335 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 10863 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 98 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 104 TPP4 tasks (seen tcr & unseen epitope).\n"
     ]
    }
   ],
   "source": [
    "# do the classification for beta data\n",
    "paired = False\n",
    "train_data_path = f'{beta_output_folder}/{train_file_name}'\n",
    "validation_data_path = f'{beta_output_folder}/{validation_file_name}'\n",
    "test_data_path = f'{beta_output_folder}/{test_file_name}'\n",
    "\n",
    "%run ../data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells the classification is checked. If the output says \"Classification is correct\", everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/788315683.py:21: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(test_file, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train+validate data has 72656 entries\n",
      "test data has 160362 entries\n",
      "test data has 82726 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 76994 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 600 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 42 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.31180423829918724/0.6881957617008128\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    160362\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification paired\n",
    "splitted_data_path = paired_output_folder\n",
    "\n",
    "%run ../data_preparation/check_task_classification_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 840756 entries\n",
      "test data has 53400 entries\n",
      "test data has 42335 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 10863 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 98 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 104 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.9516456769061926/0.04835432309380739\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    53400\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification beta\n",
    "splitted_data_path = beta_output_folder\n",
    "\n",
    "%run ../data_preparation/check_task_classification_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'validation_prenegsamples.tsv', 'test.tsv', 'train.tsv', 'test_reclassified_paired_specific.tsv', 'validation.tsv', 'train_prenegsamples.tsv', 'validate_reclassified_paired_specific.tsv', 'test_prenegsamples.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(path_to_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/data_scripts/datapipeline/wandb/run-20250412_130314-xveriwmf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xveriwmf' target=\"_blank\">wobbly-bush-211</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xveriwmf' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xveriwmf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../../../data/splitted_datasets/allele/paired)... Done. 0.2s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-bush-211</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xveriwmf' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/xveriwmf</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_130314-xveriwmf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# upload paired data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "dataset_name = f'paired_{precision}'\n",
    "#main_project_name = os.getenv(\"MAIN_PROJECT_NAME\")\n",
    "main_project_name = f\"dataset-{precision}\"\n",
    "\n",
    "%run ../upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-allele\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/arina/BA-Cancer-Immunotherapy/data_scripts/datapipeline/wandb/run-20250412_130402-a3v84x0s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/a3v84x0s' target=\"_blank\">dainty-planet-213</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/a3v84x0s' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/a3v84x0s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../../../../data/splitted_datasets/allele/beta)... Done. 1.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255544618b5544ec96db1028d6a53abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.010 MB of 0.010 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-planet-213</strong> at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/a3v84x0s' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/a3v84x0s</a><br/> View project at: <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_130402-a3v84x0s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload beta data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "dataset_name = f'beta_{precision}'\n",
    "\n",
    "%run ../upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings >> ProtBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Sollte True zurückgeben\n",
    "print(torch.version.cuda)  # Sollte die richtige CUDA-Version anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102220/1782043182.py:11: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_paired_test = pd.read_csv(path_paired_test, sep=\"\\t\", index_col=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Loading: Rostlab/prot_bert\n",
      "Model is on device: cuda:0\n",
      "Processing Batch:  0 64\n",
      "Processing Batch:  64 128\n",
      "Processing Batch:  128 192\n",
      "Processing Batch:  192 256\n",
      "Processing Batch:  256 320\n",
      "Processing Batch:  320 384\n",
      "Processing Batch:  384 448\n",
      "Processing Batch:  448 512\n",
      "Processing Batch:  512 576\n",
      "Processing Batch:  576 640\n",
      "Processing Batch:  640 704\n",
      "Processing Batch:  704 768\n",
      "Processing Batch:  768 832\n",
      "Processing Batch:  832 896\n",
      "Processing Batch:  896 960\n",
      "Processing Batch:  960 1024\n",
      "Processing Batch:  1024 1088\n",
      "Processing Batch:  1088 1152\n",
      "Processing Batch:  1152 1216\n",
      "Processing Batch:  1216 1280\n",
      "Processing Batch:  1280 1344\n",
      "Processing Batch:  1344 1408\n",
      "Processing Batch:  1408 1472\n",
      "Processing Batch:  1472 1536\n",
      "Processing Batch:  1536 1600\n",
      "Processing Batch:  1600 1664\n",
      "Processing Batch:  1664 1728\n",
      "Processing Batch:  1728 1792\n",
      "Processing Batch:  1792 1856\n",
      "Processing Batch:  1856 1920\n",
      "Processing Batch:  1920 1984\n",
      "Processing Batch:  1984 2048\n",
      "Processing Batch:  2048 2112\n",
      "Processing Batch:  2112 2176\n",
      "Processing Batch:  2176 2240\n",
      "Processing Batch:  2240 2304\n",
      "Processing Batch:  2304 2368\n",
      "Processing Batch:  2368 2432\n",
      "Processing Batch:  2432 2496\n",
      "Processing Batch:  2496 2560\n",
      "Processing Batch:  2560 2624\n",
      "Processing Batch:  2624 2688\n",
      "Processing Batch:  2688 2752\n",
      "Processing Batch:  2752 2816\n",
      "Processing Batch:  2816 2880\n",
      "Processing Batch:  2880 2944\n",
      "Processing Batch:  2944 3008\n",
      "Processing Batch:  3008 3072\n",
      "Processing Batch:  3072 3136\n",
      "Processing Batch:  3136 3200\n",
      "Processing Batch:  3200 3264\n",
      "Processing Batch:  3264 3328\n",
      "Processing Batch:  3328 3392\n",
      "Processing Batch:  3392 3456\n",
      "Processing Batch:  3456 3520\n",
      "Processing Batch:  3520 3584\n",
      "Processing Batch:  3584 3648\n",
      "Processing Batch:  3648 3712\n",
      "Processing Batch:  3712 3776\n",
      "Processing Batch:  3776 3840\n",
      "Processing Batch:  3840 3904\n",
      "Processing Batch:  3904 3968\n",
      "Processing Batch:  3968 4032\n",
      "Processing Batch:  4032 4096\n",
      "Processing Batch:  4096 4160\n",
      "Processing Batch:  4160 4224\n",
      "Processing Batch:  4224 4288\n",
      "Processing Batch:  4288 4352\n",
      "Processing Batch:  4352 4416\n",
      "Processing Batch:  4416 4480\n",
      "Processing Batch:  4480 4544\n",
      "Processing Batch:  4544 4608\n",
      "Processing Batch:  4608 4672\n",
      "Processing Batch:  4672 4736\n",
      "Processing Batch:  4736 4800\n",
      "Processing Batch:  4800 4864\n",
      "Processing Batch:  4864 4928\n",
      "Processing Batch:  4928 4992\n",
      "Processing Batch:  4992 5056\n",
      "Processing Batch:  5056 5120\n",
      "Processing Batch:  5120 5184\n",
      "Processing Batch:  5184 5248\n",
      "Processing Batch:  5248 5312\n",
      "Processing Batch:  5312 5376\n",
      "Processing Batch:  5376 5440\n",
      "Processing Batch:  5440 5504\n",
      "Processing Batch:  5504 5568\n",
      "Processing Batch:  5568 5632\n",
      "Processing Batch:  5632 5696\n",
      "Processing Batch:  5696 5760\n",
      "Processing Batch:  5760 5824\n",
      "Processing Batch:  5824 5888\n",
      "Processing Batch:  5888 5952\n",
      "Processing Batch:  5952 6016\n",
      "Processing Batch:  6016 6080\n",
      "Processing Batch:  6080 6144\n",
      "Processing Batch:  6144 6208\n",
      "Processing Batch:  6208 6272\n",
      "Processing Batch:  6272 6336\n",
      "Processing Batch:  6336 6400\n",
      "Processing Batch:  6400 6464\n",
      "Processing Batch:  6464 6528\n",
      "Processing Batch:  6528 6592\n",
      "Processing Batch:  6592 6656\n",
      "Processing Batch:  6656 6720\n",
      "Processing Batch:  6720 6784\n",
      "Processing Batch:  6784 6848\n",
      "Processing Batch:  6848 6912\n",
      "Processing Batch:  6912 6976\n",
      "Processing Batch:  6976 7040\n",
      "Processing Batch:  7040 7104\n",
      "Processing Batch:  7104 7168\n",
      "Processing Batch:  7168 7232\n",
      "Processing Batch:  7232 7296\n",
      "Processing Batch:  7296 7360\n",
      "Processing Batch:  7360 7424\n",
      "Processing Batch:  7424 7488\n",
      "Processing Batch:  7488 7552\n",
      "Processing Batch:  7552 7616\n",
      "Processing Batch:  7616 7680\n",
      "Processing Batch:  7680 7744\n",
      "Processing Batch:  7744 7808\n",
      "Processing Batch:  7808 7872\n",
      "Processing Batch:  7872 7936\n",
      "Processing Batch:  7936 8000\n",
      "Processing Batch:  8000 8064\n",
      "Processing Batch:  8064 8128\n",
      "Processing Batch:  8128 8192\n",
      "Processing Batch:  8192 8256\n",
      "Processing Batch:  8256 8320\n",
      "Processing Batch:  8320 8384\n",
      "Processing Batch:  8384 8448\n",
      "Processing Batch:  8448 8512\n",
      "Processing Batch:  8512 8576\n",
      "Processing Batch:  8576 8640\n",
      "Processing Batch:  8640 8704\n",
      "Processing Batch:  8704 8768\n",
      "Processing Batch:  8768 8832\n",
      "Processing Batch:  8832 8896\n",
      "Processing Batch:  8896 8960\n",
      "Processing Batch:  8960 9024\n",
      "Processing Batch:  9024 9088\n",
      "Processing Batch:  9088 9152\n",
      "Processing Batch:  9152 9216\n"
     ]
    }
   ],
   "source": [
    "path_paired_test = f\"{pipeline_data_splitted}/{precision}/paired/test.tsv\"\n",
    "path_paired_validation = f\"{pipeline_data_splitted}/{precision}/paired/validation.tsv\"\n",
    "path_paired_train = f\"{pipeline_data_splitted}/{precision}/paired/train.tsv\"\n",
    "path_beta_test = f\"{pipeline_data_splitted}/{precision}/beta/test.tsv\"\n",
    "path_beta_validation = f\"{pipeline_data_splitted}/{precision}/beta/validation.tsv\"\n",
    "path_beta_train = f\"{pipeline_data_splitted}/{precision}/beta/train.tsv\"\n",
    "\n",
    "\n",
    "path_paired = f\"{pipeline_data}/embeddings/temp/{precision}/paired_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_paired)])\n",
    "df_paired_test = pd.read_csv(path_paired_test, sep=\"\\t\", index_col=False)\n",
    "df_paired_validation = pd.read_csv(path_paired_validation, sep=\"\\t\", index_col=False)\n",
    "df_paired_train = pd.read_csv(path_paired_train, sep=\"\\t\", index_col=False)\n",
    "df_paired = pd.concat([df_paired_test, df_paired_validation, df_paired_train])\n",
    "df_paired.to_csv(path_paired, sep=\"\\t\", index=False)\n",
    "\n",
    "# paired\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz TRA_CDR3\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz TRB_CDR3\n",
    "#%run ../generateEmbeddingsProtBERT.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz Epitope\n",
    "\n",
    "path_beta = f\"{pipeline_data}/embeddings/temp/{precision}/beta_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_beta)])\n",
    "df_beta_test = pd.read_csv(path_beta_test, sep=\"\\t\", index_col=False)\n",
    "df_beta_validation = pd.read_csv(path_beta_validation, sep=\"\\t\", index_col=False)\n",
    "df_beta_train = pd.read_csv(path_beta_train, sep=\"\\t\", index_col=False)\n",
    "df_beta = pd.concat([df_beta_test, df_beta_validation, df_beta_train])\n",
    "df_beta.to_csv(path_beta, sep=\"\\t\", index=False)\n",
    "\n",
    "# beta\n",
    "%run ../generateEmbeddingsProtBERT.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/TRB_beta_embeddings.npz TRB_CDR3\n",
    "%run ../generateEmbeddingsProtBERT.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/Epitope_beta_embeddings.npz Epitope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 TRA Embedding Shape: (596417, 1024)\n",
      "📌 TRB Embedding Shape: (694427, 1024)\n",
      "📌 Epitope Embedding Shape: (12870, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Funktion, um Embeddings korrekt zu laden\n",
    "def load_embeddings(file_path):\n",
    "    npz_data = np.load(file_path)\n",
    "    all_keys = list(npz_data.keys())\n",
    "\n",
    "    # Falls Embeddings als einzelne Sequenzen gespeichert sind\n",
    "    if len(all_keys) > 1:\n",
    "        all_values = [npz_data[k] for k in all_keys]\n",
    "        return np.vstack(all_values)  # Alles zusammenfügen\n",
    "    else:\n",
    "        return npz_data[all_keys[0]]\n",
    "\n",
    "# Embeddings für TRA, TRB und Epitope laden\n",
    "tra_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz\")\n",
    "trb_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz\")\n",
    "epitope_embeddings = load_embeddings(f\"{pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz\")\n",
    "\n",
    "# Ausgabe der finalen Shapes\n",
    "print(f\"📌 TRA Embedding Shape: {tra_embeddings.shape}\")\n",
    "print(f\"📌 TRB Embedding Shape: {trb_embeddings.shape}\")\n",
    "print(f\"📌 Epitope Embedding Shape: {epitope_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93065/3207364397.py:24: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 67422 (Binding=1: 33711, Binding=0: 33711, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 43356 (Binding=1: 7226, Binding=0: 36130, TPP1: 27972, TPP2: 15095, TPP3: 289)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 43344 (Binding=1: 7224, Binding=0: 36120, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 154122\n",
      "\n",
      "--- Beta Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 251750 (Binding=1: 125875, Binding=0: 125875, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 161844 (Binding=1: 26974, Binding=0: 134870, TPP1: 140896, TPP2: 20645, TPP3: 299)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 161838 (Binding=1: 26973, Binding=0: 134865, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 575432\n",
      "\n",
      "       Dataset   Train  Train_Binding_1  Train_Binding_0  Train_TPP1  \\\n",
      "0  Paired Gene   67422            33711            33711           0   \n",
      "1    Beta Gene  251750           125875           125875           0   \n",
      "\n",
      "   Train_TPP2  Train_TPP3  Train_TPP4    Test  Test_Binding_1  ...  Test_TPP3  \\\n",
      "0           0           0           0   43356            7226  ...        289   \n",
      "1           0           0           0  161844           26974  ...        299   \n",
      "\n",
      "   Test_TPP4  Validation  Validation_Binding_1  Validation_Binding_0  \\\n",
      "0          0       43344                  7224                 36120   \n",
      "1          4      161838                 26973                134865   \n",
      "\n",
      "   Validation_TPP1  Validation_TPP2  Validation_TPP3  Validation_TPP4   Total  \n",
      "0                0                0                0                0  154122  \n",
      "1                0                0                0                0  575432  \n",
      "\n",
      "[2 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = pipeline_data_splitted\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen für jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "    \n",
    "    # Zähle die Anzahl der Bindings 1 und 0 in jedem Datensatz\n",
    "    train_binding_counts = train_df['Binding'].value_counts()\n",
    "    test_binding_counts = test_df['Binding'].value_counts()\n",
    "    validation_binding_counts = validation_df['Binding'].value_counts()\n",
    "    \n",
    "    # Zähle die Anzahl der TPP1, TPP2, TPP3 Einträge in jedem Datensatz\n",
    "    train_task_counts = train_df['task'].value_counts()\n",
    "    test_task_counts = test_df['task'].value_counts()\n",
    "    validation_task_counts = validation_df['task'].value_counts()\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Train_Binding_1\": train_binding_counts.get(1, 0),\n",
    "        \"Train_Binding_0\": train_binding_counts.get(0, 0),\n",
    "        \"Train_TPP1\": train_task_counts.get(\"TPP1\", 0),\n",
    "        \"Train_TPP2\": train_task_counts.get(\"TPP2\", 0),\n",
    "        \"Train_TPP3\": train_task_counts.get(\"TPP3\", 0),\n",
    "        \"Train_TPP4\": train_task_counts.get(\"TPP4\", 0),\n",
    "        \"Test\": test_length,\n",
    "        \"Test_Binding_1\": test_binding_counts.get(1, 0),\n",
    "        \"Test_Binding_0\": test_binding_counts.get(0, 0),\n",
    "        \"Test_TPP1\": test_task_counts.get(\"TPP1\", 0),\n",
    "        \"Test_TPP2\": test_task_counts.get(\"TPP2\", 0),\n",
    "        \"Test_TPP3\": test_task_counts.get(\"TPP3\", 0),\n",
    "        \"Test_TPP4\": test_task_counts.get(\"TPP4\", 0),\n",
    "        \"Validation\": validation_length,\n",
    "        \"Validation_Binding_1\": validation_binding_counts.get(1, 0),\n",
    "        \"Validation_Binding_0\": validation_binding_counts.get(0, 0),\n",
    "        \"Validation_TPP1\": validation_task_counts.get(\"TPP1\", 0),\n",
    "        \"Validation_TPP2\": validation_task_counts.get(\"TPP2\", 0),\n",
    "        \"Validation_TPP3\": validation_task_counts.get(\"TPP3\", 0),\n",
    "        \"Validation_TPP4\": validation_task_counts.get(\"TPP4\", 0),\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]} (Binding=1: {lengths[\"Train_Binding_1\"]}, Binding=0: {lengths[\"Train_Binding_0\"]}, TPP1: {lengths[\"Train_TPP1\"]}, TPP2: {lengths[\"Train_TPP2\"]}, TPP3: {lengths[\"Train_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]} (Binding=1: {lengths[\"Test_Binding_1\"]}, Binding=0: {lengths[\"Test_Binding_0\"]}, TPP1: {lengths[\"Test_TPP1\"]}, TPP2: {lengths[\"Test_TPP2\"]}, TPP3: {lengths[\"Test_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]} (Binding=1: {lengths[\"Validation_Binding_1\"]}, Binding=0: {lengths[\"Validation_Binding_0\"]}, TPP1: {lengths[\"Validation_TPP1\"]}, TPP2: {lengths[\"Validation_TPP2\"]}, TPP3: {lengths[\"Validation_TPP3\"]})')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')\n",
    "\n",
    "# Optional: Ergebnisse in einer Übersichtstabelle darstellen\n",
    "summary_data = []\n",
    "for dataset, lengths in results.items():\n",
    "    summary_data.append({\n",
    "        \"Dataset\": dataset.replace(\"_\", \" \").title(),\n",
    "        \"Train\": lengths[\"Train\"],\n",
    "        \"Train_Binding_1\": lengths[\"Train_Binding_1\"],\n",
    "        \"Train_Binding_0\": lengths[\"Train_Binding_0\"],\n",
    "        \"Train_TPP1\": lengths[\"Train_TPP1\"],\n",
    "        \"Train_TPP2\": lengths[\"Train_TPP2\"],\n",
    "        \"Train_TPP3\": lengths[\"Train_TPP3\"],\n",
    "        \"Train_TPP4\": lengths[\"Train_TPP4\"],\n",
    "        \"Test\": lengths[\"Test\"],\n",
    "        \"Test_Binding_1\": lengths[\"Test_Binding_1\"],\n",
    "        \"Test_Binding_0\": lengths[\"Test_Binding_0\"],\n",
    "        \"Test_TPP1\": lengths[\"Test_TPP1\"],\n",
    "        \"Test_TPP2\": lengths[\"Test_TPP2\"],\n",
    "        \"Test_TPP3\": lengths[\"Test_TPP3\"],\n",
    "        \"Test_TPP4\": lengths[\"Test_TPP4\"],\n",
    "        \"Validation\": lengths[\"Validation\"],\n",
    "        \"Validation_Binding_1\": lengths[\"Validation_Binding_1\"],\n",
    "        \"Validation_Binding_0\": lengths[\"Validation_Binding_0\"],\n",
    "        \"Validation_TPP1\": lengths[\"Validation_TPP1\"],\n",
    "        \"Validation_TPP2\": lengths[\"Validation_TPP2\"],\n",
    "        \"Validation_TPP3\": lengths[\"Validation_TPP3\"],\n",
    "        \"Validation_TPP4\": lengths[\"Validation_TPP4\"],\n",
    "        \"Total\": lengths[\"Total\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/splitted_datasets/allele/paired/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, paths \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Daten laden\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m     validation_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/splitted_datasets/allele/paired/train.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = pipeline_data_splitted\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"paired_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    },\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen für jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "    \n",
    "    # Zähle die Anzahl der Bindings 1 und 0 in jedem Datensatz\n",
    "    train_binding_counts = train_df['Binding'].value_counts()\n",
    "    test_binding_counts = test_df['Binding'].value_counts()\n",
    "    validation_binding_counts = validation_df['Binding'].value_counts()\n",
    "    \n",
    "    # Zähle die Anzahl der TPP1, TPP2, TPP3 Einträge in jedem Datensatz\n",
    "    train_task_counts = train_df['task'].value_counts()\n",
    "    test_task_counts = test_df['task'].value_counts()\n",
    "    validation_task_counts = validation_df['task'].value_counts()\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Train_Binding_1\": train_binding_counts.get(1, 0),\n",
    "        \"Train_Binding_0\": train_binding_counts.get(0, 0),\n",
    "        \"Train_TPP1\": train_task_counts.get(\"TPP1\", 0),\n",
    "        \"Train_TPP2\": train_task_counts.get(\"TPP2\", 0),\n",
    "        \"Train_TPP3\": train_task_counts.get(\"TPP3\", 0),\n",
    "        \"Train_TPP4\": train_task_counts.get(\"TPP4\", 0),\n",
    "        \"Test\": test_length,\n",
    "        \"Test_Binding_1\": test_binding_counts.get(1, 0),\n",
    "        \"Test_Binding_0\": test_binding_counts.get(0, 0),\n",
    "        \"Test_TPP1\": test_task_counts.get(\"TPP1\", 0),\n",
    "        \"Test_TPP2\": test_task_counts.get(\"TPP2\", 0),\n",
    "        \"Test_TPP3\": test_task_counts.get(\"TPP3\", 0),\n",
    "        \"Test_TPP4\": test_task_counts.get(\"TPP4\", 0),\n",
    "        \"Validation\": validation_length,\n",
    "        \"Validation_Binding_1\": validation_binding_counts.get(1, 0),\n",
    "        \"Validation_Binding_0\": validation_binding_counts.get(0, 0),\n",
    "        \"Validation_TPP1\": validation_task_counts.get(\"TPP1\", 0),\n",
    "        \"Validation_TPP2\": validation_task_counts.get(\"TPP2\", 0),\n",
    "        \"Validation_TPP3\": validation_task_counts.get(\"TPP3\", 0),\n",
    "        \"Validation_TPP4\": validation_task_counts.get(\"TPP4\", 0),\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]} (Binding=1: {lengths[\"Train_Binding_1\"]}, Binding=0: {lengths[\"Train_Binding_0\"]}, TPP1: {lengths[\"Train_TPP1\"]}, TPP2: {lengths[\"Train_TPP2\"]}, TPP3: {lengths[\"Train_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]} (Binding=1: {lengths[\"Test_Binding_1\"]}, Binding=0: {lengths[\"Test_Binding_0\"]}, TPP1: {lengths[\"Test_TPP1\"]}, TPP2: {lengths[\"Test_TPP2\"]}, TPP3: {lengths[\"Test_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]} (Binding=1: {lengths[\"Validation_Binding_1\"]}, Binding=0: {lengths[\"Validation_Binding_0\"]}, TPP1: {lengths[\"Validation_TPP1\"]}, TPP2: {lengths[\"Validation_TPP2\"]}, TPP3: {lengths[\"Validation_TPP3\"]})')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')\n",
    "\n",
    "# Optional: Ergebnisse in einer Übersichtstabelle darstellen\n",
    "summary_data = []\n",
    "for dataset, lengths in results.items():\n",
    "    summary_data.append({\n",
    "        \"Dataset\": dataset.replace(\"_\", \" \").title(),\n",
    "        \"Train\": lengths[\"Train\"],\n",
    "        \"Train_Binding_1\": lengths[\"Train_Binding_1\"],\n",
    "        \"Train_Binding_0\": lengths[\"Train_Binding_0\"],\n",
    "        \"Train_TPP1\": lengths[\"Train_TPP1\"],\n",
    "        \"Train_TPP2\": lengths[\"Train_TPP2\"],\n",
    "        \"Train_TPP3\": lengths[\"Train_TPP3\"],\n",
    "        \"Train_TPP4\": lengths[\"Train_TPP4\"],\n",
    "        \"Test\": lengths[\"Test\"],\n",
    "        \"Test_Binding_1\": lengths[\"Test_Binding_1\"],\n",
    "        \"Test_Binding_0\": lengths[\"Test_Binding_0\"],\n",
    "        \"Test_TPP1\": lengths[\"Test_TPP1\"],\n",
    "        \"Test_TPP2\": lengths[\"Test_TPP2\"],\n",
    "        \"Test_TPP3\": lengths[\"Test_TPP3\"],\n",
    "        \"Test_TPP4\": lengths[\"Test_TPP4\"],\n",
    "        \"Validation\": lengths[\"Validation\"],\n",
    "        \"Validation_Binding_1\": lengths[\"Validation_Binding_1\"],\n",
    "        \"Validation_Binding_0\": lengths[\"Validation_Binding_0\"],\n",
    "        \"Validation_TPP1\": lengths[\"Validation_TPP1\"],\n",
    "        \"Validation_TPP2\": lengths[\"Validation_TPP2\"],\n",
    "        \"Validation_TPP3\": lengths[\"Validation_TPP3\"],\n",
    "        \"Validation_TPP4\": lengths[\"Validation_TPP4\"],\n",
    "        \"Total\": lengths[\"Total\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
