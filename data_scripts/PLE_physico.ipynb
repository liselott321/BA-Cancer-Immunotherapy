{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac013bf-9bda-4ce5-a4ba-1e617323f6a0",
   "metadata": {},
   "source": [
    "# Roh-Deskriptoren ‚Üí descriptor_physchem_raw.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e53c9-53ae-4dd3-a1c2-9615102bb170",
   "metadata": {},
   "source": [
    "1. Imports: Wir nutzen peptides f√ºr Deskriptoren, wandb zum Download, h5py zum Speichern.\n",
    "\n",
    "2. get_descriptors: Extrahiert physico-chemische Deskriptoren oder liefert bei Fehlern {}.\n",
    "\n",
    "3. W&B: L√§dt das neueste beta_allele-Artifact herunter.\n",
    "\n",
    "4. Daten laden: Train/Validation/Test als TSVs.\n",
    "\n",
    "5. Filtern: Nur TCR, Epitope und Binding.\n",
    "\n",
    "6. Deskriptoren extrahieren: Pro Sequenz mit Peptide(seq).descriptors().\n",
    "\n",
    "7. DataFrame bauen: Zwei DataFrames (tcr_, epi_), zusammenf√ºhren und Label anh√§ngen.\n",
    "\n",
    "8. Mapping speichern: Optional, damit Du Zeile‚Üí(TCR,Epitope) nachschlagen kannst.\n",
    "\n",
    "9. Arrays: tcr_arr, epi_arr, labels als NumPy Arrays.\n",
    "\n",
    "10. HDF5: Unter den Keys \"tcr_raw\", \"epi_raw\" und \"binding\" gespeichert.\n",
    "\n",
    "11. Abschluss: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfda212-af3c-4a01-b75d-e226c6436de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import wandb\n",
    "from peptides import Peptide\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31516ad4-7587-4123-8f92-f6e51b9434b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Hilfsfunktion zum Extrahieren der physico-chemischen Deskriptoren\n",
    "def get_descriptors(seq):\n",
    "    try:\n",
    "        return Peptide(seq).descriptors()\n",
    "    except Exception as e:\n",
    "        # bei fehlerhaften Sequenzen einfach leeres Dict\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a25441f-0dcb-4ce7-a3df-d58dfb209a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/kristina/BA-Cancer-Immunotherapy/data_scripts/wandb/run-20250504_125704-gcws9fsl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/gcws9fsl' target=\"_blank\">raw_physchem_export</a></strong> to <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/gcws9fsl' target=\"_blank\">https://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/gcws9fsl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1001.05MB. 46 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   46 of 46 files downloaded.  \n",
      "Done. 0:0:1.2\n"
     ]
    }
   ],
   "source": [
    "# 3) W&B initialisieren und Dataset-Artifact herunterladen\n",
    "wandb.init(\n",
    "    project=\"dataset-allele\",\n",
    "    entity=\"ba_cancerimmunotherapy\",\n",
    "    job_type=\"physchem_raw_export\",\n",
    "    name=\"raw_physchem_export\"\n",
    ")\n",
    "\n",
    "dataset_name = \"beta_allele\"\n",
    "artifact = wandb.use_artifact(f\"ba_cancerimmunotherapy/dataset-allele/{dataset_name}:latest\")\n",
    "data_dir = artifact.download(f\"./WnB_Experiments_Datasets/{dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bdbc5ac-ad2a-4dee-9bda-797459b92379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10308/9758599.py:9: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_val   = pd.read_csv(paths[\"validation\"], sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gesamt-Samples: 993,078\n"
     ]
    }
   ],
   "source": [
    "# 4) CSVs laden (Train / Val / Test)\n",
    "paths = {\n",
    "    \"train\":      os.path.join(data_dir, \"allele/train.tsv\"),\n",
    "    \"validation\": os.path.join(data_dir, \"allele/validation.tsv\"),\n",
    "    \"test\":       os.path.join(data_dir, \"allele/test.tsv\"),\n",
    "}\n",
    "\n",
    "df_train = pd.read_csv(paths[\"train\"],      sep=\"\\t\")\n",
    "df_val   = pd.read_csv(paths[\"validation\"], sep=\"\\t\")\n",
    "df_test  = pd.read_csv(paths[\"test\"],       sep=\"\\t\")\n",
    "\n",
    "# in einem DataFrame zusammenf√ºhren\n",
    "df_beta = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "print(f\"[INFO] Gesamt-Samples: {len(df_beta):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1018ef94-111e-4b52-992c-6242648541e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Nach Dropna: 993,078 Samples\n"
     ]
    }
   ],
   "source": [
    "# 5) Auf die ben√∂tigten Spalten reduzieren und fehlende Zeilen entfernen\n",
    "df_physchem = df_beta[[\"TRB_CDR3\", \"Epitope\", \"Binding\"]].dropna()\n",
    "print(f\"[INFO] Nach Dropna: {len(df_physchem):,} Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e3721-1eb6-4492-8c8b-8042577b7b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extrahiere TCR-Deskriptoren ‚Ä¶\n",
      "[INFO] Extrahiere Epitope-Deskriptoren ‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "# 6) Roh-Deskriptoren extrahieren\n",
    "print(\"[INFO] Extrahiere TCR-Deskriptoren ‚Ä¶\")\n",
    "tcr_desc = df_physchem[\"TRB_CDR3\"].apply(get_descriptors)\n",
    "print(\"[INFO] Extrahiere Epitope-Deskriptoren ‚Ä¶\")\n",
    "epi_desc = df_physchem[\"Epitope\"].apply(get_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb07716-8b35-48ba-9756-d7b30db54e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) In DataFrame umwandeln und zusammenf√ºhren\n",
    "tcr_df  = pd.DataFrame(tcr_desc.tolist()).add_prefix(\"tcr_\")\n",
    "epi_df  = pd.DataFrame(epi_desc.tolist()).add_prefix(\"epi_\")\n",
    "desc_df = pd.concat([tcr_df, epi_df], axis=1)\n",
    "desc_df[\"binding\"] = df_physchem[\"Binding\"].astype(np.float32).values\n",
    "\n",
    "print(f\"[INFO] Feature-Matrix: {desc_df.shape[0]}√ó{desc_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d80ba9-6e25-4485-80b0-0fcc89a35713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Mapping-Datei speichern (optional, zum Nachschlagen)\n",
    "mapping = df_physchem[[\"TRB_CDR3\", \"Epitope\"]].copy()\n",
    "mapping[\"idx\"] = np.arange(len(mapping))\n",
    "mapping_path = \"../../../data/physico/ple/physchem_raw_mapping.tsv\"\n",
    "\n",
    "os.makedirs(os.path.dirname(mapping_path), exist_ok=True)\n",
    "\n",
    "mapping.to_csv(mapping_path, sep=\"\\t\", index=False)\n",
    "print(f\"[INFO] Mapping gespeichert nach `{mapping_path}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684922f-75f9-4140-b73d-59f5f42316aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Arrays erzeugen\n",
    "tcr_arr  = desc_df.filter(like=\"tcr_\").to_numpy(dtype=np.float32)\n",
    "epi_arr  = desc_df.filter(like=\"epi_\").to_numpy(dtype=np.float32)\n",
    "labels   = desc_df[\"binding\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "print(f\"[INFO] tcr_arr shape = {tcr_arr.shape}\")\n",
    "print(f\"[INFO] epi_arr shape = {epi_arr.shape}\")\n",
    "print(f\"[INFO] labels  shape = {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e7124-96ad-40ff-9550-1d16fba34406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) In HDF5 schreiben\n",
    "output_path = \"../../../data/physico/ple/descriptor_physchem_raw.h5\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with h5py.File(output_path, \"w\") as h5f:\n",
    "    h5f.create_dataset(\"tcr_raw\",   data=tcr_arr,  compression=\"gzip\")\n",
    "    h5f.create_dataset(\"epi_raw\",   data=epi_arr,  compression=\"gzip\")\n",
    "    h5f.create_dataset(\"binding\",   data=labels,   compression=\"gzip\")\n",
    "print(f\"[INFO] Roh-Deskriptoren gespeichert in `{output_path}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e9709-9d5f-4c0e-a308-bea60250b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Run beenden\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d896c-4cbc-43cd-9d2c-3ef0d4ed8c88",
   "metadata": {},
   "source": [
    "# Autoencoder trainieren ‚Üí descriptor_physchem_ple.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd43f45-86fa-40e1-ae35-106f43ddb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup & HDF5‚ÄêCheck\n",
    "# Lade raw physico‚Äêchem Deskriptoren und pr√ºfe, dass nichts leer ist.\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Pfad zu Deinem gerade erzeugten HDF5\n",
    "RAW_H5 = \"../../../data/physico/ple/descriptor_physchem_raw.h5\"\n",
    "\n",
    "with h5py.File(RAW_H5, \"r\") as f:\n",
    "    tcr_raw = f[\"tcr_raw\"][:]     # (N, D_tcr)\n",
    "    epi_raw = f[\"epi_raw\"][:]     # (N, D_epi)\n",
    "    labels  = f[\"binding\"][:]      # (N,)\n",
    "\n",
    "print(f\"tcr_raw shape: {tcr_raw.shape}\")\n",
    "print(f\"epi_raw shape: {epi_raw.shape}\")\n",
    "print(f\"labels   shape: {labels.shape}\")\n",
    "\n",
    "assert tcr_raw.size>0 and epi_raw.size>0 and labels.size>0, \"üêû mindestens eins Deiner Arrays ist leer!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ead53c-ec1a-4d9c-9cfc-b2da317c57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) DataMatrix & Train/ValSplit\n",
    "# Kombiniere TCR+Epi Deskriptoren und lege einen einfachen Dataset/Loader an.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# gesamte Feature‚ÄêMatrix\n",
    "X = np.hstack([tcr_raw, epi_raw]).astype(np.float32)  # (N, D_total)\n",
    "\n",
    "# Tensor‚ÄêDatasets\n",
    "tensor_X = torch.from_numpy(X)\n",
    "dataset = TensorDataset(tensor_X)  # nur X, unsupervised\n",
    "loader  = DataLoader(dataset, batch_size=256, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1678f-570a-4873-bbd5-fac6384afbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Autoencoder‚ÄêDefinition (PLE)\n",
    "# Ein sehr einfacher Feed-Forward Autoencoder.  \n",
    "# **latent_dim** kannst Du z.B. auf 64, 128, 256 etc. setzen.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "input_dim  = X.shape[1]\n",
    "hidden_dim = input_dim // 2\n",
    "latent_dim = 128\n",
    "\n",
    "class PLEAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_rec = self.decoder(z)\n",
    "        return x_rec\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PLEAutoencoder().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf16fac-1cff-4580-b343-4ae5391b8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Training Loop\n",
    "# Wir optimieren MSE zwischen Input und Rekonstruktion.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "n_epochs = 30\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for (batch_X,) in loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(batch_X)\n",
    "        loss  = criterion(recon, batch_X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_X.size(0)\n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f\"Epoch {epoch:02d}/{n_epochs}, MSE = {epoch_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dade90-7c56-43a7-a64f-f8fae1ac5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Embedding extrahieren & speichern\n",
    "# Nun erzeugen wir f√ºr **alle** Samples den Latent‚ÄêCode und speichern  \n",
    "# (`ple_raw.h5`) mit `ple`, plus zum Synchronisieren wieder `binding`.\n",
    "\n",
    "# Lade raw nochmal, um Reihenfolge exakt beizubehalten\n",
    "with h5py.File(RAW_H5, \"r\") as f:\n",
    "    tcr_raw = f[\"tcr_raw\"][:]\n",
    "    epi_raw = f[\"epi_raw\"][:]\n",
    "    labels  = f[\"binding\"][:]\n",
    "\n",
    "X_all = np.hstack([tcr_raw, epi_raw]).astype(np.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Z = model.encoder(torch.from_numpy(X_all).to(device)).cpu().numpy()  # (N, latent_dim)\n",
    "\n",
    "# Ausgabe‚ÄêHDF5\n",
    "PLE_H5 = \"../../../data/physico/ple/descriptor_physchem_ple.h5\"\n",
    "with h5py.File(PLE_H5, \"w\") as f:\n",
    "    f.create_dataset(\"ple\",     data=Z,      compression=\"gzip\")\n",
    "    f.create_dataset(\"binding\", data=labels, compression=\"gzip\")\n",
    "\n",
    "print(f\" PLE‚ÄêEmbedding gespeichert in `{PLE_H5}` mit shape {Z.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
