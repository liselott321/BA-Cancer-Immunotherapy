{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings', 'cleaned_datasets', 'concatenated_datasets', 'splitted_datasets', 'temp', 'plain_datasets']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List all files and directories in the current directory\n",
    "# print(os.listdir())\n",
    "\n",
    "# List files in a specific directory\n",
    "print(os.listdir(\"./../../data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tcrpeg.TCRpeg import TCRpeg\n",
    "\n",
    "# Get the absolute path of 'TCRpeg' and add it to sys.path\n",
    "tcrpeg_path = os.path.abspath(\"../../data/embeddings/TCRpeg\")\n",
    "sys.path.append(tcrpeg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../data/splitted_datasets/gene/beta/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71184/2611398523.py:22: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have loaded the data, total training seqs : 250602\n",
      "Unmapped characters: ['*']\n",
      "Generating embeddings for TCRs...\n",
      "Processing batch 1/16, sequences 0 to 10000\n",
      "Processing batch 2/16, sequences 10000 to 20000\n",
      "Processing batch 3/16, sequences 20000 to 30000\n",
      "Processing batch 4/16, sequences 30000 to 40000\n",
      "Processing batch 5/16, sequences 40000 to 50000\n",
      "Processing batch 6/16, sequences 50000 to 60000\n",
      "Processing batch 7/16, sequences 60000 to 70000\n",
      "Processing batch 8/16, sequences 70000 to 80000\n",
      "Processing batch 9/16, sequences 80000 to 90000\n",
      "Processing batch 10/16, sequences 90000 to 100000\n",
      "Processing batch 11/16, sequences 100000 to 110000\n",
      "Processing batch 12/16, sequences 110000 to 120000\n",
      "Processing batch 13/16, sequences 120000 to 130000\n",
      "Processing batch 14/16, sequences 130000 to 140000\n",
      "Processing batch 15/16, sequences 140000 to 150000\n",
      "Processing batch 16/16, sequences 150000 to 155972\n",
      "Generating embeddings for Epitopes...\n",
      "Processing batch 1/1, sequences 0 to 1104\n",
      "Processing ../../data/splitted_datasets/gene/beta/validation.tsv\n",
      "Have loaded the data, total training seqs : 161716\n",
      "Unmapped characters: ['*']\n",
      "Generating embeddings for TCRs...\n",
      "Processing batch 1/6, sequences 0 to 10000\n",
      "Processing batch 2/6, sequences 10000 to 20000\n",
      "Processing batch 3/6, sequences 20000 to 30000\n",
      "Processing batch 4/6, sequences 30000 to 40000\n",
      "Processing batch 5/6, sequences 40000 to 50000\n",
      "Processing batch 6/6, sequences 50000 to 54165\n",
      "Generating embeddings for Epitopes...\n",
      "Processing batch 1/1, sequences 0 to 1159\n",
      "Processing ../../data/splitted_datasets/gene/beta/test.tsv\n",
      "Have loaded the data, total training seqs : 161723\n",
      "Unmapped characters: ['*']\n",
      "Generating embeddings for TCRs...\n",
      "Processing batch 1/6, sequences 0 to 10000\n",
      "Processing batch 2/6, sequences 10000 to 20000\n",
      "Processing batch 3/6, sequences 20000 to 30000\n",
      "Processing batch 4/6, sequences 30000 to 40000\n",
      "Processing batch 5/6, sequences 40000 to 50000\n",
      "Processing batch 6/6, sequences 50000 to 53949\n",
      "Generating embeddings for Epitopes...\n",
      "Processing batch 1/1, sequences 0 to 1152\n",
      "Embeddings saved: ../../data/embeddings/beta/gene/TCRPeg_tcr_embeddings.npz, ../../data/embeddings/beta/gene/TCRPeg_Epitope_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "# ACHTUNG: for allele and paired the code must be revised/checked. Do the for loops work as intended????\n",
    "\n",
    "# Define paths\n",
    "precisions = ['gene']\n",
    "levels = ['beta']\n",
    "datasets = ['train', 'validation', 'test']\n",
    "file_paths = []\n",
    "embedding_paths = []\n",
    "for precision in precisions:\n",
    "    for level in levels:\n",
    "        for dataset in datasets:\n",
    "            path_dataset = f'../../data/splitted_datasets/{precision}/{level}/{dataset}.tsv'\n",
    "            file_paths.append(path_dataset)\n",
    "            path_embedding_epitope = f'../../data/embeddings/{level}/{precision}/TCRPeg_Epitope_embeddings.npz' \n",
    "            path_embedding_tcrs = f'../../data/embeddings/{level}/{precision}/TCRPeg_tcr_embeddings.npz' \n",
    "\n",
    "# Dictionaries to store all embeddings\n",
    "dictionary_tcr = {}\n",
    "dictionary_epitope = {}\n",
    "\n",
    "# Process each dataset\n",
    "for k, file_path in enumerate(file_paths):\n",
    "    print(f\"Processing {file_path}\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df.drop(columns=['TCR_name', 'TRBV', 'TRBJ', 'TRBC', 'MHC', 'task'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={'TRB_CDR3': 'CDR3.beta', 'Binding': 'Label'}, inplace=True)\n",
    "\n",
    "    # Remove sequences longer than 36\n",
    "    df = df[df['CDR3.beta'].apply(len) <= 36]\n",
    "    df = df[df['Epitope'].apply(len) <= 36]\n",
    "\n",
    "    # Identify and remove unmapped characters\n",
    "    unique_chars = set(''.join(df['CDR3.beta']))\n",
    "    df_train = df['CDR3.beta'].values\n",
    "    model = TCRpeg(hidden_size=64, num_layers=3, max_length=36, load_data=True, \n",
    "                   embedding_path='../../data/embeddings/TCRpeg/tcrpeg/data/embedding_32.txt', path_train=df_train)\n",
    "    model.create_model()\n",
    "    \n",
    "    unmapped_chars = [ch for ch in unique_chars if ch not in model.aa2idx]\n",
    "    print(\"Unmapped characters:\", unmapped_chars)\n",
    "    if unmapped_chars:\n",
    "        pattern = f\"[{''.join(unmapped_chars)}]\"\n",
    "        df = df[~df['CDR3.beta'].str.contains(pattern)]\n",
    "\n",
    "    # Unique sequences for embedding generation\n",
    "    unique_tcrs = df['CDR3.beta'].unique()\n",
    "    unique_epitopes = df['Epitope'].unique()\n",
    "\n",
    "    for seq_type, unique_seqs, model_dict in zip(['TCR', 'Epitope'], [unique_tcrs, unique_epitopes], [dictionary_tcr, dictionary_epitope]):\n",
    "        print(f\"Generating embeddings for {seq_type}s...\")\n",
    "        \n",
    "        batch_size = 10000\n",
    "        total_sequences = len(unique_seqs)\n",
    "        num_batches = (total_sequences + batch_size - 1) // batch_size\n",
    "        all_embeddings = []\n",
    "        \n",
    "        model = TCRpeg(hidden_size=512, num_layers=2, max_length=36, load_data=False, \n",
    "                       embedding_path='../../data/embeddings/TCRpeg/tcrpeg/data/embedding_32.txt')\n",
    "        model.create_model()\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, total_sequences)\n",
    "            print(f\"Processing batch {i+1}/{num_batches}, sequences {start_idx} to {end_idx}\")\n",
    "            batch_embeddings = model.get_embedding(unique_seqs[start_idx:end_idx])\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        \n",
    "        for index, element in enumerate(unique_seqs):\n",
    "            model_dict[element] = final_embeddings[index]\n",
    "\n",
    "# Save embeddings\n",
    "np.savez(path_embedding_tcrs, **dictionary_tcr)\n",
    "np.savez(path_embedding_epitope, **dictionary_epitope)\n",
    "\n",
    "print(f\"Embeddings saved: {path_embedding_tcrs}, {path_embedding_epitope}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASSPEAGYSYEQFF : [ 0.06751539  0.03299918  0.03864877 ... -0.03103181  0.02821492\n",
      "  0.00885988]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select one random key from the dictionary\n",
    "random_key = random.choice(list(dictionary_epitope.keys()))\n",
    "\n",
    "# Get the corresponding value\n",
    "random_value = dictionary_epitope[random_key]\n",
    "\n",
    "# Print the result\n",
    "print(random_key, \":\", random_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(random_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(model.max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
