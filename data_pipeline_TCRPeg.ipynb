{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings', 'cleaned_datasets', 'concatenated_datasets', 'splitted_datasets', 'temp', 'plain_datasets']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List all files and directories in the current directory\n",
    "# print(os.listdir())\n",
    "\n",
    "# List files in a specific directory\n",
    "print(os.listdir(\"./../../data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get the absolute path of 'TCRpeg' and add it to sys.path\n",
    "tcrpeg_path = os.path.abspath(\"../../data/embeddings/TCRpeg\")\n",
    "sys.path.append(tcrpeg_path)\n",
    "\n",
    "from tcrpeg.TCRpeg import TCRpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../data/splitted_datasets/gene/beta/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137966/1996193558.py:25: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have loaded the data, total training seqs : 250602\n",
      "Unmapped characters: ['*']\n",
      "Processing batch 1/25, samples 0 to 10000\n",
      "Processing batch 2/25, samples 10000 to 20000\n",
      "Processing batch 3/25, samples 20000 to 30000\n",
      "Processing batch 4/25, samples 30000 to 40000\n",
      "Processing batch 5/25, samples 40000 to 50000\n",
      "Processing batch 6/25, samples 50000 to 60000\n",
      "Processing batch 7/25, samples 60000 to 70000\n",
      "Processing batch 8/25, samples 70000 to 80000\n",
      "Processing batch 9/25, samples 80000 to 90000\n",
      "Processing batch 10/25, samples 90000 to 100000\n",
      "Processing batch 11/25, samples 100000 to 110000\n",
      "Processing batch 12/25, samples 110000 to 120000\n",
      "Processing batch 13/25, samples 120000 to 130000\n",
      "Processing batch 14/25, samples 130000 to 140000\n",
      "Processing batch 15/25, samples 140000 to 150000\n",
      "Processing batch 16/25, samples 150000 to 160000\n",
      "Processing batch 17/25, samples 160000 to 170000\n",
      "Processing batch 18/25, samples 170000 to 180000\n",
      "Processing batch 19/25, samples 180000 to 190000\n",
      "Processing batch 20/25, samples 190000 to 200000\n",
      "Processing batch 21/25, samples 200000 to 210000\n",
      "Processing batch 22/25, samples 210000 to 220000\n",
      "Processing batch 23/25, samples 220000 to 230000\n",
      "Processing batch 24/25, samples 230000 to 240000\n",
      "Processing batch 25/25, samples 240000 to 249530\n",
      "Processing ../../data/splitted_datasets/gene/beta/validation.tsv\n",
      "Have loaded the data, total training seqs : 161716\n",
      "Unmapped characters: ['*']\n",
      "Processing batch 1/17, samples 0 to 10000\n",
      "Processing batch 2/17, samples 10000 to 20000\n",
      "Processing batch 3/17, samples 20000 to 30000\n",
      "Processing batch 4/17, samples 30000 to 40000\n",
      "Processing batch 5/17, samples 40000 to 50000\n",
      "Processing batch 6/17, samples 50000 to 60000\n",
      "Processing batch 7/17, samples 60000 to 70000\n",
      "Processing batch 8/17, samples 70000 to 80000\n",
      "Processing batch 9/17, samples 80000 to 90000\n",
      "Processing batch 10/17, samples 90000 to 100000\n",
      "Processing batch 11/17, samples 100000 to 110000\n",
      "Processing batch 12/17, samples 110000 to 120000\n",
      "Processing batch 13/17, samples 120000 to 130000\n",
      "Processing batch 14/17, samples 130000 to 140000\n",
      "Processing batch 15/17, samples 140000 to 150000\n",
      "Processing batch 16/17, samples 150000 to 160000\n",
      "Processing batch 17/17, samples 160000 to 160434\n",
      "Processing ../../data/splitted_datasets/gene/beta/test.tsv\n",
      "Have loaded the data, total training seqs : 161723\n",
      "Unmapped characters: ['*']\n",
      "Processing batch 1/17, samples 0 to 10000\n",
      "Processing batch 2/17, samples 10000 to 20000\n",
      "Processing batch 3/17, samples 20000 to 30000\n",
      "Processing batch 4/17, samples 30000 to 40000\n",
      "Processing batch 5/17, samples 40000 to 50000\n",
      "Processing batch 6/17, samples 50000 to 60000\n",
      "Processing batch 7/17, samples 60000 to 70000\n",
      "Processing batch 8/17, samples 70000 to 80000\n",
      "Processing batch 9/17, samples 80000 to 90000\n",
      "Processing batch 10/17, samples 90000 to 100000\n",
      "Processing batch 11/17, samples 100000 to 110000\n",
      "Processing batch 12/17, samples 110000 to 120000\n",
      "Processing batch 13/17, samples 120000 to 130000\n",
      "Processing batch 14/17, samples 130000 to 140000\n",
      "Processing batch 15/17, samples 140000 to 150000\n",
      "Processing batch 16/17, samples 150000 to 160000\n",
      "Processing batch 17/17, samples 160000 to 160536\n",
      "Embeddings saved: ../../data/embeddings/beta/gene/TCRPeg_tcr_embeddings.npz, ../../data/embeddings/beta/gene/TCRPeg_Epitope_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "# Version for embeddings from 6.3.\n",
    "# TCRs-Embeddings contains also 'label'\n",
    "\n",
    "# Define paths\n",
    "precisions = ['gene']\n",
    "levels = ['beta']\n",
    "datasets = ['train', 'validation', 'test']\n",
    "file_paths = []\n",
    "embedding_paths = []\n",
    "for precision in precisions:\n",
    "    for level in levels:\n",
    "        for dataset in datasets:\n",
    "            path_dataset = f'../../data/splitted_datasets/{precision}/{level}/{dataset}.tsv'\n",
    "            file_paths.append((level, path_dataset))\n",
    "            path_embedding_epitope = f'../../data/embeddings/{level}/{precision}/TCRPeg_Epitope_embeddings.npz'\n",
    "            path_embedding_tcrs = f'../../data/embeddings/{level}/{precision}/TCRPeg_tcr_embeddings.npz'\n",
    "\n",
    "# Dictionaries to store all embeddings\n",
    "dictionary_tcr = {'embeddings': [], 'labels': []}\n",
    "dictionary_epitope = {'embeddings': []}\n",
    "\n",
    "# Process each dataset\n",
    "for level, file_path in file_paths:\n",
    "    print(f\"Processing {file_path}\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df.drop(columns=['TCR_name', 'TRBV', 'TRBJ', 'TRBC', 'MHC', 'task'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={'TRB_CDR3': 'CDR3.beta', 'Binding': 'Label'}, inplace=True)\n",
    "    \n",
    "    # Remove sequences longer than 36\n",
    "    df = df[df['CDR3.beta'].apply(len) <= 36]\n",
    "    df = df[df['Epitope'].apply(len) <= 36]\n",
    "    \n",
    "    # Identify and remove unmapped characters\n",
    "    unique_chars = set(''.join(df['CDR3.beta']))\n",
    "    df_train = df['CDR3.beta'].values\n",
    "    model = TCRpeg(hidden_size=64, num_layers=3, max_length=36, load_data=True, \n",
    "                   embedding_path='../../data/embeddings/TCRpeg/tcrpeg/data/embedding_32.txt', path_train=df_train)\n",
    "    model.create_model()\n",
    "    \n",
    "    unmapped_chars = [ch for ch in unique_chars if ch not in model.aa2idx]\n",
    "    print(\"Unmapped characters:\", unmapped_chars)\n",
    "    if unmapped_chars:\n",
    "        pattern = f\"[{''.join(unmapped_chars)}]\"\n",
    "        df = df[~df['CDR3.beta'].str.contains(pattern)]\n",
    "    \n",
    "    model = TCRpeg(hidden_size=512, num_layers=2, max_length=36, load_data=False, \n",
    "                   embedding_path='../../data/embeddings/TCRpeg/tcrpeg/data/embedding_32.txt')\n",
    "    model.create_model()\n",
    "    \n",
    "    batch_size = 10000\n",
    "    total_samples = len(df)\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        print(f\"Processing batch {i+1}/{num_batches}, samples {start_idx} to {end_idx}\")\n",
    "        \n",
    "        batch_tcrs = df['CDR3.beta'].iloc[start_idx:end_idx].values\n",
    "        batch_epitopes = df['Epitope'].iloc[start_idx:end_idx].values\n",
    "        batch_labels = df['Label'].iloc[start_idx:end_idx].values\n",
    "        \n",
    "        batch_tcr_embeddings = model.get_embedding(batch_tcrs)\n",
    "        batch_epitope_embeddings = model.get_embedding(batch_epitopes)\n",
    "        \n",
    "        dictionary_tcr['embeddings'].extend(batch_tcr_embeddings)\n",
    "        dictionary_tcr['labels'].extend(batch_labels)\n",
    "        dictionary_epitope['embeddings'].extend(batch_epitope_embeddings)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "dictionary_tcr['embeddings'] = np.array(dictionary_tcr['embeddings'])\n",
    "dictionary_tcr['labels'] = np.array(dictionary_tcr['labels'])\n",
    "dictionary_epitope['embeddings'] = np.array(dictionary_epitope['embeddings'])\n",
    "\n",
    "# Save embeddings\n",
    "np.savez(path_embedding_tcrs, **dictionary_tcr)\n",
    "np.savez(path_embedding_epitope, **dictionary_epitope)\n",
    "\n",
    "print(f\"Embeddings saved: {path_embedding_tcrs}, {path_embedding_epitope}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../data/splitted_datasets/gene/beta/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137966/2132429742.py:28: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m unique_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCDR3.beta\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     42\u001b[0m df_train \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCDR3.beta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 43\u001b[0m model \u001b[38;5;241m=\u001b[39m TCRpeg(hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m, load_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     44\u001b[0m                embedding_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/TCRpeg/tcrpeg/data/embedding_32.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, path_train\u001b[38;5;241m=\u001b[39mdf_train)\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39mcreate_model()\n\u001b[1;32m     47\u001b[0m unmapped_chars \u001b[38;5;241m=\u001b[39m [ch \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m unique_chars \u001b[38;5;28;01mif\u001b[39;00m ch \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39maa2idx]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# Version from arround 3.03.\n",
    "# ACHTUNG: for allele and paired the code must be revised/checked. Do the for loops work as intended????\n",
    "\n",
    "# Define paths\n",
    "precisions = ['gene']\n",
    "levels = ['beta']\n",
    "datasets = ['train', 'validation', 'test']\n",
    "file_paths = []\n",
    "embedding_paths = []\n",
    "for precision in precisions:\n",
    "    for level in levels:\n",
    "        for dataset in datasets:\n",
    "            path_dataset = f'../../data/splitted_datasets/{precision}/{level}/{dataset}.tsv'\n",
    "            file_paths.append(path_dataset)\n",
    "            path_embedding_epitope = f'../../data/embeddings/{level}/{precision}/TCRPeg_Epitope_embeddings.npz' \n",
    "            path_embedding_tcrs = f'../../data/embeddings/{level}/{precision}/TCRPeg_tcr_embeddings.npz' \n",
    "\n",
    "# Dictionaries to store all embeddings\n",
    "dictionary_tcr = {}\n",
    "dictionary_epitope = {}\n",
    "\n",
    "# Process each dataset\n",
    "for k, file_path in enumerate(file_paths):\n",
    "    print(f\"Processing {file_path}\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df.drop(columns=['TCR_name', 'TRBV', 'TRBJ', 'TRBC', 'MHC', 'task'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={'TRB_CDR3': 'CDR3.beta', 'Binding': 'Label'}, inplace=True)\n",
    "\n",
    "    # Remove sequences longer than 36\n",
    "    df = df[df['CDR3.beta'].apply(len) <= 36]\n",
    "    df = df[df['Epitope'].apply(len) <= 36]\n",
    "\n",
    "    # Identify and remove unmapped characters\n",
    "    unique_chars = set(''.join(df['CDR3.beta']))\n",
    "    df_train = df['CDR3.beta'].values\n",
    "    model = TCRpeg(hidden_size=64, num_layers=3, max_length=36, load_data=True, \n",
    "                   embedding_path='../../data/embeddings/TCRpeg/tcrpeg/data/embedding_32.txt', path_train=df_train)\n",
    "    model.create_model()\n",
    "    \n",
    "    unmapped_chars = [ch for ch in unique_chars if ch not in model.aa2idx]\n",
    "    print(\"Unmapped characters:\", unmapped_chars)\n",
    "    if unmapped_chars:\n",
    "        pattern = f\"[{''.join(unmapped_chars)}]\"\n",
    "        df = df[~df['CDR3.beta'].str.contains(pattern)]\n",
    "\n",
    "    # Unique sequences for embedding generation\n",
    "    unique_tcrs = df['CDR3.beta'].unique()\n",
    "    unique_epitopes = df['Epitope'].unique()\n",
    "\n",
    "    for seq_type, unique_seqs, model_dict in zip(['TCR', 'Epitope'], [unique_tcrs, unique_epitopes], [dictionary_tcr, dictionary_epitope]):\n",
    "        print(f\"Generating embeddings for {seq_type}s...\")\n",
    "        \n",
    "        batch_size = 10000\n",
    "        total_sequences = len(unique_seqs)\n",
    "        num_batches = (total_sequences + batch_size - 1) // batch_size\n",
    "        all_embeddings = []\n",
    "        \n",
    "        model = TCRpeg(hidden_size=512, num_layers=2, max_length=36, load_data=False, \n",
    "                       embedding_path='../../data/embeddings/TCRpeg/tcrpeg/data/embedding_32.txt')\n",
    "        model.create_model()\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, total_sequences)\n",
    "            print(f\"Processing batch {i+1}/{num_batches}, sequences {start_idx} to {end_idx}\")\n",
    "            batch_embeddings = model.get_embedding(unique_seqs[start_idx:end_idx])\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        \n",
    "        for index, element in enumerate(unique_seqs):\n",
    "            model_dict[element] = final_embeddings[index]\n",
    "\n",
    "# Save embeddings\n",
    "np.savez(path_embedding_tcrs, **dictionary_tcr)\n",
    "np.savez(path_embedding_epitope, **dictionary_epitope)\n",
    "\n",
    "print(f\"Embeddings saved: {path_embedding_tcrs}, {path_embedding_epitope}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASSPEAGYSYEQFF : [ 0.06751539  0.03299918  0.03864877 ... -0.03103181  0.02821492\n",
      "  0.00885988]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select one random key from the dictionary\n",
    "random_key = random.choice(list(dictionary_epitope.keys()))\n",
    "\n",
    "# Get the corresponding value\n",
    "random_value = dictionary_epitope[random_key]\n",
    "\n",
    "# Print the result\n",
    "print(random_key, \":\", random_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(random_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(model.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864\n",
      "199160\n",
      "Epitope embedding shape: (1024,)\n",
      "TCR embedding shape: (1024,)\n",
      "Number of epitope embeddings: 1024\n",
      "Number of TCR embeddings: 1024\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of TCR embeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_tcr_embeddings\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Compare dimensions\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epi_array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tcr_array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth embeddings have the same dimensionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# epi_embedding_path = '../../data/embeddings/beta/gene/Epitope_beta_embeddings.npz'\n",
    "# tcr_embedding_path = '../../data/embeddings/beta/gene/TRB_beta_embeddings.npz'\n",
    "\n",
    "# epi_embedding_path = '../../data/embeddings/paired/gene/Epitope_paired_embeddings.npz'\n",
    "# tcr_embedding_path = '../../data/embeddings/paired/gene/TRA_paired_embeddings.npz'\n",
    "\n",
    "# epi_embedding_path = '../../data/embeddings/beta/gene/Epitope_PLE_padded.npz'\n",
    "# tcr_embedding_path = '../../data/embeddings/beta/gene/TRB_PLE_padded.npz'\n",
    "\n",
    "# epi_embedding_path = '/home/ubuntu/data/embeddings/beta/gene/TCRPeg_Epitope_embeddings.npz'\n",
    "# tcr_embedding_path = '/home/ubuntu/data/embeddings/beta/gene/TCRPeg_tcr_embeddings.npz'\n",
    "\n",
    "\n",
    "# Load the embeddings\n",
    "epi_embeddings = np.load(epi_embedding_path)\n",
    "tcr_embeddings = np.load(tcr_embedding_path)\n",
    "\n",
    "# Assuming the embeddings are stored under specific keys, list them\n",
    "epi_keys = list(epi_embeddings.keys())\n",
    "tcr_keys = list(tcr_embeddings.keys())\n",
    "\n",
    "print(len(epi_keys))\n",
    "print(len(tcr_keys))\n",
    "# print(f\"Epitope embedding keys: {epi_keys}\")\n",
    "# print(f\"TCR embedding keys: {tcr_keys}\")\n",
    "\n",
    "# Extract and compare the arrays (assuming a single key per file)\n",
    "epi_array = epi_embeddings[epi_keys[0]]  # Modify key if needed\n",
    "tcr_array = tcr_embeddings[tcr_keys[0]]  # Modify key if needed\n",
    "\n",
    "print(f\"Epitope embedding shape: {epi_array.shape}\")\n",
    "print(f\"TCR embedding shape: {tcr_array.shape}\")\n",
    "\n",
    "# Compare the number of embeddings\n",
    "num_epi_embeddings = epi_array.shape[0]\n",
    "num_tcr_embeddings = tcr_array.shape[0]\n",
    "\n",
    "print(f\"Number of epitope embeddings: {num_epi_embeddings}\")\n",
    "print(f\"Number of TCR embeddings: {num_tcr_embeddings}\")\n",
    "\n",
    "# Compare dimensions\n",
    "if epi_array.shape[1] == tcr_array.shape[1]:\n",
    "    print(\"Both embeddings have the same dimensionality.\")\n",
    "else:\n",
    "    print(\"Warning: The embeddings have different dimensionalities!\")\n",
    "\n",
    "# Compare the number of embeddings in each file\n",
    "if num_epi_embeddings == num_tcr_embeddings:\n",
    "    print(\"Both files have the same number of embeddings.\")\n",
    "else:\n",
    "    print(\"Warning: The number of embeddings differs between the files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAC</th>\n",
       "      <th>TRBC</th>\n",
       "      <th>TRA_leader</th>\n",
       "      <th>TRB_leader</th>\n",
       "      <th>Linker</th>\n",
       "      <th>Link_order</th>\n",
       "      <th>TRA_5_prime_seq</th>\n",
       "      <th>TRA_3_prime_seq</th>\n",
       "      <th>TRB_5_prime_seq</th>\n",
       "      <th>TRB_3_prime_seq</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRA_CDR3</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>TRB_CDR3</th>\n",
       "      <th>MHC</th>\n",
       "      <th>MHC class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLFGYPVYV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAVTTDSWGKLQF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASRPGLAGGRPEQYF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>MHCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLFGYPVYV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAVTTDSWGKLQF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASRPGLMSAQPEQYF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>MHCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SLLMWITQC</td>\n",
       "      <td>TRAV21*01</td>\n",
       "      <td>TRAJ6*01</td>\n",
       "      <td>CAVRPTSGGSYIPTF</td>\n",
       "      <td>TRBV6-5*01</td>\n",
       "      <td>TRBJ2-2*01</td>\n",
       "      <td>CASSYVGNTGELFF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>MHCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAGIGILTV</td>\n",
       "      <td>TRAV35*02</td>\n",
       "      <td>TRAJ49*01</td>\n",
       "      <td>CAGGTGNQFYF</td>\n",
       "      <td>TRBV10-3</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>CAISEVGVGQPQHF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>MHCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "      <td>TRAV35*02</td>\n",
       "      <td>TRAJ49*01</td>\n",
       "      <td>CAGGTGNQFYF</td>\n",
       "      <td>TRBV10-3</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>CAISEVGVGQPQHF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>MHCI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TRAC  TRBC  TRA_leader  TRB_leader  Linker  Link_order  TRA_5_prime_seq  \\\n",
       "0   NaN   NaN         NaN         NaN     NaN         NaN              NaN   \n",
       "1   NaN   NaN         NaN         NaN     NaN         NaN              NaN   \n",
       "2   NaN   NaN         NaN         NaN     NaN         NaN              NaN   \n",
       "3   NaN   NaN         NaN         NaN     NaN         NaN              NaN   \n",
       "4   NaN   NaN         NaN         NaN     NaN         NaN              NaN   \n",
       "\n",
       "   TRA_3_prime_seq  TRB_5_prime_seq  TRB_3_prime_seq     Epitope       TRAV  \\\n",
       "0              NaN              NaN              NaN   LLFGYPVYV        NaN   \n",
       "1              NaN              NaN              NaN   LLFGYPVYV        NaN   \n",
       "2              NaN              NaN              NaN   SLLMWITQC  TRAV21*01   \n",
       "3              NaN              NaN              NaN   AAGIGILTV  TRAV35*02   \n",
       "4              NaN              NaN              NaN  ELAGIGILTV  TRAV35*02   \n",
       "\n",
       "        TRAJ         TRA_CDR3        TRBV        TRBJ          TRB_CDR3  \\\n",
       "0        NaN    CAVTTDSWGKLQF         NaN         NaN  CASRPGLAGGRPEQYF   \n",
       "1        NaN    CAVTTDSWGKLQF         NaN         NaN  CASRPGLMSAQPEQYF   \n",
       "2   TRAJ6*01  CAVRPTSGGSYIPTF  TRBV6-5*01  TRBJ2-2*01    CASSYVGNTGELFF   \n",
       "3  TRAJ49*01      CAGGTGNQFYF    TRBV10-3  TRBJ1-5*01    CAISEVGVGQPQHF   \n",
       "4  TRAJ49*01      CAGGTGNQFYF    TRBV10-3  TRBJ1-5*01    CAISEVGVGQPQHF   \n",
       "\n",
       "           MHC MHC class  \n",
       "0  HLA-A*02:01      MHCI  \n",
       "1  HLA-A*02:01      MHCI  \n",
       "2  HLA-A*02:01      MHCI  \n",
       "3  HLA-A*02:01      MHCI  \n",
       "4  HLA-A*02:01      MHCI  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/home/ubuntu/data/cleaned_datasets/IEDB/IEDB_cleaned_data_paired.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
