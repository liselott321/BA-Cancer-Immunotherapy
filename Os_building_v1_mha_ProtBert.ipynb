{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the TRBJ-, V and MHC Syntax in train/valid/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137602/648892503.py:11: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  valid_df = pd.read_csv(valid_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRBJ ---\n",
      "Length train set: 29. Length valid set: 28. Length test set: 28\n",
      "train_set (29): {'', 'trbj1401', 'trbj23', 'trbj14', 'trbj1201', 'trbj12', 'trbj1602', 'trbj2601', 'trbj2501', 'trbj2101', 'trbj24', 'trbj22', 'trbj16', 'trbj26', 'trbj1101', 'trbj21', 'trbj2301', 'trbj2701', 'trbj1601', 'trbj15', 'trbj11', 'trbj1301', 'trbv6501', 'trbj1501', 'trbj2201', 'trbj25', 'trbj27', 'trbj13', 'trbj2401'}\n",
      "valid_set (28): {'', 'trbj1401', 'trbj23', 'trbj14', 'trbj1201', 'trbj12', 'trbj1602', 'trbj2601', 'trbj2501', 'trbj2101', 'trbj24', 'trbj22', 'trbj16', 'trbj26', 'trbj1101', 'trbj21', 'trbj2301', 'trbj2701', 'trbj1601', 'trbj15', 'trbj11', 'trbj1301', 'trbj1501', 'trbj2201', 'trbj25', 'trbj27', 'trbj13', 'trbj2401'}\n",
      "test_set (28): {'', 'trbj1401', 'trbj23', 'trbj14', 'trbj1201', 'trbj1602', 'trbj12', 'trbj2601', 'trbj2501', 'trbj2101', 'trbj24', 'trbj22', 'trbj16', 'trbj26', 'trbj1101', 'trbj2301', 'trbj21', 'trbj2701', 'trbj1601', 'trbj15', 'trbj11', 'trbj1301', 'trbj1501', 'trbj2201', 'trbj25', 'trbj27', 'trbj13', 'trbj2401'} \n",
      "\n",
      "Train ∩ Test (28): {'', 'trbj1401', 'trbj23', 'trbj14', 'trbj1201', 'trbj13', 'trbj1602', 'trbj12', 'trbj2601', 'trbj2501', 'trbj24', 'trbj22', 'trbj16', 'trbj26', 'trbj1101', 'trbj2301', 'trbj21', 'trbj2701', 'trbj1601', 'trbj15', 'trbj11', 'trbj1301', 'trbj1501', 'trbj2201', 'trbj25', 'trbj27', 'trbj2101', 'trbj2401'}\n",
      "Valid ∩ Test (28): {'', 'trbj1401', 'trbj23', 'trbj14', 'trbj1201', 'trbj13', 'trbj1602', 'trbj12', 'trbj2601', 'trbj2501', 'trbj24', 'trbj22', 'trbj16', 'trbj26', 'trbj1101', 'trbj2301', 'trbj21', 'trbj2701', 'trbj1601', 'trbj15', 'trbj11', 'trbj1301', 'trbj1501', 'trbj2201', 'trbj25', 'trbj27', 'trbj2101', 'trbj2401'}\n",
      "Test - (Train ∪ Valid) (0): set()\n",
      "Train ∩ Valid (28): {'', 'trbj1401', 'trbj23', 'trbj14', 'trbj1201', 'trbj13', 'trbj12', 'trbj1602', 'trbj2601', 'trbj2501', 'trbj24', 'trbj22', 'trbj16', 'trbj26', 'trbj1101', 'trbj21', 'trbj2301', 'trbj2701', 'trbj1601', 'trbj15', 'trbj11', 'trbj1301', 'trbj1501', 'trbj2201', 'trbj25', 'trbj27', 'trbj2101', 'trbj2401'}\n",
      "\n",
      "--- TRBV ---\n",
      "Length train set: 159. Length valid set: 134. Length test set: 118\n",
      "train_set (159): {'', 'trbv31', 'trbv11201', 'trbv2', 'trbv12101', 'trbv7502', 'trbv22101', 'trbv1', 'trbv11202', 'trbv16', 'trbv12201', 'trbv7303', 'trbv71', 'trbv12301', 'trbv8101', 'trbv10102', 'trbv1902', 'trbv20or92', 'trbv14', 'trbv23101', 'trbv19', 'trbv12401', 'trbv111', 'trbv10101', 'trbv123', 'trbv7301', 'trbv7907', 'trbv3001', 'trbv903', 'trbv7903', 'trbv72', 'trbv121', 'trbv15', 'trbv5601', 'trbv11304', 'trbv102', 'trbv5501', 'trbv69', 'trbv6501', 'trbv7202', 'trbv78', 'trbv74', 'trbv76', 'trbv5401', 'trbv7501', 'trbv7201', 'trbv17', 'trbv7901', 'trbv26', 'trbv901', 'trbv6601', 'trbv6401', 'trbv1801', 'trbv42', 'trbv75', 'trbv12402', 'trbv1402', 'trbv251', 'trbv66', 'trbv11302', 'trbv1401', 'trbv122', 'trbv291', 'trbv231', 'trbv30', 'trbv1501', 'trbv113', 'trbv6201', 'trbj2701', 'trbv2701', 'trbv7803', 'trbv902', 'trbv77', 'trbv81', 'trbv3005', 'trbv10201', 'trbv63', 'trbv65', 'trbv241', 'trbv7801', 'trbv6602', 'trbv4304', 'trbv56', 'trbv67', 'trbv18', 'trbv29101', 'trbv51', 'trbv101', 'trbv11203', 'trbv10301', 'trbv20101', 'trbv1701', 'trbv11301', 'trbv2801', 'trbv124', 'trbv5101', 'trbv7904', 'trbv7701', 'trbv125', 'trbv61', 'trbv7802', 'trbv221', 'trbv4201', 'trbv73', 'trbv5402', 'trbv7601', 'trbv82', 'trbv202', 'trbv1901', 'trbv112', 'trbv6901', 'trbv10302', 'trbv1301', 'trbv5201', 'trbv20102', 'trbv27', 'trbv7101', 'trbv6604', 'trbv12501', 'trbv28', 'trbv24101', 'trbv4301', 'trbv1601', 'trbv54', 'trbv32', 'trbv68', 'trbv11101', 'trbv64', 'trbv6701', 'trbv6101', 'trbv5801', 'trbv79', 'trbv13', 'trbv9', 'trbv203', 'trbv1502', 'trbv53', 'trbv41', 'trbv25101', 'trbv201', 'trbv21101', 'trbv3002', 'trbv55', 'trbv5701', 'trbv3101', 'trbv4101', 'trbv8201', 'trbv6801', 'trbv62', 'trbv57', 'trbv4102', 'trbv20104', 'trbv211', 'trbv2601', 'trbv6402', 'trbv43', 'trbv103', 'trbv58', 'trbv7401'}\n",
      "valid_set (134): {'', 'trbv31', 'trbv11201', 'trbv2', 'trbv7502', 'trbv22101', 'trbv1', 'trbv11202', 'trbv16', 'trbv12201', 'trbv71', 'trbv12301', 'trbv10102', 'trbv1902', 'trbv14', 'trbv23101', 'trbv19', 'trbv12401', 'trbv111', 'trbv10101', 'trbv123', 'trbv7301', 'trbv3001', 'trbv7903', 'trbv72', 'trbv121', 'trbv15', 'trbv5601', 'trbv102', 'trbv5501', 'trbv69', 'trbv6501', 'trbv7202', 'trbv78', 'trbv74', 'trbv76', 'trbv5401', 'trbv7201', 'trbv17', 'trbv7901', 'trbv901', 'trbv6601', 'trbv6401', 'trbv1801', 'trbv42', 'trbv251', 'trbv66', 'trbv1401', 'trbv122', 'trbv291', 'trbv231', 'trbv30', 'trbv1501', 'trbv113', 'trbv6201', 'trbv2701', 'trbv7803', 'trbv902', 'trbv77', 'trbv81', 'trbv3005', 'trbv10201', 'trbv63', 'trbv65', 'trbv241', 'trbv7801', 'trbv6602', 'trbv56', 'trbv67', 'trbv18', 'trbv29101', 'trbv51', 'trbv101', 'trbv11203', 'trbv10301', 'trbv20101', 'trbv1701', 'trbv11301', 'trbv2801', 'trbv124', 'trbv5101', 'trbv7701', 'trbv125', 'trbv61', 'trbv4201', 'trbv73', 'trbv7601', 'trbv82', 'trbv1901', 'trbv112', 'trbv10302', 'trbv6901', 'trbv1301', 'trbv27', 'trbv7101', 'trbv6604', 'trbv12501', 'trbv28', 'trbv24101', 'trbv1601', 'trbv54', 'trbv32', 'trbv68', 'trbv11101', 'trbv10202', 'trbv64', 'trbv6701', 'trbv6101', 'trbv5801', 'trbv79', 'trbv13', 'trbv9', 'trbv1502', 'trbv7401', 'trbv53', 'trbv41', 'trbv25101', 'trbv201', 'trbv21101', 'trbv3002', 'trbv55', 'trbv5701', 'trbv3101', 'trbv4101', 'trbv6801', 'trbv62', 'trbv57', 'trbv20104', 'trbv211', 'trbv2601', 'trbv43', 'trbv103', 'trbv58', 'trbv4301'}\n",
      "test_set (118): {'', 'trbv11201', 'trbv31', 'trbv2', 'trbv12201', 'trbv16', 'trbv11202', 'trbv12301', 'trbv71', 'trbv1902', 'trbv14', 'trbv23101', 'trbv19', 'trbv12401', 'trbv111', 'trbv10101', 'trbv7301', 'trbv123', 'trbv3001', 'trbv7903', 'trbv72', 'trbv121', 'trbv5601', 'trbv15', 'trbv102', 'trbv5501', 'trbv69', 'trbv6501', 'trbv78', 'trbv74', 'trbv76', 'trbv5401', 'trbv7201', 'trbv7901', 'trbv901', 'trbv6601', 'trbv6401', 'trbv1801', 'trbv42', 'trbv251', 'trbv66', 'trbv1401', 'trbv291', 'trbv231', 'trbv30', 'trbv1501', 'trbv113', 'trbv6201', 'trbv2701', 'trbv902', 'trbv77', 'trbv10201', 'trbv63', 'trbv65', 'trbv241', 'trbv7801', 'trbv6602', 'trbv56', 'trbv67', 'trbv18', 'trbv29101', 'trbv51', 'trbv101', 'trbv10301', 'trbv20101', 'trbv11301', 'trbv2801', 'trbv124', 'trbv5101', 'trbv7701', 'trbv125', 'trbv61', 'trbv7802', 'trbv4201', 'trbv73', 'trbv7601', 'trbv1901', 'trbv112', 'trbv6901', 'trbv10302', 'trbv1301', 'trbv20102', 'trbv27', 'trbv7101', 'trbv12501', 'trbv24101', 'trbv28', 'trbv1601', 'trbv54', 'trbv68', 'trbv11101', 'trbv7203', 'trbv64', 'trbv6101', 'trbv5801', 'trbv79', 'trbv13', 'trbv9', 'trbv7401', 'trbv1502', 'trbv53', 'trbv41', 'trbv25101', 'trbv201', 'trbv3002', 'trbv55', 'trbv3101', 'trbv6301', 'trbv4101', 'trbv6801', 'trbv62', 'trbv57', 'trbv20104', 'trbv211', 'trbv43', 'trbv103', 'trbv58', 'trbv4301'} \n",
      "\n",
      "Train ∩ Test (116): {'', 'trbv11201', 'trbv31', 'trbv2', 'trbv11202', 'trbv12201', 'trbv16', 'trbv71', 'trbv12301', 'trbv1902', 'trbv14', 'trbv23101', 'trbv19', 'trbv12401', 'trbv111', 'trbv10101', 'trbv7301', 'trbv123', 'trbv3001', 'trbv7903', 'trbv72', 'trbv121', 'trbv5601', 'trbv15', 'trbv102', 'trbv5501', 'trbv69', 'trbv6501', 'trbv78', 'trbv74', 'trbv76', 'trbv5401', 'trbv7201', 'trbv7901', 'trbv901', 'trbv6601', 'trbv6401', 'trbv1801', 'trbv42', 'trbv251', 'trbv66', 'trbv1401', 'trbv291', 'trbv231', 'trbv30', 'trbv1501', 'trbv113', 'trbv6201', 'trbv2701', 'trbv902', 'trbv77', 'trbv10201', 'trbv63', 'trbv65', 'trbv241', 'trbv7801', 'trbv6602', 'trbv56', 'trbv67', 'trbv18', 'trbv29101', 'trbv51', 'trbv101', 'trbv10301', 'trbv20101', 'trbv11301', 'trbv2801', 'trbv124', 'trbv5101', 'trbv7701', 'trbv125', 'trbv61', 'trbv7802', 'trbv4201', 'trbv73', 'trbv7601', 'trbv1901', 'trbv112', 'trbv6901', 'trbv10302', 'trbv1301', 'trbv20102', 'trbv27', 'trbv7101', 'trbv12501', 'trbv24101', 'trbv28', 'trbv4301', 'trbv1601', 'trbv54', 'trbv68', 'trbv11101', 'trbv64', 'trbv6101', 'trbv5801', 'trbv79', 'trbv13', 'trbv9', 'trbv1502', 'trbv53', 'trbv41', 'trbv25101', 'trbv201', 'trbv3002', 'trbv55', 'trbv3101', 'trbv4101', 'trbv6801', 'trbv62', 'trbv57', 'trbv20104', 'trbv211', 'trbv43', 'trbv103', 'trbv58', 'trbv7401'}\n",
      "Valid ∩ Test (114): {'', 'trbv11201', 'trbv31', 'trbv2', 'trbv11202', 'trbv12201', 'trbv16', 'trbv71', 'trbv12301', 'trbv1902', 'trbv14', 'trbv23101', 'trbv19', 'trbv12401', 'trbv111', 'trbv10101', 'trbv7301', 'trbv123', 'trbv3001', 'trbv7903', 'trbv72', 'trbv121', 'trbv5601', 'trbv15', 'trbv102', 'trbv5501', 'trbv69', 'trbv6501', 'trbv78', 'trbv74', 'trbv76', 'trbv5401', 'trbv7201', 'trbv7901', 'trbv901', 'trbv6601', 'trbv6401', 'trbv1801', 'trbv42', 'trbv251', 'trbv66', 'trbv1401', 'trbv291', 'trbv231', 'trbv30', 'trbv1501', 'trbv113', 'trbv6201', 'trbv2701', 'trbv902', 'trbv77', 'trbv10201', 'trbv63', 'trbv65', 'trbv241', 'trbv7801', 'trbv6602', 'trbv56', 'trbv67', 'trbv18', 'trbv29101', 'trbv51', 'trbv101', 'trbv10301', 'trbv20101', 'trbv11301', 'trbv2801', 'trbv124', 'trbv5101', 'trbv7701', 'trbv125', 'trbv61', 'trbv4201', 'trbv73', 'trbv7601', 'trbv1901', 'trbv112', 'trbv6901', 'trbv10302', 'trbv1301', 'trbv27', 'trbv7101', 'trbv12501', 'trbv24101', 'trbv28', 'trbv4301', 'trbv1601', 'trbv54', 'trbv68', 'trbv11101', 'trbv64', 'trbv6101', 'trbv5801', 'trbv79', 'trbv13', 'trbv9', 'trbv1502', 'trbv53', 'trbv41', 'trbv25101', 'trbv201', 'trbv3002', 'trbv55', 'trbv3101', 'trbv4101', 'trbv6801', 'trbv62', 'trbv57', 'trbv20104', 'trbv211', 'trbv43', 'trbv103', 'trbv58', 'trbv7401'}\n",
      "Test - (Train ∪ Valid) (2): {'trbv6301', 'trbv7203'}\n",
      "Train ∩ Valid (133): {'', 'trbv31', 'trbv11201', 'trbv2', 'trbv7502', 'trbv22101', 'trbv1', 'trbv12201', 'trbv11202', 'trbv16', 'trbv12301', 'trbv71', 'trbv10102', 'trbv1902', 'trbv14', 'trbv23101', 'trbv19', 'trbv12401', 'trbv111', 'trbv10101', 'trbv123', 'trbv7301', 'trbv3001', 'trbv7903', 'trbv72', 'trbv121', 'trbv15', 'trbv5601', 'trbv102', 'trbv5501', 'trbv69', 'trbv6501', 'trbv7202', 'trbv78', 'trbv74', 'trbv76', 'trbv5401', 'trbv7201', 'trbv17', 'trbv7901', 'trbv901', 'trbv6601', 'trbv6401', 'trbv1801', 'trbv42', 'trbv251', 'trbv66', 'trbv1401', 'trbv122', 'trbv291', 'trbv231', 'trbv30', 'trbv1501', 'trbv113', 'trbv6201', 'trbv2701', 'trbv7803', 'trbv902', 'trbv77', 'trbv81', 'trbv3005', 'trbv10201', 'trbv63', 'trbv65', 'trbv241', 'trbv7801', 'trbv6602', 'trbv56', 'trbv67', 'trbv18', 'trbv29101', 'trbv51', 'trbv101', 'trbv11203', 'trbv10301', 'trbv20101', 'trbv1701', 'trbv11301', 'trbv2801', 'trbv124', 'trbv5101', 'trbv7701', 'trbv125', 'trbv61', 'trbv4201', 'trbv73', 'trbv7601', 'trbv82', 'trbv1901', 'trbv112', 'trbv10302', 'trbv6901', 'trbv1301', 'trbv27', 'trbv7101', 'trbv6604', 'trbv12501', 'trbv28', 'trbv24101', 'trbv4301', 'trbv1601', 'trbv54', 'trbv32', 'trbv68', 'trbv11101', 'trbv64', 'trbv6701', 'trbv6101', 'trbv5801', 'trbv79', 'trbv13', 'trbv9', 'trbv1502', 'trbv53', 'trbv41', 'trbv25101', 'trbv201', 'trbv21101', 'trbv3002', 'trbv55', 'trbv5701', 'trbv3101', 'trbv4101', 'trbv6801', 'trbv62', 'trbv57', 'trbv20104', 'trbv211', 'trbv2601', 'trbv43', 'trbv103', 'trbv58', 'trbv7401'}\n",
      "\n",
      "--- MHC ---\n",
      "Length train set: 62. Length valid set: 59. Length test set: 63\n",
      "train_set (62): {'', 'hlaa2902', 'hlab0801', 'hlab08', 'hlab4405', 'hlab3801', 'hlaa0214', 'hlaa8001', 'hlaa6801', 'hlab1801', 'hlab57', 'hlaa01', 'hlaa020148', 'hlaa0206', 'hlaa0207', 'hlab3501', 'hlaa0201', 'hlac0702', 'hlab18', 'hlaa0202', 'hlaa0203', 'hlab07', 'hlab0702', 'hlaa02', 'hlab4403', 'hlaa2402', 'hlab15', 'hlaa0215', 'hlab3502', 'hlac0304', 'hlab5703', 'hlaa0208', 'hlab27', 'hlaa0217', 'hlae0103', 'hlaa0209', 'hlab3701', 'hlab5101', 'hlaa02266', 'hlab4001', 'hlab2705', 'hlaa11', 'hlac1202', 'hlaa0213', 'hlab4102', 'hlaa1101', 'hlab3508', 'hlaa0205', 'hlab3901', 'hlaa0204', 'hlab1501', 'hlab2709', 'hlac0401', 'hlaa0101', 'hlaa0301', 'hlaa3001', 'hlab4402', 'hlaa0216', 'hlab5701', 'hlab35', 'hlaa0210', 'hlab4201'}\n",
      "valid_set (59): {'', 'hlaa2902', 'hlab0801', 'hlab08', 'hlab4405', 'hlab3801', 'hlab5001', 'hlaa8001', 'hlab1801', 'hlaa6801', 'hlab4801', 'hlab57', 'hlaa01', 'hlac0701', 'hlaa0206', 'hlaa020148', 'hlab18', 'hlaa0201', 'hlab3501', 'hlac0702', 'hlaa3002', 'hlab07', 'hlab0702', 'hlac1601', 'hlaa02', 'hlab4403', 'hlab42', 'hlaa2402', 'hlac0501', 'hlab15', 'hlac0801', 'hlab4102', 'hlac0304', 'hlab5703', 'hlab27', 'hlac03', 'hlab5101', 'hlab3701', 'hlab4001', 'hlab2705', 'hlaa11', 'hlac1202', 'hlaa3201', 'hlaa2301', 'hlaa1101', 'hlab3508', 'hlaa0205', 'hlac0303', 'hlac1402', 'hlab3901', 'hlab1501', 'hlac0802', 'hlaa0101', 'hlaa0301', 'hlab4402', 'hlac0602', 'hlab5701', 'hlaa0210', 'hlab4201'}\n",
      "test_set (63): {'', 'hlaa2902', 'hlab08', 'hlab0801', 'hlaa03', 'hlab3801', 'hlaa6801', 'hlab57', 'hlab354202', 'hlaa020148', 'hlaa01', 'hlab3501', 'hlaa0201', 'hlab18', 'hlaa3002', 'hlab07', 'hlab0702', 'hlaa02', 'hlab4403', 'hlab350145', 'hlab42', 'hlab1402', 'hlaa2402', 'hlaa020198', 'hlab53', 'hlab15', 'hlaa240284', 'hlab58', 'hlac0304', 'hlaa02060103', 'hlab27', 'hlae0103', 'hlab354201', 'hlab080129', 'hlab270531', 'hlab8101', 'hlab5706', 'hlaa110118', 'hlab5101', 'hlaa02266', 'hlaa11', 'hlab2705', 'hlab070248', 'hlaa3201', 'hlaa2501', 'hlac0102', 'hlaa1101', 'hlab3508', 'hlac0303', 'hlac1402', 'hlac0401', 'hlab1501', 'hlae01010103', 'hlab440308', 'hlaa010173', 'hlac0802', 'hlaa240233', 'hlaa0101', 'hlaa0301', 'hlac0602', 'hlab5701', 'hlab35', 'hlab4201'} \n",
      "\n",
      "Train ∩ Test (34): {'', 'hlaa2902', 'hlab0801', 'hlab08', 'hlab3801', 'hlaa6801', 'hlab57', 'hlaa01', 'hlaa020148', 'hlab3501', 'hlaa0201', 'hlab18', 'hlab07', 'hlab0702', 'hlaa02', 'hlab4403', 'hlaa2402', 'hlab15', 'hlac0304', 'hlab27', 'hlae0103', 'hlab5101', 'hlaa02266', 'hlaa11', 'hlab2705', 'hlaa1101', 'hlab3508', 'hlab1501', 'hlac0401', 'hlaa0101', 'hlaa0301', 'hlab5701', 'hlab35', 'hlab4201'}\n",
      "Valid ∩ Test (37): {'', 'hlaa2902', 'hlab0801', 'hlab08', 'hlab3801', 'hlaa6801', 'hlab57', 'hlaa01', 'hlaa020148', 'hlab18', 'hlaa0201', 'hlab3501', 'hlaa3002', 'hlab07', 'hlab0702', 'hlaa02', 'hlab4403', 'hlab42', 'hlaa2402', 'hlab15', 'hlac0304', 'hlab27', 'hlab5101', 'hlaa11', 'hlab2705', 'hlaa3201', 'hlaa1101', 'hlab3508', 'hlac0303', 'hlac1402', 'hlab1501', 'hlac0802', 'hlaa0101', 'hlaa0301', 'hlac0602', 'hlab5701', 'hlab4201'}\n",
      "Test - (Train ∪ Valid) (22): {'hlaa03', 'hlab354202', 'hlab350145', 'hlab1402', 'hlaa020198', 'hlab53', 'hlaa240284', 'hlab58', 'hlaa02060103', 'hlab080129', 'hlab354201', 'hlab270531', 'hlab8101', 'hlab5706', 'hlaa110118', 'hlab070248', 'hlaa2501', 'hlac0102', 'hlae01010103', 'hlab440308', 'hlaa010173', 'hlaa240233'}\n",
      "Train ∩ Valid (44): {'', 'hlaa2902', 'hlab0801', 'hlab08', 'hlab4405', 'hlab3801', 'hlaa8001', 'hlaa6801', 'hlab1801', 'hlab57', 'hlaa01', 'hlaa020148', 'hlaa0206', 'hlab18', 'hlaa0201', 'hlab3501', 'hlac0702', 'hlab07', 'hlab0702', 'hlaa02', 'hlab4403', 'hlaa2402', 'hlab15', 'hlac0304', 'hlab5703', 'hlab27', 'hlab5101', 'hlab3701', 'hlab4001', 'hlab4102', 'hlab2705', 'hlaa11', 'hlac1202', 'hlaa0205', 'hlaa1101', 'hlab3508', 'hlab3901', 'hlab1501', 'hlaa0101', 'hlaa0301', 'hlab4402', 'hlab5701', 'hlaa0210', 'hlab4201'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "def normalize(value):\n",
    "    if pd.isna(value):\n",
    "        return ''\n",
    "    value = value.lower()  # lowercase\n",
    "    value = re.sub(r'[^a-z0-9]', '', value)  # remove non-alphanumeric\n",
    "    return value\n",
    "\n",
    "def normalize_columns(df, columns):\n",
    "    return df[columns].map(normalize)\n",
    "\n",
    "# Normalize columns\n",
    "train_norm = normalize_columns(train_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "valid_norm = normalize_columns(valid_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "test_norm  = normalize_columns(test_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "\n",
    "# Compare sets\n",
    "for col in ['TRBJ', 'TRBV', 'MHC']:\n",
    "    train_set = set(train_norm[col])\n",
    "    valid_set = set(valid_norm[col])\n",
    "    test_set = set(test_norm[col])\n",
    "\n",
    "\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Length train set: {len(train_set)}. Length valid set: {len(valid_set)}. Length test set: {len(test_set)}\")\n",
    "    print(f\"train_set ({len(train_set)}): {train_set}\")\n",
    "    print(f\"valid_set ({len(valid_set)}): {valid_set}\")\n",
    "    print(f\"test_set ({len(test_set)}): {test_set} \\n\")\n",
    "    print(f\"Train ∩ Test ({len(train_set & test_set)}): {train_set & test_set}\")\n",
    "    print(f\"Valid ∩ Test ({len(valid_set & test_set)}): {valid_set & test_set}\")\n",
    "    print(f\"Test - (Train ∪ Valid) ({len(test_set - (train_set | valid_set))}): {test_set - (train_set | valid_set)}\")\n",
    "    print(f\"Train ∩ Valid ({len(train_set & valid_set)}): {train_set & valid_set}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128975/1985604591.py:11: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  valid_df = pd.read_csv(valid_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRBJ ---\n",
      "Length train set: 29. Length valid set: 28. Length test set: 28\n",
      "train_set (29): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBV6-5*01', 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-6', 'TRBJ2-1*01', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "valid_set (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "test_set (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'} \n",
      "\n",
      "Train ∩ Test (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "Valid ∩ Test (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "Test - (Train ∪ Valid) (0): set()\n",
      "Train ∩ Valid (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "\n",
      "--- TRBV ---\n",
      "Length train set: 161. Length valid set: 134. Length test set: 113\n",
      "train_set (161): {'TRBV27*01', 'TRBV19', 'TRBV4-3*01', 'TRBV18*01', 'TRBV11-3*04', 'TRBV7-5*01', 'TRBV1*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV22-1', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV7-3*03', 'TRBV5-6*01', 'TRBV12-1*01', 'TRBV30*01', 'TRBV8-1*01', 'TRBV4-1', 'TRBV14*02', 'TRBV3-2', 'TRBV20/OR9-2', 'TRBV8-2', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV1', 'TRBV7-9*07', 'TRBV16', 'TRBV7-9*03', 'TRBV7-1*01', 'TRBV21-1*01', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV26', 'TRBV7-1', 'TRBV9*01', 'TRBV15*02', 'TRBV7-4', 'TRBV6-6*02', 'TRBV5-7*01', 'TRBJ2-7*01', 'TRBV11-1', 'TRBV17', 'TRBV20-1*01', 'TRBV13*01', 'TRBV6-4*02', 'TRBV6-5', 'TRBV7-2', 'TRBV28*01', 'TRBV29-1*01', 'TRBV23-1', 'TRBV8-1', 'TRBV6-7*01', 'TRBV27', 'TRBV9*03', 'TRBV19*01', 'TRBV2*02', 'TRBV8-2*01', 'TRBV7-5', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV5-4*02', 'TRBV4-3*04', 'TRBV16*01', 'TRBV6-8*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV12-2*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-2', 'TRBV12-3', 'TRBV7-3', 'TRBV5-8', 'TRBV6-1*01', 'TRBV11-2*03', 'TRBV4-1*02', 'TRBV17*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV30*02', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV7-8*03', 'TRBV7-2*02', 'TRBV5-4*01', 'TRBV6-9*01', 'TRBV5-2*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV6-6*04', 'TRBV12-5', 'TRBV5-6', 'TRBV11-3*02', 'TRBV25-1*01', 'TRBV2*03', 'TRBV7-8*02', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV20-1*02', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV23-1*01', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV7-5*02', 'TRBV10-1', 'TRBV10-1*01', 'TRBV5-4', 'TRBV14', 'TRBV10-1*02', 'TRBV11-2*02', 'TRBV6-2*01', 'TRBV22-1*01', 'TRBV20-1*04', 'TRBV24-1', 'TRBV12-4*02', 'TRBV30*05', 'TRBV7-9*04', 'TRBV6-1', 'TRBV26*01', 'TRBV4-3', 'TRBV5-5*01', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "valid_set (134): {'TRBV27*01', 'TRBV19', 'TRBV18*01', 'TRBV4-3*01', 'TRBV1*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV10-2*02', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV3-2', 'TRBV8-2', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV1', 'TRBV16', 'TRBV7-9*03', 'TRBV7-1*01', 'TRBV21-1*01', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV7-1', 'TRBV9*01', 'TRBV15*02', 'TRBV6-6*02', 'TRBV7-4', 'TRBV5-7*01', 'TRBV11-1', 'TRBV17', 'TRBV20-1*01', 'TRBV13*01', 'TRBV6-5', 'TRBV7-2', 'TRBV28*01', 'TRBV29-1*01', 'TRBV23-1', 'TRBV8-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV16*01', 'TRBV6-8*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV12-2*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-2', 'TRBV12-3', 'TRBV5-8', 'TRBV7-3', 'TRBV6-1*01', 'TRBV11-2*03', 'TRBV17*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV30*02', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV7-2*02', 'TRBV7-8*03', 'TRBV5-4*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV6-6*04', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV23-1*01', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV7-5*02', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV10-1*02', 'TRBV11-2*02', 'TRBV6-2*01', 'TRBV20-1*04', 'TRBV22-1*01', 'TRBV24-1', 'TRBV30*05', 'TRBV6-1', 'TRBV26*01', 'TRBV4-3', 'TRBV5-5*01', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "test_set (113): {'TRBV27*01', 'TRBV19', 'TRBV4-3*01', 'TRBV18*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV7-2*03', 'TRBV16', 'TRBV7-9*03', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV9*01', 'TRBV15*02', 'TRBV6-6*02', 'TRBV7-4', 'TRBV11-1', 'TRBV20-1*01', 'TRBV13*01', 'TRBV29-1*01', 'TRBV7-2', 'TRBV28*01', 'TRBV6-5', 'TRBV23-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV16*01', 'TRBV6-8*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-3', 'TRBV5-8', 'TRBV7-3', 'TRBV6-1*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV5-4*01', 'TRBV6-9*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV7-8*02', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV20-1*02', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV6-2*01', 'TRBV11-2*02', 'TRBV24-1', 'TRBV6-3*01', 'TRBV6-1', 'TRBV5-5*01', 'TRBV4-3', 'TRBV25-1', 'TRBV24-1*01'} \n",
      "\n",
      "Train ∩ Test (111): {'TRBV27*01', 'TRBV19', 'TRBV4-3*01', 'TRBV18*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV16', 'TRBV7-9*03', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV9*01', 'TRBV15*02', 'TRBV6-6*02', 'TRBV7-4', 'TRBV11-1', 'TRBV20-1*01', 'TRBV13*01', 'TRBV29-1*01', 'TRBV7-2', 'TRBV28*01', 'TRBV6-5', 'TRBV23-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV6-8*01', 'TRBV16*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-3', 'TRBV5-8', 'TRBV7-3', 'TRBV6-1*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV5-4*01', 'TRBV6-9*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV7-8*02', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV20-1*02', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV6-2*01', 'TRBV11-2*02', 'TRBV24-1', 'TRBV6-1', 'TRBV5-5*01', 'TRBV4-3', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "Valid ∩ Test (108): {'TRBV27*01', 'TRBV19', 'TRBV4-3*01', 'TRBV18*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV16', 'TRBV7-9*03', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV9*01', 'TRBV15*02', 'TRBV6-6*02', 'TRBV7-4', 'TRBV11-1', 'TRBV20-1*01', 'TRBV13*01', 'TRBV29-1*01', 'TRBV7-2', 'TRBV28*01', 'TRBV6-5', 'TRBV23-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV6-8*01', 'TRBV16*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-3', 'TRBV5-8', 'TRBV7-3', 'TRBV6-1*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV5-4*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV6-2*01', 'TRBV11-2*02', 'TRBV24-1', 'TRBV6-1', 'TRBV5-5*01', 'TRBV4-3', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "Test - (Train ∪ Valid) (2): {'TRBV7-2*03', 'TRBV6-3*01'}\n",
      "Train ∩ Valid (133): {'TRBV27*01', 'TRBV19', 'TRBV18*01', 'TRBV4-3*01', 'TRBV1*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV3-2', 'TRBV8-2', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV1', 'TRBV16', 'TRBV7-9*03', 'TRBV7-1*01', 'TRBV21-1*01', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV7-1', 'TRBV9*01', 'TRBV15*02', 'TRBV6-6*02', 'TRBV7-4', 'TRBV5-7*01', 'TRBV11-1', 'TRBV17', 'TRBV20-1*01', 'TRBV13*01', 'TRBV6-5', 'TRBV7-2', 'TRBV28*01', 'TRBV29-1*01', 'TRBV23-1', 'TRBV8-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV6-8*01', 'TRBV16*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV12-2*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-2', 'TRBV12-3', 'TRBV5-8', 'TRBV7-3', 'TRBV6-1*01', 'TRBV11-2*03', 'TRBV17*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV30*02', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV7-2*02', 'TRBV7-8*03', 'TRBV5-4*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV6-6*04', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV23-1*01', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV7-5*02', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV10-1*02', 'TRBV11-2*02', 'TRBV6-2*01', 'TRBV20-1*04', 'TRBV22-1*01', 'TRBV24-1', 'TRBV30*05', 'TRBV6-1', 'TRBV26*01', 'TRBV4-3', 'TRBV5-5*01', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "\n",
      "--- MHC ---\n",
      "Length train set: 60. Length valid set: 57. Length test set: 61\n",
      "train_set (60): {nan, 'HLA-B*27:05', 'HLA-B*44:05', 'HLA-B*38:01', 'HLA-B*18', 'HLA-A*02:14', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-A*02:02', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:04', 'HLA-A*02:09', 'HLA-A*02:01', 'HLA-A*02:08', 'HLA-B*41:02', 'HLA-C*04:01', 'HLA-A*02:11', 'HLA-B*15:01', 'HLA-B*44:08', 'HLA-A*01:01', 'HLA-A*02:16', 'HLA-A*68:01', 'HLA-A*02:03', 'HLA-B*08', 'HLA-B*35:08', 'HLA-C*07:02', 'HLA-B*57:03', 'HLA-A*80:01', 'HLA-A*02:15', 'HLA-A*30:01', 'HLA-B*51:01', 'HLA-B*35', 'HLA-A*02', 'HLA-B*44:02', 'HLA-A*01', 'HLA-A*02:13', 'HLA-B*18:01', 'HLA-A*02:17', 'HLA-C*03:04', 'HLA-B*35:01', 'HLA-B*27', 'HLA-B*44:03', 'HLA-A*11', 'HLA-A*02:05', 'HLA-A*02:07', 'HLA-B*27:09', 'HLA-B*35:02', 'HLA-B*39:01', 'HLA-B*07:02', 'HLA-A*11:01', 'HLA-A*02:06', 'HLA-A*03:01', 'HLA-B*57', 'HLA-A*02:10', 'HLA-C*12:02', 'HLA-B*40:01', 'HLA-B*37:01', 'HLA-B*07'}\n",
      "valid_set (57): {'HLA-B*48:01', 'HLA-C*08:01', nan, 'HLA-A*30:02', 'HLA-B*27:05', 'HLA-B*44:05', 'HLA-B*38:01', 'HLA-B*18', 'HLA-C*14:02', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-C*08:02', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-C*07:01', 'HLA-B*42:01', 'HLA-A*02:01', 'HLA-C*03', 'HLA-B*50:01', 'HLA-B*41:02', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-C*06:02', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*15', 'HLA-B*35:08', 'HLA-B*42', 'HLA-C*07:02', 'HLA-B*57:03', 'HLA-A*80:01', 'HLA-B*51:01', 'HLA-C*05:01', 'HLA-A*02', 'HLA-B*44:02', 'HLA-B*18:01', 'HLA-C*03:04', 'HLA-B*44:03', 'HLA-B*35:01', 'HLA-B*27', 'HLA-A*11', 'HLA-A*02:05', 'HLA-A*23:01', 'HLA-B*39:01', 'HLA-B*07:02', 'HLA-C*03:03', 'HLA-A*11:01', 'HLA-A*02:06', 'HLA-A*03:01', 'HLA-B*57', 'HLA-C*16:01', 'HLA-A*32:01', 'HLA-A*02:10', 'HLA-C*12:02', 'HLA-B*40:01', 'HLA-B*37:01', 'HLA-B*07'}\n",
      "test_set (61): {'HLA-E*01:03', nan, 'HLA-A*30:02', 'HLA-B*27:05', 'HLA-A*02:06:01:03', 'HLA-B*35:42:02', 'HLA-B*38:01', 'HLA-C*01:02', 'HLA-C*14:02', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-E*01:01:01:03', 'HLA-C*08:02', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:01:48', 'HLA-A*24:02:33', 'HLA-A*02:01', 'HLA-B*35:42:01', 'HLA-B*27:05:31', 'HLA-C*04:01', 'HLA-A*01:01:73', 'HLA-B*53', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*24:02:84', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*15', 'HLA-A*03', 'HLA-B*08:01:29', 'HLA-B*42', 'HLA-B*35:08', 'HLA-B*58', 'HLA-A*25:01', 'HLA-B*07:02:48', 'HLA-B*51:01', 'HLA-B*35', 'HLA-A*01', 'HLA-C*03:04', 'HLA-B*35:01', 'HLA-B*27', 'HLA-B*44:03:08', 'HLA-B*44:03', 'HLA-A*11', 'HLA-B*35:01:45', 'HLA-B*14:02', 'HLA-B*07:02', 'HLA-C*03:03', 'HLA-B*57:06', 'HLA-A*11:01', 'HLA-A*03:01', 'HLA-B*57', 'HLA-A*32:01', 'HLA-A*11:01:18', 'HLA-A*02:266', 'HLA-A*02', 'HLA-B*81:01', 'HLA-A*02:01:98', 'HLA-B*07'} \n",
      "\n",
      "Train ∩ Test (29): {nan, 'HLA-B*27:05', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-B*07', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:01', 'HLA-C*04:01', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*35:08', 'HLA-B*51:01', 'HLA-B*35', 'HLA-A*01', 'HLA-C*03:04', 'HLA-B*35:01', 'HLA-B*27', 'HLA-B*44:03', 'HLA-A*11', 'HLA-B*07:02', 'HLA-A*11:01', 'HLA-A*03:01', 'HLA-B*57', 'HLA-A*02', 'HLA-B*38:01'}\n",
      "Valid ∩ Test (33): {nan, 'HLA-A*30:02', 'HLA-B*27:05', 'HLA-C*14:02', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-B*07', 'HLA-C*08:02', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:01', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*15', 'HLA-B*35:08', 'HLA-B*42', 'HLA-B*51:01', 'HLA-C*03:04', 'HLA-B*44:03', 'HLA-B*35:01', 'HLA-B*27', 'HLA-A*11', 'HLA-B*07:02', 'HLA-C*03:03', 'HLA-A*11:01', 'HLA-A*03:01', 'HLA-B*57', 'HLA-A*32:01', 'HLA-A*02', 'HLA-B*38:01'}\n",
      "Test - (Train ∪ Valid) (25): {'HLA-E*01:03', 'HLA-A*02:06:01:03', 'HLA-B*35:42:02', 'HLA-C*01:02', 'HLA-E*01:01:01:03', 'HLA-A*02:01:48', 'HLA-A*24:02:33', 'HLA-B*35:42:01', 'HLA-B*27:05:31', 'HLA-A*01:01:73', 'HLA-B*53', 'HLA-A*24:02:84', 'HLA-A*03', 'HLA-B*08:01:29', 'HLA-B*58', 'HLA-A*25:01', 'HLA-B*07:02:48', 'HLA-B*44:03:08', 'HLA-B*35:01:45', 'HLA-B*14:02', 'HLA-B*57:06', 'HLA-A*11:01:18', 'HLA-B*81:01', 'HLA-A*02:01:98', 'HLA-A*02:266'}\n",
      "Train ∩ Valid (41): {nan, 'HLA-B*44:05', 'HLA-B*27:05', 'HLA-B*18', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-B*07', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:01', 'HLA-B*41:02', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*35:08', 'HLA-C*07:02', 'HLA-B*57:03', 'HLA-A*80:01', 'HLA-B*51:01', 'HLA-B*37:01', 'HLA-B*44:02', 'HLA-B*18:01', 'HLA-C*03:04', 'HLA-B*44:03', 'HLA-B*35:01', 'HLA-B*27', 'HLA-A*11', 'HLA-A*02:05', 'HLA-B*39:01', 'HLA-B*07:02', 'HLA-A*11:01', 'HLA-A*02:06', 'HLA-A*03:01', 'HLA-B*57', 'HLA-A*02:10', 'HLA-C*12:02', 'HLA-B*40:01', 'HLA-A*02', 'HLA-B*38:01'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# def normalize(value):\n",
    "#     if pd.isna(value):\n",
    "#         return ''\n",
    "#     value = value.lower()  # lowercase\n",
    "#     value = re.sub(r'[^a-z0-9]', '', value)  # remove non-alphanumeric\n",
    "#     return value\n",
    "\n",
    "# def normalize_columns(df, columns):\n",
    "#     return df[columns].map(normalize)\n",
    "\n",
    "# # Normalize columns\n",
    "# train_norm = normalize_columns(train_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "# valid_norm = normalize_columns(valid_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "# test_norm  = normalize_columns(test_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "\n",
    "# Compare sets\n",
    "for col in ['TRBJ', 'TRBV', 'MHC']:\n",
    "    train_set = set(train_df[col])\n",
    "    valid_set = set(valid_df[col])\n",
    "    test_set = set(test_df[col])\n",
    "\n",
    "\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Length train set: {len(train_set)}. Length valid set: {len(valid_set)}. Length test set: {len(test_set)}\")\n",
    "    print(f\"train_set ({len(train_set)}): {train_set}\")\n",
    "    print(f\"valid_set ({len(valid_set)}): {valid_set}\")\n",
    "    print(f\"test_set ({len(test_set)}): {test_set} \\n\")\n",
    "    print(f\"Train ∩ Test ({len(train_set & test_set)}): {train_set & test_set}\")\n",
    "    print(f\"Valid ∩ Test ({len(valid_set & test_set)}): {valid_set & test_set}\")\n",
    "    print(f\"Test - (Train ∪ Valid) ({len(test_set - (train_set | valid_set))}): {test_set - (train_set | valid_set)}\")\n",
    "    print(f\"Train ∩ Valid ({len(train_set & valid_set)}): {train_set & valid_set}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train dataset (Total rows: 757732):\n",
      "  TRBJ: 3793 empty or missing values (Missing: 3793, Empty strings: 0) => 0.50%\n",
      "  TRBV: 2118 empty or missing values (Missing: 2118, Empty strings: 0) => 0.28%\n",
      "  MHC: 256689 empty or missing values (Missing: 256689, Empty strings: 0) => 33.88%\n",
      "\n",
      "Validation dataset (Total rows: 107700):\n",
      "  TRBJ: 15760 empty or missing values (Missing: 15760, Empty strings: 0) => 14.63%\n",
      "  TRBV: 14491 empty or missing values (Missing: 14491, Empty strings: 0) => 13.45%\n",
      "  MHC: 5819 empty or missing values (Missing: 5819, Empty strings: 0) => 5.40%\n",
      "\n",
      "Test dataset (Total rows: 45112):\n",
      "  TRBJ: 18 empty or missing values (Missing: 18, Empty strings: 0) => 0.04%\n",
      "  TRBV: 12 empty or missing values (Missing: 12, Empty strings: 0) => 0.03%\n",
      "  MHC: 782 empty or missing values (Missing: 782, Empty strings: 0) => 1.73%\n"
     ]
    }
   ],
   "source": [
    "# Columns to check\n",
    "columns_to_check = ['TRBJ', 'TRBV', 'MHC']\n",
    "\n",
    "# Function to count and report missing/empty values with percentage\n",
    "def count_empty_or_missing(df, name):\n",
    "    total_rows = len(df)\n",
    "    print(f\"\\n{name} dataset (Total rows: {total_rows}):\")\n",
    "    for col in columns_to_check:\n",
    "        missing = df[col].isna().sum()\n",
    "        empty = (df[col].astype(str).str.strip() == '').sum()\n",
    "        total = missing + empty\n",
    "        percent = (total / total_rows) * 100 if total_rows > 0 else 0\n",
    "        print(f\"  {col}: {total} empty or missing values \"\n",
    "              f\"(Missing: {missing}, Empty strings: {empty}) \"\n",
    "              f\"=> {percent:.2f}%\")\n",
    "\n",
    "# Run the check\n",
    "count_empty_or_missing(train_df, \"Train\")\n",
    "count_empty_or_missing(valid_df, \"Validation\")\n",
    "count_empty_or_missing(test_df, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97337/136104671.py:11: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  valid_df = pd.read_csv(valid_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRB_CDR3 ---\n",
      "Length train set: 186941. Length valid set: 63634. Length test set: 14016\n",
      "Train ∩ Test (6019)\n",
      "Valid ∩ Test (3963)\n",
      "Test - (Train ∪ Valid) (7550)\n",
      "Train ∩ Valid (46672)\n",
      "\n",
      "--- Epitope ---\n",
      "Length train set: 993. Length valid set: 1455. Length test set: 306\n",
      "Train ∩ Test (130)\n",
      "Valid ∩ Test (155)\n",
      "Test - (Train ∪ Valid) (142)\n",
      "Train ∩ Valid (694)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# def normalize(value):\n",
    "#     if pd.isna(value):\n",
    "#         return ''\n",
    "#     value = value.lower()  # lowercase\n",
    "#     value = re.sub(r'[^a-z0-9]', '', value)  # remove non-alphanumeric\n",
    "#     return value\n",
    "\n",
    "# def normalize_columns(df, columns):\n",
    "#     return df[columns].map(normalize)\n",
    "\n",
    "# # Normalize columns\n",
    "# train_norm = normalize_columns(train_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "# valid_norm = normalize_columns(valid_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "# test_norm  = normalize_columns(test_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "\n",
    "# Compare sets\n",
    "for col in ['TRB_CDR3', 'Epitope']:\n",
    "    train_set = set(train_df[col])\n",
    "    valid_set = set(valid_df[col])\n",
    "    test_set = set(test_df[col])\n",
    "\n",
    "\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Length train set: {len(train_set)}. Length valid set: {len(valid_set)}. Length test set: {len(test_set)}\")\n",
    "    # print(f\"train_set ({len(train_set)}): {train_set}\")\n",
    "    # print(f\"valid_set ({len(valid_set)}): {valid_set}\")\n",
    "    # print(f\"test_set ({len(test_set)}): {test_set} \\n\")\n",
    "    print(f\"Train ∩ Test ({len(train_set & test_set)})\")\n",
    "    print(f\"Valid ∩ Test ({len(valid_set & test_set)})\")\n",
    "    print(f\"Test - (Train ∪ Valid) ({len(test_set - (train_set | valid_set))})\")\n",
    "    print(f\"Train ∩ Valid ({len(train_set & valid_set)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### again.. investigations on NEW DATA--- 15.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128975/903881231.py:10: DtypeWarning: Columns (1,7,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRBJ ---\n",
      "Length train set: 29. Length valid set: 28. Length test set: 28\n",
      "train_set (29): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBV6-5*01', 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-6', 'TRBJ2-1*01', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "valid_set (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "test_set (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ1-4*01', 'TRBJ2-7', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'} \n",
      "\n",
      "Train ∩ Test (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "Valid ∩ Test (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "Test - (Train ∪ Valid) (0): set()\n",
      "Train ∩ Valid (28): {'TRBJ2-4*01', 'TRBJ1-2*01', nan, 'TRBJ2-7', 'TRBJ1-4*01', 'TRBJ2-1*01', 'TRBJ2-6', 'TRBJ2-2', 'TRBJ1-6*01', 'TRBJ1-1', 'TRBJ2-3', 'TRBJ2-2*01', 'TRBJ2-5*01', 'TRBJ1-4', 'TRBJ2-7*01', 'TRBJ1-1*01', 'TRBJ1-2', 'TRBJ1-5', 'TRBJ2-5', 'TRBJ2-6*01', 'TRBJ2-1', 'TRBJ1-6', 'TRBJ2-4', 'TRBJ1-3', 'TRBJ1-5*01', 'TRBJ2-3*01', 'TRBJ1-3*01', 'TRBJ1-6*02'}\n",
      "\n",
      "--- TRBV ---\n",
      "Length train set: 161. Length valid set: 134. Length test set: 108\n",
      "train_set (161): {'TRBV27*01', 'TRBV19', 'TRBV4-3*01', 'TRBV18*01', 'TRBV11-3*04', 'TRBV7-5*01', 'TRBV1*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV22-1', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV7-3*03', 'TRBV5-6*01', 'TRBV8-1*01', 'TRBV30*01', 'TRBV12-1*01', 'TRBV4-1', 'TRBV14*02', 'TRBV3-2', 'TRBV20/OR9-2', 'TRBV8-2', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV1', 'TRBV7-9*07', 'TRBV16', 'TRBV7-9*03', 'TRBV7-1*01', 'TRBV21-1*01', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV26', 'TRBV7-1', 'TRBV9*01', 'TRBV15*02', 'TRBV6-6*02', 'TRBV7-4', 'TRBV5-7*01', 'TRBJ2-7*01', 'TRBV11-1', 'TRBV17', 'TRBV20-1*01', 'TRBV13*01', 'TRBV6-4*02', 'TRBV6-5', 'TRBV29-1*01', 'TRBV7-2', 'TRBV28*01', 'TRBV23-1', 'TRBV8-1', 'TRBV6-7*01', 'TRBV27', 'TRBV9*03', 'TRBV19*01', 'TRBV2*02', 'TRBV8-2*01', 'TRBV7-5', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV5-4*02', 'TRBV16*01', 'TRBV6-8*01', 'TRBV4-3*04', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV12-2*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-2', 'TRBV12-3', 'TRBV5-8', 'TRBV7-3', 'TRBV6-1*01', 'TRBV11-2*03', 'TRBV4-1*02', 'TRBV17*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV30*02', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV7-8*03', 'TRBV7-2*02', 'TRBV5-4*01', 'TRBV6-9*01', 'TRBV5-2*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV6-6*04', 'TRBV11-1*01', 'TRBV12-5', 'TRBV5-6', 'TRBV11-3*02', 'TRBV25-1*01', 'TRBV2*03', 'TRBV7-8*02', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV20-1*02', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV23-1*01', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV7-5*02', 'TRBV10-1', 'TRBV10-1*01', 'TRBV5-4', 'TRBV14', 'TRBV10-1*02', 'TRBV11-2*02', 'TRBV6-2*01', 'TRBV20-1*04', 'TRBV22-1*01', 'TRBV24-1', 'TRBV12-4*02', 'TRBV30*05', 'TRBV7-9*04', 'TRBV6-1', 'TRBV26*01', 'TRBV4-3', 'TRBV5-5*01', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "valid_set (134): {'TRBV27*01', 'TRBV19', 'TRBV18*01', 'TRBV4-3*01', 'TRBV1*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV10-2*02', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV3-2', 'TRBV8-2', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV1', 'TRBV16', 'TRBV7-9*03', 'TRBV7-1*01', 'TRBV21-1*01', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV7-1', 'TRBV9*01', 'TRBV15*02', 'TRBV6-6*02', 'TRBV7-4', 'TRBV5-7*01', 'TRBV11-1', 'TRBV17', 'TRBV20-1*01', 'TRBV13*01', 'TRBV6-5', 'TRBV7-2', 'TRBV28*01', 'TRBV29-1*01', 'TRBV23-1', 'TRBV8-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV16*01', 'TRBV6-8*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV12-2*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-2', 'TRBV12-3', 'TRBV5-8', 'TRBV7-3', 'TRBV6-1*01', 'TRBV11-2*03', 'TRBV17*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV30*02', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV7-2*02', 'TRBV7-8*03', 'TRBV5-4*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV6-6*04', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV23-1*01', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV7-5*02', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV10-1*02', 'TRBV11-2*02', 'TRBV6-2*01', 'TRBV20-1*04', 'TRBV22-1*01', 'TRBV24-1', 'TRBV30*05', 'TRBV6-1', 'TRBV26*01', 'TRBV4-3', 'TRBV5-5*01', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "test_set (108): {'TRBV27*01', 'TRBV19', 'TRBV18*01', 'TRBV4-3*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-2*03', 'TRBV7-9', 'TRBV16', 'TRBV7-9*03', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV9*01', 'TRBV6-6*02', 'TRBV7-4', 'TRBV11-1', 'TRBV20-1*01', 'TRBV13*01', 'TRBV29-1*01', 'TRBV6-5', 'TRBV28*01', 'TRBV7-2', 'TRBV23-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV6-8*01', 'TRBV16*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV5-1*01', 'TRBV13', 'TRBV15*01', 'TRBV2*01', 'TRBV7-3', 'TRBV5-8', 'TRBV12-3', 'TRBV6-1*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV5-4*01', 'TRBV6-9*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV7-8*02', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV20-1*02', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV6-2*01', 'TRBV24-1', 'TRBV6-3*01', 'TRBV6-1', 'TRBV5-5*01', 'TRBV4-3', 'TRBV25-1', 'TRBV24-1*01'} \n",
      "\n",
      "Train ∩ Test (106): {'TRBV27*01', 'TRBV19', 'TRBV18*01', 'TRBV4-3*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV16', 'TRBV7-9*03', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV9*01', 'TRBV6-6*02', 'TRBV7-4', 'TRBV11-1', 'TRBV20-1*01', 'TRBV13*01', 'TRBV29-1*01', 'TRBV6-5', 'TRBV28*01', 'TRBV7-2', 'TRBV23-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV16*01', 'TRBV6-8*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV5-1*01', 'TRBV13', 'TRBV15*01', 'TRBV2*01', 'TRBV7-3', 'TRBV5-8', 'TRBV12-3', 'TRBV6-1*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV5-4*01', 'TRBV6-9*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV7-8*02', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV20-1*02', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV6-2*01', 'TRBV24-1', 'TRBV6-1', 'TRBV5-5*01', 'TRBV4-3', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "Valid ∩ Test (103): {'TRBV27*01', 'TRBV19', 'TRBV18*01', 'TRBV4-3*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV16', 'TRBV7-9*03', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV9*01', 'TRBV6-6*02', 'TRBV7-4', 'TRBV11-1', 'TRBV20-1*01', 'TRBV13*01', 'TRBV29-1*01', 'TRBV6-5', 'TRBV28*01', 'TRBV7-2', 'TRBV23-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV16*01', 'TRBV6-8*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV5-1*01', 'TRBV13', 'TRBV15*01', 'TRBV2*01', 'TRBV7-3', 'TRBV5-8', 'TRBV12-3', 'TRBV6-1*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV5-4*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV6-2*01', 'TRBV24-1', 'TRBV6-1', 'TRBV5-5*01', 'TRBV4-3', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "Test - (Train ∪ Valid) (2): {'TRBV7-2*03', 'TRBV6-3*01'}\n",
      "Train ∩ Valid (133): {'TRBV27*01', 'TRBV19', 'TRBV18*01', 'TRBV4-3*01', 'TRBV1*01', 'TRBV6-6*01', 'TRBV10-3', 'TRBV4-2', 'TRBV5-8*01', 'TRBV12-1', 'TRBV11-2', 'TRBV5-3', 'TRBV10-2', 'TRBV6-9', 'TRBV2', 'TRBV7-7*01', 'TRBV5-6*01', 'TRBV30*01', 'TRBV4-1', 'TRBV3-2', 'TRBV8-2', 'TRBV18', 'TRBV10-3*02', 'TRBV28', 'TRBV12-3*01', 'TRBV7-9', 'TRBV1', 'TRBV16', 'TRBV7-9*03', 'TRBV7-1*01', 'TRBV21-1*01', 'TRBV7-2*01', 'TRBV10-2*01', 'TRBV4-2*01', 'TRBV9*02', 'TRBV7-1', 'TRBV9*01', 'TRBV15*02', 'TRBV6-6*02', 'TRBV7-4', 'TRBV5-7*01', 'TRBV11-1', 'TRBV17', 'TRBV20-1*01', 'TRBV13*01', 'TRBV6-5', 'TRBV7-2', 'TRBV28*01', 'TRBV29-1*01', 'TRBV23-1', 'TRBV8-1', 'TRBV27', 'TRBV19*01', 'TRBV7-8', 'TRBV6-6', 'TRBV5-1', 'TRBV5-5', 'TRBV6-7', 'TRBV6-8*01', 'TRBV16*01', 'TRBV4-1*01', 'TRBV6-5*01', 'TRBV12-2*01', 'TRBV7-4*01', 'TRBV3-1', 'TRBV11-3', 'TRBV6-2', 'TRBV20-1', 'TRBV7-3*01', 'TRBV7-8*01', 'TRBV13', 'TRBV5-1*01', 'TRBV15*01', 'TRBV2*01', 'TRBV12-2', 'TRBV12-3', 'TRBV5-8', 'TRBV7-3', 'TRBV6-1*01', 'TRBV11-2*03', 'TRBV17*01', 'TRBV15', 'TRBV11-2*01', 'TRBV5-7', 'TRBV30*02', 'TRBV7-6*01', 'TRBV6-4*01', 'TRBV7-2*02', 'TRBV7-8*03', 'TRBV5-4*01', 'TRBV9', 'TRBV30', 'TRBV7-7', 'TRBV7-6', 'TRBV11-1*01', 'TRBV6-6*04', 'TRBV12-5', 'TRBV5-6', 'TRBV25-1*01', 'TRBV11-3*01', 'TRBV7-9*01', 'TRBV19*02', nan, 'TRBV12-5*01', 'TRBV6-3', 'TRBV12-4*01', 'TRBV12-4', 'TRBV29-1', 'TRBV21-1', 'TRBV23-1*01', 'TRBV3-1*01', 'TRBV6-4', 'TRBV6-8', 'TRBV14*01', 'TRBV10-3*01', 'TRBV7-5*02', 'TRBV10-1*01', 'TRBV10-1', 'TRBV5-4', 'TRBV14', 'TRBV10-1*02', 'TRBV11-2*02', 'TRBV6-2*01', 'TRBV20-1*04', 'TRBV22-1*01', 'TRBV24-1', 'TRBV30*05', 'TRBV6-1', 'TRBV26*01', 'TRBV4-3', 'TRBV5-5*01', 'TRBV25-1', 'TRBV24-1*01'}\n",
      "\n",
      "--- MHC ---\n",
      "Length train set: 55. Length valid set: 57. Length test set: 59\n",
      "train_set (55): {nan, 'HLA-B*27:05', 'HLA-B*44:05', 'HLA-B*18', 'HLA-A*02:14', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-B*07', 'HLA-A*02:02', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:04', 'HLA-A*02:09', 'HLA-A*02:01', 'HLA-B*41:02', 'HLA-C*04:01', 'HLA-A*02:08', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*02:16', 'HLA-A*68:01', 'HLA-A*02:03', 'HLA-B*08', 'HLA-B*35:08', 'HLA-C*07:02', 'HLA-B*57:03', 'HLA-A*80:01', 'HLA-A*30:01', 'HLA-A*02:15', 'HLA-B*51:01', 'HLA-B*35', 'HLA-B*37:01', 'HLA-B*44:02', 'HLA-A*01', 'HLA-A*02:07', 'HLA-B*18:01', 'HLA-A*02:17', 'HLA-C*03:04', 'HLA-B*44:03', 'HLA-B*35:01', 'HLA-B*27', 'HLA-A*11', 'HLA-A*02:05', 'HLA-B*35:02', 'HLA-B*39:01', 'HLA-B*07:02', 'HLA-A*02:06', 'HLA-A*11:01', 'HLA-B*57', 'HLA-A*03:01', 'HLA-C*12:02', 'HLA-B*40:01', 'HLA-A*02', 'HLA-B*38:01'}\n",
      "valid_set (57): {'HLA-B*48:01', 'HLA-C*08:01', nan, 'HLA-A*30:02', 'HLA-B*27:05', 'HLA-B*44:05', 'HLA-B*38:01', 'HLA-B*18', 'HLA-C*14:02', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-C*08:02', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-C*07:01', 'HLA-B*42:01', 'HLA-A*02:01', 'HLA-C*03', 'HLA-B*50:01', 'HLA-B*41:02', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-C*06:02', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*15', 'HLA-B*35:08', 'HLA-B*42', 'HLA-C*07:02', 'HLA-B*57:03', 'HLA-A*80:01', 'HLA-B*51:01', 'HLA-C*05:01', 'HLA-A*02', 'HLA-B*44:02', 'HLA-B*18:01', 'HLA-C*03:04', 'HLA-B*44:03', 'HLA-B*35:01', 'HLA-B*27', 'HLA-A*11', 'HLA-A*02:05', 'HLA-A*23:01', 'HLA-B*39:01', 'HLA-B*07:02', 'HLA-C*03:03', 'HLA-A*11:01', 'HLA-A*02:06', 'HLA-A*03:01', 'HLA-B*57', 'HLA-C*16:01', 'HLA-A*32:01', 'HLA-A*02:10', 'HLA-C*12:02', 'HLA-B*40:01', 'HLA-B*37:01', 'HLA-B*07'}\n",
      "test_set (59): {'HLA-E*01:03', nan, 'HLA-A*30:02', 'HLA-B*27:05', 'HLA-A*02:06:01:03', 'HLA-B*35:42:02', 'HLA-C*01:02', 'HLA-C*14:02', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-B*07', 'HLA-C*08:02', 'HLA-E*01:01:01:03', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:01:48', 'HLA-A*24:02:33', 'HLA-A*02:01', 'HLA-B*35:42:01', 'HLA-B*27:05:31', 'HLA-C*04:01', 'HLA-A*01:01:73', 'HLA-B*53', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*24:02:84', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*15', 'HLA-A*03', 'HLA-B*08:01:29', 'HLA-B*35:08', 'HLA-B*58', 'HLA-B*42', 'HLA-A*25:01', 'HLA-B*51:01', 'HLA-B*35', 'HLA-A*01', 'HLA-C*03:04', 'HLA-B*35:01', 'HLA-B*44:03', 'HLA-B*27', 'HLA-A*11', 'HLA-B*35:01:45', 'HLA-B*44:03:08', 'HLA-B*14:02', 'HLA-B*07:02', 'HLA-C*03:03', 'HLA-A*11:01', 'HLA-A*03:01', 'HLA-A*32:01', 'HLA-B*57', 'HLA-A*11:01:18', 'HLA-A*02:266', 'HLA-A*02', 'HLA-B*81:01', 'HLA-A*02:01:98', 'HLA-B*38:01'} \n",
      "\n",
      "Train ∩ Test (29): {nan, 'HLA-B*27:05', 'HLA-B*38:01', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:01', 'HLA-C*04:01', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*35:08', 'HLA-B*51:01', 'HLA-B*35', 'HLA-A*01', 'HLA-C*03:04', 'HLA-B*44:03', 'HLA-B*35:01', 'HLA-B*27', 'HLA-A*11', 'HLA-B*07:02', 'HLA-A*11:01', 'HLA-B*57', 'HLA-A*03:01', 'HLA-A*02', 'HLA-B*07'}\n",
      "Valid ∩ Test (33): {nan, 'HLA-A*30:02', 'HLA-B*27:05', 'HLA-C*14:02', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-B*07', 'HLA-C*08:02', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:01', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*15', 'HLA-B*35:08', 'HLA-B*42', 'HLA-B*51:01', 'HLA-C*03:04', 'HLA-B*44:03', 'HLA-B*35:01', 'HLA-B*27', 'HLA-A*11', 'HLA-B*07:02', 'HLA-C*03:03', 'HLA-A*11:01', 'HLA-A*03:01', 'HLA-B*57', 'HLA-A*32:01', 'HLA-A*02', 'HLA-B*38:01'}\n",
      "Test - (Train ∪ Valid) (23): {'HLA-E*01:03', 'HLA-A*02:06:01:03', 'HLA-B*35:42:02', 'HLA-C*01:02', 'HLA-E*01:01:01:03', 'HLA-A*02:01:48', 'HLA-A*24:02:33', 'HLA-B*35:42:01', 'HLA-B*27:05:31', 'HLA-A*01:01:73', 'HLA-B*53', 'HLA-A*24:02:84', 'HLA-A*03', 'HLA-B*08:01:29', 'HLA-B*58', 'HLA-A*25:01', 'HLA-B*44:03:08', 'HLA-B*35:01:45', 'HLA-B*14:02', 'HLA-A*11:01:18', 'HLA-B*81:01', 'HLA-A*02:01:98', 'HLA-A*02:266'}\n",
      "Train ∩ Valid (40): {nan, 'HLA-B*44:05', 'HLA-B*27:05', 'HLA-B*18', 'HLA-B*38:01', 'HLA-A*24:02', 'HLA-B*08:01', 'HLA-B*57:01', 'HLA-A*29:02', 'HLA-B*42:01', 'HLA-A*02:01', 'HLA-B*41:02', 'HLA-B*15:01', 'HLA-A*01:01', 'HLA-A*68:01', 'HLA-B*08', 'HLA-B*35:08', 'HLA-C*07:02', 'HLA-B*57:03', 'HLA-A*80:01', 'HLA-B*51:01', 'HLA-A*02', 'HLA-B*44:02', 'HLA-B*18:01', 'HLA-C*03:04', 'HLA-B*44:03', 'HLA-B*35:01', 'HLA-B*27', 'HLA-A*11', 'HLA-A*02:05', 'HLA-B*39:01', 'HLA-B*07:02', 'HLA-A*02:06', 'HLA-A*11:01', 'HLA-B*57', 'HLA-A*03:01', 'HLA-C*12:02', 'HLA-B*40:01', 'HLA-B*37:01', 'HLA-B*07'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128975/903881231.py:11: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  valid_df = pd.read_csv(valid_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "test_path = '../../data/splitted_datasets/allele/beta/new/test.tsv'\n",
    "train_path = '../../data/splitted_datasets/allele/beta/new/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/new/validation.tsv'\n",
    "# valid_path = '../../data/ba_splitted/validation.tsv'\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Compare sets\n",
    "for col in ['TRBJ', 'TRBV', 'MHC']:\n",
    "    train_set = set(train_df[col])\n",
    "    valid_set = set(valid_df[col])\n",
    "    test_set = set(test_df[col])\n",
    "\n",
    "\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Length train set: {len(train_set)}. Length valid set: {len(valid_set)}. Length test set: {len(test_set)}\")\n",
    "    print(f\"train_set ({len(train_set)}): {train_set}\")\n",
    "    print(f\"valid_set ({len(valid_set)}): {valid_set}\")\n",
    "    print(f\"test_set ({len(test_set)}): {test_set} \\n\")\n",
    "    print(f\"Train ∩ Test ({len(train_set & test_set)}): {train_set & test_set}\")\n",
    "    print(f\"Valid ∩ Test ({len(valid_set & test_set)}): {valid_set & test_set}\")\n",
    "    print(f\"Test - (Train ∪ Valid) ({len(test_set - (train_set | valid_set))}): {test_set - (train_set | valid_set)}\")\n",
    "    print(f\"Train ∩ Valid ({len(train_set & valid_set)}): {train_set & valid_set}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train dataset (Total rows: 442266):\n",
      "  TRBJ: 2171 empty or missing values (Missing: 2171, Empty strings: 0) => 0.49%\n",
      "  TRBV: 1513 empty or missing values (Missing: 1513, Empty strings: 0) => 0.34%\n",
      "  MHC: 80937 empty or missing values (Missing: 80937, Empty strings: 0) => 18.30%\n",
      "\n",
      "Validation dataset (Total rows: 107700):\n",
      "  TRBJ: 15760 empty or missing values (Missing: 15760, Empty strings: 0) => 14.63%\n",
      "  TRBV: 14491 empty or missing values (Missing: 14491, Empty strings: 0) => 13.45%\n",
      "  MHC: 5819 empty or missing values (Missing: 5819, Empty strings: 0) => 5.40%\n",
      "\n",
      "Test dataset (Total rows: 31255):\n",
      "  TRBJ: 1 empty or missing values (Missing: 1, Empty strings: 0) => 0.00%\n",
      "  TRBV: 0 empty or missing values (Missing: 0, Empty strings: 0) => 0.00%\n",
      "  MHC: 135 empty or missing values (Missing: 135, Empty strings: 0) => 0.43%\n"
     ]
    }
   ],
   "source": [
    "# Columns to check\n",
    "columns_to_check = ['TRBJ', 'TRBV', 'MHC']\n",
    "\n",
    "# Function to count and report missing/empty values with percentage\n",
    "def count_empty_or_missing(df, name):\n",
    "    total_rows = len(df)\n",
    "    print(f\"\\n{name} dataset (Total rows: {total_rows}):\")\n",
    "    for col in columns_to_check:\n",
    "        missing = df[col].isna().sum()\n",
    "        empty = (df[col].astype(str).str.strip() == '').sum()\n",
    "        total = missing + empty\n",
    "        percent = (total / total_rows) * 100 if total_rows > 0 else 0\n",
    "        print(f\"  {col}: {total} empty or missing values \"\n",
    "              f\"(Missing: {missing}, Empty strings: {empty}) \"\n",
    "              f\"=> {percent:.2f}%\")\n",
    "\n",
    "# Run the check\n",
    "count_empty_or_missing(train_df, \"Train\")\n",
    "count_empty_or_missing(valid_df, \"Validation\")\n",
    "count_empty_or_missing(test_df, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dataset', 'generated', '10X'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()\n",
    "valid_df['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train dataset (Total rows: 757732):\n",
      "  TRBJ: 3793 empty or missing values (Missing: 3793, Empty strings: 0) => 0.50%\n",
      "  TRBV: 2118 empty or missing values (Missing: 2118, Empty strings: 0) => 0.28%\n",
      "  MHC: 256689 empty or missing values (Missing: 256689, Empty strings: 0) => 33.88%\n",
      "\n",
      "Train dataset - Missing/empty values by source category:\n",
      "\n",
      "Source: 'generated' (Rows: 315617):\n",
      "  TRBJ: 1501 empty or missing values (Missing: 1501, Empty strings: 0) => 0.48%\n",
      "  TRBV: 484 empty or missing values (Missing: 484, Empty strings: 0) => 0.15%\n",
      "  MHC: 175752 empty or missing values (Missing: 175752, Empty strings: 0) => 55.69%\n",
      "\n",
      "Source: '10X' (Rows: 315652):\n",
      "  TRBJ: 30 empty or missing values (Missing: 30, Empty strings: 0) => 0.01%\n",
      "  TRBV: 0 empty or missing values (Missing: 0, Empty strings: 0) => 0.00%\n",
      "  MHC: 0 empty or missing values (Missing: 0, Empty strings: 0) => 0.00%\n",
      "\n",
      "Source: 'dataset' (Rows: 126463):\n",
      "  TRBJ: 2262 empty or missing values (Missing: 2262, Empty strings: 0) => 1.79%\n",
      "  TRBV: 1634 empty or missing values (Missing: 1634, Empty strings: 0) => 1.29%\n",
      "  MHC: 80937 empty or missing values (Missing: 80937, Empty strings: 0) => 64.00%\n",
      "\n",
      "Validation dataset (Total rows: 169591):\n",
      "  TRBJ: 75779 empty or missing values (Missing: 75779, Empty strings: 0) => 44.68%\n",
      "  TRBV: 69898 empty or missing values (Missing: 69898, Empty strings: 0) => 41.22%\n",
      "  MHC: 35848 empty or missing values (Missing: 35848, Empty strings: 0) => 21.14%\n",
      "\n",
      "Validation dataset - Missing/empty values by source category:\n",
      "\n",
      "Source: 'dataset' (Rows: 29559):\n",
      "  TRBJ: 17022 empty or missing values (Missing: 17022, Empty strings: 0) => 57.59%\n",
      "  TRBV: 15763 empty or missing values (Missing: 15763, Empty strings: 0) => 53.33%\n",
      "  MHC: 5626 empty or missing values (Missing: 5626, Empty strings: 0) => 19.03%\n",
      "\n",
      "Source: 'generated' (Rows: 85279):\n",
      "  TRBJ: 58754 empty or missing values (Missing: 58754, Empty strings: 0) => 68.90%\n",
      "  TRBV: 54135 empty or missing values (Missing: 54135, Empty strings: 0) => 63.48%\n",
      "  MHC: 30222 empty or missing values (Missing: 30222, Empty strings: 0) => 35.44%\n",
      "\n",
      "Source: '10X' (Rows: 54753):\n",
      "  TRBJ: 3 empty or missing values (Missing: 3, Empty strings: 0) => 0.01%\n",
      "  TRBV: 0 empty or missing values (Missing: 0, Empty strings: 0) => 0.00%\n",
      "  MHC: 0 empty or missing values (Missing: 0, Empty strings: 0) => 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Columns to check\n",
    "columns_to_check = ['TRBJ', 'TRBV', 'MHC']\n",
    "\n",
    "# Function to count and report missing/empty values with percentage\n",
    "def count_empty_or_missing(df, name):\n",
    "    total_rows = len(df)\n",
    "    print(f\"\\n{name} dataset (Total rows: {total_rows}):\")\n",
    "    \n",
    "    # Overall check for columns of interest\n",
    "    for col in columns_to_check:\n",
    "        missing = df[col].isna().sum()\n",
    "        empty = (df[col].astype(str).str.strip() == '').sum()\n",
    "        total = missing + empty\n",
    "        percent = (total / total_rows) * 100 if total_rows > 0 else 0\n",
    "        print(f\"  {col}: {total} empty or missing values \"\n",
    "              f\"(Missing: {missing}, Empty strings: {empty}) \"\n",
    "              f\"=> {percent:.2f}%\")\n",
    "    \n",
    "    # Check by source category\n",
    "    print(f\"\\n{name} dataset - Missing/empty values by source category:\")\n",
    "    \n",
    "    # Get unique source categories\n",
    "    source_categories = df['source'].unique()\n",
    "    \n",
    "    for category in source_categories:\n",
    "        subset = df[df['source'] == category]\n",
    "        category_rows = len(subset)\n",
    "        \n",
    "        print(f\"\\nSource: '{category}' (Rows: {category_rows}):\")\n",
    "        \n",
    "        for col in columns_to_check:\n",
    "            missing = subset[col].isna().sum()\n",
    "            empty = (subset[col].astype(str).str.strip() == '').sum()\n",
    "            total = missing + empty\n",
    "            percent = (total / category_rows) * 100 if category_rows > 0 else 0\n",
    "            print(f\"  {col}: {total} empty or missing values \"\n",
    "                  f\"(Missing: {missing}, Empty strings: {empty}) \"\n",
    "                  f\"=> {percent:.2f}%\")\n",
    "\n",
    "# Run the check\n",
    "count_empty_or_missing(train_df, \"Train\")\n",
    "count_empty_or_missing(valid_df, \"Validation\")\n",
    "# count_empty_or_missing(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186941"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['TRB_CDR3'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column TRBJ: 2170 missing values among non-unique TRB_CDR3 rows\n",
      "Column TRBV: 724 missing values among non-unique TRB_CDR3 rows\n",
      "Column MHC: 255464 missing values among non-unique TRB_CDR3 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify the duplicated TRB_CDR3 values\n",
    "duplicated_cdr3 = train_df[train_df['TRB_CDR3'].duplicated(keep=False)]\n",
    "\n",
    "# Step 2: Count missing values in the specified columns for these rows\n",
    "missing_counts = {\n",
    "    'TRBJ': duplicated_cdr3['TRBJ'].isna().sum(),\n",
    "    'TRBV': duplicated_cdr3['TRBV'].isna().sum(),\n",
    "    'MHC': duplicated_cdr3['MHC'].isna().sum()\n",
    "}\n",
    "\n",
    "# Print the results\n",
    "for column, count in missing_counts.items():\n",
    "    print(f\"Column {column}: {count} missing values among non-unique TRB_CDR3 rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 757732\n",
      "Rows with non-unique TRB_CDR3: 739837\n",
      "\n",
      "Missing values summary:\n",
      "--------------------------------------------------------------------------------\n",
      "Column: TRBJ\n",
      "  Missing in non-unique TRB_CDR3 rows: 2170 (0.29%)\n",
      "  Missing in all rows: 3793 (0.50%)\n",
      "--------------------------------------------------------------------------------\n",
      "Column: TRBV\n",
      "  Missing in non-unique TRB_CDR3 rows: 724 (0.10%)\n",
      "  Missing in all rows: 2118 (0.28%)\n",
      "--------------------------------------------------------------------------------\n",
      "Column: MHC\n",
      "  Missing in non-unique TRB_CDR3 rows: 255464 (34.53%)\n",
      "  Missing in all rows: 256689 (33.88%)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify the duplicated TRB_CDR3 values\n",
    "duplicated_cdr3 = train_df[train_df['TRB_CDR3'].duplicated(keep=False)]\n",
    "\n",
    "# List of columns to check\n",
    "columns_to_check = ['TRBJ', 'TRBV', 'MHC']\n",
    "\n",
    "# Step 2: Count missing values in both datasets\n",
    "results = {}\n",
    "\n",
    "for column in columns_to_check:\n",
    "    # Count for duplicated TRB_CDR3 rows\n",
    "    dup_missing = duplicated_cdr3[column].isna().sum()\n",
    "    \n",
    "    # Count for all rows\n",
    "    all_missing = train_df[column].isna().sum()\n",
    "    \n",
    "    results[column] = {\n",
    "        'missing_in_non_unique_TRB_CDR3': dup_missing,\n",
    "        'missing_in_all_rows': all_missing,\n",
    "        'percentage_in_non_unique': f\"{(dup_missing / len(duplicated_cdr3) * 100):.2f}%\" if len(duplicated_cdr3) > 0 else \"0%\",\n",
    "        'percentage_in_all': f\"{(all_missing / len(train_df) * 100):.2f}%\"\n",
    "    }\n",
    "\n",
    "# Print the results in a readable format\n",
    "print(f\"Total rows in dataset: {len(train_df)}\")\n",
    "print(f\"Rows with non-unique TRB_CDR3: {len(duplicated_cdr3)}\")\n",
    "print(\"\\nMissing values summary:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for column, stats in results.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"  Missing in non-unique TRB_CDR3 rows: {stats['missing_in_non_unique_TRB_CDR3']} ({stats['percentage_in_non_unique']})\")\n",
    "    print(f\"  Missing in all rows: {stats['missing_in_all_rows']} ({stats['percentage_in_all']})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCR_name</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>TRB_CDR3</th>\n",
       "      <th>TRBC</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>MHC</th>\n",
       "      <th>is_duplicated</th>\n",
       "      <th>Binding</th>\n",
       "      <th>task</th>\n",
       "      <th>source</th>\n",
       "      <th>TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class</th>\n",
       "      <th>pair</th>\n",
       "      <th>pair_count</th>\n",
       "      <th>epi_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2852</td>\n",
       "      <td>TRBV28*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>CASSLYEQYF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ISPRTLNAW</td>\n",
       "      <td>HLA-B*57:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1604</td>\n",
       "      <td>TRBV5-6*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>CASSLVGVPNYGYTF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GLCTLVAML</td>\n",
       "      <td>HLA-A*02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7336</td>\n",
       "      <td>TRBV12-3*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>CASSPPGGGGPYGYTF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAKFKQLL</td>\n",
       "      <td>HLA-B*08:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2192</td>\n",
       "      <td>TRBV30*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>CAWREGGSEAFF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RLRAEAQVK</td>\n",
       "      <td>HLA-A*03:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4829</td>\n",
       "      <td>TRBV25-1*01</td>\n",
       "      <td>TRBJ2-2*01</td>\n",
       "      <td>CASSESGSGRAETGELFF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLLGIGILV</td>\n",
       "      <td>HLA-A*02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TCR_name         TRBV        TRBJ            TRB_CDR3 TRBC    Epitope  \\\n",
       "0     2852    TRBV28*01  TRBJ2-7*01          CASSLYEQYF  NaN  ISPRTLNAW   \n",
       "1     1604   TRBV5-6*01  TRBJ1-2*01     CASSLVGVPNYGYTF  NaN  GLCTLVAML   \n",
       "2     7336  TRBV12-3*01  TRBJ1-2*01    CASSPPGGGGPYGYTF  NaN   RAKFKQLL   \n",
       "3     2192    TRBV30*01  TRBJ1-1*01        CAWREGGSEAFF  NaN  RLRAEAQVK   \n",
       "4     4829  TRBV25-1*01  TRBJ2-2*01  CASSESGSGRAETGELFF  NaN  LLLGIGILV   \n",
       "\n",
       "           MHC is_duplicated  Binding  task    source  \\\n",
       "0  HLA-B*57:01           NaN        1  TPP2  datasets   \n",
       "1     HLA-A*02           NaN        1  TPP2  datasets   \n",
       "2  HLA-B*08:01           NaN        1  TPP2  datasets   \n",
       "3  HLA-A*03:01           NaN        1  TPP2  datasets   \n",
       "4     HLA-A*02           NaN        1  TPP2  datasets   \n",
       "\n",
       "   TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class  \\\n",
       "0                                                NaN                                                                                                                                               \n",
       "1                                                NaN                                                                                                                                               \n",
       "2                                                NaN                                                                                                                                               \n",
       "3                                                NaN                                                                                                                                               \n",
       "4                                                NaN                                                                                                                                               \n",
       "\n",
       "   pair  pair_count  epi_count  \n",
       "0   NaN         NaN        NaN  \n",
       "1   NaN         NaN        NaN  \n",
       "2   NaN         NaN        NaN  \n",
       "3   NaN         NaN        NaN  \n",
       "4   NaN         NaN        NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr_len_20 = test_df[test_df['TRB_CDR3'].apply(lambda x: len(x) == 20)]\n",
    "epi_len_15 = test_df[test_df['Epitope'].apply(lambda x: len(x) == 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCR_name</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>TRB_CDR3</th>\n",
       "      <th>TRBC</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>MHC</th>\n",
       "      <th>is_duplicated</th>\n",
       "      <th>Binding</th>\n",
       "      <th>task</th>\n",
       "      <th>source</th>\n",
       "      <th>TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class</th>\n",
       "      <th>pair</th>\n",
       "      <th>pair_count</th>\n",
       "      <th>epi_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>6109</td>\n",
       "      <td>TRBV6-6*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>CASSFGQRDRGYWRNTEAFF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>5372</td>\n",
       "      <td>TRBV28*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>CASSFSQTSGTGRIYYGYTF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "      <td>HLA-A*02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>7056</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ1-4*01</td>\n",
       "      <td>CASSVNPNPIRDRDNEKLFF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAKFKQLL</td>\n",
       "      <td>HLA-B*08:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP1</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>5748</td>\n",
       "      <td>TRBV7-3*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>CASSLRLRLAGALRNYEQYF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GTSGSPIVNR</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>5116</td>\n",
       "      <td>TRBV6-2*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>CASSFLPGQGSYYSNQPQHF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HLA-B*08:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP2</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TCR_name        TRBV        TRBJ              TRB_CDR3 TRBC     Epitope  \\\n",
       "59      6109  TRBV6-6*01  TRBJ1-1*01  CASSFGQRDRGYWRNTEAFF  NaN   NLVPMVATV   \n",
       "153     5372   TRBV28*01  TRBJ1-2*01  CASSFSQTSGTGRIYYGYTF  NaN  ELAGIGILTV   \n",
       "555     7056    TRBV9*01  TRBJ1-4*01  CASSVNPNPIRDRDNEKLFF  NaN    RAKFKQLL   \n",
       "744     5748  TRBV7-3*01  TRBJ2-7*01  CASSLRLRLAGALRNYEQYF  NaN  GTSGSPIVNR   \n",
       "828     5116  TRBV6-2*01  TRBJ1-5*01  CASSFLPGQGSYYSNQPQHF  NaN    FLKEKGGL   \n",
       "\n",
       "             MHC is_duplicated  Binding  task    source  \\\n",
       "59   HLA-A*02:01           NaN        1  TPP2  datasets   \n",
       "153     HLA-A*02           NaN        1  TPP2  datasets   \n",
       "555  HLA-B*08:01           NaN        1  TPP1  datasets   \n",
       "744  HLA-A*11:01           NaN        1  TPP2  datasets   \n",
       "828  HLA-B*08:01           NaN        1  TPP2  datasets   \n",
       "\n",
       "     TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class  \\\n",
       "59                                                 NaN                                                                                                                                               \n",
       "153                                                NaN                                                                                                                                               \n",
       "555                                                NaN                                                                                                                                               \n",
       "744                                                NaN                                                                                                                                               \n",
       "828                                                NaN                                                                                                                                               \n",
       "\n",
       "     pair  pair_count  epi_count  \n",
       "59    NaN         NaN        NaN  \n",
       "153   NaN         NaN        NaN  \n",
       "555   NaN         NaN        NaN  \n",
       "744   NaN         NaN        NaN  \n",
       "828   NaN         NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcr_len_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCR_name</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>TRB_CDR3</th>\n",
       "      <th>TRBC</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>MHC</th>\n",
       "      <th>is_duplicated</th>\n",
       "      <th>Binding</th>\n",
       "      <th>task</th>\n",
       "      <th>source</th>\n",
       "      <th>TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class</th>\n",
       "      <th>pair</th>\n",
       "      <th>pair_count</th>\n",
       "      <th>epi_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>3323</td>\n",
       "      <td>TRBV6-5*01</td>\n",
       "      <td>TRBJ1-6*01</td>\n",
       "      <td>CASTDGQKNSPLHF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARNLVPMVATVQGQN</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP3</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>3324</td>\n",
       "      <td>TRBV6-6*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>CASSYNSAGYNEQFF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARNLVPMVATVQGQN</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP3</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>3326</td>\n",
       "      <td>TRBV6-5*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>CASSFATGTAYGYTF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LSEFCRVLCCYVLEE</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP3</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>3325</td>\n",
       "      <td>TRBV25-1*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>CATESTGIGGYTF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARNLVPMVATVQGQN</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP3</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7770</th>\n",
       "      <td>3327</td>\n",
       "      <td>TRBV6-2*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>CASSNPQGAKYEQFF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LSEFCRVLCCYVLEE</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP3</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TCR_name         TRBV        TRBJ         TRB_CDR3 TRBC          Epitope  \\\n",
       "4648     3323   TRBV6-5*01  TRBJ1-6*01   CASTDGQKNSPLHF  NaN  ARNLVPMVATVQGQN   \n",
       "5997     3324   TRBV6-6*01  TRBJ2-1*01  CASSYNSAGYNEQFF  NaN  ARNLVPMVATVQGQN   \n",
       "6089     3326   TRBV6-5*01  TRBJ1-2*01  CASSFATGTAYGYTF  NaN  LSEFCRVLCCYVLEE   \n",
       "6702     3325  TRBV25-1*01  TRBJ1-2*01    CATESTGIGGYTF  NaN  ARNLVPMVATVQGQN   \n",
       "7770     3327   TRBV6-2*01  TRBJ2-1*01  CASSNPQGAKYEQFF  NaN  LSEFCRVLCCYVLEE   \n",
       "\n",
       "              MHC is_duplicated  Binding  task    source  \\\n",
       "4648  HLA-A*02:01           NaN        1  TPP3  datasets   \n",
       "5997  HLA-A*02:01           NaN        1  TPP3  datasets   \n",
       "6089  HLA-A*02:01           NaN        1  TPP3  datasets   \n",
       "6702  HLA-A*02:01           NaN        1  TPP3  datasets   \n",
       "7770  HLA-A*02:01           NaN        1  TPP3  datasets   \n",
       "\n",
       "      TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class  \\\n",
       "4648                                                NaN                                                                                                                                               \n",
       "5997                                                NaN                                                                                                                                               \n",
       "6089                                                NaN                                                                                                                                               \n",
       "6702                                                NaN                                                                                                                                               \n",
       "7770                                                NaN                                                                                                                                               \n",
       "\n",
       "      pair  pair_count  epi_count  \n",
       "4648   NaN         NaN        NaN  \n",
       "5997   NaN         NaN        NaN  \n",
       "6089   NaN         NaN        NaN  \n",
       "6702   NaN         NaN        NaN  \n",
       "7770   NaN         NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi_len_15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class</th>\n",
       "      <th>TRBC</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>TRB_CDR3</th>\n",
       "      <th>MHC</th>\n",
       "      <th>TCR_name</th>\n",
       "      <th>Binding</th>\n",
       "      <th>task</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBC2</td>\n",
       "      <td>TPRVTGGGAM</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>CASSEDSYEQYF</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>TCTGGAATCCAAACAC-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LPRRSGAAGA</td>\n",
       "      <td>TRBV7-9</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>CASSLSGRGEQFF</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>277334</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBC1</td>\n",
       "      <td>SLEGGGLGY</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>CASSESLAGSYEQYF</td>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>TATGCCCGTACATGTC-8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MIELSLIDFYLCFLAFLLFLVLIML</td>\n",
       "      <td>TRBV4-1</td>\n",
       "      <td>TRBJ2-2</td>\n",
       "      <td>CASSQRGTGELFF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228116</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GLCTLVAML</td>\n",
       "      <td>TRBV4-2</td>\n",
       "      <td>TRBJ1-6</td>\n",
       "      <td>CASSQENRGPSPLHF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>260489</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class  \\\n",
       "0                                                NaN                                                                                                                                               \n",
       "1                                                NaN                                                                                                                                               \n",
       "2                                                NaN                                                                                                                                               \n",
       "3                                                NaN                                                                                                                                               \n",
       "4                                                NaN                                                                                                                                               \n",
       "\n",
       "    TRBC                    Epitope     TRBV     TRBJ         TRB_CDR3  \\\n",
       "0  TRBC2                 TPRVTGGGAM  TRBV6-1  TRBJ2-7     CASSEDSYEQYF   \n",
       "1    NaN                 LPRRSGAAGA  TRBV7-9  TRBJ2-1    CASSLSGRGEQFF   \n",
       "2  TRBC1                  SLEGGGLGY  TRBV6-1  TRBJ2-7  CASSESLAGSYEQYF   \n",
       "3    NaN  MIELSLIDFYLCFLAFLLFLVLIML  TRBV4-1  TRBJ2-2    CASSQRGTGELFF   \n",
       "4    NaN                  GLCTLVAML  TRBV4-2  TRBJ1-6  CASSQENRGPSPLHF   \n",
       "\n",
       "           MHC            TCR_name  Binding  task     source  \n",
       "0  HLA-B*07:02  TCTGGAATCCAAACAC-1        0   NaN        10X  \n",
       "1  HLA-B*07:02              277334        0   NaN  generated  \n",
       "2  HLA-A*01:01  TATGCCCGTACATGTC-8        0   NaN  generated  \n",
       "3          NaN              228116        0   NaN  generated  \n",
       "4  HLA-A*02:01              260489        0   NaN  generated  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50       TRBJ1-2\n",
       "51       TRBJ2-7\n",
       "52       TRBJ2-1\n",
       "53    TRBJ2-7*01\n",
       "54    TRBJ1-2*01\n",
       "55       TRBJ2-1\n",
       "56    TRBJ2-3*01\n",
       "57       TRBJ1-1\n",
       "58       TRBJ2-3\n",
       "59       TRBJ2-7\n",
       "60    TRBJ2-7*01\n",
       "61       TRBJ2-7\n",
       "62       TRBJ2-2\n",
       "63       TRBJ2-2\n",
       "64    TRBJ1-5*01\n",
       "65       TRBJ2-1\n",
       "66       TRBJ2-7\n",
       "67       TRBJ2-3\n",
       "68    TRBJ1-2*01\n",
       "69       TRBJ2-7\n",
       "70       TRBJ1-1\n",
       "71       TRBJ2-3\n",
       "72       TRBJ1-2\n",
       "73       TRBJ1-1\n",
       "74       TRBJ2-7\n",
       "75       TRBJ2-1\n",
       "76       TRBJ1-2\n",
       "77    TRBJ1-2*01\n",
       "78       TRBJ2-7\n",
       "79       TRBJ2-3\n",
       "Name: TRBJ, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['TRBJ'][50:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class</th>\n",
       "      <th>TRBC</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>TRB_CDR3</th>\n",
       "      <th>MHC</th>\n",
       "      <th>TCR_name</th>\n",
       "      <th>Binding</th>\n",
       "      <th>task</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBC1</td>\n",
       "      <td>YLLEMLWRL</td>\n",
       "      <td>TRBV7-8</td>\n",
       "      <td>TRBJ1-3</td>\n",
       "      <td>CASSLARGLGNTIYF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>GAAACTCTCTGCAGTA-8</td>\n",
       "      <td>0</td>\n",
       "      <td>TPP1</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YVLDHLIVV</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>CASSAGGYYEQYF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>151755</td>\n",
       "      <td>1</td>\n",
       "      <td>TPP1</td>\n",
       "      <td>datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBC2</td>\n",
       "      <td>FLYALALLL</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>CASSLHGDYEQYF</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>TGTATTCTCAACCAAC-22</td>\n",
       "      <td>0</td>\n",
       "      <td>TPP1</td>\n",
       "      <td>10X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBC2</td>\n",
       "      <td>IVTDFSVIK</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>CSVVGGGTSAYEQYF</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>ACATCAGCATGCATGT-3</td>\n",
       "      <td>0</td>\n",
       "      <td>TPP1</td>\n",
       "      <td>10X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBC2</td>\n",
       "      <td>QYDPVAALF</td>\n",
       "      <td>TRBV10-3</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "      <td>CAISDGSQETQYF</td>\n",
       "      <td>HLA-A*24:02</td>\n",
       "      <td>TTTACTGTCAAACAAG-21</td>\n",
       "      <td>0</td>\n",
       "      <td>TPP1</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class  \\\n",
       "0                                                NaN                                                                                                                                               \n",
       "1                                                NaN                                                                                                                                               \n",
       "2                                                NaN                                                                                                                                               \n",
       "3                                                NaN                                                                                                                                               \n",
       "4                                                NaN                                                                                                                                               \n",
       "\n",
       "    TRBC    Epitope      TRBV        TRBJ         TRB_CDR3          MHC  \\\n",
       "0  TRBC1  YLLEMLWRL   TRBV7-8     TRBJ1-3  CASSLARGLGNTIYF  HLA-A*02:01   \n",
       "1    NaN  YVLDHLIVV  TRBV9*01  TRBJ2-7*01    CASSAGGYYEQYF  HLA-A*02:01   \n",
       "2  TRBC2  FLYALALLL  TRBV12-3     TRBJ2-7    CASSLHGDYEQYF  HLA-A*02:01   \n",
       "3  TRBC2  IVTDFSVIK  TRBV29-1     TRBJ2-7  CSVVGGGTSAYEQYF  HLA-A*11:01   \n",
       "4  TRBC2  QYDPVAALF  TRBV10-3     TRBJ2-5    CAISDGSQETQYF  HLA-A*24:02   \n",
       "\n",
       "              TCR_name  Binding  task     source  \n",
       "0   GAAACTCTCTGCAGTA-8        0  TPP1  generated  \n",
       "1               151755        1  TPP1   datasets  \n",
       "2  TGTATTCTCAACCAAC-22        0  TPP1        10X  \n",
       "3   ACATCAGCATGCATGT-3        0  TPP1        10X  \n",
       "4  TTTACTGTCAAACAAG-21        0  TPP1  generated  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check again the \"Sternli-Verdacht\" on new datasets (12.05.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22205/3656393471.py:11: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  valid_df = pd.read_csv(valid_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRBJ ---\n",
      "Length train set: 29. Length valid set: 29. Length test set: 28\n",
      "train_set (29): {nan, 'TRBJ2-1', 'TRBJ1-6*01', 'TRBJ2-2*01', 'TRBJ2-2', 'TRBJ2-6', 'TRBJ2-7', 'TRBJ1-6*02', 'TRBJ1-1*01', 'TRBV6-5*01', 'TRBJ2-5', 'TRBJ2-4', 'TRBJ2-7*01', 'TRBJ1-1', 'TRBJ1-4', 'TRBJ1-4*01', 'TRBJ1-3*01', 'TRBJ1-3', 'TRBJ2-3*01', 'TRBJ1-5*01', 'TRBJ2-4*01', 'TRBJ2-1*01', 'TRBJ1-2', 'TRBJ2-3', 'TRBJ2-5*01', 'TRBJ1-2*01', 'TRBJ1-6', 'TRBJ2-6*01', 'TRBJ1-5'}\n",
      "valid_set (29): {nan, 'TRBJ2-1', 'TRBJ1-6*01', 'TRBJ2-2*01', 'TRBJ2-2', 'TRBJ2-6', 'TRBJ2-7', 'TRBJ1-6*02', 'TRBJ1-1*01', 'TRBJ2-5', 'TRBJ2-4', 'TRBJ2-7*01', 'TRBJ1-1', 'TRBJ1-4', 'TRBJ1-4*01', 'TRBJ1-3*01', 'TRBJ1-3', 'TRBJ2-3*01', 'TRBJ1-5*01', 'TRBJ2-4*01', 'TRAJ26', 'TRBJ2-1*01', 'TRBJ1-2', 'TRBJ2-3', 'TRBJ2-5*01', 'TRBJ1-2*01', 'TRBJ1-6', 'TRBJ2-6*01', 'TRBJ1-5'}\n",
      "test_set (28): {nan, 'TRBJ2-1', 'TRBJ1-6*01', 'TRBJ2-2*01', 'TRBJ2-2', 'TRBJ2-6', 'TRBJ2-7', 'TRBJ1-6*02', 'TRBJ1-1*01', 'TRBJ2-5', 'TRBJ2-4', 'TRBJ2-7*01', 'TRBJ1-1', 'TRBJ1-4', 'TRBJ1-4*01', 'TRBJ1-3*01', 'TRBJ1-3', 'TRBJ2-3*01', 'TRBJ1-5*01', 'TRBJ2-4*01', 'TRBJ2-1*01', 'TRBJ1-2', 'TRBJ2-3', 'TRBJ2-5*01', 'TRBJ1-2*01', 'TRBJ1-6', 'TRBJ2-6*01', 'TRBJ1-5'} \n",
      "\n",
      "Train ∩ Test (28): {nan, 'TRBJ2-1', 'TRBJ1-6*01', 'TRBJ2-2*01', 'TRBJ2-2', 'TRBJ2-6', 'TRBJ2-7', 'TRBJ1-6*02', 'TRBJ1-1*01', 'TRBJ2-5', 'TRBJ2-4', 'TRBJ2-7*01', 'TRBJ1-1', 'TRBJ1-4', 'TRBJ1-4*01', 'TRBJ1-3*01', 'TRBJ1-3', 'TRBJ2-3*01', 'TRBJ1-5*01', 'TRBJ2-4*01', 'TRBJ2-1*01', 'TRBJ1-2', 'TRBJ2-3', 'TRBJ2-5*01', 'TRBJ1-2*01', 'TRBJ1-6', 'TRBJ2-6*01', 'TRBJ1-5'}\n",
      "Valid ∩ Test (28): {nan, 'TRBJ2-1', 'TRBJ1-6*01', 'TRBJ2-2*01', 'TRBJ2-2', 'TRBJ2-6', 'TRBJ2-7', 'TRBJ1-6*02', 'TRBJ1-1*01', 'TRBJ2-5', 'TRBJ2-4', 'TRBJ2-7*01', 'TRBJ1-1', 'TRBJ1-4', 'TRBJ1-4*01', 'TRBJ1-3*01', 'TRBJ1-3', 'TRBJ2-3*01', 'TRBJ1-5*01', 'TRBJ2-4*01', 'TRBJ2-1*01', 'TRBJ1-2', 'TRBJ2-3', 'TRBJ2-5*01', 'TRBJ1-2*01', 'TRBJ1-6', 'TRBJ2-6*01', 'TRBJ1-5'}\n",
      "Test - (Train ∪ Valid) (0): set()\n",
      "Train ∩ Valid (28): {nan, 'TRBJ2-1', 'TRBJ1-6*01', 'TRBJ2-2*01', 'TRBJ2-2', 'TRBJ2-6', 'TRBJ2-7', 'TRBJ1-6*02', 'TRBJ1-1*01', 'TRBJ2-5', 'TRBJ2-4', 'TRBJ2-7*01', 'TRBJ1-1', 'TRBJ1-4', 'TRBJ1-4*01', 'TRBJ1-3*01', 'TRBJ1-3', 'TRBJ2-3*01', 'TRBJ1-5*01', 'TRBJ2-4*01', 'TRBJ2-1*01', 'TRBJ1-2', 'TRBJ2-3', 'TRBJ2-5*01', 'TRBJ1-2*01', 'TRBJ1-6', 'TRBJ2-6*01', 'TRBJ1-5'}\n",
      "\n",
      "--- TRBV ---\n",
      "Length train set: 161. Length valid set: 138. Length test set: 113\n",
      "train_set (161): {'TRBV24-1', 'TRBV4-3*04', 'TRBV8-1*01', 'TRBV7-9', 'TRBV17*01', 'TRBV1', 'TRBV7-6', 'TRBV12-4*01', 'TRBV6-4', 'TRBV5-5', 'TRBV4-1', 'TRBV10-1*02', 'TRBV11-3', 'TRBV7-8', 'TRBV18', 'TRBV9*01', 'TRBV5-7*01', 'TRBV11-1', 'TRBV4-1*02', 'TRBV12-3', 'TRBV7-5*02', 'TRBV5-8*01', 'TRBV19', 'TRBV2*03', 'TRBV6-4*02', 'TRBV5-6', 'TRBV12-3*01', 'TRBV5-4*01', 'TRBV7-7', 'TRBV7-8*01', 'TRBV9', 'TRBV7-2', 'TRBV6-2', 'TRBV12-1', 'TRBV7-4*01', 'TRBV6-2*01', 'TRBV16', 'TRBV11-2*01', 'TRBV11-3*02', 'TRBV5-1*01', 'TRBV25-1*01', 'TRBV12-4*02', 'TRBV7-2*01', 'TRBV7-7*01', 'TRBV22-1*01', 'TRBV16*01', 'TRBV13*01', 'TRBV7-3', 'TRBV7-9*01', 'TRBV29-1*01', 'TRBV11-1*01', 'TRBV10-1*01', 'TRBV22-1', 'TRBV6-5', 'TRBV10-3*01', 'TRBV10-1', 'TRBV9*02', 'TRBV10-3*02', 'TRBV12-4', 'TRBV20-1*04', 'TRBV14*01', 'TRBV4-1*01', 'TRBV7-5', 'TRBV6-6*02', 'TRBV21-1', 'TRBV24-1*01', 'TRBV21-1*01', 'TRBV1*01', 'TRBV6-9*01', 'TRBV6-5*01', 'TRBV7-8*03', 'TRBV5-6*01', 'TRBV3-2', 'TRBV30', 'TRBV4-2*01', 'TRBV11-3*01', 'TRBV12-2*01', 'TRBV6-8', 'TRBV26', 'TRBV7-8*02', 'TRBV7-1*01', 'TRBV6-7', 'TRBV5-8', 'TRBV11-2', 'TRBV6-8*01', 'TRBV12-5', 'TRBV4-2', 'TRBV2*01', 'TRBV7-6*01', 'TRBV7-3*01', 'TRBV20-1*02', 'TRBV5-4', 'TRBV4-3', 'TRBV6-6*04', 'TRBV11-2*03', 'TRBV10-2', 'TRBV23-1', 'TRBV30*05', 'TRBV11-3*04', 'TRBV4-3*01', 'TRBV5-7', 'TRBV30*02', 'TRBV12-5*01', 'TRBV15*02', 'TRBV6-6*01', 'TRBV7-2*02', 'TRBV20/OR9-2', 'TRBV6-7*01', 'TRBV3-1*01', 'TRBV25-1', 'TRBV23-1*01', 'TRBV9*03', 'TRBV2*02', 'TRBV5-1', 'TRBV15*01', 'TRBV26*01', 'TRBV10-2*01', 'TRBV5-2*01', 'TRBV8-2*01', 'TRBV15', 'TRBV6-9', 'TRBV19*01', 'TRBV18*01', 'TRBV12-2', 'TRBV7-9*03', 'TRBV7-5*01', 'TRBV6-3', 'TRBV27*01', 'TRBV10-3', 'TRBV7-9*07', 'TRBV7-1', nan, 'TRBV20-1', 'TRBV7-3*03', 'TRBV7-4', 'TRBV19*02', 'TRBV30*01', 'TRBV29-1', 'TRBV7-9*04', 'TRBV28', 'TRBV2', 'TRBV5-3', 'TRBV11-2*02', 'TRBV17', 'TRBV8-1', 'TRBV13', 'TRBV6-1', 'TRBV6-4*01', 'TRBV27', 'TRBV5-5*01', 'TRBJ2-7*01', 'TRBV6-6', 'TRBV8-2', 'TRBV28*01', 'TRBV14', 'TRBV6-1*01', 'TRBV5-4*02', 'TRBV3-1', 'TRBV14*02', 'TRBV12-1*01', 'TRBV20-1*01'}\n",
      "valid_set (138): {'TRBV24-1', 'TRBV7-9', 'TRBV17*01', 'TRBV10-2*02', 'TRBV1', 'TRBV7-6', 'TRBV12-4*01', 'TRBV6-4', 'TRBV5-5', 'TRBV4-1', 'TRBV10-1*02', 'TRBV18', 'TRBV7-8', 'TRBV11-3', 'TRBV9*01', 'TRBV5-7*01', 'TRBV11-1', 'TRBV12-3', 'TRBV7-5*02', 'TRBV5-8*01', 'TRBV19', 'TRBV5-6', 'TRBV12-3*01', 'TRBV5-4*01', 'TRBV7-7', 'TRBV7-8*01', 'TRBV9', 'TRBV12-1', 'TRBV7-2', 'TRBV7-4*01', 'TRBV6-2', 'TRBV6-2*01', 'TRBV16', 'TRBV11-2*01', 'TRBV5-1*01', 'TRBV25-1*01', 'TRBV7-2*01', 'TRBV7-7*01', 'TRBV22-1*01', 'TRBV16*01', 'TRBV13*01', 'TRBV7-3', 'TRBV7-9*01', 'TRBV29-1*01', 'TRBV10-1*01', 'TRBV11-1*01', 'TRBV6-5', 'TRBV10-3*01', 'TRBV10-1', 'TRBV9*02', 'TRBV10-3*02', 'TRBV12-4', 'TRBV6-6*02', 'TRBV14*01', 'TRBV4-1*01', 'TRBV20-1*04', 'TRBV21-1', 'TRBV24-1*01', 'TRBV21-1*01', 'TRBV1*01', 'TRBV6-9*01', 'TRBV6-5*01', 'TRBV3-2', 'TRBV5-6*01', 'TRBV7-8*03', 'TRBV30', 'TRBV4-2*01', 'TRBV11-3*01', 'TRBV12-2*01', 'TRBV6-8', 'TRBV7-1*01', 'TRBV6-7', 'TRBV5-8', 'TRBV11-2', 'TRBV6-8*01', 'TRBV12-5', 'TRBV4-2', 'TRBV2*01', 'TRBV7-6*01', 'TRBV7-3*01', 'TRBV5-4', 'TRBV4-3', 'TRBV6-6*04', 'TRBV11-2*03', 'TRBV10-2', 'TRBV23-1', 'TRBV30*05', 'TRBV4-3*01', 'TRBV5-7', 'TRBV30*02', 'TRBV12-5*01', 'TRBV15*02', 'TRBV6-6*01', 'TRBV7-2*02', 'TRBV6-7*01', 'TRBV3-1*01', 'TRBV25-1', 'TRBV23-1*01', 'TRBV5-1', 'TRBV15*01', 'TRBV26*01', 'TRBV10-2*01', 'TRBV5-2*01', 'TRBV15', 'TRBV6-9', 'TRBV19*01', 'TRBV18*01', 'TRBV12-2', 'TRBV7-9*03', 'TRBV7-5*01', 'TRBV6-3', 'TRBV27*01', 'TRBV10-3', 'TRBV7-1', nan, 'TRBV20-1', 'TRBV7-4', 'TRBV19*02', 'TRBV30*01', 'TRBV29-1', 'TRBV28', 'TRBV2', 'TRBV5-3', 'TRBV11-2*02', 'TRBV17', 'TRBV8-1', 'TRBV13', 'TRBV6-1', 'TRBV6-4*01', 'TRBV27', 'TRBV5-5*01', 'TRBV6-6', 'TRBV8-2', 'TRBV28*01', 'TRBV14', 'TRBV6-1*01', 'TRBV3-1', 'TRBV20-1*01'}\n",
      "test_set (113): {'TRBV24-1', 'TRBV7-9', 'TRBV12-4*01', 'TRBV7-6', 'TRBV6-4', 'TRBV5-5', 'TRBV4-1', 'TRBV18', 'TRBV7-8', 'TRBV11-3', 'TRBV9*01', 'TRBV11-1', 'TRBV12-3', 'TRBV5-8*01', 'TRBV19', 'TRBV5-6', 'TRBV12-3*01', 'TRBV5-4*01', 'TRBV7-7', 'TRBV7-8*01', 'TRBV9', 'TRBV7-2', 'TRBV6-2*01', 'TRBV7-4*01', 'TRBV12-1', 'TRBV6-2', 'TRBV16', 'TRBV11-2*01', 'TRBV5-1*01', 'TRBV25-1*01', 'TRBV7-2*01', 'TRBV7-7*01', 'TRBV16*01', 'TRBV13*01', 'TRBV7-3', 'TRBV7-9*01', 'TRBV29-1*01', 'TRBV10-1*01', 'TRBV11-1*01', 'TRBV6-5', 'TRBV10-3*01', 'TRBV9*02', 'TRBV10-1', 'TRBV10-3*02', 'TRBV12-4', 'TRBV6-6*02', 'TRBV14*01', 'TRBV4-1*01', 'TRBV21-1', 'TRBV24-1*01', 'TRBV6-9*01', 'TRBV6-5*01', 'TRBV5-6*01', 'TRBV30', 'TRBV4-2*01', 'TRBV11-3*01', 'TRBV6-8', 'TRBV7-8*02', 'TRBV6-7', 'TRBV5-8', 'TRBV11-2', 'TRBV6-8*01', 'TRBV12-5', 'TRBV4-2', 'TRBV2*01', 'TRBV7-6*01', 'TRBV20-1*02', 'TRBV7-3*01', 'TRBV4-3', 'TRBV5-4', 'TRBV6-3*01', 'TRBV10-2', 'TRBV23-1', 'TRBV4-3*01', 'TRBV5-7', 'TRBV12-5*01', 'TRBV7-2*03', 'TRBV15*02', 'TRBV6-6*01', 'TRBV3-1*01', 'TRBV25-1', 'TRBV5-1', 'TRBV15*01', 'TRBV10-2*01', 'TRBV15', 'TRBV6-9', 'TRBV19*01', 'TRBV18*01', 'TRBV7-9*03', 'TRBV6-3', 'TRBV27*01', 'TRBV10-3', nan, 'TRBV20-1', 'TRBV7-4', 'TRBV19*02', 'TRBV30*01', 'TRBV29-1', 'TRBV28', 'TRBV2', 'TRBV5-3', 'TRBV11-2*02', 'TRBV13', 'TRBV6-1', 'TRBV6-4*01', 'TRBV27', 'TRBV5-5*01', 'TRBV6-6', 'TRBV28*01', 'TRBV14', 'TRBV6-1*01', 'TRBV3-1', 'TRBV20-1*01'} \n",
      "\n",
      "Train ∩ Test (111): {'TRBV24-1', 'TRBV7-9', 'TRBV7-6', 'TRBV12-4*01', 'TRBV6-4', 'TRBV5-5', 'TRBV4-1', 'TRBV11-3', 'TRBV18', 'TRBV7-8', 'TRBV9*01', 'TRBV11-1', 'TRBV12-3', 'TRBV5-8*01', 'TRBV19', 'TRBV5-6', 'TRBV12-3*01', 'TRBV5-4*01', 'TRBV7-7', 'TRBV7-8*01', 'TRBV9', 'TRBV7-2', 'TRBV6-2*01', 'TRBV7-4*01', 'TRBV12-1', 'TRBV6-2', 'TRBV16', 'TRBV11-2*01', 'TRBV25-1*01', 'TRBV5-1*01', 'TRBV7-2*01', 'TRBV7-7*01', 'TRBV16*01', 'TRBV13*01', 'TRBV7-3', 'TRBV7-9*01', 'TRBV29-1*01', 'TRBV10-1*01', 'TRBV11-1*01', 'TRBV6-5', 'TRBV10-3*01', 'TRBV9*02', 'TRBV10-1', 'TRBV10-3*02', 'TRBV12-4', 'TRBV6-6*02', 'TRBV14*01', 'TRBV4-1*01', 'TRBV21-1', 'TRBV24-1*01', 'TRBV6-9*01', 'TRBV6-5*01', 'TRBV5-6*01', 'TRBV30', 'TRBV4-2*01', 'TRBV11-3*01', 'TRBV6-8', 'TRBV7-8*02', 'TRBV6-7', 'TRBV5-8', 'TRBV11-2', 'TRBV6-8*01', 'TRBV12-5', 'TRBV4-2', 'TRBV2*01', 'TRBV7-6*01', 'TRBV20-1*02', 'TRBV7-3*01', 'TRBV5-4', 'TRBV4-3', 'TRBV10-2', 'TRBV23-1', 'TRBV4-3*01', 'TRBV5-7', 'TRBV12-5*01', 'TRBV15*02', 'TRBV6-6*01', 'TRBV3-1*01', 'TRBV25-1', 'TRBV5-1', 'TRBV15*01', 'TRBV10-2*01', 'TRBV15', 'TRBV6-9', 'TRBV19*01', 'TRBV18*01', 'TRBV7-9*03', 'TRBV6-3', 'TRBV27*01', 'TRBV10-3', nan, 'TRBV20-1', 'TRBV7-4', 'TRBV19*02', 'TRBV30*01', 'TRBV29-1', 'TRBV28', 'TRBV2', 'TRBV5-3', 'TRBV11-2*02', 'TRBV13', 'TRBV6-1', 'TRBV6-4*01', 'TRBV27', 'TRBV5-5*01', 'TRBV6-6', 'TRBV28*01', 'TRBV14', 'TRBV6-1*01', 'TRBV3-1', 'TRBV20-1*01'}\n",
      "Valid ∩ Test (109): {'TRBV24-1', 'TRBV7-9', 'TRBV7-6', 'TRBV12-4*01', 'TRBV6-4', 'TRBV5-5', 'TRBV4-1', 'TRBV11-3', 'TRBV18', 'TRBV7-8', 'TRBV9*01', 'TRBV11-1', 'TRBV12-3', 'TRBV5-8*01', 'TRBV19', 'TRBV5-6', 'TRBV12-3*01', 'TRBV5-4*01', 'TRBV7-7', 'TRBV7-8*01', 'TRBV9', 'TRBV7-2', 'TRBV6-2*01', 'TRBV7-4*01', 'TRBV12-1', 'TRBV6-2', 'TRBV16', 'TRBV11-2*01', 'TRBV25-1*01', 'TRBV5-1*01', 'TRBV7-2*01', 'TRBV7-7*01', 'TRBV16*01', 'TRBV13*01', 'TRBV7-3', 'TRBV7-9*01', 'TRBV29-1*01', 'TRBV10-1*01', 'TRBV11-1*01', 'TRBV6-5', 'TRBV10-3*01', 'TRBV9*02', 'TRBV10-1', 'TRBV10-3*02', 'TRBV12-4', 'TRBV6-6*02', 'TRBV14*01', 'TRBV4-1*01', 'TRBV21-1', 'TRBV24-1*01', 'TRBV6-9*01', 'TRBV6-5*01', 'TRBV5-6*01', 'TRBV30', 'TRBV4-2*01', 'TRBV11-3*01', 'TRBV6-8', 'TRBV6-7', 'TRBV5-8', 'TRBV11-2', 'TRBV6-8*01', 'TRBV12-5', 'TRBV4-2', 'TRBV2*01', 'TRBV7-6*01', 'TRBV7-3*01', 'TRBV5-4', 'TRBV4-3', 'TRBV10-2', 'TRBV23-1', 'TRBV4-3*01', 'TRBV5-7', 'TRBV12-5*01', 'TRBV15*02', 'TRBV6-6*01', 'TRBV3-1*01', 'TRBV25-1', 'TRBV5-1', 'TRBV15*01', 'TRBV10-2*01', 'TRBV15', 'TRBV6-9', 'TRBV19*01', 'TRBV18*01', 'TRBV7-9*03', 'TRBV6-3', 'TRBV27*01', 'TRBV10-3', nan, 'TRBV20-1', 'TRBV7-4', 'TRBV19*02', 'TRBV30*01', 'TRBV29-1', 'TRBV28', 'TRBV2', 'TRBV5-3', 'TRBV11-2*02', 'TRBV13', 'TRBV6-1', 'TRBV6-4*01', 'TRBV27', 'TRBV5-5*01', 'TRBV6-6', 'TRBV28*01', 'TRBV14', 'TRBV6-1*01', 'TRBV3-1', 'TRBV20-1*01'}\n",
      "Test - (Train ∪ Valid) (2): {'TRBV7-2*03', 'TRBV6-3*01'}\n",
      "Train ∩ Valid (137): {'TRBV24-1', 'TRBV7-9', 'TRBV17*01', 'TRBV1', 'TRBV7-6', 'TRBV12-4*01', 'TRBV6-4', 'TRBV5-5', 'TRBV4-1', 'TRBV10-1*02', 'TRBV11-3', 'TRBV18', 'TRBV7-8', 'TRBV9*01', 'TRBV5-7*01', 'TRBV11-1', 'TRBV12-3', 'TRBV7-5*02', 'TRBV5-8*01', 'TRBV19', 'TRBV5-6', 'TRBV12-3*01', 'TRBV5-4*01', 'TRBV7-7', 'TRBV7-8*01', 'TRBV9', 'TRBV12-1', 'TRBV7-2', 'TRBV7-4*01', 'TRBV6-2', 'TRBV6-2*01', 'TRBV16', 'TRBV11-2*01', 'TRBV25-1*01', 'TRBV5-1*01', 'TRBV7-2*01', 'TRBV7-7*01', 'TRBV22-1*01', 'TRBV16*01', 'TRBV13*01', 'TRBV7-3', 'TRBV7-9*01', 'TRBV29-1*01', 'TRBV10-1*01', 'TRBV11-1*01', 'TRBV6-5', 'TRBV10-3*01', 'TRBV10-1', 'TRBV9*02', 'TRBV10-3*02', 'TRBV12-4', 'TRBV6-6*02', 'TRBV14*01', 'TRBV4-1*01', 'TRBV20-1*04', 'TRBV21-1', 'TRBV24-1*01', 'TRBV21-1*01', 'TRBV1*01', 'TRBV6-9*01', 'TRBV6-5*01', 'TRBV3-2', 'TRBV5-6*01', 'TRBV7-8*03', 'TRBV30', 'TRBV4-2*01', 'TRBV11-3*01', 'TRBV12-2*01', 'TRBV6-8', 'TRBV7-1*01', 'TRBV6-7', 'TRBV5-8', 'TRBV11-2', 'TRBV6-8*01', 'TRBV12-5', 'TRBV4-2', 'TRBV2*01', 'TRBV7-6*01', 'TRBV7-3*01', 'TRBV5-4', 'TRBV4-3', 'TRBV6-6*04', 'TRBV11-2*03', 'TRBV10-2', 'TRBV23-1', 'TRBV30*05', 'TRBV4-3*01', 'TRBV5-7', 'TRBV30*02', 'TRBV12-5*01', 'TRBV15*02', 'TRBV6-6*01', 'TRBV7-2*02', 'TRBV6-7*01', 'TRBV3-1*01', 'TRBV25-1', 'TRBV23-1*01', 'TRBV5-1', 'TRBV15*01', 'TRBV26*01', 'TRBV10-2*01', 'TRBV5-2*01', 'TRBV15', 'TRBV6-9', 'TRBV19*01', 'TRBV18*01', 'TRBV12-2', 'TRBV7-9*03', 'TRBV7-5*01', 'TRBV6-3', 'TRBV27*01', 'TRBV10-3', 'TRBV7-1', nan, 'TRBV20-1', 'TRBV7-4', 'TRBV19*02', 'TRBV30*01', 'TRBV29-1', 'TRBV28', 'TRBV2', 'TRBV5-3', 'TRBV11-2*02', 'TRBV17', 'TRBV8-1', 'TRBV13', 'TRBV6-1', 'TRBV6-4*01', 'TRBV27', 'TRBV5-5*01', 'TRBV6-6', 'TRBV8-2', 'TRBV28*01', 'TRBV14', 'TRBV6-1*01', 'TRBV3-1', 'TRBV20-1*01'}\n",
      "\n",
      "--- MHC ---\n",
      "Length train set: 60. Length valid set: 60. Length test set: 61\n",
      "train_set (60): {nan, 'HLA-A*02:03', 'HLA-B*18', 'HLA-B*27:05', 'HLA-B*44:03', 'HLA-A*29:02', 'HLA-A*02:08', 'HLA-B*35:02', 'HLA-B*44:08', 'HLA-A*02:06', 'HLA-A*02:14', 'HLA-A*02:01', 'HLA-B*38:01', 'HLA-A*03:01', 'HLA-C*04:01', 'HLA-A*02:07', 'HLA-A*02:11', 'HLA-A*24:02', 'HLA-B*07', 'HLA-B*37:01', 'HLA-B*07:02', 'HLA-B*08', 'HLA-A*02:10', 'HLA-A*68:01', 'HLA-C*07:02', 'HLA-B*44:05', 'HLA-A*11:01', 'HLA-C*03:04', 'HLA-B*35:08', 'HLA-A*01', 'HLA-B*57:01', 'HLA-A*02:17', 'HLA-A*02', 'HLA-A*02:02', 'HLA-B*40:01', 'HLA-B*27:09', 'HLA-A*11', 'HLA-B*51:01', 'HLA-B*39:01', 'HLA-A*02:16', 'HLA-B*08:01', 'HLA-B*42:01', 'HLA-A*02:09', 'HLA-B*35:01', 'HLA-A*30:01', 'HLA-A*02:13', 'HLA-A*80:01', 'HLA-C*12:02', 'HLA-B*18:01', 'HLA-B*15:01', 'HLA-B*41:02', 'HLA-A*02:15', 'HLA-B*27', 'HLA-B*57:03', 'HLA-B*57', 'HLA-A*01:01', 'HLA-A*02:05', 'HLA-A*02:04', 'HLA-B*44:02', 'HLA-B*35'}\n",
      "valid_set (60): {nan, 'HLA-B*18', 'HLA-B*27:05', 'HLA-B*44:03', 'HLA-A*29:02', 'HLA-C*06:02', 'HLA-A*02:06', 'HLA-A*23:01', 'HLA-A*02:01', 'HLA-B*38:01', 'HLA-A*03:01', 'HLA-A*02:12', 'HLA-A*24:02', 'HLA-B*07', 'HLA-C*03', 'HLA-B*37:01', 'HLA-B*07:02', 'HLA-B*08', 'HLA-C*08:01', 'HLA-A*02:10', 'HLA-A*68:01', 'HLA-B*15', 'HLA-C*07:02', 'HLA-B*44:05', 'HLA-A*11:01', 'HLA-C*03:04', 'HLA-B*35:08', 'HLA-B*57:01', 'HLA-A*01', 'HLA-C*08:02', 'HLA-A*32:01', 'HLA-A*02', 'HLA-B*50:01', 'HLA-B*40:01', 'HLA-A*11', 'HLA-B*51:01', 'HLA-B*39:01', 'HLA-C*05:01', 'HLA-C*16:01', 'HLA-C*03:03', 'HLA-B*08:01', 'HLA-B*42:01', 'HLA-B*35:01', 'HLA-A*02:13', 'HLA-A*80:01', 'HLA-B*41:02', 'HLA-B*18:01', 'HLA-B*15:01', 'HLA-B*48:01', 'HLA-B*42', 'HLA-C*12:02', 'HLA-B*27', 'HLA-C*07:01', 'HLA-B*57:03', 'HLA-C*14:02', 'HLA-B*57', 'HLA-A*30:02', 'HLA-A*01:01', 'HLA-A*02:05', 'HLA-B*44:02'}\n",
      "test_set (61): {nan, 'HLA-A*02:01:48', 'HLA-B*27:05', 'HLA-B*57:06', 'HLA-B*08:01:29', 'HLA-B*44:03', 'HLA-A*24:02:84', 'HLA-A*29:02', 'HLA-B*53', 'HLA-B*14:02', 'HLA-A*02:01', 'HLA-B*38:01', 'HLA-A*03:01', 'HLA-C*04:01', 'HLA-A*24:02', 'HLA-C*01:02', 'HLA-B*07', 'HLA-A*01:01:73', 'HLA-B*07:02', 'HLA-B*08', 'HLA-A*68:01', 'HLA-B*15', 'HLA-A*11:01', 'HLA-C*03:04', 'HLA-B*35:08', 'HLA-B*57:01', 'HLA-A*01', 'HLA-C*08:02', 'HLA-B*35:01:45', 'HLA-A*32:01', 'HLA-A*02', 'HLA-A*02:01:98', 'HLA-A*11', 'HLA-B*51:01', 'HLA-A*02:06:01:03', 'HLA-C*03:03', 'HLA-B*07:02:48', 'HLA-A*24:02:33', 'HLA-E*01:01:01:03', 'HLA-B*08:01', 'HLA-B*42:01', 'HLA-B*35:01', 'HLA-A*25:01', 'HLA-B*81:01', 'HLA-B*58', 'HLA-E*01:03', 'HLA-B*27:05:31', 'HLA-B*15:01', 'HLA-B*35:42:02', 'HLA-B*27', 'HLA-A*02:266', 'HLA-B*57', 'HLA-C*14:02', 'HLA-B*35:42:01', 'HLA-A*11:01:18', 'HLA-B*44:03:08', 'HLA-A*03', 'HLA-A*30:02', 'HLA-A*01:01', 'HLA-B*42', 'HLA-B*35'} \n",
      "\n",
      "Train ∩ Test (29): {nan, 'HLA-B*27:05', 'HLA-B*44:03', 'HLA-A*29:02', 'HLA-B*38:01', 'HLA-A*02:01', 'HLA-A*03:01', 'HLA-C*04:01', 'HLA-A*24:02', 'HLA-B*07', 'HLA-B*07:02', 'HLA-B*08', 'HLA-A*68:01', 'HLA-C*03:04', 'HLA-A*11:01', 'HLA-B*35:08', 'HLA-B*57:01', 'HLA-A*01', 'HLA-A*02', 'HLA-A*11', 'HLA-B*51:01', 'HLA-B*08:01', 'HLA-B*42:01', 'HLA-B*35:01', 'HLA-B*15:01', 'HLA-B*27', 'HLA-B*57', 'HLA-A*01:01', 'HLA-B*35'}\n",
      "Valid ∩ Test (34): {nan, 'HLA-B*27:05', 'HLA-B*44:03', 'HLA-A*29:02', 'HLA-B*38:01', 'HLA-A*02:01', 'HLA-A*03:01', 'HLA-A*24:02', 'HLA-B*07', 'HLA-B*07:02', 'HLA-B*08', 'HLA-A*68:01', 'HLA-B*15', 'HLA-C*03:04', 'HLA-A*11:01', 'HLA-B*35:08', 'HLA-A*01', 'HLA-B*57:01', 'HLA-C*08:02', 'HLA-A*32:01', 'HLA-A*02', 'HLA-A*11', 'HLA-B*51:01', 'HLA-C*03:03', 'HLA-B*08:01', 'HLA-B*42:01', 'HLA-B*35:01', 'HLA-B*15:01', 'HLA-B*27', 'HLA-C*14:02', 'HLA-B*57', 'HLA-A*30:02', 'HLA-A*01:01', 'HLA-B*42'}\n",
      "Test - (Train ∪ Valid) (25): {'HLA-A*02:01:48', 'HLA-B*57:06', 'HLA-B*08:01:29', 'HLA-A*24:02:84', 'HLA-B*53', 'HLA-B*14:02', 'HLA-C*01:02', 'HLA-A*01:01:73', 'HLA-B*35:01:45', 'HLA-A*02:01:98', 'HLA-A*02:06:01:03', 'HLA-B*07:02:48', 'HLA-A*24:02:33', 'HLA-E*01:01:01:03', 'HLA-A*25:01', 'HLA-B*81:01', 'HLA-B*58', 'HLA-E*01:03', 'HLA-B*27:05:31', 'HLA-B*35:42:02', 'HLA-A*02:266', 'HLA-B*35:42:01', 'HLA-A*11:01:18', 'HLA-B*44:03:08', 'HLA-A*03'}\n",
      "Train ∩ Valid (43): {nan, 'HLA-B*18', 'HLA-B*27:05', 'HLA-B*44:03', 'HLA-A*29:02', 'HLA-A*02:06', 'HLA-B*38:01', 'HLA-A*02:01', 'HLA-A*03:01', 'HLA-A*24:02', 'HLA-B*07', 'HLA-B*37:01', 'HLA-B*07:02', 'HLA-B*08', 'HLA-A*02:10', 'HLA-A*68:01', 'HLA-C*07:02', 'HLA-B*44:05', 'HLA-A*11:01', 'HLA-C*03:04', 'HLA-B*35:08', 'HLA-B*57:01', 'HLA-A*01', 'HLA-A*02', 'HLA-B*40:01', 'HLA-A*11', 'HLA-B*51:01', 'HLA-B*39:01', 'HLA-B*08:01', 'HLA-B*42:01', 'HLA-B*35:01', 'HLA-A*02:13', 'HLA-A*80:01', 'HLA-B*41:02', 'HLA-B*18:01', 'HLA-B*15:01', 'HLA-C*12:02', 'HLA-B*27', 'HLA-B*57:03', 'HLA-B*57', 'HLA-A*01:01', 'HLA-A*02:05', 'HLA-B*44:02'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# def normalize(value):\n",
    "#     if pd.isna(value):\n",
    "#         return ''\n",
    "#     value = value.lower()  # lowercase\n",
    "#     value = re.sub(r'[^a-z0-9]', '', value)  # remove non-alphanumeric\n",
    "#     return value\n",
    "\n",
    "# def normalize_columns(df, columns):\n",
    "#     return df[columns].map(normalize)\n",
    "\n",
    "# # Normalize columns\n",
    "# train_norm = normalize_columns(train_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "# valid_norm = normalize_columns(valid_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "# test_norm  = normalize_columns(test_df, ['TRBJ', 'TRBV', 'MHC'])\n",
    "\n",
    "# Compare sets\n",
    "for col in ['TRBJ', 'TRBV', 'MHC']:\n",
    "    train_set = set(train_df[col])\n",
    "    valid_set = set(valid_df[col])\n",
    "    test_set = set(test_df[col])\n",
    "\n",
    "\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Length train set: {len(train_set)}. Length valid set: {len(valid_set)}. Length test set: {len(test_set)}\")\n",
    "    print(f\"train_set ({len(train_set)}): {train_set}\")\n",
    "    print(f\"valid_set ({len(valid_set)}): {valid_set}\")\n",
    "    print(f\"test_set ({len(test_set)}): {test_set} \\n\")\n",
    "    print(f\"Train ∩ Test ({len(train_set & test_set)}): {train_set & test_set}\")\n",
    "    print(f\"Valid ∩ Test ({len(valid_set & test_set)}): {valid_set & test_set}\")\n",
    "    print(f\"Test - (Train ∪ Valid) ({len(test_set - (train_set | valid_set))}): {test_set - (train_set | valid_set)}\")\n",
    "    print(f\"Train ∩ Valid ({len(train_set & valid_set)}): {train_set & valid_set}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TRBV  Binding  count\n",
      "0  TRBV6-3*01        0    240\n",
      "1  TRBV6-3*01        1     86\n",
      "2  TRBV7-2*03        0     41\n",
      "3  TRBV7-2*03        1     13\n"
     ]
    }
   ],
   "source": [
    "# how often appear those and in which binding category? \n",
    "# Test - (Train ∪ Valid) (2): {'TRBV7-2*03', 'TRBV6-3*01'}\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "\n",
    "# Define the alleles of interest\n",
    "alleles_of_interest = {'TRBV7-2*03', 'TRBV6-3*01'}\n",
    "\n",
    "# Filter the dataframe\n",
    "filtered_df = test_df[test_df['TRBV'].isin(alleles_of_interest)]\n",
    "\n",
    "# Count appearances per Binding category\n",
    "binding_counts = filtered_df.groupby(['TRBV', 'Binding']).size().reset_index(name='count')\n",
    "\n",
    "print(binding_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short inspection in train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210016/3671801919.py:8: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 280252\n",
      "Validation dataset length: 210192\n",
      "Number of matching rows: 18862\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "\n",
    "print(f\"Train dataset length: {len(train_df)}\")\n",
    "print(f\"Validation dataset length: {len(valid_df)}\")\n",
    "\n",
    "# Select only the relevant columns\n",
    "target_columns = ['Epitope', 'TRB_CDR3']\n",
    "\n",
    "# Convert valid_df pairs into a set for fast lookup\n",
    "valid_pairs = set(map(tuple, valid_df[target_columns].values))\n",
    "\n",
    "# Count how many rows in train_df have the same (Epitope, TRB_CDR3) pair as in valid_df\n",
    "matching_rows = train_df[target_columns].apply(tuple, axis=1).isin(valid_pairs).sum()\n",
    "\n",
    "print(f\"Number of matching rows: {matching_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of df:  18862\n",
      "Number of binding samples 1704\n"
     ]
    }
   ],
   "source": [
    "# matching_rows[matching_rows['Binding'] == 1].count()\n",
    "# matching_rows.head()\n",
    "\n",
    "matching_df = train_df[target_columns].apply(tuple, axis=1).isin(valid_pairs)\n",
    "# matching_df.head()\n",
    "# len(matching_df)\n",
    "double_pairs_df = train_df[matching_df]\n",
    "print(\"length of df: \" , len(double_pairs_df))\n",
    "print(\"Number of binding samples\" , len(double_pairs_df[double_pairs_df['Binding'] == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common 'Epitope' and 'TRB_CDR3' pairs: 177715\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_df and valid_df are your DataFrames\n",
    "# Merge the two DataFrames on 'Epitope' and 'TRB_CDR3'\n",
    "merged_df = pd.merge(train_df, valid_df, on=['Epitope', 'TRB_CDR3'], how='inner')\n",
    "\n",
    "# The number of rows in merged_df will give you the count of common pairs\n",
    "common_pairs_count = len(merged_df)\n",
    "\n",
    "print(f\"Number of common 'Epitope' and 'TRB_CDR3' pairs: {common_pairs_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching rows: 18862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming train_df and valid_df are already defined\n",
    "\n",
    "# Select only the relevant columns\n",
    "target_columns = ['Epitope', 'TRB_CDR3']\n",
    "\n",
    "# Convert valid_df pairs into a set for fast lookup\n",
    "valid_pairs = set(map(tuple, valid_df[target_columns].values))\n",
    "\n",
    "# Count how many rows in train_df have the same (Epitope, TRB_CDR3) pair as in valid_df\n",
    "matching_rows = train_df[target_columns].apply(tuple, axis=1).isin(valid_pairs).sum()\n",
    "\n",
    "print(f\"Number of matching rows: {matching_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218269/3552120905.py:10: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset - Binding: 140126, Non-Binding: 140126\n",
      "Validation Dataset - Binding: 35032, Non-Binding: 175160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "t_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "v_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Function to count binding and non-binding rows\n",
    "def count_binding_rows(file_path):\n",
    "    # Read the TSV file\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Count the number of binding (1) and non-binding (0) rows\n",
    "    binding_count = df[df['Binding'] == 1].shape[0]\n",
    "    non_binding_count = df[df['Binding'] == 0].shape[0]\n",
    "    \n",
    "    return binding_count, non_binding_count\n",
    "\n",
    "# Count for training dataset\n",
    "train_binding, train_non_binding = count_binding_rows(t_path)\n",
    "print(f\"Training Dataset - Binding: {train_binding}, Non-Binding: {train_non_binding}\")\n",
    "\n",
    "# Count for validation dataset\n",
    "val_binding, val_non_binding = count_binding_rows(v_path)\n",
    "print(f\"Validation Dataset - Binding: {val_binding}, Non-Binding: {val_non_binding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218269/1833262170.py:9: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File paths\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "\n",
    "# Combine datasets\n",
    "data = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "\n",
    "# Ensure the columns exist\n",
    "if 'TRB_CDR3' not in data.columns or 'Epitope' not in data.columns:\n",
    "    raise ValueError(\"Columns 'TRB_CDR3' and 'Epitope' must exist in the dataset\")\n",
    "\n",
    "# Compute lengths\n",
    "data['tcr_length'] = data['TRB_CDR3'].astype(str).apply(len)\n",
    "data['epitope_length'] = data['Epitope'].astype(str).apply(len)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data['tcr_length'], bins=20, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('TCR Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('TCR Length Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data['epitope_length'], bins=20, color='green', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Epitope Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Epitope Length Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epitopes longer than 26: 3112\n",
      "Epitopes longer than 26:\n",
      "                                            Epitope  epitope_length\n",
      "100                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "114                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "119                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "164                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "306                     GLEAPFLYLYALVYFLQSINFVRIIMR              27\n",
      "...                                             ...             ...\n",
      "315193  LPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERD              43\n",
      "315197  ILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASA              43\n",
      "315255  MIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGV              43\n",
      "315271  ILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASA              43\n",
      "315279       WESGVKDCVVLHSYFTSDYYQLYSTQLSTDTGVEHVTF              38\n",
      "\n",
      "[3112 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter epitopes longer than 26\n",
    "long_epitopes = data[data['epitope_length'] > 26]\n",
    "\n",
    "# Count the number of long epitopes\n",
    "num_long_epitopes = long_epitopes.shape[0]\n",
    "\n",
    "# Display the long epitopes\n",
    "print(f\"Number of epitopes longer than 26: {num_long_epitopes}\")\n",
    "print(\"Epitopes longer than 26:\")\n",
    "print(long_epitopes[['Epitope', 'epitope_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigations on ProtBERT-Embeddings in oder to get shape, type, etc., and be able to use them in the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 1222\n",
      "\n",
      "Key: GLCTLVAML\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: YLNTLTLAV\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: KLSYGIATV\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: SEHDYQIGGYTEKW\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: IMLIIFWFSL\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: AYSNNSIAIPTNFTISV\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: IPSINVHHY\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: QPRAPIRPI\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: MLDLQPETT\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: ELRRKMMYM\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "Number of keys in the NPZ file: 5000\n",
      "\n",
      "Key: CASSYSPRDPAYEQYF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSLDPEKLFF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASRLTTGLAETQYF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSLDVSYEQYF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSMGSGLTYEQYF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CATSVQGGYTF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CSARDTIYF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSIGGNTGELFF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASRLAGANTGELFF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASLRPGTAYGYTF\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to the embeddings file\n",
    "# paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "# paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "# paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "# === Lade die reduzierten Isomap-Embeddings ===\n",
    "tcr_embeddings_path = '../../data/embeddings/beta/allele/isomap/TRB_beta_embeddings_reduced.npz'\n",
    "epitope_embeddings_path = '../../data/embeddings/beta/allele/isomap/Epitope_beta_embeddings_reduced.npz'\n",
    "\n",
    "beta_all_epi_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "beta_all_trb_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "# paths = [beta_all_epi_path, beta_all_trb_path ]\n",
    "# paths = [tcr_embeddings_path, epitope_embeddings_path]\n",
    "\n",
    "paths_beta = [beta_all_epi_path, beta_all_trb_path]\n",
    "\n",
    "train_epi_path = \"../../data/embeddings/beta/allele/dimension_1024/train_epitope_padded_batches/batch_0.npz\"\n",
    "train_tcr_path = \"../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_0.npz\"\n",
    "embeddings_paths = [train_epi_path, train_tcr_path]\n",
    "\n",
    "for path in embeddings_paths:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    # Inspect the shape and size of each stored array\n",
    "    for key in data.files[:10]:\n",
    "        array = data[key]\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        print(f\"Shape: {array.shape}\")\n",
    "        print(f\"Size: {array.size}\")\n",
    "        print(f\"Data Type: {array.dtype}\")\n",
    "        # print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 1383\n",
      "Number of keys in the NPZ file: 41519\n",
      "Number of keys in the NPZ file: 45261\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to the embeddings file\n",
    "paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "paths = [paired_all_epi_path, paired_all_tra_path,paired_all_trb_path ]\n",
    "for path in paths:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    # Inspect the shape and size of each stored array\n",
    "    # for key in data.files[:10]:\n",
    "    #     array = data[key]\n",
    "    #     print(f\"\\nKey: {key}\")\n",
    "    #     print(f\"Shape: {array.shape}\")\n",
    "    #     print(f\"Size: {array.size}\")\n",
    "    #     print(f\"Data Type: {array.dtype}\")\n",
    "    #     print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 41519 but got size 45261 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m padded_trb \u001b[38;5;241m=\u001b[39m pad_embeddings(trb_embeddings, max_len)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Concatenate along sequence dimension\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m padded_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([padded_tra, padded_trb, padded_epi], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save padded embeddings\u001b[39;00m\n\u001b[1;32m     37\u001b[0m padd_paired_all_epi_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/paired/allele/padded_Epitope_paired_embeddings.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 41519 but got size 45261 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# raising error, because length tra != length trb.  To be solved.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Paths to the embeddings files\n",
    "paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "\n",
    "# Load NPZ files\n",
    "epi_data = np.load(paired_all_epi_path, allow_pickle=True)\n",
    "tra_data = np.load(paired_all_tra_path, allow_pickle=True)\n",
    "trb_data = np.load(paired_all_trb_path, allow_pickle=True)\n",
    "\n",
    "# Extract embeddings\n",
    "epi_embeddings = [torch.tensor(epi_data[key]) for key in epi_data]\n",
    "tra_embeddings = [torch.tensor(tra_data[key]) for key in tra_data]\n",
    "trb_embeddings = [torch.tensor(trb_data[key]) for key in trb_data]\n",
    "\n",
    "# Find max sequence length\n",
    "max_len = max(max(e.shape[0] for e in epi_embeddings), \n",
    "              max(e.shape[0] for e in tra_embeddings), \n",
    "              max(e.shape[0] for e in trb_embeddings))\n",
    "\n",
    "# Pad sequences\n",
    "def pad_embeddings(embeddings, max_len):\n",
    "    return pad_sequence([torch.nn.functional.pad(e, (0, 0, 0, max_len - e.shape[0])) for e in embeddings], batch_first=True, padding_value=0.0)\n",
    "\n",
    "padded_epi = pad_embeddings(epi_embeddings, max_len)\n",
    "padded_tra = pad_embeddings(tra_embeddings, max_len)\n",
    "padded_trb = pad_embeddings(trb_embeddings, max_len)\n",
    "\n",
    "# Concatenate along sequence dimension\n",
    "padded_combined = torch.cat([padded_tra, padded_trb, padded_epi], dim=1)\n",
    "\n",
    "# Save padded embeddings\n",
    "padd_paired_all_epi_path = '../../data/embeddings/paired/allele/padded_Epitope_paired_embeddings.npz'\n",
    "padd_paired_all_tra_path = '../../data/embeddings/paired/allele/padded_TRA_paired_embeddings.npz'\n",
    "padd_paired_all_trb_path = '../../data/embeddings/paired/allele/padded_TRB_paired_embeddings.npz'\n",
    "padd_paired_combined_path = '../../data/embeddings/paired/allele/padded_Combined_paired_embeddings.npz'\n",
    "\n",
    "np.savez(padd_paired_all_epi_path, **{key: padded_epi[i].numpy() for i, key in enumerate(epi_data)})\n",
    "np.savez(padd_paired_all_tra_path, **{key: padded_tra[i].numpy() for i, key in enumerate(tra_data)})\n",
    "np.savez(padd_paired_all_trb_path, **{key: padded_trb[i].numpy() for i, key in enumerate(trb_data)})\n",
    "np.savez(padd_paired_combined_path, combined=padded_combined.numpy())\n",
    "\n",
    "print(\"Padded and concatenated embeddings saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_all_epi_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "beta_all_trb_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from data_scripts.pca_analysis import perform_pca_on_embeddings\n",
    "\n",
    "def process_chunk(chunk, chunk_index, output_path, device='cuda', n_components=512):\n",
    "    \"\"\"\n",
    "    Führt PCA auf einem Chunk durch und speichert das reduzierte Ergebnis.\n",
    "    \"\"\"\n",
    "    # Auf CUDA verschieben\n",
    "    chunk_tensor = torch.tensor(chunk, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Zurück auf CPU und in NumPy konvertieren für PCA\n",
    "    chunk_cpu = chunk_tensor.cpu().numpy()\n",
    "\n",
    "    # PCA durchführen\n",
    "    pca_df = perform_pca_on_embeddings([chunk_cpu], n_components=n_components)\n",
    "\n",
    "    # Speichern des reduzierten Chunks\n",
    "    chunk_output_path = f\"{output_path}_chunk_{chunk_index}.npz\"\n",
    "    np.savez_compressed(chunk_output_path, embeddings=pca_df.values)\n",
    "    print(f\"✅ Reduzierter Chunk {chunk_index} gespeichert unter: {chunk_output_path}\")\n",
    "\n",
    "def load_and_process_embeddings_in_chunks(file_path, output_path, chunk_size=100_000, device='cuda'):\n",
    "    \"\"\"\n",
    "    Lädt Embeddings in Blöcken, führt PCA durch und speichert sie sofort.\n",
    "    \"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    print(f\"🔍 Verfügbare Keys: {len(data.files)}\")\n",
    "\n",
    "    current_chunk = []\n",
    "    chunk_index = 0\n",
    "\n",
    "    for idx, key in enumerate(data.files):\n",
    "        current_chunk.append(data[key])\n",
    "\n",
    "        # Wenn Chunk voll, dann verarbeiten und speichern\n",
    "        if len(current_chunk) * 8 >= chunk_size:  # Weil jeder Key 8 Samples hat\n",
    "            combined_chunk = np.concatenate(current_chunk, axis=0)\n",
    "            process_chunk(combined_chunk, chunk_index, output_path, device)\n",
    "            current_chunk = []  # Speicher freigeben\n",
    "            chunk_index += 1\n",
    "\n",
    "    # Verarbeite den letzten, unvollständigen Chunk\n",
    "    if current_chunk:\n",
    "        combined_chunk = np.concatenate(current_chunk, axis=0)\n",
    "        process_chunk(combined_chunk, chunk_index, output_path, device)\n",
    "        print(f\"✅ Letzter Chunk verarbeitet mit Shape: {combined_chunk.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Verfügbare Keys: 211529\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183794, 1024)\n",
      "Gesamte Datenform vor PCA: (183794, 1024)\n",
      "Gesamte Datenform nach PCA: (183794, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32249368e-01 9.58742275e-02 7.89532647e-02 5.11123302e-02\n",
      " 4.80737901e-02 4.30722391e-02 4.12162620e-02 3.67663143e-02\n",
      " 3.44270099e-02 3.00138312e-02 2.75371255e-02 2.53238905e-02\n",
      " 2.09061986e-02 1.88502574e-02 1.82200435e-02 1.76574678e-02\n",
      " 1.67295576e-02 1.39572318e-02 1.24031297e-02 1.13262613e-02\n",
      " 1.04606786e-02 9.60687032e-03 8.80338193e-03 8.47489202e-03\n",
      " 8.19804270e-03 7.24789258e-03 6.62234060e-03 6.46977093e-03\n",
      " 6.05263837e-03 5.62309360e-03 5.19391063e-03 4.63077482e-03\n",
      " 4.57076437e-03 4.37590878e-03 4.22315599e-03 3.85675877e-03\n",
      " 3.70877228e-03 3.50729256e-03 3.36813172e-03 3.19846308e-03\n",
      " 3.02063081e-03 2.81538770e-03 2.59925467e-03 2.51338564e-03\n",
      " 2.38639413e-03 2.29968182e-03 2.23009345e-03 2.17231169e-03\n",
      " 2.08578046e-03 1.99314245e-03 1.89377655e-03 1.78039604e-03\n",
      " 1.69840526e-03 1.66242810e-03 1.60023223e-03 1.50027460e-03\n",
      " 1.45836685e-03 1.40550723e-03 1.38354467e-03 1.30071209e-03\n",
      " 1.22352720e-03 1.17135390e-03 1.14210816e-03 1.09223570e-03\n",
      " 1.06541732e-03 1.00208841e-03 9.88587380e-04 9.66312127e-04\n",
      " 9.48534597e-04 9.07367833e-04 8.98717355e-04 8.62741397e-04\n",
      " 8.25732197e-04 7.99276721e-04 7.63160787e-04 7.54369732e-04\n",
      " 7.36028923e-04 7.30822350e-04 6.78140344e-04 6.65148766e-04\n",
      " 6.59425804e-04 6.32058288e-04 6.20421845e-04 5.92764806e-04\n",
      " 5.80061272e-04 5.71998489e-04 5.52067357e-04 5.27997250e-04\n",
      " 5.24235475e-04 5.16245570e-04 4.97477990e-04 4.80747064e-04\n",
      " 4.65446140e-04 4.56599779e-04 4.53016870e-04 4.41956930e-04\n",
      " 4.28339137e-04 4.22203600e-04 4.19148039e-04 4.10068709e-04\n",
      " 3.94203350e-04 3.90213183e-04 3.80060590e-04 3.74439962e-04\n",
      " 3.66189527e-04 3.56702455e-04 3.55999477e-04 3.41677829e-04\n",
      " 3.36665866e-04 3.35212976e-04 3.30815298e-04 3.22832746e-04\n",
      " 3.18989630e-04 3.14833246e-04 3.05139631e-04 2.96561199e-04\n",
      " 2.88623641e-04 2.81425753e-04 2.80811376e-04 2.76561697e-04\n",
      " 2.74668371e-04 2.72339625e-04 2.62400417e-04 2.54755527e-04\n",
      " 2.53086222e-04 2.47829429e-04 2.46898824e-04 2.43470688e-04\n",
      " 2.38360599e-04 2.37120678e-04 2.33134812e-04 2.29676341e-04\n",
      " 2.25141763e-04 2.23836941e-04 2.20442122e-04 2.17229615e-04\n",
      " 2.11744570e-04 2.09139665e-04 2.07593064e-04 2.05150862e-04\n",
      " 2.00507568e-04 1.96684915e-04 1.95408971e-04 1.93661620e-04\n",
      " 1.89769719e-04 1.87748185e-04 1.84336824e-04 1.81109565e-04\n",
      " 1.78549896e-04 1.75441746e-04 1.74692018e-04 1.73733355e-04\n",
      " 1.71824560e-04 1.69543676e-04 1.66107123e-04 1.62391186e-04\n",
      " 1.61769295e-04 1.61371564e-04 1.60635361e-04 1.57998040e-04\n",
      " 1.55910819e-04 1.54358461e-04 1.53969608e-04 1.50173637e-04\n",
      " 1.49692982e-04 1.48973276e-04 1.46293435e-04 1.44811065e-04\n",
      " 1.43333701e-04 1.42951611e-04 1.41029730e-04 1.40153320e-04\n",
      " 1.39130646e-04 1.37250198e-04 1.34972608e-04 1.33773750e-04\n",
      " 1.32691192e-04 1.31035597e-04 1.29954773e-04 1.28409952e-04\n",
      " 1.27596814e-04 1.25049238e-04 1.23333008e-04 1.23185888e-04\n",
      " 1.21958533e-04 1.20159541e-04 1.18940787e-04 1.18080377e-04\n",
      " 1.17006432e-04 1.16466887e-04 1.15390409e-04 1.15020239e-04\n",
      " 1.14196488e-04 1.13735011e-04 1.12560315e-04 1.11397145e-04\n",
      " 1.10185686e-04 1.09691308e-04 1.07350597e-04 1.06698429e-04\n",
      " 1.06341611e-04 1.04710525e-04 1.03929883e-04 1.03132398e-04\n",
      " 1.02526322e-04 1.01521231e-04 1.01104124e-04 1.00450037e-04\n",
      " 9.87141079e-05 9.80707833e-05 9.73115752e-05 9.60420356e-05\n",
      " 9.56048135e-05 9.46452774e-05 9.37319491e-05 9.31051793e-05\n",
      " 9.21008343e-05 9.12831757e-05 9.10631369e-05 9.09332824e-05\n",
      " 8.94095541e-05 8.87031895e-05 8.84329083e-05 8.76011978e-05\n",
      " 8.69328214e-05 8.65039576e-05 8.60259092e-05 8.53331052e-05\n",
      " 8.50488299e-05 8.45394580e-05 8.31376825e-05 8.22110421e-05\n",
      " 8.21087365e-05 8.17999123e-05 8.12756157e-05 8.07364926e-05\n",
      " 7.99666082e-05 7.92107411e-05 7.87746751e-05 7.86245144e-05\n",
      " 7.81833096e-05 7.77557233e-05 7.75314533e-05 7.66396855e-05\n",
      " 7.58083276e-05 7.49885477e-05 7.46844055e-05 7.39702895e-05\n",
      " 7.36486619e-05 7.32864276e-05 7.24267521e-05 7.23897639e-05\n",
      " 7.13687774e-05 7.11523051e-05 7.06331009e-05 7.04109350e-05\n",
      " 6.97196916e-05 6.88054442e-05 6.86669713e-05 6.82499282e-05\n",
      " 6.79955660e-05 6.68010952e-05 6.65641202e-05 6.58116634e-05\n",
      " 6.54294409e-05 6.52274309e-05 6.50737442e-05 6.47137931e-05\n",
      " 6.43868187e-05 6.41439477e-05 6.35529233e-05 6.32491684e-05\n",
      " 6.30386613e-05 6.22518291e-05 6.18925602e-05 6.12005596e-05\n",
      " 6.05164434e-05 6.03410170e-05 5.99106734e-05 5.93408049e-05\n",
      " 5.89244381e-05 5.87526591e-05 5.84414476e-05 5.80553060e-05\n",
      " 5.75424544e-05 5.72881210e-05 5.68863437e-05 5.63384808e-05\n",
      " 5.63042903e-05 5.60479570e-05 5.57723753e-05 5.51455419e-05\n",
      " 5.50746580e-05 5.48370760e-05 5.45470898e-05 5.38189855e-05\n",
      " 5.37137897e-05 5.36803333e-05 5.33007177e-05 5.30049743e-05\n",
      " 5.29144316e-05 5.25497754e-05 5.24284295e-05 5.18886186e-05\n",
      " 5.14568242e-05 5.11334682e-05 5.08372335e-05 5.06842346e-05\n",
      " 5.05278023e-05 4.99996271e-05 4.97299933e-05 4.95114458e-05\n",
      " 4.93145803e-05 4.93091179e-05 4.89130053e-05 4.86653135e-05\n",
      " 4.80139312e-05 4.78027969e-05 4.76622980e-05 4.74435280e-05\n",
      " 4.71454638e-05 4.69189770e-05 4.64865092e-05 4.60363305e-05\n",
      " 4.58168582e-05 4.57373097e-05 4.51052422e-05 4.50274480e-05\n",
      " 4.48829520e-05 4.45298362e-05 4.42332517e-05 4.41705701e-05\n",
      " 4.39982275e-05 4.36944668e-05 4.35905514e-05 4.30351654e-05\n",
      " 4.29667757e-05 4.26682722e-05 4.25229207e-05 4.23615665e-05\n",
      " 4.22390587e-05 4.19738295e-05 4.17639814e-05 4.15442518e-05\n",
      " 4.14766396e-05 4.10943102e-05 4.10744288e-05 4.08071129e-05\n",
      " 4.04264915e-05 4.02555882e-05 4.00675376e-05 3.99650181e-05\n",
      " 3.96617285e-05 3.95832002e-05 3.91486833e-05 3.90867069e-05\n",
      " 3.87236316e-05 3.84920813e-05 3.82616755e-05 3.81238182e-05\n",
      " 3.80217987e-05 3.77110497e-05 3.75932500e-05 3.73406392e-05\n",
      " 3.72915875e-05 3.71346176e-05 3.69501074e-05 3.68728392e-05\n",
      " 3.65888385e-05 3.63620425e-05 3.61535499e-05 3.60605419e-05\n",
      " 3.60044354e-05 3.58361650e-05 3.56467214e-05 3.54430668e-05\n",
      " 3.52566087e-05 3.49801802e-05 3.49156113e-05 3.45245854e-05\n",
      " 3.44216439e-05 3.42781392e-05 3.41148572e-05 3.40719558e-05\n",
      " 3.38643620e-05 3.38249143e-05 3.37568513e-05 3.35729047e-05\n",
      " 3.33974985e-05 3.32332570e-05 3.31314976e-05 3.27393503e-05\n",
      " 3.25866000e-05 3.25385628e-05 3.22943941e-05 3.22039353e-05\n",
      " 3.21326902e-05 3.18307156e-05 3.15757840e-05 3.14728744e-05\n",
      " 3.13904010e-05 3.13276645e-05 3.11853679e-05 3.10635971e-05\n",
      " 3.08304544e-05 3.07730935e-05 3.05546270e-05 3.04188621e-05\n",
      " 3.03525591e-05 3.02119619e-05 2.98079506e-05 2.97399597e-05\n",
      " 2.95692674e-05 2.94663751e-05 2.93447749e-05 2.91941951e-05\n",
      " 2.90991611e-05 2.90764098e-05 2.88628016e-05 2.87185339e-05\n",
      " 2.86412571e-05 2.85016570e-05 2.83484905e-05 2.82583438e-05\n",
      " 2.81085906e-05 2.79081962e-05 2.78462371e-05 2.77399980e-05\n",
      " 2.76506403e-05 2.74181421e-05 2.73537899e-05 2.72553485e-05\n",
      " 2.71956322e-05 2.71219593e-05 2.68623948e-05 2.67864243e-05\n",
      " 2.65704606e-05 2.64725567e-05 2.63530634e-05 2.62384545e-05\n",
      " 2.61055624e-05 2.60344069e-05 2.58893791e-05 2.58387870e-05\n",
      " 2.56221441e-05 2.55446505e-05 2.54522176e-05 2.53246579e-05\n",
      " 2.50694402e-05 2.50223365e-05 2.49106697e-05 2.47774164e-05\n",
      " 2.46809576e-05 2.45673168e-05 2.43960349e-05 2.43066656e-05\n",
      " 2.41833703e-05 2.40160840e-05 2.39062424e-05 2.38601690e-05\n",
      " 2.34681561e-05 2.34413297e-05 2.32404858e-05 2.31891813e-05\n",
      " 2.30594005e-05 2.29829777e-05 2.27778795e-05 2.26755869e-05\n",
      " 2.25951439e-05 2.24469109e-05 2.23388019e-05 2.21877973e-05\n",
      " 2.20793126e-05 2.20459443e-05 2.19125306e-05 2.18362595e-05\n",
      " 2.16245400e-05 2.15294685e-05 2.14183826e-05 2.13354699e-05\n",
      " 2.11948698e-05 2.11543253e-05 2.10945469e-05 2.09591462e-05\n",
      " 2.08500372e-05 2.08022037e-05 2.06741411e-05 2.05506363e-05\n",
      " 2.04585315e-05 2.03921678e-05 2.02261329e-05 2.01593777e-05\n",
      " 2.00619434e-05 1.99562506e-05 1.97359835e-05 1.96640317e-05\n",
      " 1.95811176e-05 1.94621257e-05 1.92684364e-05 1.91874139e-05\n",
      " 1.91309779e-05 1.89619879e-05 1.88031408e-05 1.86715768e-05\n",
      " 1.86401016e-05 1.85672279e-05 1.84634266e-05 1.81899966e-05\n",
      " 1.80857518e-05 1.79561617e-05 1.78468388e-05 1.76249836e-05]\n",
      "Gesamte erklärte Varianz: 0.995497975928254\n",
      "✅ Reduzierter Chunk 0 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_0.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183495, 1024)\n",
      "Gesamte Datenform vor PCA: (183495, 1024)\n",
      "Gesamte Datenform nach PCA: (183495, 512)\n",
      "Erklärte Varianz pro Komponente: [1.31756638e-01 9.61254466e-02 7.89165674e-02 5.14080141e-02\n",
      " 4.82924308e-02 4.33002527e-02 4.12940918e-02 3.66310449e-02\n",
      " 3.42996750e-02 2.95654184e-02 2.75189168e-02 2.53992416e-02\n",
      " 2.09440484e-02 1.87875455e-02 1.80446636e-02 1.73623310e-02\n",
      " 1.65152838e-02 1.38279082e-02 1.24752531e-02 1.13219732e-02\n",
      " 1.04393919e-02 9.71366194e-03 8.80060443e-03 8.45327955e-03\n",
      " 8.23607385e-03 7.23191910e-03 6.59071427e-03 6.48976518e-03\n",
      " 6.04608266e-03 5.63241455e-03 5.23254885e-03 4.65122332e-03\n",
      " 4.55993263e-03 4.41691479e-03 4.21486747e-03 3.91425301e-03\n",
      " 3.74500444e-03 3.52088628e-03 3.35504484e-03 3.19062288e-03\n",
      " 3.00811977e-03 2.78825010e-03 2.64057675e-03 2.52915026e-03\n",
      " 2.41278714e-03 2.30185497e-03 2.24770657e-03 2.19085543e-03\n",
      " 2.10638786e-03 1.98181278e-03 1.90315642e-03 1.77471679e-03\n",
      " 1.70087283e-03 1.68038944e-03 1.62516785e-03 1.48331051e-03\n",
      " 1.47562527e-03 1.42660473e-03 1.38698435e-03 1.31381311e-03\n",
      " 1.24732518e-03 1.18238658e-03 1.15192334e-03 1.09213300e-03\n",
      " 1.06531722e-03 1.01973888e-03 9.88839370e-04 9.69194200e-04\n",
      " 9.55687141e-04 9.14584882e-04 9.08658401e-04 8.70767146e-04\n",
      " 8.33756193e-04 8.01913064e-04 7.70514561e-04 7.59882142e-04\n",
      " 7.37799345e-04 7.29722688e-04 6.79072003e-04 6.72369928e-04\n",
      " 6.59762922e-04 6.41565892e-04 6.29929727e-04 5.95391564e-04\n",
      " 5.89527935e-04 5.70611242e-04 5.59348730e-04 5.36973084e-04\n",
      " 5.32460502e-04 5.13774360e-04 5.00320647e-04 4.79452672e-04\n",
      " 4.66523652e-04 4.57315422e-04 4.50605364e-04 4.45969015e-04\n",
      " 4.30823921e-04 4.17253457e-04 4.16703522e-04 4.11004269e-04\n",
      " 4.04469384e-04 3.93632579e-04 3.83546932e-04 3.74974932e-04\n",
      " 3.65789292e-04 3.60087985e-04 3.58351702e-04 3.47677565e-04\n",
      " 3.41079759e-04 3.37757656e-04 3.33535154e-04 3.23978491e-04\n",
      " 3.20711880e-04 3.12555641e-04 3.05994545e-04 2.97033826e-04\n",
      " 2.89940308e-04 2.84196361e-04 2.81510487e-04 2.79435323e-04\n",
      " 2.74877425e-04 2.73538353e-04 2.66624832e-04 2.58173298e-04\n",
      " 2.57433446e-04 2.53407530e-04 2.46956449e-04 2.45537727e-04\n",
      " 2.40584302e-04 2.39098459e-04 2.32730881e-04 2.29760787e-04\n",
      " 2.27960914e-04 2.22935846e-04 2.21969757e-04 2.16739127e-04\n",
      " 2.12974381e-04 2.11261402e-04 2.08177741e-04 2.06156661e-04\n",
      " 2.03107679e-04 1.98235957e-04 1.95510743e-04 1.94372572e-04\n",
      " 1.92866252e-04 1.89475447e-04 1.87404425e-04 1.84042392e-04\n",
      " 1.81089823e-04 1.77563888e-04 1.76145685e-04 1.75391435e-04\n",
      " 1.74058673e-04 1.72028641e-04 1.67271226e-04 1.65914999e-04\n",
      " 1.63552719e-04 1.62493564e-04 1.60896495e-04 1.59174852e-04\n",
      " 1.56869195e-04 1.55261489e-04 1.54469352e-04 1.51534469e-04\n",
      " 1.50841940e-04 1.49653584e-04 1.47686150e-04 1.45593520e-04\n",
      " 1.44994300e-04 1.43106298e-04 1.42718743e-04 1.41266368e-04\n",
      " 1.39677744e-04 1.38570677e-04 1.36606023e-04 1.35124749e-04\n",
      " 1.33805289e-04 1.30985243e-04 1.29972707e-04 1.28943386e-04\n",
      " 1.28185236e-04 1.26819214e-04 1.25269538e-04 1.23681606e-04\n",
      " 1.22481287e-04 1.21943397e-04 1.20867847e-04 1.19188314e-04\n",
      " 1.18012879e-04 1.17167326e-04 1.16473459e-04 1.15488381e-04\n",
      " 1.14202886e-04 1.13825172e-04 1.13249787e-04 1.11634341e-04\n",
      " 1.10928591e-04 1.08858745e-04 1.07945968e-04 1.07551780e-04\n",
      " 1.06679912e-04 1.06232436e-04 1.05628694e-04 1.04109013e-04\n",
      " 1.03104923e-04 1.02729909e-04 1.01923800e-04 1.01412641e-04\n",
      " 1.00687290e-04 9.86428833e-05 9.82355778e-05 9.68612378e-05\n",
      " 9.56032056e-05 9.48151927e-05 9.42342417e-05 9.40182388e-05\n",
      " 9.32251440e-05 9.29150704e-05 9.24285536e-05 9.20162527e-05\n",
      " 9.02249279e-05 8.95936361e-05 8.93024771e-05 8.86397823e-05\n",
      " 8.73888568e-05 8.69047511e-05 8.64949595e-05 8.58484796e-05\n",
      " 8.53471265e-05 8.44691038e-05 8.39179810e-05 8.31204215e-05\n",
      " 8.25884324e-05 8.17923092e-05 8.15647581e-05 8.12729645e-05\n",
      " 7.99365400e-05 7.99053100e-05 7.92406886e-05 7.84739726e-05\n",
      " 7.83935613e-05 7.78712113e-05 7.76247860e-05 7.68414340e-05\n",
      " 7.63362102e-05 7.57935901e-05 7.55884895e-05 7.50100766e-05\n",
      " 7.42278436e-05 7.35270140e-05 7.31924537e-05 7.27793279e-05\n",
      " 7.23027904e-05 7.17860991e-05 7.12577095e-05 7.04789722e-05\n",
      " 7.00378698e-05 6.98311656e-05 6.93693548e-05 6.88808190e-05\n",
      " 6.77976668e-05 6.75467537e-05 6.68729144e-05 6.65979068e-05\n",
      " 6.63214629e-05 6.59244021e-05 6.49524586e-05 6.48365026e-05\n",
      " 6.43407102e-05 6.42031170e-05 6.40887357e-05 6.39716491e-05\n",
      " 6.36328720e-05 6.27242250e-05 6.23571252e-05 6.18190101e-05\n",
      " 6.16657096e-05 6.05884124e-05 6.03796085e-05 6.00771261e-05\n",
      " 5.99097565e-05 5.94242779e-05 5.92363729e-05 5.87671266e-05\n",
      " 5.83272586e-05 5.81011669e-05 5.72146589e-05 5.69255533e-05\n",
      " 5.66473212e-05 5.64280477e-05 5.59805482e-05 5.58230310e-05\n",
      " 5.54503935e-05 5.50420267e-05 5.48752685e-05 5.45088205e-05\n",
      " 5.41861546e-05 5.37014836e-05 5.34261991e-05 5.32222983e-05\n",
      " 5.30708322e-05 5.28246492e-05 5.24609123e-05 5.20107536e-05\n",
      " 5.17272087e-05 5.14696216e-05 5.12685359e-05 5.11195674e-05\n",
      " 5.07382716e-05 5.03658014e-05 5.02551292e-05 4.97124399e-05\n",
      " 4.95255270e-05 4.93092300e-05 4.91035295e-05 4.84939442e-05\n",
      " 4.81987790e-05 4.80432692e-05 4.78429707e-05 4.76829328e-05\n",
      " 4.73917045e-05 4.69858152e-05 4.68265126e-05 4.64715149e-05\n",
      " 4.62303879e-05 4.61330695e-05 4.56124615e-05 4.54655466e-05\n",
      " 4.51848254e-05 4.49318645e-05 4.46126596e-05 4.45087451e-05\n",
      " 4.42137212e-05 4.39997517e-05 4.38578841e-05 4.38426556e-05\n",
      " 4.35900120e-05 4.33398633e-05 4.32133425e-05 4.28496460e-05\n",
      " 4.25183418e-05 4.20602112e-05 4.20165359e-05 4.18022088e-05\n",
      " 4.15898660e-05 4.13200124e-05 4.12010309e-05 4.09327435e-05\n",
      " 4.07915565e-05 4.05282068e-05 4.03950293e-05 4.02111823e-05\n",
      " 3.99254455e-05 3.94789382e-05 3.91682618e-05 3.90980923e-05\n",
      " 3.87858757e-05 3.86807672e-05 3.86104737e-05 3.83863836e-05\n",
      " 3.82380266e-05 3.81691809e-05 3.78956846e-05 3.78458602e-05\n",
      " 3.73685209e-05 3.72614309e-05 3.71831914e-05 3.68846921e-05\n",
      " 3.68139111e-05 3.65845949e-05 3.64337286e-05 3.62729321e-05\n",
      " 3.61203324e-05 3.60475412e-05 3.59770747e-05 3.57071288e-05\n",
      " 3.55154253e-05 3.53231998e-05 3.51847182e-05 3.50234429e-05\n",
      " 3.48745091e-05 3.47184714e-05 3.44403720e-05 3.43981157e-05\n",
      " 3.41549871e-05 3.38625734e-05 3.38073815e-05 3.35910008e-05\n",
      " 3.34865527e-05 3.33292634e-05 3.32443759e-05 3.30317908e-05\n",
      " 3.28898107e-05 3.27207395e-05 3.25144794e-05 3.24622871e-05\n",
      " 3.20627632e-05 3.18622110e-05 3.16939762e-05 3.16193160e-05\n",
      " 3.15025005e-05 3.13941328e-05 3.11968109e-05 3.11180696e-05\n",
      " 3.09908825e-05 3.09334900e-05 3.08619274e-05 3.07071589e-05\n",
      " 3.04167122e-05 3.03360183e-05 3.01517791e-05 3.00516169e-05\n",
      " 3.00150886e-05 2.97934357e-05 2.95729421e-05 2.94034296e-05\n",
      " 2.93514565e-05 2.91624324e-05 2.91288460e-05 2.89285187e-05\n",
      " 2.88194011e-05 2.86651315e-05 2.84403319e-05 2.84308198e-05\n",
      " 2.81957871e-05 2.81555901e-05 2.78999614e-05 2.78283181e-05\n",
      " 2.77866070e-05 2.76246279e-05 2.74874558e-05 2.73835297e-05\n",
      " 2.73127690e-05 2.71916935e-05 2.70395755e-05 2.69787883e-05\n",
      " 2.67696297e-05 2.67117469e-05 2.64688172e-05 2.63639452e-05\n",
      " 2.62415026e-05 2.61439968e-05 2.60327679e-05 2.58617123e-05\n",
      " 2.56754167e-05 2.55792664e-05 2.54580814e-05 2.53715816e-05\n",
      " 2.52621640e-05 2.51582841e-05 2.50617849e-05 2.49706877e-05\n",
      " 2.47811617e-05 2.47220877e-05 2.45388349e-05 2.44051180e-05\n",
      " 2.43382740e-05 2.42058378e-05 2.40538005e-05 2.38819921e-05\n",
      " 2.38490302e-05 2.37758366e-05 2.35808451e-05 2.34718761e-05\n",
      " 2.32354604e-05 2.31427033e-05 2.30864011e-05 2.29420055e-05\n",
      " 2.27281802e-05 2.26396788e-05 2.25546197e-05 2.24537437e-05\n",
      " 2.23366412e-05 2.21452997e-05 2.20075276e-05 2.18686841e-05\n",
      " 2.17862020e-05 2.17186802e-05 2.16624890e-05 2.15043387e-05\n",
      " 2.14733134e-05 2.13002937e-05 2.11618439e-05 2.10344780e-05\n",
      " 2.08767647e-05 2.08188905e-05 2.07655806e-05 2.06510941e-05\n",
      " 2.04607086e-05 2.04426795e-05 2.02519985e-05 2.00768776e-05\n",
      " 1.99967851e-05 1.98391972e-05 1.97936227e-05 1.96812387e-05\n",
      " 1.96499236e-05 1.94883166e-05 1.93785602e-05 1.92496888e-05\n",
      " 1.92329893e-05 1.91418979e-05 1.89142429e-05 1.88417257e-05\n",
      " 1.88315359e-05 1.85774488e-05 1.85516237e-05 1.84346496e-05\n",
      " 1.83046634e-05 1.81900226e-05 1.80840271e-05 1.80159559e-05]\n",
      "Gesamte erklärte Varianz: 0.99547169052299\n",
      "✅ Reduzierter Chunk 1 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_1.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183992, 1024)\n",
      "Gesamte Datenform vor PCA: (183992, 1024)\n",
      "Gesamte Datenform nach PCA: (183992, 512)\n",
      "Erklärte Varianz pro Komponente: [1.31722005e-01 9.60018661e-02 7.85979188e-02 5.09952533e-02\n",
      " 4.79842820e-02 4.35047058e-02 4.12788401e-02 3.66105572e-02\n",
      " 3.44090390e-02 2.98924303e-02 2.75794466e-02 2.51826362e-02\n",
      " 2.12486970e-02 1.90877161e-02 1.80849450e-02 1.76061757e-02\n",
      " 1.67440072e-02 1.39912520e-02 1.23939912e-02 1.13875126e-02\n",
      " 1.05711129e-02 9.65205067e-03 8.80799389e-03 8.49272866e-03\n",
      " 8.19668311e-03 7.20923482e-03 6.58322364e-03 6.41539261e-03\n",
      " 6.04578024e-03 5.60221690e-03 5.28152484e-03 4.62801708e-03\n",
      " 4.55024662e-03 4.38804057e-03 4.19476892e-03 3.85519197e-03\n",
      " 3.73787937e-03 3.53317285e-03 3.33976381e-03 3.20292987e-03\n",
      " 3.00898898e-03 2.78414841e-03 2.62848759e-03 2.52167821e-03\n",
      " 2.39747066e-03 2.30558837e-03 2.22657317e-03 2.15544279e-03\n",
      " 2.10896169e-03 1.98555302e-03 1.88070881e-03 1.77088251e-03\n",
      " 1.68845777e-03 1.66988945e-03 1.61185249e-03 1.50058201e-03\n",
      " 1.45163859e-03 1.39221426e-03 1.36971946e-03 1.28925953e-03\n",
      " 1.21483617e-03 1.17752432e-03 1.14372669e-03 1.09601025e-03\n",
      " 1.07281334e-03 1.00703668e-03 9.90402872e-04 9.68698770e-04\n",
      " 9.49578342e-04 9.20047986e-04 9.03170537e-04 8.69160762e-04\n",
      " 8.38951868e-04 8.01951516e-04 7.70669106e-04 7.65453046e-04\n",
      " 7.39829786e-04 7.24605635e-04 6.82238350e-04 6.72076945e-04\n",
      " 6.59288174e-04 6.32387257e-04 6.30039721e-04 5.89823703e-04\n",
      " 5.85539907e-04 5.69168024e-04 5.56137865e-04 5.29782522e-04\n",
      " 5.28011684e-04 5.16387403e-04 4.95122385e-04 4.84736535e-04\n",
      " 4.70804774e-04 4.55200424e-04 4.52503813e-04 4.44619089e-04\n",
      " 4.31345608e-04 4.19552268e-04 4.17257679e-04 4.09857203e-04\n",
      " 3.98316025e-04 3.88569073e-04 3.81314890e-04 3.76754371e-04\n",
      " 3.63474675e-04 3.59963500e-04 3.57095896e-04 3.48985852e-04\n",
      " 3.39528630e-04 3.37176905e-04 3.32936479e-04 3.25423090e-04\n",
      " 3.16279401e-04 3.14319223e-04 3.07968322e-04 2.95542255e-04\n",
      " 2.90718018e-04 2.86773653e-04 2.81739567e-04 2.77311413e-04\n",
      " 2.74442060e-04 2.73458644e-04 2.64679454e-04 2.59700408e-04\n",
      " 2.53566264e-04 2.52126223e-04 2.47966920e-04 2.45803315e-04\n",
      " 2.40699754e-04 2.37414935e-04 2.33130472e-04 2.29814853e-04\n",
      " 2.27896986e-04 2.24113209e-04 2.21802713e-04 2.16848782e-04\n",
      " 2.13422551e-04 2.10196111e-04 2.06926095e-04 2.04337356e-04\n",
      " 2.02483853e-04 1.99206723e-04 1.97369495e-04 1.94933540e-04\n",
      " 1.92604616e-04 1.88907831e-04 1.86794927e-04 1.83941654e-04\n",
      " 1.79313203e-04 1.77436818e-04 1.75727248e-04 1.75141998e-04\n",
      " 1.73820753e-04 1.70842285e-04 1.67801973e-04 1.66283895e-04\n",
      " 1.64277344e-04 1.62914858e-04 1.60939591e-04 1.59686450e-04\n",
      " 1.57995169e-04 1.54884635e-04 1.54240144e-04 1.50448079e-04\n",
      " 1.49816962e-04 1.49437489e-04 1.47011018e-04 1.46160612e-04\n",
      " 1.44978062e-04 1.44136347e-04 1.42882124e-04 1.40098808e-04\n",
      " 1.37906209e-04 1.36631832e-04 1.34966921e-04 1.34503597e-04\n",
      " 1.33507625e-04 1.32639850e-04 1.30833401e-04 1.28864176e-04\n",
      " 1.28309302e-04 1.26373721e-04 1.24217723e-04 1.22945027e-04\n",
      " 1.22159055e-04 1.20338010e-04 1.19230091e-04 1.18740237e-04\n",
      " 1.18231494e-04 1.17390481e-04 1.16197998e-04 1.15698613e-04\n",
      " 1.15331478e-04 1.14362681e-04 1.13199917e-04 1.11254184e-04\n",
      " 1.09819991e-04 1.08968492e-04 1.08254376e-04 1.07350852e-04\n",
      " 1.06711920e-04 1.05984844e-04 1.05165804e-04 1.04765244e-04\n",
      " 1.02913352e-04 1.01850484e-04 1.00721801e-04 1.00373289e-04\n",
      " 9.90024123e-05 9.79709774e-05 9.71834856e-05 9.64495211e-05\n",
      " 9.50265156e-05 9.44129112e-05 9.42746358e-05 9.36541829e-05\n",
      " 9.29986476e-05 9.24654983e-05 9.15600634e-05 9.12030171e-05\n",
      " 8.99043485e-05 8.93156288e-05 8.90428877e-05 8.84933825e-05\n",
      " 8.74642381e-05 8.67697241e-05 8.64700553e-05 8.57647967e-05\n",
      " 8.50444831e-05 8.45073225e-05 8.42430642e-05 8.34252205e-05\n",
      " 8.27853618e-05 8.19381330e-05 8.14505402e-05 8.10559714e-05\n",
      " 8.04431670e-05 7.99675504e-05 7.95238050e-05 7.87499180e-05\n",
      " 7.85938653e-05 7.79017281e-05 7.71362607e-05 7.69380066e-05\n",
      " 7.62619662e-05 7.56415824e-05 7.54568813e-05 7.47311234e-05\n",
      " 7.38789972e-05 7.35449594e-05 7.30716506e-05 7.27778749e-05\n",
      " 7.23987985e-05 7.20323716e-05 7.11548888e-05 7.10263680e-05\n",
      " 7.01238739e-05 6.98153193e-05 6.89442073e-05 6.82990771e-05\n",
      " 6.80100722e-05 6.70534639e-05 6.69130417e-05 6.64731177e-05\n",
      " 6.58563596e-05 6.56786796e-05 6.50792383e-05 6.46396768e-05\n",
      " 6.42747807e-05 6.40290478e-05 6.38386147e-05 6.31305592e-05\n",
      " 6.28888030e-05 6.22994387e-05 6.20709132e-05 6.16062772e-05\n",
      " 6.12248009e-05 6.08125987e-05 6.04913716e-05 6.02053306e-05\n",
      " 5.96293410e-05 5.94821798e-05 5.90325700e-05 5.85901138e-05\n",
      " 5.81737507e-05 5.80825569e-05 5.73328812e-05 5.69030226e-05\n",
      " 5.63644578e-05 5.60831935e-05 5.60608007e-05 5.56293193e-05\n",
      " 5.54646974e-05 5.50643159e-05 5.49255973e-05 5.44841424e-05\n",
      " 5.41851008e-05 5.40826215e-05 5.38290445e-05 5.33937589e-05\n",
      " 5.29184704e-05 5.27766325e-05 5.22585785e-05 5.18228729e-05\n",
      " 5.16158021e-05 5.14022741e-05 5.10512994e-05 5.09599790e-05\n",
      " 5.07593480e-05 5.05490774e-05 5.03174327e-05 5.00118765e-05\n",
      " 4.97442630e-05 4.92432036e-05 4.88513510e-05 4.84681483e-05\n",
      " 4.82182200e-05 4.80720256e-05 4.77907526e-05 4.76549834e-05\n",
      " 4.74505081e-05 4.68893232e-05 4.66289467e-05 4.63363191e-05\n",
      " 4.61714526e-05 4.61025019e-05 4.55100961e-05 4.52585074e-05\n",
      " 4.52015041e-05 4.49378616e-05 4.46011880e-05 4.43391569e-05\n",
      " 4.41482203e-05 4.39798691e-05 4.36796766e-05 4.35153079e-05\n",
      " 4.32380922e-05 4.31229029e-05 4.28016959e-05 4.25262729e-05\n",
      " 4.23484749e-05 4.20992746e-05 4.20710664e-05 4.18164276e-05\n",
      " 4.15261538e-05 4.12641055e-05 4.10696525e-05 4.10037088e-05\n",
      " 4.07541890e-05 4.04887164e-05 4.04395600e-05 3.99937170e-05\n",
      " 3.97601041e-05 3.95124058e-05 3.94605791e-05 3.92748852e-05\n",
      " 3.90462993e-05 3.88063703e-05 3.86648864e-05 3.85036110e-05\n",
      " 3.84543050e-05 3.82073319e-05 3.79293220e-05 3.76123852e-05\n",
      " 3.74794360e-05 3.71594059e-05 3.69817288e-05 3.67834890e-05\n",
      " 3.65785246e-05 3.63443649e-05 3.62744445e-05 3.60760983e-05\n",
      " 3.60466959e-05 3.57551532e-05 3.56275503e-05 3.55273240e-05\n",
      " 3.53941734e-05 3.52473057e-05 3.50148150e-05 3.48248279e-05\n",
      " 3.46381586e-05 3.44164384e-05 3.43899941e-05 3.41605392e-05\n",
      " 3.40686203e-05 3.40086733e-05 3.37357019e-05 3.36270129e-05\n",
      " 3.35653164e-05 3.32936657e-05 3.31770233e-05 3.28973704e-05\n",
      " 3.25902028e-05 3.24642315e-05 3.23842110e-05 3.21888487e-05\n",
      " 3.21113346e-05 3.20486309e-05 3.18379978e-05 3.17873882e-05\n",
      " 3.15281627e-05 3.15168109e-05 3.12994156e-05 3.10790047e-05\n",
      " 3.10480342e-05 3.09734033e-05 3.07302169e-05 3.05054178e-05\n",
      " 3.04120170e-05 3.03189844e-05 3.00442434e-05 2.99712987e-05\n",
      " 2.98773569e-05 2.97169994e-05 2.95172001e-05 2.94282852e-05\n",
      " 2.92718210e-05 2.90852323e-05 2.88989255e-05 2.87959742e-05\n",
      " 2.86780024e-05 2.85252934e-05 2.84545413e-05 2.83523929e-05\n",
      " 2.80806876e-05 2.80612731e-05 2.79371002e-05 2.78155171e-05\n",
      " 2.77053462e-05 2.74845727e-05 2.73763096e-05 2.73315558e-05\n",
      " 2.72578083e-05 2.70741430e-05 2.69743196e-05 2.68574326e-05\n",
      " 2.65961525e-05 2.65498449e-05 2.64654074e-05 2.62459864e-05\n",
      " 2.62234210e-05 2.60217714e-05 2.59959832e-05 2.58204067e-05\n",
      " 2.57504028e-05 2.56581443e-05 2.56238071e-05 2.55204731e-05\n",
      " 2.52697275e-05 2.52031795e-05 2.50194942e-05 2.48919172e-05\n",
      " 2.46821646e-05 2.46542960e-05 2.45239652e-05 2.43450191e-05\n",
      " 2.41158923e-05 2.40760273e-05 2.40180658e-05 2.38691378e-05\n",
      " 2.36330747e-05 2.35323348e-05 2.35063567e-05 2.33301888e-05\n",
      " 2.32709123e-05 2.31101045e-05 2.30593137e-05 2.27551387e-05\n",
      " 2.27379183e-05 2.25297080e-05 2.24072314e-05 2.23994334e-05\n",
      " 2.23073360e-05 2.20781904e-05 2.19592100e-05 2.18467241e-05\n",
      " 2.17838766e-05 2.16774004e-05 2.15666899e-05 2.14828179e-05\n",
      " 2.14072101e-05 2.13621830e-05 2.12839926e-05 2.10735119e-05\n",
      " 2.08497070e-05 2.07987450e-05 2.07284275e-05 2.06382768e-05\n",
      " 2.05588246e-05 2.04639864e-05 2.03764571e-05 2.02634648e-05\n",
      " 2.02241294e-05 2.01186229e-05 1.99996928e-05 1.98693664e-05\n",
      " 1.97008412e-05 1.96180310e-05 1.95285392e-05 1.94278828e-05\n",
      " 1.93094016e-05 1.90723803e-05 1.90636600e-05 1.89535092e-05\n",
      " 1.89267787e-05 1.87828331e-05 1.85375936e-05 1.84507002e-05\n",
      " 1.83277244e-05 1.81662836e-05 1.81267424e-05 1.80443897e-05]\n",
      "Gesamte erklärte Varianz: 0.9954788024792924\n",
      "✅ Reduzierter Chunk 2 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_2.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183726, 1024)\n",
      "Gesamte Datenform vor PCA: (183726, 1024)\n",
      "Gesamte Datenform nach PCA: (183726, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32405806e-01 9.63381820e-02 7.84095011e-02 5.14826679e-02\n",
      " 4.79844361e-02 4.33826356e-02 4.11837117e-02 3.67562801e-02\n",
      " 3.42264615e-02 2.97791585e-02 2.78092476e-02 2.53811984e-02\n",
      " 2.11469737e-02 1.87715211e-02 1.80004718e-02 1.73718024e-02\n",
      " 1.67193293e-02 1.39215149e-02 1.23034026e-02 1.13663863e-02\n",
      " 1.04044598e-02 9.62449779e-03 8.75348407e-03 8.43343052e-03\n",
      " 8.25480484e-03 7.25266778e-03 6.60698157e-03 6.47785542e-03\n",
      " 6.05368493e-03 5.60749030e-03 5.23709090e-03 4.62919126e-03\n",
      " 4.60949729e-03 4.33824922e-03 4.29524831e-03 3.80967721e-03\n",
      " 3.71536659e-03 3.51946212e-03 3.34636197e-03 3.17794767e-03\n",
      " 3.00048523e-03 2.78117607e-03 2.65683796e-03 2.52581034e-03\n",
      " 2.37476204e-03 2.28114235e-03 2.20872837e-03 2.18596231e-03\n",
      " 2.07500118e-03 1.98931499e-03 1.90462536e-03 1.78651294e-03\n",
      " 1.68912542e-03 1.64238145e-03 1.59691621e-03 1.47664380e-03\n",
      " 1.43269919e-03 1.39785778e-03 1.36962984e-03 1.29408186e-03\n",
      " 1.22644021e-03 1.17269463e-03 1.14311718e-03 1.09748859e-03\n",
      " 1.08178411e-03 1.01032770e-03 9.84796283e-04 9.66436857e-04\n",
      " 9.49052188e-04 9.15955365e-04 8.96486102e-04 8.57070546e-04\n",
      " 8.30045025e-04 7.98973940e-04 7.70783747e-04 7.52968336e-04\n",
      " 7.39681156e-04 7.28513955e-04 6.82103712e-04 6.70375588e-04\n",
      " 6.63891654e-04 6.35839867e-04 6.28715551e-04 5.87643463e-04\n",
      " 5.84176147e-04 5.68514949e-04 5.46674256e-04 5.29530827e-04\n",
      " 5.22226850e-04 5.10910896e-04 4.96283771e-04 4.83745121e-04\n",
      " 4.70332839e-04 4.54945958e-04 4.48932772e-04 4.45796319e-04\n",
      " 4.31287274e-04 4.16589792e-04 4.15407749e-04 4.09108696e-04\n",
      " 3.98405267e-04 3.87279506e-04 3.82550619e-04 3.72482607e-04\n",
      " 3.65669104e-04 3.60480489e-04 3.54442287e-04 3.47214351e-04\n",
      " 3.37084943e-04 3.35687738e-04 3.29272338e-04 3.24105711e-04\n",
      " 3.18631991e-04 3.14428203e-04 3.09316597e-04 2.96731634e-04\n",
      " 2.88589390e-04 2.84861431e-04 2.80984579e-04 2.75347413e-04\n",
      " 2.72397573e-04 2.70930865e-04 2.63970663e-04 2.57360150e-04\n",
      " 2.53065400e-04 2.49733187e-04 2.46232217e-04 2.42909567e-04\n",
      " 2.40119962e-04 2.35829439e-04 2.31649696e-04 2.29846719e-04\n",
      " 2.26397813e-04 2.24332345e-04 2.19714886e-04 2.16568709e-04\n",
      " 2.11188168e-04 2.09353754e-04 2.03931682e-04 2.03360086e-04\n",
      " 2.02843026e-04 1.98573939e-04 1.95676856e-04 1.93966666e-04\n",
      " 1.90973470e-04 1.88363455e-04 1.84496455e-04 1.83084224e-04\n",
      " 1.79449236e-04 1.76851752e-04 1.75257944e-04 1.74252132e-04\n",
      " 1.73295299e-04 1.68934049e-04 1.65058421e-04 1.63924365e-04\n",
      " 1.62023717e-04 1.61831526e-04 1.59938535e-04 1.59117984e-04\n",
      " 1.57360211e-04 1.55286312e-04 1.53406187e-04 1.50652973e-04\n",
      " 1.49575312e-04 1.47717384e-04 1.46053854e-04 1.45464207e-04\n",
      " 1.43656784e-04 1.43418430e-04 1.42460536e-04 1.41110821e-04\n",
      " 1.38747602e-04 1.38183306e-04 1.34858519e-04 1.34330418e-04\n",
      " 1.32837031e-04 1.30622740e-04 1.29481258e-04 1.28624512e-04\n",
      " 1.26799534e-04 1.26021946e-04 1.24311075e-04 1.22333878e-04\n",
      " 1.22106092e-04 1.20877518e-04 1.19480336e-04 1.18397062e-04\n",
      " 1.17637757e-04 1.17074084e-04 1.16288215e-04 1.15441644e-04\n",
      " 1.14370301e-04 1.12632520e-04 1.11948689e-04 1.11025303e-04\n",
      " 1.10194739e-04 1.08200207e-04 1.07934978e-04 1.06165414e-04\n",
      " 1.05270312e-04 1.05174049e-04 1.03524135e-04 1.02818880e-04\n",
      " 1.02742874e-04 1.01656795e-04 1.01358789e-04 1.00742822e-04\n",
      " 9.89755092e-05 9.71188981e-05 9.65825394e-05 9.63411196e-05\n",
      " 9.54364438e-05 9.42937668e-05 9.37436876e-05 9.31224651e-05\n",
      " 9.24120880e-05 9.20731176e-05 9.17768158e-05 9.08864553e-05\n",
      " 8.98242708e-05 8.90065204e-05 8.83480457e-05 8.77037246e-05\n",
      " 8.73493764e-05 8.63512440e-05 8.54564024e-05 8.53350371e-05\n",
      " 8.49452268e-05 8.43077140e-05 8.36934730e-05 8.30695132e-05\n",
      " 8.25910509e-05 8.18698984e-05 8.17226744e-05 8.02478121e-05\n",
      " 7.97478567e-05 7.95566762e-05 7.90347078e-05 7.85191262e-05\n",
      " 7.78687764e-05 7.75201508e-05 7.65591782e-05 7.62233433e-05\n",
      " 7.55956820e-05 7.55733458e-05 7.51899800e-05 7.43259460e-05\n",
      " 7.42710004e-05 7.37517519e-05 7.30802151e-05 7.22789859e-05\n",
      " 7.22275397e-05 7.13715902e-05 7.06895493e-05 7.02897778e-05\n",
      " 7.00143570e-05 6.92317453e-05 6.86977080e-05 6.81471090e-05\n",
      " 6.76288769e-05 6.75406522e-05 6.67531666e-05 6.61253436e-05\n",
      " 6.56745937e-05 6.55304187e-05 6.47097059e-05 6.44692332e-05\n",
      " 6.41683695e-05 6.37745170e-05 6.36948098e-05 6.32311594e-05\n",
      " 6.27723886e-05 6.23852173e-05 6.16820759e-05 6.15265826e-05\n",
      " 6.11341622e-05 6.09332399e-05 6.04805440e-05 5.96212913e-05\n",
      " 5.94691358e-05 5.87257856e-05 5.86223448e-05 5.83522945e-05\n",
      " 5.77574446e-05 5.77105604e-05 5.70024066e-05 5.68118556e-05\n",
      " 5.65030517e-05 5.63161098e-05 5.58309778e-05 5.50956371e-05\n",
      " 5.50396174e-05 5.46145646e-05 5.44576854e-05 5.40351330e-05\n",
      " 5.38555884e-05 5.34556610e-05 5.31262360e-05 5.30149705e-05\n",
      " 5.27021706e-05 5.21999746e-05 5.18767631e-05 5.16636852e-05\n",
      " 5.15669371e-05 5.13666038e-05 5.10465915e-05 5.05025121e-05\n",
      " 5.02988584e-05 4.98706065e-05 4.97675063e-05 4.96108754e-05\n",
      " 4.93828874e-05 4.91154010e-05 4.89806444e-05 4.84795398e-05\n",
      " 4.83567626e-05 4.77215048e-05 4.74605755e-05 4.73483370e-05\n",
      " 4.70080297e-05 4.68993108e-05 4.62040749e-05 4.60907941e-05\n",
      " 4.59648351e-05 4.57697134e-05 4.51880932e-05 4.50588513e-05\n",
      " 4.48173911e-05 4.47177875e-05 4.46153168e-05 4.43918619e-05\n",
      " 4.41415002e-05 4.40206171e-05 4.36216078e-05 4.33975899e-05\n",
      " 4.30600718e-05 4.28954055e-05 4.25690525e-05 4.22764985e-05\n",
      " 4.21679586e-05 4.19325965e-05 4.18235658e-05 4.16650754e-05\n",
      " 4.14830246e-05 4.12656678e-05 4.08957684e-05 4.07254285e-05\n",
      " 4.05026060e-05 4.03733958e-05 4.02045833e-05 3.99881446e-05\n",
      " 3.97993981e-05 3.96133859e-05 3.95363755e-05 3.91441659e-05\n",
      " 3.87290916e-05 3.85481611e-05 3.82407547e-05 3.81260736e-05\n",
      " 3.78526364e-05 3.77623300e-05 3.74971996e-05 3.73214171e-05\n",
      " 3.72490524e-05 3.70659766e-05 3.68856553e-05 3.67631091e-05\n",
      " 3.67084754e-05 3.65358920e-05 3.61912827e-05 3.61555955e-05\n",
      " 3.59720144e-05 3.56936053e-05 3.55185244e-05 3.54351417e-05\n",
      " 3.52628500e-05 3.50431949e-05 3.47279465e-05 3.45426042e-05\n",
      " 3.44824268e-05 3.43241617e-05 3.41399800e-05 3.39846224e-05\n",
      " 3.39245432e-05 3.37349132e-05 3.37219318e-05 3.35517075e-05\n",
      " 3.32461749e-05 3.31843865e-05 3.30002308e-05 3.28901491e-05\n",
      " 3.27787739e-05 3.26068893e-05 3.24538704e-05 3.21898055e-05\n",
      " 3.18665680e-05 3.17575691e-05 3.15670036e-05 3.14857257e-05\n",
      " 3.14153302e-05 3.12370473e-05 3.10669759e-05 3.08712479e-05\n",
      " 3.07792293e-05 3.07104535e-05 3.05246434e-05 3.03682030e-05\n",
      " 3.02880252e-05 3.02228644e-05 3.01103487e-05 2.97515713e-05\n",
      " 2.96838378e-05 2.95671557e-05 2.93867074e-05 2.91641100e-05\n",
      " 2.91467255e-05 2.90581688e-05 2.89354926e-05 2.87259199e-05\n",
      " 2.85086064e-05 2.82814095e-05 2.82427484e-05 2.81190502e-05\n",
      " 2.80813996e-05 2.79608774e-05 2.77879159e-05 2.77625508e-05\n",
      " 2.75765415e-05 2.73851358e-05 2.73425999e-05 2.72502897e-05\n",
      " 2.71123772e-05 2.70255990e-05 2.68843459e-05 2.67309546e-05\n",
      " 2.65482223e-05 2.63954200e-05 2.63419816e-05 2.62594218e-05\n",
      " 2.61242956e-05 2.60543678e-05 2.59500896e-05 2.57943913e-05\n",
      " 2.57112482e-05 2.53800013e-05 2.53261327e-05 2.51610160e-05\n",
      " 2.51001543e-05 2.50131047e-05 2.48981811e-05 2.48249675e-05\n",
      " 2.46217498e-05 2.45498210e-05 2.45092341e-05 2.43498140e-05\n",
      " 2.42421057e-05 2.41275834e-05 2.40444389e-05 2.39788983e-05\n",
      " 2.38662672e-05 2.35728989e-05 2.35135343e-05 2.33221387e-05\n",
      " 2.31237226e-05 2.30027817e-05 2.29050419e-05 2.28208175e-05\n",
      " 2.27310727e-05 2.24926644e-05 2.24291998e-05 2.22571203e-05\n",
      " 2.20814476e-05 2.20051042e-05 2.19927522e-05 2.18600687e-05\n",
      " 2.17028184e-05 2.15739490e-05 2.14567631e-05 2.13619698e-05\n",
      " 2.11912401e-05 2.11623670e-05 2.10253640e-05 2.09098253e-05\n",
      " 2.07939430e-05 2.06880249e-05 2.05711508e-05 2.04568724e-05\n",
      " 2.04147350e-05 2.03562452e-05 2.01355160e-05 2.00864432e-05\n",
      " 1.99627926e-05 1.98570188e-05 1.97516622e-05 1.96150288e-05\n",
      " 1.95591362e-05 1.95326221e-05 1.93454145e-05 1.91993338e-05\n",
      " 1.91226324e-05 1.89579213e-05 1.89095473e-05 1.88058364e-05\n",
      " 1.86790979e-05 1.84757574e-05 1.84191849e-05 1.83744417e-05\n",
      " 1.82227148e-05 1.81409418e-05 1.79724801e-05 1.79466401e-05]\n",
      "Gesamte erklärte Varianz: 0.9954871730816074\n",
      "✅ Reduzierter Chunk 3 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_3.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183396, 1024)\n",
      "Gesamte Datenform vor PCA: (183396, 1024)\n",
      "Gesamte Datenform nach PCA: (183396, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32701745e-01 9.63894465e-02 7.92096142e-02 5.12135132e-02\n",
      " 4.81423486e-02 4.34124626e-02 4.11329077e-02 3.67094262e-02\n",
      " 3.43188180e-02 2.96224483e-02 2.77444148e-02 2.51777114e-02\n",
      " 2.08857510e-02 1.88560193e-02 1.81359810e-02 1.75355663e-02\n",
      " 1.66431454e-02 1.39866960e-02 1.22468843e-02 1.13198978e-02\n",
      " 1.04212582e-02 9.61556769e-03 8.81597350e-03 8.41628320e-03\n",
      " 8.19018804e-03 7.29229751e-03 6.66446883e-03 6.46443641e-03\n",
      " 6.00571929e-03 5.58800600e-03 5.21883015e-03 4.63188146e-03\n",
      " 4.53189417e-03 4.38970382e-03 4.19726210e-03 3.82726413e-03\n",
      " 3.68854503e-03 3.51660004e-03 3.33412412e-03 3.18273185e-03\n",
      " 2.95943824e-03 2.79099750e-03 2.61272947e-03 2.53436231e-03\n",
      " 2.39431215e-03 2.28392428e-03 2.21620857e-03 2.16389118e-03\n",
      " 2.06231605e-03 1.98141193e-03 1.88226381e-03 1.77033945e-03\n",
      " 1.67816368e-03 1.64581998e-03 1.60268114e-03 1.46030685e-03\n",
      " 1.40876374e-03 1.40005827e-03 1.36865191e-03 1.29448300e-03\n",
      " 1.23447674e-03 1.16823532e-03 1.14959863e-03 1.09004542e-03\n",
      " 1.07420462e-03 1.00206354e-03 9.93056223e-04 9.70774178e-04\n",
      " 9.50171673e-04 9.11410090e-04 8.93660676e-04 8.55492221e-04\n",
      " 8.34377890e-04 8.01149134e-04 7.70526410e-04 7.50920902e-04\n",
      " 7.30626733e-04 7.15271847e-04 6.75246090e-04 6.66854517e-04\n",
      " 6.55852274e-04 6.35327640e-04 6.27552463e-04 5.87307969e-04\n",
      " 5.83435955e-04 5.71125967e-04 5.48932410e-04 5.29964805e-04\n",
      " 5.23143888e-04 5.13465613e-04 4.95257124e-04 4.69123878e-04\n",
      " 4.56791688e-04 4.53275476e-04 4.49567410e-04 4.41490663e-04\n",
      " 4.28616101e-04 4.20075180e-04 4.14071040e-04 4.08178750e-04\n",
      " 3.97592475e-04 3.87767495e-04 3.81836106e-04 3.67679456e-04\n",
      " 3.65018379e-04 3.57756602e-04 3.54769898e-04 3.44513955e-04\n",
      " 3.38060752e-04 3.34788053e-04 3.28815086e-04 3.19883065e-04\n",
      " 3.17803156e-04 3.11656135e-04 3.02519490e-04 2.92889353e-04\n",
      " 2.88232311e-04 2.83370948e-04 2.81101643e-04 2.77114744e-04\n",
      " 2.74574253e-04 2.69612791e-04 2.63730003e-04 2.55192418e-04\n",
      " 2.52723820e-04 2.50537950e-04 2.46571098e-04 2.41556748e-04\n",
      " 2.39186302e-04 2.31938547e-04 2.30175921e-04 2.27442349e-04\n",
      " 2.24811215e-04 2.22737912e-04 2.17230718e-04 2.14920414e-04\n",
      " 2.09392710e-04 2.07386988e-04 2.04949470e-04 2.02412780e-04\n",
      " 2.00742442e-04 1.96133005e-04 1.93577658e-04 1.93314793e-04\n",
      " 1.88832628e-04 1.87896967e-04 1.86022667e-04 1.80462736e-04\n",
      " 1.78406073e-04 1.75932538e-04 1.75675745e-04 1.72396707e-04\n",
      " 1.72004773e-04 1.68200816e-04 1.65484278e-04 1.64055704e-04\n",
      " 1.63036435e-04 1.60957082e-04 1.60083221e-04 1.57163450e-04\n",
      " 1.55961667e-04 1.54213886e-04 1.53705642e-04 1.50018968e-04\n",
      " 1.49213141e-04 1.47440341e-04 1.45379089e-04 1.43988246e-04\n",
      " 1.43421586e-04 1.42035297e-04 1.41141515e-04 1.39859670e-04\n",
      " 1.38993700e-04 1.35447162e-04 1.34468984e-04 1.32774079e-04\n",
      " 1.31461978e-04 1.30573550e-04 1.28935346e-04 1.27720596e-04\n",
      " 1.27066434e-04 1.25811520e-04 1.23557696e-04 1.23251434e-04\n",
      " 1.21026453e-04 1.19751920e-04 1.19097862e-04 1.18321886e-04\n",
      " 1.16928157e-04 1.16015325e-04 1.15097532e-04 1.14703142e-04\n",
      " 1.13047880e-04 1.12802525e-04 1.11603708e-04 1.09868338e-04\n",
      " 1.09258304e-04 1.08765796e-04 1.07206230e-04 1.06967792e-04\n",
      " 1.06012049e-04 1.04507248e-04 1.03916404e-04 1.02281781e-04\n",
      " 1.02005299e-04 1.01284865e-04 1.01069314e-04 1.00303859e-04\n",
      " 9.86105309e-05 9.69680049e-05 9.53656666e-05 9.48316620e-05\n",
      " 9.44249589e-05 9.38328513e-05 9.29870505e-05 9.22519395e-05\n",
      " 9.16576070e-05 9.08136139e-05 9.01747424e-05 8.93357670e-05\n",
      " 8.90701552e-05 8.86276100e-05 8.75451735e-05 8.73772035e-05\n",
      " 8.69117369e-05 8.65724362e-05 8.55679589e-05 8.47898849e-05\n",
      " 8.42827358e-05 8.34413384e-05 8.25595622e-05 8.24987998e-05\n",
      " 8.23032847e-05 8.13400612e-05 8.06042492e-05 7.99886119e-05\n",
      " 7.94500590e-05 7.87691979e-05 7.82269657e-05 7.78339655e-05\n",
      " 7.73900065e-05 7.68156346e-05 7.59377172e-05 7.53102948e-05\n",
      " 7.51895464e-05 7.47869919e-05 7.44170637e-05 7.35488398e-05\n",
      " 7.30256237e-05 7.29068047e-05 7.20048999e-05 7.14904561e-05\n",
      " 7.10344976e-05 7.09171271e-05 7.02843799e-05 6.95175812e-05\n",
      " 6.94137631e-05 6.85582282e-05 6.81535066e-05 6.76399029e-05\n",
      " 6.69661685e-05 6.68943626e-05 6.59273266e-05 6.56336948e-05\n",
      " 6.50734662e-05 6.49662470e-05 6.44287949e-05 6.42183195e-05\n",
      " 6.38352330e-05 6.35318266e-05 6.34330087e-05 6.29019301e-05\n",
      " 6.24321470e-05 6.18921860e-05 6.14647749e-05 6.12736575e-05\n",
      " 6.07020784e-05 6.04169059e-05 5.97010080e-05 5.90958233e-05\n",
      " 5.86770163e-05 5.83741777e-05 5.82660314e-05 5.79488756e-05\n",
      " 5.75969786e-05 5.72727251e-05 5.66763242e-05 5.63221095e-05\n",
      " 5.58785214e-05 5.56990153e-05 5.53846349e-05 5.49704922e-05\n",
      " 5.44702786e-05 5.42447328e-05 5.41469463e-05 5.39000378e-05\n",
      " 5.37624635e-05 5.29727839e-05 5.28683343e-05 5.26985217e-05\n",
      " 5.23951616e-05 5.21706240e-05 5.17082399e-05 5.14156799e-05\n",
      " 5.12224825e-05 5.06953844e-05 5.04418591e-05 5.01498959e-05\n",
      " 4.98106879e-05 4.97757960e-05 4.94381235e-05 4.92374633e-05\n",
      " 4.89234710e-05 4.87265712e-05 4.84883567e-05 4.79098421e-05\n",
      " 4.78218504e-05 4.75379932e-05 4.74433268e-05 4.70534612e-05\n",
      " 4.68723792e-05 4.61577705e-05 4.61271141e-05 4.59689881e-05\n",
      " 4.57763933e-05 4.55919767e-05 4.52501150e-05 4.47372443e-05\n",
      " 4.45672318e-05 4.43423002e-05 4.40622236e-05 4.39481587e-05\n",
      " 4.37834333e-05 4.35377647e-05 4.34363395e-05 4.32457291e-05\n",
      " 4.26005943e-05 4.22594250e-05 4.21472373e-05 4.21165432e-05\n",
      " 4.18377384e-05 4.17728390e-05 4.16079688e-05 4.14694587e-05\n",
      " 4.09069994e-05 4.06749614e-05 4.05761870e-05 4.03175658e-05\n",
      " 4.00930861e-05 3.99957892e-05 3.96277355e-05 3.93986293e-05\n",
      " 3.93379219e-05 3.91893996e-05 3.88242400e-05 3.86777340e-05\n",
      " 3.84436855e-05 3.81805884e-05 3.79935500e-05 3.77213505e-05\n",
      " 3.75991361e-05 3.75617934e-05 3.74224779e-05 3.72567762e-05\n",
      " 3.70965209e-05 3.69644480e-05 3.68456029e-05 3.64147183e-05\n",
      " 3.61768862e-05 3.60907400e-05 3.58821100e-05 3.57798939e-05\n",
      " 3.56338515e-05 3.53921056e-05 3.51776989e-05 3.51057453e-05\n",
      " 3.49382011e-05 3.47680756e-05 3.46775433e-05 3.45649066e-05\n",
      " 3.43794963e-05 3.41596056e-05 3.41240532e-05 3.39079721e-05\n",
      " 3.38355375e-05 3.36727184e-05 3.35412509e-05 3.33644159e-05\n",
      " 3.32192368e-05 3.28721112e-05 3.27825465e-05 3.25911452e-05\n",
      " 3.23302933e-05 3.23126996e-05 3.20268289e-05 3.20120048e-05\n",
      " 3.16671997e-05 3.15271571e-05 3.14752538e-05 3.12981870e-05\n",
      " 3.11742258e-05 3.10514263e-05 3.08567253e-05 3.06939090e-05\n",
      " 3.06227376e-05 3.05740761e-05 3.04084410e-05 3.01228398e-05\n",
      " 3.00015641e-05 2.98446491e-05 2.96163454e-05 2.95882963e-05\n",
      " 2.95271515e-05 2.94011131e-05 2.93301415e-05 2.91438883e-05\n",
      " 2.89285691e-05 2.87538576e-05 2.85678158e-05 2.84439560e-05\n",
      " 2.82935014e-05 2.82384809e-05 2.80710931e-05 2.80016483e-05\n",
      " 2.77428533e-05 2.76745818e-05 2.74944789e-05 2.74679595e-05\n",
      " 2.73227166e-05 2.70873731e-05 2.70424112e-05 2.69424200e-05\n",
      " 2.69021947e-05 2.66750816e-05 2.66093798e-05 2.64923859e-05\n",
      " 2.64650148e-05 2.63534297e-05 2.60994553e-05 2.60575236e-05\n",
      " 2.59898315e-05 2.56780845e-05 2.55547520e-05 2.54596858e-05\n",
      " 2.53722648e-05 2.53487786e-05 2.52249970e-05 2.50369534e-05\n",
      " 2.49531827e-05 2.49153127e-05 2.47980870e-05 2.47124391e-05\n",
      " 2.46030616e-05 2.45195372e-05 2.42721014e-05 2.40817141e-05\n",
      " 2.40402604e-05 2.37595986e-05 2.36645831e-05 2.36153292e-05\n",
      " 2.34547784e-05 2.33749926e-05 2.32928660e-05 2.32234038e-05\n",
      " 2.30644189e-05 2.29833150e-05 2.28197759e-05 2.26612690e-05\n",
      " 2.24450749e-05 2.23824381e-05 2.22927908e-05 2.21845257e-05\n",
      " 2.20711720e-05 2.19274255e-05 2.18672526e-05 2.17748285e-05\n",
      " 2.15741509e-05 2.14759125e-05 2.13306537e-05 2.12547717e-05\n",
      " 2.12254001e-05 2.11527613e-05 2.10689095e-05 2.07699718e-05\n",
      " 2.07066744e-05 2.05653571e-05 2.04070790e-05 2.03361321e-05\n",
      " 2.02018343e-05 2.01750671e-05 2.01035683e-05 2.00877433e-05\n",
      " 1.99548939e-05 1.99097900e-05 1.98197907e-05 1.96668678e-05\n",
      " 1.95928066e-05 1.92809234e-05 1.92366292e-05 1.90324694e-05\n",
      " 1.88765828e-05 1.87969665e-05 1.86667448e-05 1.86311243e-05\n",
      " 1.85209892e-05 1.84497091e-05 1.83070548e-05 1.81766578e-05\n",
      " 1.80714708e-05 1.79310182e-05 1.77382264e-05 1.75264444e-05]\n",
      "Gesamte erklärte Varianz: 0.9955425936638228\n",
      "✅ Reduzierter Chunk 4 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_4.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183618, 1024)\n",
      "Gesamte Datenform vor PCA: (183618, 1024)\n",
      "Gesamte Datenform nach PCA: (183618, 512)\n",
      "Erklärte Varianz pro Komponente: [1.31881628e-01 9.61627554e-02 7.87874714e-02 5.12529983e-02\n",
      " 4.79393944e-02 4.33543762e-02 4.14431788e-02 3.68011161e-02\n",
      " 3.44355900e-02 3.00895456e-02 2.77179448e-02 2.53329952e-02\n",
      " 2.14455372e-02 1.88817855e-02 1.82565642e-02 1.74776653e-02\n",
      " 1.66458806e-02 1.39368246e-02 1.23114751e-02 1.13124017e-02\n",
      " 1.03684641e-02 9.66379584e-03 8.63976715e-03 8.31622965e-03\n",
      " 8.21439938e-03 7.18549486e-03 6.56981692e-03 6.45040309e-03\n",
      " 5.99276851e-03 5.51615913e-03 5.25140228e-03 4.55232819e-03\n",
      " 4.51776011e-03 4.32666069e-03 4.22417006e-03 3.81028313e-03\n",
      " 3.67999195e-03 3.49167354e-03 3.34057053e-03 3.17927304e-03\n",
      " 2.96112795e-03 2.78777181e-03 2.65024223e-03 2.52768489e-03\n",
      " 2.38945045e-03 2.28187866e-03 2.22840244e-03 2.16612152e-03\n",
      " 2.07215953e-03 1.96783558e-03 1.87506799e-03 1.75623437e-03\n",
      " 1.69041384e-03 1.65987030e-03 1.61640814e-03 1.48725882e-03\n",
      " 1.43316564e-03 1.39917556e-03 1.37553674e-03 1.30281361e-03\n",
      " 1.21578201e-03 1.16114445e-03 1.14919411e-03 1.09586775e-03\n",
      " 1.08024013e-03 1.01090759e-03 9.90914097e-04 9.67538415e-04\n",
      " 9.33769929e-04 9.08986565e-04 8.97123202e-04 8.74212961e-04\n",
      " 8.32896767e-04 8.02778134e-04 7.79762821e-04 7.58276512e-04\n",
      " 7.42061302e-04 7.26625413e-04 6.75876521e-04 6.73394011e-04\n",
      " 6.58824282e-04 6.34216116e-04 6.26434855e-04 5.89168354e-04\n",
      " 5.86861043e-04 5.69040654e-04 5.52469091e-04 5.32717686e-04\n",
      " 5.28904776e-04 5.13444511e-04 4.98709370e-04 4.81039158e-04\n",
      " 4.71583886e-04 4.57941392e-04 4.50054828e-04 4.45015297e-04\n",
      " 4.35437501e-04 4.22214284e-04 4.18374967e-04 4.11789534e-04\n",
      " 4.03311126e-04 3.90504138e-04 3.80991298e-04 3.72469541e-04\n",
      " 3.61105228e-04 3.54671911e-04 3.53468835e-04 3.46745437e-04\n",
      " 3.40231584e-04 3.37608361e-04 3.32437212e-04 3.25637225e-04\n",
      " 3.19308565e-04 3.13297225e-04 3.07504940e-04 2.98937249e-04\n",
      " 2.90904455e-04 2.85256876e-04 2.81901882e-04 2.77743284e-04\n",
      " 2.77292223e-04 2.70173785e-04 2.66845429e-04 2.58251122e-04\n",
      " 2.53246862e-04 2.49406875e-04 2.46809921e-04 2.43695750e-04\n",
      " 2.41492081e-04 2.36330165e-04 2.32149984e-04 2.30659504e-04\n",
      " 2.27392317e-04 2.24165963e-04 2.20413507e-04 2.15657779e-04\n",
      " 2.09037699e-04 2.08755904e-04 2.05449500e-04 2.05076643e-04\n",
      " 2.03646663e-04 1.97755954e-04 1.96098839e-04 1.93304423e-04\n",
      " 1.91481770e-04 1.89613701e-04 1.87305386e-04 1.82371320e-04\n",
      " 1.78399600e-04 1.77895852e-04 1.75790031e-04 1.75426638e-04\n",
      " 1.73720449e-04 1.70592290e-04 1.66412548e-04 1.65331534e-04\n",
      " 1.62779048e-04 1.61832526e-04 1.59998655e-04 1.59224665e-04\n",
      " 1.56850864e-04 1.54771703e-04 1.53829036e-04 1.51005074e-04\n",
      " 1.50675728e-04 1.48843957e-04 1.47832850e-04 1.47106918e-04\n",
      " 1.45171483e-04 1.44305416e-04 1.43050865e-04 1.41429910e-04\n",
      " 1.40512772e-04 1.38256762e-04 1.36887455e-04 1.34097875e-04\n",
      " 1.33348896e-04 1.31817420e-04 1.29854343e-04 1.28980832e-04\n",
      " 1.28142614e-04 1.26541591e-04 1.25026435e-04 1.22986838e-04\n",
      " 1.22161432e-04 1.19991694e-04 1.18787533e-04 1.18551096e-04\n",
      " 1.18248665e-04 1.16870991e-04 1.16652535e-04 1.15349765e-04\n",
      " 1.15140161e-04 1.13755608e-04 1.13192561e-04 1.11515479e-04\n",
      " 1.10339884e-04 1.09255534e-04 1.07759745e-04 1.06936220e-04\n",
      " 1.06389342e-04 1.05632422e-04 1.05271118e-04 1.04169272e-04\n",
      " 1.03249110e-04 1.02992568e-04 1.00985853e-04 1.00784063e-04\n",
      " 9.98878959e-05 9.95288428e-05 9.79430770e-05 9.64551246e-05\n",
      " 9.54648779e-05 9.46389692e-05 9.43673885e-05 9.34735414e-05\n",
      " 9.27309097e-05 9.19513283e-05 9.10373184e-05 8.99945410e-05\n",
      " 8.97037438e-05 8.91876353e-05 8.87188234e-05 8.81673983e-05\n",
      " 8.74150776e-05 8.70323877e-05 8.57529654e-05 8.53650011e-05\n",
      " 8.47596487e-05 8.47207657e-05 8.40360258e-05 8.35113189e-05\n",
      " 8.29901034e-05 8.22665843e-05 8.13812721e-05 8.13289664e-05\n",
      " 8.05428295e-05 8.02136109e-05 7.92555936e-05 7.90385402e-05\n",
      " 7.84411514e-05 7.78884220e-05 7.69355810e-05 7.60759484e-05\n",
      " 7.54880235e-05 7.54099459e-05 7.47941716e-05 7.41551068e-05\n",
      " 7.36711699e-05 7.31007764e-05 7.29312390e-05 7.20996693e-05\n",
      " 7.15293451e-05 7.12659875e-05 7.09214362e-05 7.01838193e-05\n",
      " 7.00444800e-05 6.93246772e-05 6.90610716e-05 6.86958727e-05\n",
      " 6.83558341e-05 6.72421463e-05 6.66442439e-05 6.64280734e-05\n",
      " 6.60819929e-05 6.54259275e-05 6.52465914e-05 6.51420725e-05\n",
      " 6.49187002e-05 6.39491414e-05 6.38657663e-05 6.37163225e-05\n",
      " 6.29951232e-05 6.25576001e-05 6.21330842e-05 6.16937895e-05\n",
      " 6.12925815e-05 6.10814950e-05 6.08002367e-05 6.01998933e-05\n",
      " 5.97671657e-05 5.93055268e-05 5.89653267e-05 5.84273587e-05\n",
      " 5.82507233e-05 5.77512229e-05 5.70436021e-05 5.69443864e-05\n",
      " 5.64853675e-05 5.64155478e-05 5.59739506e-05 5.57446460e-05\n",
      " 5.53425550e-05 5.50867629e-05 5.45143208e-05 5.40827646e-05\n",
      " 5.38676155e-05 5.36452992e-05 5.34804995e-05 5.30766196e-05\n",
      " 5.29983747e-05 5.28273195e-05 5.25110331e-05 5.16680317e-05\n",
      " 5.16167590e-05 5.15181723e-05 5.13457034e-05 5.11258512e-05\n",
      " 5.08556323e-05 5.04764683e-05 5.01484443e-05 4.97160398e-05\n",
      " 4.94875662e-05 4.90753823e-05 4.88930339e-05 4.85606068e-05\n",
      " 4.83936777e-05 4.83520883e-05 4.77598584e-05 4.77319282e-05\n",
      " 4.73144381e-05 4.67799557e-05 4.63129386e-05 4.60477027e-05\n",
      " 4.59990355e-05 4.57729538e-05 4.55564284e-05 4.52104518e-05\n",
      " 4.51282510e-05 4.45611785e-05 4.44961514e-05 4.43789427e-05\n",
      " 4.42018514e-05 4.40397697e-05 4.38276127e-05 4.36326726e-05\n",
      " 4.33179674e-05 4.29084380e-05 4.25799696e-05 4.23786356e-05\n",
      " 4.22647162e-05 4.20692683e-05 4.19766947e-05 4.17987118e-05\n",
      " 4.15659130e-05 4.13695880e-05 4.12412216e-05 4.08675831e-05\n",
      " 4.06778828e-05 4.04959527e-05 4.04317393e-05 4.00940811e-05\n",
      " 3.97478707e-05 3.95503078e-05 3.94026182e-05 3.91471233e-05\n",
      " 3.90720522e-05 3.89672390e-05 3.86239630e-05 3.84230214e-05\n",
      " 3.80895816e-05 3.80481133e-05 3.76720078e-05 3.76578177e-05\n",
      " 3.74975884e-05 3.72560239e-05 3.71214684e-05 3.70017235e-05\n",
      " 3.67313949e-05 3.65241921e-05 3.63081197e-05 3.61587393e-05\n",
      " 3.60564277e-05 3.58597160e-05 3.57576582e-05 3.55222367e-05\n",
      " 3.53386389e-05 3.52353982e-05 3.50395636e-05 3.49230474e-05\n",
      " 3.46132386e-05 3.45184029e-05 3.44931907e-05 3.42301448e-05\n",
      " 3.40959471e-05 3.38196142e-05 3.37674297e-05 3.34778588e-05\n",
      " 3.34251549e-05 3.32858935e-05 3.30889596e-05 3.30238892e-05\n",
      " 3.26821511e-05 3.26739798e-05 3.24536371e-05 3.23930505e-05\n",
      " 3.20620286e-05 3.19715324e-05 3.19291524e-05 3.16543949e-05\n",
      " 3.15243579e-05 3.13224439e-05 3.12551806e-05 3.11676246e-05\n",
      " 3.09959549e-05 3.09093770e-05 3.05736549e-05 3.05538614e-05\n",
      " 3.03275778e-05 3.01113755e-05 2.99482897e-05 2.98812082e-05\n",
      " 2.96098179e-05 2.95803873e-05 2.95517848e-05 2.94586716e-05\n",
      " 2.91914795e-05 2.90724270e-05 2.89036426e-05 2.87575083e-05\n",
      " 2.86943912e-05 2.86001499e-05 2.84227527e-05 2.83263070e-05\n",
      " 2.80278809e-05 2.79962979e-05 2.78861323e-05 2.77041590e-05\n",
      " 2.76867199e-05 2.75165506e-05 2.74615702e-05 2.74124240e-05\n",
      " 2.73018833e-05 2.70626964e-05 2.69300857e-05 2.68505827e-05\n",
      " 2.66886597e-05 2.65130024e-05 2.63930324e-05 2.62010642e-05\n",
      " 2.60738780e-05 2.60098664e-05 2.59085387e-05 2.58033388e-05\n",
      " 2.57299326e-05 2.56053402e-05 2.53869740e-05 2.52909611e-05\n",
      " 2.51669371e-05 2.50597867e-05 2.49588976e-05 2.48408088e-05\n",
      " 2.47934486e-05 2.45711266e-05 2.44615554e-05 2.43771329e-05\n",
      " 2.42723023e-05 2.40768833e-05 2.39891167e-05 2.37820898e-05\n",
      " 2.36882150e-05 2.36478590e-05 2.35472670e-05 2.34920601e-05\n",
      " 2.32143999e-05 2.31199523e-05 2.30214334e-05 2.28624823e-05\n",
      " 2.28079088e-05 2.26964491e-05 2.24832159e-05 2.23629372e-05\n",
      " 2.22439482e-05 2.20445935e-05 2.20116371e-05 2.19342433e-05\n",
      " 2.18018345e-05 2.16789489e-05 2.15940417e-05 2.14811826e-05\n",
      " 2.13734248e-05 2.13501761e-05 2.12256819e-05 2.10972434e-05\n",
      " 2.08514230e-05 2.07385380e-05 2.06739840e-05 2.05897130e-05\n",
      " 2.04972996e-05 2.03115435e-05 2.02786996e-05 2.01684042e-05\n",
      " 2.00794358e-05 1.99674077e-05 1.98346397e-05 1.96500134e-05\n",
      " 1.94618784e-05 1.93811145e-05 1.93293787e-05 1.91384652e-05\n",
      " 1.90706695e-05 1.88781820e-05 1.88066455e-05 1.87357120e-05\n",
      " 1.87047796e-05 1.84884836e-05 1.84000476e-05 1.83167230e-05\n",
      " 1.81831904e-05 1.80226884e-05 1.79657445e-05 1.78819107e-05]\n",
      "Gesamte erklärte Varianz: 0.9954606824631933\n",
      "✅ Reduzierter Chunk 5 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_5.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183526, 1024)\n",
      "Gesamte Datenform vor PCA: (183526, 1024)\n",
      "Gesamte Datenform nach PCA: (183526, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32228177e-01 9.64800759e-02 7.89757483e-02 5.13230412e-02\n",
      " 4.82111791e-02 4.34177620e-02 4.11313778e-02 3.69634241e-02\n",
      " 3.42222053e-02 2.97550201e-02 2.78360414e-02 2.52661855e-02\n",
      " 2.10395802e-02 1.90115410e-02 1.81844249e-02 1.74454556e-02\n",
      " 1.67146458e-02 1.39217394e-02 1.24038923e-02 1.13292145e-02\n",
      " 1.02606439e-02 9.62038329e-03 8.71840829e-03 8.39319676e-03\n",
      " 8.23583279e-03 7.18145630e-03 6.63056031e-03 6.49076960e-03\n",
      " 5.98493844e-03 5.51914933e-03 5.16468972e-03 4.60263759e-03\n",
      " 4.52385720e-03 4.32820844e-03 4.19326182e-03 3.79469519e-03\n",
      " 3.69194477e-03 3.49363756e-03 3.36789636e-03 3.18444884e-03\n",
      " 2.99907593e-03 2.76905902e-03 2.63315960e-03 2.50601204e-03\n",
      " 2.38235883e-03 2.26604279e-03 2.22065093e-03 2.14117541e-03\n",
      " 2.08764229e-03 1.96104170e-03 1.88452909e-03 1.75204355e-03\n",
      " 1.68184281e-03 1.63933062e-03 1.59871756e-03 1.49384993e-03\n",
      " 1.41641402e-03 1.39717294e-03 1.37201306e-03 1.29716115e-03\n",
      " 1.21149935e-03 1.15416225e-03 1.14860961e-03 1.09425530e-03\n",
      " 1.07537094e-03 1.00053725e-03 9.93352675e-04 9.60118874e-04\n",
      " 9.46787156e-04 9.10581286e-04 8.97937116e-04 8.56218746e-04\n",
      " 8.30908024e-04 7.90325209e-04 7.71941900e-04 7.58930742e-04\n",
      " 7.36238742e-04 7.25359270e-04 6.74279092e-04 6.68556415e-04\n",
      " 6.64519452e-04 6.33701641e-04 6.25441307e-04 5.90874583e-04\n",
      " 5.84234205e-04 5.67316398e-04 5.51782889e-04 5.28466279e-04\n",
      " 5.24472276e-04 5.10926913e-04 4.95972327e-04 4.79002403e-04\n",
      " 4.64671919e-04 4.55116120e-04 4.48690126e-04 4.45658126e-04\n",
      " 4.30160734e-04 4.20755411e-04 4.16678678e-04 4.06744099e-04\n",
      " 4.01823700e-04 3.90333398e-04 3.81090829e-04 3.72806980e-04\n",
      " 3.64031031e-04 3.61136328e-04 3.54069932e-04 3.43956529e-04\n",
      " 3.36452854e-04 3.35019653e-04 3.29364121e-04 3.21855891e-04\n",
      " 3.18523632e-04 3.09658017e-04 3.05159151e-04 2.96729774e-04\n",
      " 2.88840174e-04 2.83711541e-04 2.77505619e-04 2.76500980e-04\n",
      " 2.74381156e-04 2.71442382e-04 2.64664892e-04 2.60262790e-04\n",
      " 2.54904928e-04 2.47049941e-04 2.45902548e-04 2.42972237e-04\n",
      " 2.39246932e-04 2.35966882e-04 2.32647409e-04 2.29868407e-04\n",
      " 2.25328616e-04 2.22678679e-04 2.19700830e-04 2.13790474e-04\n",
      " 2.09947826e-04 2.07395070e-04 2.05193788e-04 2.03679985e-04\n",
      " 2.02732548e-04 1.98213150e-04 1.95647331e-04 1.92780374e-04\n",
      " 1.90780516e-04 1.88329566e-04 1.87035442e-04 1.81426489e-04\n",
      " 1.77805937e-04 1.76595849e-04 1.73860675e-04 1.73009112e-04\n",
      " 1.72073341e-04 1.68594976e-04 1.66470215e-04 1.64019357e-04\n",
      " 1.63054221e-04 1.61668546e-04 1.59234105e-04 1.58553401e-04\n",
      " 1.56572492e-04 1.55237038e-04 1.53993435e-04 1.50484861e-04\n",
      " 1.48964445e-04 1.47229078e-04 1.46377018e-04 1.44502042e-04\n",
      " 1.43311989e-04 1.41599258e-04 1.39925382e-04 1.38689040e-04\n",
      " 1.38201553e-04 1.36538036e-04 1.34396431e-04 1.34052067e-04\n",
      " 1.32452690e-04 1.30652247e-04 1.29149623e-04 1.28293066e-04\n",
      " 1.27901673e-04 1.25820011e-04 1.23855646e-04 1.22900314e-04\n",
      " 1.22048428e-04 1.20403050e-04 1.19936950e-04 1.19584124e-04\n",
      " 1.17249152e-04 1.16138176e-04 1.15314591e-04 1.14559273e-04\n",
      " 1.13771342e-04 1.12478606e-04 1.12086287e-04 1.11119729e-04\n",
      " 1.09262614e-04 1.07938224e-04 1.07186259e-04 1.06058866e-04\n",
      " 1.05646131e-04 1.04675596e-04 1.04352932e-04 1.03100254e-04\n",
      " 1.02189454e-04 1.00957378e-04 1.00486712e-04 9.93394687e-05\n",
      " 9.83580896e-05 9.78281046e-05 9.67637821e-05 9.54156517e-05\n",
      " 9.51277051e-05 9.43636587e-05 9.38658535e-05 9.36807986e-05\n",
      " 9.25555560e-05 9.19684661e-05 9.12064603e-05 9.00209914e-05\n",
      " 8.98235375e-05 8.89942705e-05 8.85384774e-05 8.77510550e-05\n",
      " 8.69498347e-05 8.66008987e-05 8.57659843e-05 8.51880737e-05\n",
      " 8.48648376e-05 8.39614365e-05 8.33636760e-05 8.25337616e-05\n",
      " 8.23038679e-05 8.14361668e-05 8.09804373e-05 8.02298709e-05\n",
      " 7.96053817e-05 7.95388720e-05 7.91069001e-05 7.86515290e-05\n",
      " 7.82554651e-05 7.76001141e-05 7.69771508e-05 7.62711979e-05\n",
      " 7.57915085e-05 7.51671174e-05 7.46879714e-05 7.38228021e-05\n",
      " 7.32263480e-05 7.24827181e-05 7.22381074e-05 7.20247574e-05\n",
      " 7.17650598e-05 7.11130094e-05 7.08909019e-05 7.01456362e-05\n",
      " 6.93636648e-05 6.84568532e-05 6.82863593e-05 6.79440131e-05\n",
      " 6.74909657e-05 6.70593984e-05 6.66891847e-05 6.62242180e-05\n",
      " 6.54248822e-05 6.46448645e-05 6.43579410e-05 6.42274072e-05\n",
      " 6.36735664e-05 6.35863628e-05 6.28858203e-05 6.27580033e-05\n",
      " 6.24692244e-05 6.22665854e-05 6.20172579e-05 6.15985463e-05\n",
      " 6.08929286e-05 6.03080064e-05 5.99724753e-05 5.99649145e-05\n",
      " 5.93315233e-05 5.89817955e-05 5.82919988e-05 5.82410963e-05\n",
      " 5.77054928e-05 5.75148309e-05 5.69920079e-05 5.66699105e-05\n",
      " 5.65120006e-05 5.61124974e-05 5.57235839e-05 5.51535348e-05\n",
      " 5.48372120e-05 5.45547164e-05 5.42908626e-05 5.40081127e-05\n",
      " 5.36075863e-05 5.33255705e-05 5.32316268e-05 5.29339113e-05\n",
      " 5.28760492e-05 5.23775098e-05 5.20660831e-05 5.17108291e-05\n",
      " 5.13733615e-05 5.12322784e-05 5.10959180e-05 5.05828350e-05\n",
      " 5.02340956e-05 5.01999043e-05 4.97157466e-05 4.94554710e-05\n",
      " 4.91948254e-05 4.87485932e-05 4.84627304e-05 4.81050949e-05\n",
      " 4.78330301e-05 4.77165427e-05 4.74755708e-05 4.71384385e-05\n",
      " 4.69539791e-05 4.66367777e-05 4.65027005e-05 4.59699699e-05\n",
      " 4.56517916e-05 4.55200266e-05 4.53241508e-05 4.50719157e-05\n",
      " 4.47667694e-05 4.46357963e-05 4.44060125e-05 4.41550231e-05\n",
      " 4.38349835e-05 4.36251075e-05 4.35594967e-05 4.31312416e-05\n",
      " 4.28302225e-05 4.25619786e-05 4.22053460e-05 4.20454823e-05\n",
      " 4.18874134e-05 4.18677281e-05 4.17147547e-05 4.14393776e-05\n",
      " 4.13280754e-05 4.10242066e-05 4.08668371e-05 4.07110457e-05\n",
      " 4.04140932e-05 4.02738859e-05 3.99910781e-05 3.96862960e-05\n",
      " 3.95410019e-05 3.94671568e-05 3.92484425e-05 3.88874370e-05\n",
      " 3.86866825e-05 3.85414837e-05 3.84067851e-05 3.81538969e-05\n",
      " 3.79637350e-05 3.77518879e-05 3.76826325e-05 3.75662087e-05\n",
      " 3.72678892e-05 3.72477443e-05 3.68173274e-05 3.66089485e-05\n",
      " 3.64742297e-05 3.61578751e-05 3.58559197e-05 3.58009969e-05\n",
      " 3.56802407e-05 3.55496693e-05 3.53295128e-05 3.52195516e-05\n",
      " 3.51157928e-05 3.49348825e-05 3.48592975e-05 3.46867313e-05\n",
      " 3.42873148e-05 3.42191172e-05 3.41567550e-05 3.39289394e-05\n",
      " 3.37701191e-05 3.35731536e-05 3.34324029e-05 3.32273767e-05\n",
      " 3.32116799e-05 3.29127476e-05 3.28066766e-05 3.26696225e-05\n",
      " 3.24704258e-05 3.22556740e-05 3.20537779e-05 3.19252181e-05\n",
      " 3.17509466e-05 3.16547515e-05 3.14518178e-05 3.13380068e-05\n",
      " 3.11203156e-05 3.10423376e-05 3.09861402e-05 3.08647193e-05\n",
      " 3.07814030e-05 3.06900923e-05 3.04947049e-05 3.03758101e-05\n",
      " 3.02099231e-05 3.00218162e-05 2.98344203e-05 2.97164821e-05\n",
      " 2.95857373e-05 2.94018877e-05 2.93531211e-05 2.91908527e-05\n",
      " 2.91240453e-05 2.90835447e-05 2.89220798e-05 2.86833045e-05\n",
      " 2.86280840e-05 2.84588675e-05 2.81312065e-05 2.79129575e-05\n",
      " 2.78394824e-05 2.76608698e-05 2.75722500e-05 2.75290412e-05\n",
      " 2.74694507e-05 2.74019959e-05 2.73025320e-05 2.72152879e-05\n",
      " 2.69507289e-05 2.68402446e-05 2.68190477e-05 2.67218439e-05\n",
      " 2.65151269e-05 2.63722172e-05 2.62757504e-05 2.61690089e-05\n",
      " 2.59870668e-05 2.58402004e-05 2.57894945e-05 2.56114628e-05\n",
      " 2.54580212e-05 2.54058759e-05 2.53041952e-05 2.51721700e-05\n",
      " 2.51598866e-05 2.50988423e-05 2.50443155e-05 2.46926049e-05\n",
      " 2.46722722e-05 2.45483888e-05 2.42913011e-05 2.41898834e-05\n",
      " 2.40235759e-05 2.39681372e-05 2.37435442e-05 2.36249152e-05\n",
      " 2.34961401e-05 2.34468243e-05 2.33240378e-05 2.31411997e-05\n",
      " 2.29575495e-05 2.29259855e-05 2.28710367e-05 2.26952218e-05\n",
      " 2.26053476e-05 2.24565766e-05 2.23414202e-05 2.22625462e-05\n",
      " 2.20829394e-05 2.20561673e-05 2.18846257e-05 2.17794290e-05\n",
      " 2.16905389e-05 2.16005606e-05 2.14000937e-05 2.13049636e-05\n",
      " 2.12125764e-05 2.11057005e-05 2.10534670e-05 2.09163768e-05\n",
      " 2.08290041e-05 2.07251932e-05 2.06364000e-05 2.05982795e-05\n",
      " 2.04399346e-05 2.03405486e-05 2.01860824e-05 2.01319111e-05\n",
      " 2.00731414e-05 1.99622466e-05 1.98777511e-05 1.96558113e-05\n",
      " 1.96051169e-05 1.94756857e-05 1.93787103e-05 1.92012581e-05\n",
      " 1.91431807e-05 1.89577010e-05 1.89401805e-05 1.87211569e-05\n",
      " 1.86918429e-05 1.86462367e-05 1.84073183e-05 1.82979582e-05\n",
      " 1.81321681e-05 1.79581597e-05 1.78845979e-05 1.78059522e-05]\n",
      "Gesamte erklärte Varianz: 0.9955156471041637\n",
      "✅ Reduzierter Chunk 6 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_6.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183399, 1024)\n",
      "Gesamte Datenform vor PCA: (183399, 1024)\n",
      "Gesamte Datenform nach PCA: (183399, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32325875e-01 9.58561152e-02 7.87568565e-02 5.06639692e-02\n",
      " 4.80169821e-02 4.36204039e-02 4.11305548e-02 3.68254573e-02\n",
      " 3.42883513e-02 2.96846471e-02 2.76344469e-02 2.54713117e-02\n",
      " 2.10856325e-02 1.90545822e-02 1.81449689e-02 1.75080903e-02\n",
      " 1.66676563e-02 1.39699347e-02 1.23477922e-02 1.13583984e-02\n",
      " 1.04956163e-02 9.61330233e-03 8.77757511e-03 8.43401515e-03\n",
      " 8.18391235e-03 7.21390983e-03 6.56812196e-03 6.46087189e-03\n",
      " 6.02430907e-03 5.62136617e-03 5.23817550e-03 4.60744830e-03\n",
      " 4.53264813e-03 4.36894054e-03 4.18504012e-03 3.84175600e-03\n",
      " 3.74266608e-03 3.54252090e-03 3.37596972e-03 3.19533277e-03\n",
      " 3.02121034e-03 2.79186280e-03 2.64562035e-03 2.52345623e-03\n",
      " 2.39774170e-03 2.28883769e-03 2.20328185e-03 2.17878549e-03\n",
      " 2.10101807e-03 2.01082733e-03 1.89471473e-03 1.77369046e-03\n",
      " 1.69128739e-03 1.68582773e-03 1.63157492e-03 1.49894579e-03\n",
      " 1.43645240e-03 1.39891782e-03 1.37140607e-03 1.29530370e-03\n",
      " 1.23276550e-03 1.18981792e-03 1.15844380e-03 1.08822587e-03\n",
      " 1.07943147e-03 1.01645711e-03 1.00142949e-03 9.74980752e-04\n",
      " 9.56873495e-04 9.15111571e-04 9.02048914e-04 8.66483650e-04\n",
      " 8.36996657e-04 7.93170908e-04 7.76742037e-04 7.56226752e-04\n",
      " 7.41535832e-04 7.26967469e-04 6.81712657e-04 6.79228772e-04\n",
      " 6.61417908e-04 6.37728297e-04 6.32650421e-04 5.94442709e-04\n",
      " 5.89192709e-04 5.66129207e-04 5.53559966e-04 5.32119727e-04\n",
      " 5.26425740e-04 5.15807535e-04 4.98297321e-04 4.83310657e-04\n",
      " 4.62037498e-04 4.55932800e-04 4.52482492e-04 4.42711347e-04\n",
      " 4.28784134e-04 4.18551877e-04 4.16529428e-04 4.11052413e-04\n",
      " 4.03687769e-04 3.90660795e-04 3.83341670e-04 3.72590934e-04\n",
      " 3.65418849e-04 3.60144064e-04 3.57955646e-04 3.46253206e-04\n",
      " 3.41659861e-04 3.36969438e-04 3.29908661e-04 3.22857974e-04\n",
      " 3.21880767e-04 3.09426631e-04 3.07179341e-04 2.98466907e-04\n",
      " 2.87808853e-04 2.87258647e-04 2.80279004e-04 2.78125644e-04\n",
      " 2.75364574e-04 2.70064172e-04 2.66063150e-04 2.57435250e-04\n",
      " 2.52574832e-04 2.51210774e-04 2.48424989e-04 2.47231388e-04\n",
      " 2.40281851e-04 2.35576132e-04 2.31210867e-04 2.28988524e-04\n",
      " 2.27064333e-04 2.24961630e-04 2.20609694e-04 2.17114468e-04\n",
      " 2.10028283e-04 2.09061165e-04 2.06665332e-04 2.05297710e-04\n",
      " 2.02690783e-04 1.98284697e-04 1.95562920e-04 1.92845066e-04\n",
      " 1.89777757e-04 1.88483665e-04 1.86502789e-04 1.82789423e-04\n",
      " 1.80775004e-04 1.77944950e-04 1.75127762e-04 1.74100720e-04\n",
      " 1.73719099e-04 1.71617102e-04 1.66718501e-04 1.64568069e-04\n",
      " 1.63825271e-04 1.61866842e-04 1.61082910e-04 1.59949500e-04\n",
      " 1.57247891e-04 1.55075649e-04 1.54397195e-04 1.50583361e-04\n",
      " 1.49674109e-04 1.48679256e-04 1.48414787e-04 1.46055136e-04\n",
      " 1.43881702e-04 1.43402007e-04 1.41854902e-04 1.41523196e-04\n",
      " 1.38948447e-04 1.37422874e-04 1.35522217e-04 1.34104853e-04\n",
      " 1.32434347e-04 1.31718207e-04 1.30724419e-04 1.29443033e-04\n",
      " 1.28380747e-04 1.26647551e-04 1.25177865e-04 1.22875615e-04\n",
      " 1.22497291e-04 1.20952499e-04 1.20211958e-04 1.19024756e-04\n",
      " 1.17316090e-04 1.16951488e-04 1.16219185e-04 1.15392847e-04\n",
      " 1.14302213e-04 1.13967811e-04 1.13082197e-04 1.12078343e-04\n",
      " 1.10892310e-04 1.09943821e-04 1.08954708e-04 1.07874788e-04\n",
      " 1.06094211e-04 1.05143327e-04 1.04199721e-04 1.03830503e-04\n",
      " 1.02482852e-04 1.02001248e-04 1.01540622e-04 1.01328799e-04\n",
      " 9.93467779e-05 9.82005012e-05 9.73339091e-05 9.68064110e-05\n",
      " 9.62038833e-05 9.48441463e-05 9.42814678e-05 9.40530351e-05\n",
      " 9.22755183e-05 9.21950447e-05 9.15609119e-05 9.11573061e-05\n",
      " 8.99138189e-05 8.93298620e-05 8.86988706e-05 8.80413247e-05\n",
      " 8.77166825e-05 8.70327151e-05 8.63605323e-05 8.60602173e-05\n",
      " 8.54059574e-05 8.47396525e-05 8.42282607e-05 8.33067141e-05\n",
      " 8.23287087e-05 8.18411234e-05 8.13766967e-05 8.10311521e-05\n",
      " 8.07486500e-05 7.99080802e-05 7.97934908e-05 7.90156279e-05\n",
      " 7.85138570e-05 7.75430890e-05 7.72883969e-05 7.69241774e-05\n",
      " 7.66981456e-05 7.61923482e-05 7.47452357e-05 7.41578077e-05\n",
      " 7.35357141e-05 7.28682869e-05 7.26299787e-05 7.24045370e-05\n",
      " 7.20855701e-05 7.16421093e-05 7.08713219e-05 7.04348266e-05\n",
      " 6.96927979e-05 6.93953987e-05 6.87709157e-05 6.86473590e-05\n",
      " 6.78395225e-05 6.72268127e-05 6.69189825e-05 6.65440197e-05\n",
      " 6.59738839e-05 6.54242397e-05 6.50111807e-05 6.47542322e-05\n",
      " 6.44602810e-05 6.42504365e-05 6.37838750e-05 6.37180556e-05\n",
      " 6.32750229e-05 6.26744854e-05 6.25668209e-05 6.17103821e-05\n",
      " 6.13631366e-05 6.07125620e-05 6.04948206e-05 6.03621636e-05\n",
      " 6.00744547e-05 5.93642278e-05 5.89589500e-05 5.85015408e-05\n",
      " 5.80996648e-05 5.80177681e-05 5.73789607e-05 5.69955861e-05\n",
      " 5.69217309e-05 5.61978218e-05 5.60184195e-05 5.55647361e-05\n",
      " 5.52718552e-05 5.50351553e-05 5.49862579e-05 5.44631914e-05\n",
      " 5.40137601e-05 5.37147815e-05 5.34026760e-05 5.32568690e-05\n",
      " 5.30838769e-05 5.25330467e-05 5.23881190e-05 5.19835702e-05\n",
      " 5.18371731e-05 5.15808437e-05 5.12749574e-05 5.09624064e-05\n",
      " 5.04219666e-05 5.02905419e-05 5.01113537e-05 4.98076485e-05\n",
      " 4.94388933e-05 4.92328902e-05 4.91114683e-05 4.86076147e-05\n",
      " 4.83983054e-05 4.79943669e-05 4.79222675e-05 4.74298139e-05\n",
      " 4.72574958e-05 4.72511898e-05 4.65333668e-05 4.62993021e-05\n",
      " 4.59566676e-05 4.58081994e-05 4.54759379e-05 4.54195398e-05\n",
      " 4.50876139e-05 4.47217022e-05 4.47000767e-05 4.43171504e-05\n",
      " 4.41881064e-05 4.41143553e-05 4.37103358e-05 4.35555587e-05\n",
      " 4.32530771e-05 4.29324354e-05 4.28360291e-05 4.25499025e-05\n",
      " 4.24587174e-05 4.23700374e-05 4.20711687e-05 4.17104088e-05\n",
      " 4.15406246e-05 4.15162830e-05 4.11587976e-05 4.09232981e-05\n",
      " 4.05920057e-05 4.04506331e-05 4.03455691e-05 4.01948244e-05\n",
      " 3.98763782e-05 3.97579907e-05 3.94758386e-05 3.92551033e-05\n",
      " 3.88903226e-05 3.86625639e-05 3.83447888e-05 3.82352325e-05\n",
      " 3.81189016e-05 3.80142687e-05 3.76798579e-05 3.76272435e-05\n",
      " 3.74021981e-05 3.72775914e-05 3.71799470e-05 3.69122148e-05\n",
      " 3.67497143e-05 3.66093629e-05 3.62898580e-05 3.62227253e-05\n",
      " 3.59551927e-05 3.56847096e-05 3.56370589e-05 3.55427180e-05\n",
      " 3.54294157e-05 3.52190650e-05 3.50305827e-05 3.48304952e-05\n",
      " 3.46866205e-05 3.45240621e-05 3.43862823e-05 3.42334577e-05\n",
      " 3.40266274e-05 3.39530817e-05 3.38933051e-05 3.38551973e-05\n",
      " 3.33620668e-05 3.32124328e-05 3.29799533e-05 3.28846810e-05\n",
      " 3.27932240e-05 3.26862480e-05 3.25938335e-05 3.24631522e-05\n",
      " 3.20568128e-05 3.20099056e-05 3.19168606e-05 3.16929838e-05\n",
      " 3.15970404e-05 3.14836716e-05 3.13107431e-05 3.12789876e-05\n",
      " 3.11450202e-05 3.08419340e-05 3.07196125e-05 3.05239333e-05\n",
      " 3.03784040e-05 3.03304004e-05 3.01008252e-05 2.99988014e-05\n",
      " 2.97705480e-05 2.96672225e-05 2.96044202e-05 2.93863577e-05\n",
      " 2.93044002e-05 2.90482762e-05 2.89803278e-05 2.88574364e-05\n",
      " 2.87727309e-05 2.87187974e-05 2.86257871e-05 2.84545450e-05\n",
      " 2.84052427e-05 2.82350564e-05 2.79860310e-05 2.79596326e-05\n",
      " 2.77342835e-05 2.77062046e-05 2.75558127e-05 2.74608094e-05\n",
      " 2.73833066e-05 2.71984228e-05 2.70695899e-05 2.69670367e-05\n",
      " 2.67589451e-05 2.65859010e-05 2.64349942e-05 2.62845735e-05\n",
      " 2.62492831e-05 2.60867653e-05 2.59162204e-05 2.58512919e-05\n",
      " 2.57118112e-05 2.56332612e-05 2.55301527e-05 2.54648829e-05\n",
      " 2.52166413e-05 2.50926768e-05 2.50041732e-05 2.49185305e-05\n",
      " 2.48473770e-05 2.47150903e-05 2.45761331e-05 2.45316818e-05\n",
      " 2.44461837e-05 2.43090344e-05 2.40720091e-05 2.38851424e-05\n",
      " 2.37909764e-05 2.37794970e-05 2.35666383e-05 2.34492198e-05\n",
      " 2.32938223e-05 2.32893647e-05 2.30478616e-05 2.28787803e-05\n",
      " 2.28600142e-05 2.27592039e-05 2.26403636e-05 2.24443764e-05\n",
      " 2.22209263e-05 2.20878282e-05 2.20051852e-05 2.19437120e-05\n",
      " 2.18706262e-05 2.16952347e-05 2.15830418e-05 2.14971343e-05\n",
      " 2.14611512e-05 2.13373515e-05 2.12512243e-05 2.11729737e-05\n",
      " 2.10793502e-05 2.09619100e-05 2.07773935e-05 2.07641004e-05\n",
      " 2.05789562e-05 2.05472976e-05 2.03717701e-05 2.02415082e-05\n",
      " 2.02081487e-05 2.00385077e-05 1.99242783e-05 1.97846284e-05\n",
      " 1.97619572e-05 1.96777493e-05 1.95591202e-05 1.94659074e-05\n",
      " 1.92524529e-05 1.91609352e-05 1.89668860e-05 1.88747319e-05\n",
      " 1.88361136e-05 1.86988789e-05 1.85942546e-05 1.84957078e-05\n",
      " 1.83454722e-05 1.82375560e-05 1.80873609e-05 1.79235992e-05]\n",
      "Gesamte erklärte Varianz: 0.9954670552339666\n",
      "✅ Reduzierter Chunk 7 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_7.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183892, 1024)\n",
      "Gesamte Datenform vor PCA: (183892, 1024)\n",
      "Gesamte Datenform nach PCA: (183892, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32381343e-01 9.65414218e-02 7.86150443e-02 5.13325275e-02\n",
      " 4.80012738e-02 4.33396232e-02 4.13017639e-02 3.68909205e-02\n",
      " 3.46720207e-02 2.99237469e-02 2.73116809e-02 2.50837484e-02\n",
      " 2.11846206e-02 1.88955728e-02 1.79740621e-02 1.74508428e-02\n",
      " 1.64712124e-02 1.37253544e-02 1.22852760e-02 1.13731238e-02\n",
      " 1.05402468e-02 9.62006027e-03 8.70744624e-03 8.43893872e-03\n",
      " 8.17876484e-03 7.21537458e-03 6.60691030e-03 6.47276738e-03\n",
      " 6.00844521e-03 5.60867622e-03 5.14510960e-03 4.63499785e-03\n",
      " 4.57969531e-03 4.33224061e-03 4.21599898e-03 3.84211726e-03\n",
      " 3.72378668e-03 3.49818432e-03 3.31626226e-03 3.17054329e-03\n",
      " 2.97216344e-03 2.77123890e-03 2.65901538e-03 2.53827312e-03\n",
      " 2.39553350e-03 2.28529622e-03 2.22406507e-03 2.14139224e-03\n",
      " 2.08881505e-03 1.98910585e-03 1.89063176e-03 1.78752126e-03\n",
      " 1.66500391e-03 1.65280198e-03 1.61182819e-03 1.48283228e-03\n",
      " 1.44407823e-03 1.40833920e-03 1.38143702e-03 1.30336864e-03\n",
      " 1.22280231e-03 1.17419649e-03 1.15513158e-03 1.10035478e-03\n",
      " 1.07443020e-03 1.00662526e-03 9.84722676e-04 9.70995595e-04\n",
      " 9.65659882e-04 9.20652375e-04 9.04206036e-04 8.70827514e-04\n",
      " 8.33277830e-04 7.98129388e-04 7.73807454e-04 7.64849650e-04\n",
      " 7.44468839e-04 7.33752016e-04 6.73732015e-04 6.68932673e-04\n",
      " 6.61092961e-04 6.35333718e-04 6.24826797e-04 5.94767588e-04\n",
      " 5.88906636e-04 5.71175208e-04 5.55573973e-04 5.32014336e-04\n",
      " 5.28609398e-04 5.18123121e-04 5.03105086e-04 4.82207809e-04\n",
      " 4.72632631e-04 4.58005997e-04 4.52685109e-04 4.42340660e-04\n",
      " 4.30888418e-04 4.21140377e-04 4.17873665e-04 4.09872267e-04\n",
      " 3.98436835e-04 3.89101532e-04 3.79616041e-04 3.72124573e-04\n",
      " 3.64100984e-04 3.61215652e-04 3.58888326e-04 3.51100386e-04\n",
      " 3.39634150e-04 3.35820354e-04 3.30267674e-04 3.22826660e-04\n",
      " 3.19574519e-04 3.14442266e-04 3.06484834e-04 2.95539082e-04\n",
      " 2.90541706e-04 2.85167685e-04 2.80925748e-04 2.77814234e-04\n",
      " 2.74441001e-04 2.72535073e-04 2.65005827e-04 2.57498195e-04\n",
      " 2.54707145e-04 2.49722563e-04 2.47637444e-04 2.42837639e-04\n",
      " 2.41759104e-04 2.37900902e-04 2.33884432e-04 2.29840644e-04\n",
      " 2.28056685e-04 2.23825717e-04 2.21309918e-04 2.16085345e-04\n",
      " 2.12198256e-04 2.10518117e-04 2.05327835e-04 2.04499334e-04\n",
      " 2.03443590e-04 1.97817870e-04 1.95846616e-04 1.93553662e-04\n",
      " 1.90670281e-04 1.88839366e-04 1.87625606e-04 1.81542663e-04\n",
      " 1.80760576e-04 1.77347591e-04 1.75501227e-04 1.74004758e-04\n",
      " 1.73025412e-04 1.71054111e-04 1.68850302e-04 1.64432386e-04\n",
      " 1.63722390e-04 1.61957124e-04 1.61000673e-04 1.58697432e-04\n",
      " 1.56588691e-04 1.55683445e-04 1.53381497e-04 1.51956646e-04\n",
      " 1.49969147e-04 1.49637230e-04 1.47227375e-04 1.45127720e-04\n",
      " 1.44587656e-04 1.43023089e-04 1.42612372e-04 1.40924624e-04\n",
      " 1.39445242e-04 1.37138456e-04 1.36420148e-04 1.35567967e-04\n",
      " 1.35070772e-04 1.30996226e-04 1.30573940e-04 1.29113355e-04\n",
      " 1.27716548e-04 1.26095720e-04 1.23298066e-04 1.22926766e-04\n",
      " 1.21310301e-04 1.20184324e-04 1.19223139e-04 1.18082672e-04\n",
      " 1.17578018e-04 1.16583293e-04 1.16184479e-04 1.15709475e-04\n",
      " 1.14867477e-04 1.13462404e-04 1.12663806e-04 1.12007199e-04\n",
      " 1.10435844e-04 1.09233249e-04 1.07911247e-04 1.06815104e-04\n",
      " 1.05869852e-04 1.05101644e-04 1.04037228e-04 1.02667439e-04\n",
      " 1.02341006e-04 1.02231806e-04 1.01326745e-04 9.98143078e-05\n",
      " 9.83389787e-05 9.75235014e-05 9.67737947e-05 9.64721154e-05\n",
      " 9.49262937e-05 9.46476655e-05 9.41949331e-05 9.29688251e-05\n",
      " 9.25991471e-05 9.15765735e-05 9.10875873e-05 9.08094268e-05\n",
      " 9.03438728e-05 8.98646602e-05 8.91831311e-05 8.85312920e-05\n",
      " 8.78757871e-05 8.67713829e-05 8.60737706e-05 8.58628757e-05\n",
      " 8.50685157e-05 8.39370425e-05 8.35812310e-05 8.33134212e-05\n",
      " 8.28187257e-05 8.20141592e-05 8.11860509e-05 8.09564115e-05\n",
      " 8.07942108e-05 8.02545850e-05 7.95527354e-05 7.85697349e-05\n",
      " 7.78526507e-05 7.73354929e-05 7.69525142e-05 7.64286367e-05\n",
      " 7.59666677e-05 7.53713451e-05 7.49325253e-05 7.47112392e-05\n",
      " 7.41206676e-05 7.34856911e-05 7.29287120e-05 7.26964689e-05\n",
      " 7.23480005e-05 7.14888168e-05 7.11626490e-05 7.06235600e-05\n",
      " 7.01175785e-05 6.95657488e-05 6.89393912e-05 6.86452109e-05\n",
      " 6.77950733e-05 6.75802644e-05 6.70578648e-05 6.68367115e-05\n",
      " 6.60415433e-05 6.54170618e-05 6.50777088e-05 6.44735825e-05\n",
      " 6.43445702e-05 6.41454346e-05 6.38195497e-05 6.35768058e-05\n",
      " 6.30442691e-05 6.25962646e-05 6.20831824e-05 6.16548404e-05\n",
      " 6.11527672e-05 6.07412704e-05 6.06674122e-05 6.02796766e-05\n",
      " 5.98574596e-05 5.90533088e-05 5.87654729e-05 5.86724776e-05\n",
      " 5.85607608e-05 5.80279180e-05 5.74596529e-05 5.68888997e-05\n",
      " 5.66250675e-05 5.65534375e-05 5.61643049e-05 5.59618096e-05\n",
      " 5.53844983e-05 5.51616882e-05 5.48521405e-05 5.47535455e-05\n",
      " 5.40021530e-05 5.37346377e-05 5.37019343e-05 5.31660378e-05\n",
      " 5.31255641e-05 5.28357135e-05 5.25993082e-05 5.22830987e-05\n",
      " 5.17408230e-05 5.14894487e-05 5.13825407e-05 5.09342996e-05\n",
      " 5.06138989e-05 5.05314795e-05 5.00495824e-05 4.95728232e-05\n",
      " 4.92876487e-05 4.91958656e-05 4.90114105e-05 4.88698826e-05\n",
      " 4.85312453e-05 4.79240883e-05 4.78811352e-05 4.75677659e-05\n",
      " 4.74593714e-05 4.72970495e-05 4.67988267e-05 4.64596036e-05\n",
      " 4.60892242e-05 4.59895207e-05 4.58417754e-05 4.53198030e-05\n",
      " 4.51256523e-05 4.50364065e-05 4.47697859e-05 4.44828564e-05\n",
      " 4.43963758e-05 4.42448232e-05 4.37144715e-05 4.33453044e-05\n",
      " 4.32396520e-05 4.29743939e-05 4.27636866e-05 4.26795209e-05\n",
      " 4.24952851e-05 4.23518435e-05 4.21981549e-05 4.20366615e-05\n",
      " 4.17425101e-05 4.15786988e-05 4.10882867e-05 4.09349821e-05\n",
      " 4.08909841e-05 4.06246926e-05 4.02768563e-05 4.00569875e-05\n",
      " 3.99641076e-05 3.95713467e-05 3.93386014e-05 3.90158829e-05\n",
      " 3.87431865e-05 3.86807123e-05 3.85849027e-05 3.82384808e-05\n",
      " 3.81316074e-05 3.80439837e-05 3.77046884e-05 3.75274812e-05\n",
      " 3.73217645e-05 3.72546548e-05 3.70077544e-05 3.67107050e-05\n",
      " 3.66272898e-05 3.64749781e-05 3.62477777e-05 3.61349323e-05\n",
      " 3.59848374e-05 3.58152994e-05 3.56437149e-05 3.54921854e-05\n",
      " 3.53154516e-05 3.51476426e-05 3.51252582e-05 3.49821629e-05\n",
      " 3.47824703e-05 3.45762168e-05 3.45222490e-05 3.43246462e-05\n",
      " 3.39169537e-05 3.38316825e-05 3.36666272e-05 3.34116101e-05\n",
      " 3.32822861e-05 3.31713890e-05 3.30679332e-05 3.28743454e-05\n",
      " 3.27210638e-05 3.24980488e-05 3.24663932e-05 3.22645155e-05\n",
      " 3.21840127e-05 3.19612372e-05 3.18969100e-05 3.16337590e-05\n",
      " 3.14645385e-05 3.13858426e-05 3.13447887e-05 3.11579869e-05\n",
      " 3.10580035e-05 3.07562059e-05 3.06782605e-05 3.05149341e-05\n",
      " 3.03320204e-05 3.02249739e-05 3.00173060e-05 2.98625091e-05\n",
      " 2.96969169e-05 2.95949735e-05 2.94089626e-05 2.93152197e-05\n",
      " 2.92546593e-05 2.91141705e-05 2.89805081e-05 2.87880748e-05\n",
      " 2.87422122e-05 2.86584246e-05 2.85394513e-05 2.83081232e-05\n",
      " 2.81852561e-05 2.81116057e-05 2.80560475e-05 2.77593358e-05\n",
      " 2.76782730e-05 2.75818919e-05 2.74825695e-05 2.74411635e-05\n",
      " 2.73486328e-05 2.72500465e-05 2.71375503e-05 2.68731091e-05\n",
      " 2.67706953e-05 2.66872540e-05 2.65849268e-05 2.64687562e-05\n",
      " 2.61684943e-05 2.61332537e-05 2.60149616e-05 2.59522104e-05\n",
      " 2.57593471e-05 2.57114321e-05 2.55135811e-05 2.53749049e-05\n",
      " 2.52500433e-05 2.50664541e-05 2.50649041e-05 2.48303721e-05\n",
      " 2.46947729e-05 2.46624967e-05 2.45520817e-05 2.43815594e-05\n",
      " 2.43569179e-05 2.42638850e-05 2.41884739e-05 2.40129263e-05\n",
      " 2.39481589e-05 2.36847467e-05 2.35999358e-05 2.33144495e-05\n",
      " 2.31726286e-05 2.31183260e-05 2.29590104e-05 2.28488955e-05\n",
      " 2.28058140e-05 2.27311894e-05 2.26247576e-05 2.25075479e-05\n",
      " 2.23311013e-05 2.22382041e-05 2.22351964e-05 2.20169743e-05\n",
      " 2.18175255e-05 2.17162447e-05 2.17079981e-05 2.16203254e-05\n",
      " 2.14935471e-05 2.13305282e-05 2.11798646e-05 2.11507779e-05\n",
      " 2.10744879e-05 2.09284094e-05 2.08610847e-05 2.07436051e-05\n",
      " 2.05715328e-05 2.05212236e-05 2.03684804e-05 2.02577089e-05\n",
      " 2.02023094e-05 2.00390321e-05 1.99039669e-05 1.98054282e-05\n",
      " 1.97088435e-05 1.95741045e-05 1.95290139e-05 1.93249296e-05\n",
      " 1.92265034e-05 1.91706392e-05 1.91101712e-05 1.88901176e-05\n",
      " 1.87839601e-05 1.85761046e-05 1.84968415e-05 1.84623096e-05\n",
      " 1.82944602e-05 1.81341415e-05 1.79300008e-05 1.78651671e-05]\n",
      "Gesamte erklärte Varianz: 0.9954654252569454\n",
      "✅ Reduzierter Chunk 8 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_8.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (184013, 1024)\n",
      "Gesamte Datenform vor PCA: (184013, 1024)\n",
      "Gesamte Datenform nach PCA: (184013, 512)\n",
      "Erklärte Varianz pro Komponente: [1.31773544e-01 9.61337863e-02 7.86483961e-02 5.12845886e-02\n",
      " 4.82018538e-02 4.36229547e-02 4.11882717e-02 3.65491088e-02\n",
      " 3.43106900e-02 2.97557354e-02 2.75521363e-02 2.55007001e-02\n",
      " 2.11706880e-02 1.90578732e-02 1.80822462e-02 1.76493133e-02\n",
      " 1.65835102e-02 1.39307157e-02 1.23128906e-02 1.13271153e-02\n",
      " 1.03794610e-02 9.60109820e-03 8.76664855e-03 8.40575578e-03\n",
      " 8.26574783e-03 7.22398808e-03 6.62459223e-03 6.52551904e-03\n",
      " 5.97939442e-03 5.64139987e-03 5.21307609e-03 4.62475379e-03\n",
      " 4.54994557e-03 4.37998840e-03 4.25852731e-03 3.86852961e-03\n",
      " 3.70908486e-03 3.51160196e-03 3.35348861e-03 3.22899357e-03\n",
      " 2.98961860e-03 2.78172399e-03 2.64117957e-03 2.51485108e-03\n",
      " 2.38007632e-03 2.28645043e-03 2.22961855e-03 2.16610277e-03\n",
      " 2.09452585e-03 2.01486864e-03 1.88826327e-03 1.75989221e-03\n",
      " 1.68269254e-03 1.66315707e-03 1.59860813e-03 1.47996043e-03\n",
      " 1.43953231e-03 1.39840361e-03 1.37448658e-03 1.29812272e-03\n",
      " 1.21263639e-03 1.16794013e-03 1.15339309e-03 1.09011507e-03\n",
      " 1.08145709e-03 1.00917663e-03 9.84919486e-04 9.65982520e-04\n",
      " 9.48771757e-04 9.11812337e-04 9.07478741e-04 8.64065273e-04\n",
      " 8.34061846e-04 8.02232894e-04 7.72629329e-04 7.65237252e-04\n",
      " 7.41223437e-04 7.28343233e-04 6.76813811e-04 6.67771241e-04\n",
      " 6.60470062e-04 6.32220407e-04 6.24453320e-04 5.92472440e-04\n",
      " 5.89255726e-04 5.65667778e-04 5.52692809e-04 5.32659672e-04\n",
      " 5.24709591e-04 5.15113251e-04 4.97780569e-04 4.82703375e-04\n",
      " 4.64268309e-04 4.54554606e-04 4.50757566e-04 4.46123935e-04\n",
      " 4.33370621e-04 4.21648446e-04 4.15995367e-04 4.08872212e-04\n",
      " 4.00119873e-04 3.90738930e-04 3.84839250e-04 3.75677591e-04\n",
      " 3.63274848e-04 3.60157271e-04 3.57152915e-04 3.46053246e-04\n",
      " 3.38646349e-04 3.36525345e-04 3.31932699e-04 3.22858257e-04\n",
      " 3.16599838e-04 3.10841109e-04 3.07142285e-04 2.96887171e-04\n",
      " 2.88425180e-04 2.87048557e-04 2.82734130e-04 2.77190958e-04\n",
      " 2.73240909e-04 2.70309019e-04 2.65435553e-04 2.59054201e-04\n",
      " 2.55195350e-04 2.50679657e-04 2.49354076e-04 2.44346724e-04\n",
      " 2.41486678e-04 2.38493184e-04 2.33417393e-04 2.30758291e-04\n",
      " 2.25471339e-04 2.24128406e-04 2.19530812e-04 2.15908229e-04\n",
      " 2.12313054e-04 2.09130444e-04 2.05818689e-04 2.05101094e-04\n",
      " 2.02321523e-04 1.98012504e-04 1.96158536e-04 1.94639329e-04\n",
      " 1.91238561e-04 1.88840305e-04 1.86353865e-04 1.82109269e-04\n",
      " 1.78229868e-04 1.75310267e-04 1.75005940e-04 1.74331390e-04\n",
      " 1.73088147e-04 1.70924939e-04 1.67228773e-04 1.64319182e-04\n",
      " 1.63985135e-04 1.62154317e-04 1.61423364e-04 1.58713566e-04\n",
      " 1.58036232e-04 1.55247547e-04 1.54799117e-04 1.50452461e-04\n",
      " 1.50041748e-04 1.48430146e-04 1.46515943e-04 1.45879755e-04\n",
      " 1.44347258e-04 1.43059302e-04 1.42635679e-04 1.40665142e-04\n",
      " 1.38926305e-04 1.36758506e-04 1.34975934e-04 1.34116778e-04\n",
      " 1.33530642e-04 1.31898110e-04 1.30008324e-04 1.28934360e-04\n",
      " 1.27418996e-04 1.26293104e-04 1.24468603e-04 1.23130250e-04\n",
      " 1.22113944e-04 1.21001687e-04 1.19205607e-04 1.18825040e-04\n",
      " 1.17181324e-04 1.16446148e-04 1.16049346e-04 1.15597245e-04\n",
      " 1.14144797e-04 1.13942679e-04 1.12775492e-04 1.11817718e-04\n",
      " 1.10503528e-04 1.09106654e-04 1.07732920e-04 1.07125681e-04\n",
      " 1.06352638e-04 1.04964924e-04 1.04598164e-04 1.03274942e-04\n",
      " 1.02654805e-04 1.01755309e-04 1.01022929e-04 1.00510444e-04\n",
      " 9.96479056e-05 9.85895785e-05 9.73930392e-05 9.67872848e-05\n",
      " 9.60416151e-05 9.47499945e-05 9.43435156e-05 9.36728433e-05\n",
      " 9.32454174e-05 9.25324253e-05 9.20039867e-05 9.14184429e-05\n",
      " 8.98988099e-05 8.91608207e-05 8.90340168e-05 8.79329500e-05\n",
      " 8.77574175e-05 8.65889820e-05 8.63624080e-05 8.59579370e-05\n",
      " 8.51723984e-05 8.41390513e-05 8.40711245e-05 8.33401828e-05\n",
      " 8.24440756e-05 8.17176731e-05 8.11226367e-05 8.07308857e-05\n",
      " 8.02806776e-05 7.95967617e-05 7.93779256e-05 7.87781199e-05\n",
      " 7.80889168e-05 7.71810906e-05 7.66697847e-05 7.64919336e-05\n",
      " 7.53504571e-05 7.50947265e-05 7.44370734e-05 7.39159699e-05\n",
      " 7.35689550e-05 7.31701335e-05 7.26668473e-05 7.25447207e-05\n",
      " 7.23409405e-05 7.16252905e-05 7.13995277e-05 7.07637652e-05\n",
      " 6.99096440e-05 6.93431372e-05 6.91061789e-05 6.86047050e-05\n",
      " 6.80802590e-05 6.75786297e-05 6.66616100e-05 6.60688116e-05\n",
      " 6.56832049e-05 6.54235967e-05 6.51258742e-05 6.48100813e-05\n",
      " 6.45024348e-05 6.41457432e-05 6.37236446e-05 6.32196336e-05\n",
      " 6.30754611e-05 6.21898706e-05 6.19819136e-05 6.15273102e-05\n",
      " 6.11259170e-05 6.07292241e-05 6.03013380e-05 6.01778307e-05\n",
      " 5.93300207e-05 5.91211317e-05 5.85177130e-05 5.83932276e-05\n",
      " 5.81516093e-05 5.79465749e-05 5.76065730e-05 5.67748659e-05\n",
      " 5.65319186e-05 5.60708084e-05 5.59196920e-05 5.51914427e-05\n",
      " 5.48418044e-05 5.46801253e-05 5.46264777e-05 5.42200391e-05\n",
      " 5.40781161e-05 5.37851054e-05 5.36213840e-05 5.31575928e-05\n",
      " 5.29727230e-05 5.25052268e-05 5.23091385e-05 5.19233362e-05\n",
      " 5.15843123e-05 5.13456568e-05 5.10887583e-05 5.09545157e-05\n",
      " 5.07696171e-05 5.03944983e-05 5.01549684e-05 4.98803216e-05\n",
      " 4.95926728e-05 4.90079517e-05 4.86467784e-05 4.85318316e-05\n",
      " 4.81740555e-05 4.81185037e-05 4.75487895e-05 4.73680044e-05\n",
      " 4.70003790e-05 4.67040660e-05 4.63549109e-05 4.62995892e-05\n",
      " 4.61368458e-05 4.58500887e-05 4.55345317e-05 4.52689777e-05\n",
      " 4.51258897e-05 4.48685740e-05 4.45952476e-05 4.42955518e-05\n",
      " 4.41927671e-05 4.38217819e-05 4.35602406e-05 4.34524795e-05\n",
      " 4.32615459e-05 4.29122182e-05 4.28417889e-05 4.26339240e-05\n",
      " 4.24450730e-05 4.21681911e-05 4.20325850e-05 4.19152421e-05\n",
      " 4.15180890e-05 4.09896245e-05 4.09312248e-05 4.06927707e-05\n",
      " 4.05018544e-05 4.03808496e-05 4.01847814e-05 3.98825687e-05\n",
      " 3.97159448e-05 3.95485615e-05 3.92788508e-05 3.92050013e-05\n",
      " 3.89137079e-05 3.86793529e-05 3.84776869e-05 3.84282708e-05\n",
      " 3.81749334e-05 3.79244237e-05 3.77986640e-05 3.76542700e-05\n",
      " 3.75343037e-05 3.71775143e-05 3.69374493e-05 3.68046996e-05\n",
      " 3.66743518e-05 3.66175343e-05 3.64527600e-05 3.62204790e-05\n",
      " 3.59517262e-05 3.57597887e-05 3.56444709e-05 3.54368447e-05\n",
      " 3.53742166e-05 3.51726110e-05 3.49537030e-05 3.48700042e-05\n",
      " 3.48087971e-05 3.46116300e-05 3.43454863e-05 3.43292367e-05\n",
      " 3.42270129e-05 3.38945993e-05 3.38465955e-05 3.34847750e-05\n",
      " 3.34255095e-05 3.32147335e-05 3.31770738e-05 3.29046563e-05\n",
      " 3.26652472e-05 3.26584010e-05 3.23606642e-05 3.21925905e-05\n",
      " 3.20277846e-05 3.18689245e-05 3.17461996e-05 3.16062413e-05\n",
      " 3.14900547e-05 3.14022021e-05 3.12153446e-05 3.10290768e-05\n",
      " 3.09407410e-05 3.08396649e-05 3.05888129e-05 3.04607088e-05\n",
      " 3.03151355e-05 3.00475276e-05 2.99592378e-05 2.99015573e-05\n",
      " 2.97680770e-05 2.96894696e-05 2.95406486e-05 2.93645494e-05\n",
      " 2.92497637e-05 2.91523841e-05 2.90783130e-05 2.87506227e-05\n",
      " 2.86804695e-05 2.84572007e-05 2.84101231e-05 2.82456538e-05\n",
      " 2.80988953e-05 2.79274935e-05 2.78558560e-05 2.76923186e-05\n",
      " 2.75908974e-05 2.75348278e-05 2.74888059e-05 2.73291404e-05\n",
      " 2.71236630e-05 2.70705016e-05 2.69801867e-05 2.67863967e-05\n",
      " 2.66840923e-05 2.65539172e-05 2.64481006e-05 2.63075123e-05\n",
      " 2.62351585e-05 2.61198206e-05 2.60502484e-05 2.59091366e-05\n",
      " 2.57454295e-05 2.56889169e-05 2.56105857e-05 2.53646296e-05\n",
      " 2.53053497e-05 2.51447839e-05 2.49465784e-05 2.49384406e-05\n",
      " 2.48090507e-05 2.46771151e-05 2.45663106e-05 2.44203892e-05\n",
      " 2.42680445e-05 2.40896555e-05 2.40229009e-05 2.39218982e-05\n",
      " 2.36318303e-05 2.35523887e-05 2.34447484e-05 2.33821332e-05\n",
      " 2.32920340e-05 2.32371251e-05 2.31912471e-05 2.28951786e-05\n",
      " 2.27333859e-05 2.26427258e-05 2.25093533e-05 2.23666335e-05\n",
      " 2.23373345e-05 2.21309337e-05 2.19679228e-05 2.19216579e-05\n",
      " 2.18569327e-05 2.16927481e-05 2.15983974e-05 2.14916287e-05\n",
      " 2.14307855e-05 2.13541528e-05 2.12075152e-05 2.09778288e-05\n",
      " 2.09654553e-05 2.09335990e-05 2.07965417e-05 2.06691611e-05\n",
      " 2.05558194e-05 2.04076371e-05 2.03836956e-05 2.02400783e-05\n",
      " 2.00601015e-05 1.99336068e-05 1.98454695e-05 1.97616053e-05\n",
      " 1.96834193e-05 1.95643303e-05 1.93805996e-05 1.92812725e-05\n",
      " 1.90891840e-05 1.90313841e-05 1.89057338e-05 1.88456700e-05\n",
      " 1.86856507e-05 1.86657177e-05 1.86201273e-05 1.84213724e-05\n",
      " 1.83033492e-05 1.82398408e-05 1.81700860e-05 1.81150635e-05]\n",
      "Gesamte erklärte Varianz: 0.9954765349569855\n",
      "✅ Reduzierter Chunk 9 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_9.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183435, 1024)\n",
      "Gesamte Datenform vor PCA: (183435, 1024)\n",
      "Gesamte Datenform nach PCA: (183435, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32350887e-01 9.63140697e-02 7.90467282e-02 5.13973826e-02\n",
      " 4.79458611e-02 4.38243244e-02 4.13998554e-02 3.71596484e-02\n",
      " 3.42984787e-02 2.96941291e-02 2.73314398e-02 2.51719289e-02\n",
      " 2.09473374e-02 1.88773246e-02 1.80851798e-02 1.74764578e-02\n",
      " 1.65022284e-02 1.38180643e-02 1.22702852e-02 1.12791713e-02\n",
      " 1.03471595e-02 9.56063010e-03 8.76302356e-03 8.34676686e-03\n",
      " 8.22744984e-03 7.20894974e-03 6.63080530e-03 6.46273261e-03\n",
      " 6.01520629e-03 5.60995737e-03 5.23821463e-03 4.62526272e-03\n",
      " 4.53142606e-03 4.36796779e-03 4.26076256e-03 3.83282760e-03\n",
      " 3.72376370e-03 3.51856205e-03 3.36732719e-03 3.17561427e-03\n",
      " 3.00470203e-03 2.78741771e-03 2.64048762e-03 2.51393376e-03\n",
      " 2.41404622e-03 2.28741875e-03 2.20263033e-03 2.15191233e-03\n",
      " 2.07750126e-03 1.99399467e-03 1.87968776e-03 1.76314819e-03\n",
      " 1.67701769e-03 1.66779497e-03 1.59728833e-03 1.49976085e-03\n",
      " 1.43625530e-03 1.38961501e-03 1.37491121e-03 1.29981361e-03\n",
      " 1.22845820e-03 1.17340077e-03 1.15714860e-03 1.08831354e-03\n",
      " 1.06908154e-03 1.00451917e-03 9.90500971e-04 9.56153811e-04\n",
      " 9.44381165e-04 9.12918342e-04 9.06541670e-04 8.66939087e-04\n",
      " 8.37381848e-04 7.92801511e-04 7.69119007e-04 7.58567645e-04\n",
      " 7.40299842e-04 7.25742339e-04 6.78033372e-04 6.68465072e-04\n",
      " 6.61646956e-04 6.33322544e-04 6.24378932e-04 5.87784362e-04\n",
      " 5.85459676e-04 5.71802769e-04 5.52845380e-04 5.30290383e-04\n",
      " 5.26602426e-04 5.16026078e-04 4.93738552e-04 4.72774107e-04\n",
      " 4.66803648e-04 4.54580380e-04 4.50589304e-04 4.42933775e-04\n",
      " 4.29263054e-04 4.21552175e-04 4.17958413e-04 4.10387807e-04\n",
      " 4.01872567e-04 3.85285942e-04 3.81038821e-04 3.69682379e-04\n",
      " 3.62862756e-04 3.58041243e-04 3.54420943e-04 3.47454674e-04\n",
      " 3.38212463e-04 3.33367564e-04 3.28870865e-04 3.20233733e-04\n",
      " 3.18337698e-04 3.13833281e-04 3.06043436e-04 2.96932898e-04\n",
      " 2.89188875e-04 2.82681596e-04 2.81411894e-04 2.76539159e-04\n",
      " 2.74345034e-04 2.72893409e-04 2.63078220e-04 2.55420048e-04\n",
      " 2.53989723e-04 2.48285068e-04 2.47569292e-04 2.45490198e-04\n",
      " 2.40034310e-04 2.37778462e-04 2.32510244e-04 2.29125361e-04\n",
      " 2.25318085e-04 2.23269354e-04 2.19786359e-04 2.16636730e-04\n",
      " 2.11447200e-04 2.09418449e-04 2.04719454e-04 2.02878791e-04\n",
      " 2.01315750e-04 1.98175995e-04 1.93709705e-04 1.92113022e-04\n",
      " 1.90113544e-04 1.88727049e-04 1.86344813e-04 1.82939823e-04\n",
      " 1.78432995e-04 1.76254805e-04 1.75173272e-04 1.73517149e-04\n",
      " 1.71879396e-04 1.69578351e-04 1.65752858e-04 1.64573978e-04\n",
      " 1.62761591e-04 1.60761800e-04 1.59763197e-04 1.57163681e-04\n",
      " 1.55086986e-04 1.54087224e-04 1.53601209e-04 1.50896883e-04\n",
      " 1.49636069e-04 1.47508337e-04 1.45564845e-04 1.44964668e-04\n",
      " 1.43826013e-04 1.43377569e-04 1.42660193e-04 1.40066785e-04\n",
      " 1.39575694e-04 1.36733008e-04 1.34735894e-04 1.34114266e-04\n",
      " 1.32168746e-04 1.30664253e-04 1.29468847e-04 1.29233789e-04\n",
      " 1.27489592e-04 1.25615030e-04 1.23268279e-04 1.21845233e-04\n",
      " 1.21623143e-04 1.19324730e-04 1.18682705e-04 1.17345637e-04\n",
      " 1.17055034e-04 1.16022927e-04 1.15607709e-04 1.14761673e-04\n",
      " 1.13041279e-04 1.12617913e-04 1.11930646e-04 1.10454638e-04\n",
      " 1.10040324e-04 1.08521912e-04 1.07344087e-04 1.06394852e-04\n",
      " 1.06011051e-04 1.04990452e-04 1.04276426e-04 1.03447634e-04\n",
      " 1.01703645e-04 1.01119832e-04 1.00900153e-04 9.98560049e-05\n",
      " 9.86900232e-05 9.75482624e-05 9.66993644e-05 9.61072242e-05\n",
      " 9.48997427e-05 9.36186775e-05 9.29830326e-05 9.24860313e-05\n",
      " 9.20616751e-05 9.12853004e-05 9.10516452e-05 9.03283592e-05\n",
      " 8.90472651e-05 8.85807542e-05 8.82367495e-05 8.78501383e-05\n",
      " 8.75105605e-05 8.70822814e-05 8.54147731e-05 8.49828319e-05\n",
      " 8.44305748e-05 8.39381859e-05 8.32853199e-05 8.26695325e-05\n",
      " 8.19729416e-05 8.12663088e-05 8.06743830e-05 7.99842065e-05\n",
      " 7.93803962e-05 7.89422955e-05 7.87998101e-05 7.83589744e-05\n",
      " 7.77429494e-05 7.70320578e-05 7.60808415e-05 7.58391494e-05\n",
      " 7.58059240e-05 7.52347133e-05 7.49792593e-05 7.47973659e-05\n",
      " 7.36445260e-05 7.31561469e-05 7.25324269e-05 7.18285871e-05\n",
      " 7.15651253e-05 7.07698317e-05 7.03836493e-05 7.01655811e-05\n",
      " 6.92047692e-05 6.87630470e-05 6.81948378e-05 6.80600068e-05\n",
      " 6.74934838e-05 6.71743083e-05 6.64255152e-05 6.60819045e-05\n",
      " 6.56310445e-05 6.51301783e-05 6.45453622e-05 6.43634456e-05\n",
      " 6.39558758e-05 6.35305519e-05 6.33404767e-05 6.31686018e-05\n",
      " 6.25412371e-05 6.24227639e-05 6.18171863e-05 6.11940110e-05\n",
      " 6.08922478e-05 6.03592631e-05 5.98677723e-05 5.96752403e-05\n",
      " 5.93657647e-05 5.88677784e-05 5.88139190e-05 5.82386928e-05\n",
      " 5.78871437e-05 5.74780036e-05 5.71389996e-05 5.66390548e-05\n",
      " 5.64406124e-05 5.61983871e-05 5.56018894e-05 5.53396618e-05\n",
      " 5.48503382e-05 5.45366384e-05 5.44188431e-05 5.40327766e-05\n",
      " 5.35952843e-05 5.34372524e-05 5.31945172e-05 5.28518742e-05\n",
      " 5.25679232e-05 5.23223256e-05 5.22416321e-05 5.15648408e-05\n",
      " 5.13383243e-05 5.11542116e-05 5.10242711e-05 5.09346194e-05\n",
      " 5.07114290e-05 5.02055101e-05 4.99374532e-05 4.95488139e-05\n",
      " 4.92034649e-05 4.88812473e-05 4.85170403e-05 4.83765482e-05\n",
      " 4.79845799e-05 4.77793465e-05 4.76718653e-05 4.69857797e-05\n",
      " 4.68764732e-05 4.62781967e-05 4.59521055e-05 4.56905268e-05\n",
      " 4.56718021e-05 4.55377707e-05 4.53024032e-05 4.52967391e-05\n",
      " 4.48012445e-05 4.45909408e-05 4.43709228e-05 4.42472027e-05\n",
      " 4.38165797e-05 4.37244625e-05 4.34000431e-05 4.32695927e-05\n",
      " 4.29152391e-05 4.25984712e-05 4.24978739e-05 4.23379732e-05\n",
      " 4.19637911e-05 4.18769758e-05 4.17156004e-05 4.16398931e-05\n",
      " 4.12060919e-05 4.11979594e-05 4.09767071e-05 4.04816702e-05\n",
      " 4.02432461e-05 4.01435643e-05 3.99804014e-05 3.98215176e-05\n",
      " 3.95156462e-05 3.92581120e-05 3.91957290e-05 3.90111498e-05\n",
      " 3.89371113e-05 3.83407295e-05 3.81676320e-05 3.80177732e-05\n",
      " 3.79139917e-05 3.77828142e-05 3.77415142e-05 3.73590547e-05\n",
      " 3.72612937e-05 3.69837282e-05 3.68483699e-05 3.66789055e-05\n",
      " 3.64277713e-05 3.62028541e-05 3.61273930e-05 3.58999436e-05\n",
      " 3.58337247e-05 3.57054009e-05 3.54995271e-05 3.53430220e-05\n",
      " 3.52427028e-05 3.49855510e-05 3.48427035e-05 3.46202749e-05\n",
      " 3.43648644e-05 3.42634240e-05 3.41312150e-05 3.39939417e-05\n",
      " 3.39113331e-05 3.37301930e-05 3.36915875e-05 3.34006918e-05\n",
      " 3.31184964e-05 3.30951564e-05 3.29295656e-05 3.25742763e-05\n",
      " 3.25245396e-05 3.23665917e-05 3.22736691e-05 3.20024745e-05\n",
      " 3.18898450e-05 3.16564418e-05 3.15517709e-05 3.13763643e-05\n",
      " 3.12931329e-05 3.10854773e-05 3.09646168e-05 3.08183360e-05\n",
      " 3.07195522e-05 3.05873027e-05 3.04559948e-05 3.03557248e-05\n",
      " 3.03323123e-05 3.02216818e-05 3.00595156e-05 2.98381996e-05\n",
      " 2.97545683e-05 2.96249031e-05 2.92901956e-05 2.92285051e-05\n",
      " 2.91227535e-05 2.89233376e-05 2.88611140e-05 2.85475765e-05\n",
      " 2.84879661e-05 2.83780860e-05 2.82147580e-05 2.81461286e-05\n",
      " 2.79284573e-05 2.78333849e-05 2.78127683e-05 2.76978905e-05\n",
      " 2.74688592e-05 2.73220656e-05 2.73032192e-05 2.71935303e-05\n",
      " 2.70201663e-05 2.69307377e-05 2.68136435e-05 2.66975721e-05\n",
      " 2.65938891e-05 2.64811031e-05 2.63556563e-05 2.60314107e-05\n",
      " 2.59878973e-05 2.58786256e-05 2.58178505e-05 2.57891622e-05\n",
      " 2.56194545e-05 2.55740116e-05 2.54819552e-05 2.53125314e-05\n",
      " 2.51042152e-05 2.49560628e-05 2.48301147e-05 2.47828147e-05\n",
      " 2.46585557e-05 2.44329286e-05 2.43694910e-05 2.42621055e-05\n",
      " 2.41439248e-05 2.40668008e-05 2.39775634e-05 2.37219472e-05\n",
      " 2.36482592e-05 2.35164153e-05 2.32052289e-05 2.31168056e-05\n",
      " 2.30535781e-05 2.28495658e-05 2.28391923e-05 2.27047191e-05\n",
      " 2.25801936e-05 2.25086148e-05 2.23584633e-05 2.22196588e-05\n",
      " 2.20491471e-05 2.19516990e-05 2.18534324e-05 2.17507780e-05\n",
      " 2.16612002e-05 2.15564656e-05 2.14223126e-05 2.13521244e-05\n",
      " 2.12437958e-05 2.11662589e-05 2.11219458e-05 2.11033051e-05\n",
      " 2.08460998e-05 2.08193077e-05 2.07798548e-05 2.05595021e-05\n",
      " 2.04748612e-05 2.03915588e-05 2.02679618e-05 2.01427380e-05\n",
      " 2.00448090e-05 1.99204167e-05 1.98420019e-05 1.96967236e-05\n",
      " 1.96941074e-05 1.96020047e-05 1.94620920e-05 1.94180237e-05\n",
      " 1.92499384e-05 1.90439720e-05 1.90034543e-05 1.89084326e-05\n",
      " 1.87192121e-05 1.85568126e-05 1.83886418e-05 1.82865075e-05\n",
      " 1.81836459e-05 1.80429292e-05 1.77475805e-05 1.75780017e-05]\n",
      "Gesamte erklärte Varianz: 0.9955131814840964\n",
      "✅ Reduzierter Chunk 10 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_10.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183548, 1024)\n",
      "Gesamte Datenform vor PCA: (183548, 1024)\n",
      "Gesamte Datenform nach PCA: (183548, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32264565e-01 9.59936278e-02 7.89241214e-02 5.12819171e-02\n",
      " 4.80828678e-02 4.31480530e-02 4.12972381e-02 3.66598773e-02\n",
      " 3.44373936e-02 3.00753834e-02 2.76795041e-02 2.53716326e-02\n",
      " 2.11286360e-02 1.87414177e-02 1.80516097e-02 1.76534264e-02\n",
      " 1.65807323e-02 1.39154234e-02 1.23178808e-02 1.13601709e-02\n",
      " 1.03718047e-02 9.60394214e-03 8.70702538e-03 8.40876835e-03\n",
      " 8.13582612e-03 7.22078940e-03 6.60579418e-03 6.42627316e-03\n",
      " 6.01127810e-03 5.57338513e-03 5.26354537e-03 4.64512147e-03\n",
      " 4.55152263e-03 4.34763307e-03 4.18300836e-03 3.85832335e-03\n",
      " 3.72467955e-03 3.49258644e-03 3.37594722e-03 3.18397085e-03\n",
      " 3.01455970e-03 2.78336936e-03 2.64724778e-03 2.50996751e-03\n",
      " 2.38783724e-03 2.30046607e-03 2.23020883e-03 2.16758960e-03\n",
      " 2.07989832e-03 1.99056265e-03 1.91319051e-03 1.78170327e-03\n",
      " 1.67904537e-03 1.66756034e-03 1.61702102e-03 1.50729178e-03\n",
      " 1.43484977e-03 1.41809887e-03 1.38733029e-03 1.30348983e-03\n",
      " 1.21929375e-03 1.18684155e-03 1.15766065e-03 1.10555151e-03\n",
      " 1.07907293e-03 1.00864583e-03 9.91079121e-04 9.71430317e-04\n",
      " 9.52064060e-04 9.09960327e-04 9.00511959e-04 8.66517320e-04\n",
      " 8.35310397e-04 7.93028754e-04 7.66928941e-04 7.54632399e-04\n",
      " 7.35319801e-04 7.28033458e-04 6.71546374e-04 6.67774095e-04\n",
      " 6.55984036e-04 6.28970527e-04 6.23024521e-04 5.94553365e-04\n",
      " 5.87520656e-04 5.73370494e-04 5.52638643e-04 5.30545870e-04\n",
      " 5.29084938e-04 5.14589411e-04 5.00431338e-04 4.79271227e-04\n",
      " 4.66564327e-04 4.56159613e-04 4.52455658e-04 4.47985294e-04\n",
      " 4.30145471e-04 4.20861922e-04 4.16917100e-04 4.05771142e-04\n",
      " 4.00202327e-04 3.86951266e-04 3.85329516e-04 3.72601568e-04\n",
      " 3.61026379e-04 3.59630393e-04 3.54835666e-04 3.47580292e-04\n",
      " 3.37847043e-04 3.35125384e-04 3.32499874e-04 3.22516899e-04\n",
      " 3.19387172e-04 3.12953140e-04 3.06322602e-04 2.95078877e-04\n",
      " 2.90060791e-04 2.87214119e-04 2.79518019e-04 2.77854915e-04\n",
      " 2.76309399e-04 2.72786638e-04 2.65850309e-04 2.57570775e-04\n",
      " 2.52800102e-04 2.50773661e-04 2.47481105e-04 2.43928484e-04\n",
      " 2.40465327e-04 2.35115984e-04 2.31998216e-04 2.28598340e-04\n",
      " 2.27993650e-04 2.24882612e-04 2.19012400e-04 2.15338282e-04\n",
      " 2.10147004e-04 2.08604264e-04 2.05809425e-04 2.04750247e-04\n",
      " 2.03047429e-04 1.96321691e-04 1.96056492e-04 1.92602008e-04\n",
      " 1.90427252e-04 1.87426933e-04 1.86043657e-04 1.82388227e-04\n",
      " 1.79409083e-04 1.76843870e-04 1.74818690e-04 1.73810720e-04\n",
      " 1.73637355e-04 1.69747950e-04 1.65453530e-04 1.64410877e-04\n",
      " 1.62234617e-04 1.61312768e-04 1.60662641e-04 1.57664057e-04\n",
      " 1.56910057e-04 1.54227985e-04 1.53938187e-04 1.49979765e-04\n",
      " 1.49575894e-04 1.48843751e-04 1.46787798e-04 1.44729924e-04\n",
      " 1.44268728e-04 1.42396038e-04 1.41531538e-04 1.41062166e-04\n",
      " 1.38809811e-04 1.36662901e-04 1.36182254e-04 1.33972190e-04\n",
      " 1.32392524e-04 1.31261229e-04 1.29932075e-04 1.28998986e-04\n",
      " 1.28095398e-04 1.24994606e-04 1.23799659e-04 1.22723099e-04\n",
      " 1.20813449e-04 1.19988107e-04 1.19610720e-04 1.18395165e-04\n",
      " 1.17513885e-04 1.17162714e-04 1.16432549e-04 1.15412401e-04\n",
      " 1.14703861e-04 1.12894465e-04 1.12640853e-04 1.11504736e-04\n",
      " 1.09764044e-04 1.07986531e-04 1.07646312e-04 1.06825885e-04\n",
      " 1.06384199e-04 1.04419073e-04 1.03841468e-04 1.02682718e-04\n",
      " 1.02190078e-04 1.01483701e-04 1.00401185e-04 9.98369257e-05\n",
      " 9.87585850e-05 9.79874577e-05 9.76912190e-05 9.64080924e-05\n",
      " 9.54588010e-05 9.48297332e-05 9.41697822e-05 9.29944665e-05\n",
      " 9.26683334e-05 9.19194564e-05 9.11744535e-05 9.08229014e-05\n",
      " 8.96320313e-05 8.91924321e-05 8.84430116e-05 8.79737261e-05\n",
      " 8.70725492e-05 8.66355694e-05 8.61902341e-05 8.55266460e-05\n",
      " 8.54085657e-05 8.46134940e-05 8.32807552e-05 8.32133510e-05\n",
      " 8.20876359e-05 8.19131492e-05 8.08833740e-05 8.05376735e-05\n",
      " 8.02478301e-05 7.98498750e-05 7.91789839e-05 7.89951935e-05\n",
      " 7.81926163e-05 7.75671856e-05 7.64044233e-05 7.62953461e-05\n",
      " 7.61293768e-05 7.51630623e-05 7.46730819e-05 7.41196871e-05\n",
      " 7.36457526e-05 7.34551102e-05 7.24944219e-05 7.22038788e-05\n",
      " 7.17486095e-05 7.14388056e-05 7.08583901e-05 7.03166003e-05\n",
      " 6.99538710e-05 6.94528175e-05 6.89935180e-05 6.82228996e-05\n",
      " 6.72746085e-05 6.71476986e-05 6.65315832e-05 6.63962659e-05\n",
      " 6.53834212e-05 6.51294049e-05 6.49327084e-05 6.45862389e-05\n",
      " 6.44900677e-05 6.37841995e-05 6.32791851e-05 6.28048401e-05\n",
      " 6.24084867e-05 6.19176621e-05 6.18296729e-05 6.17188783e-05\n",
      " 6.10218917e-05 6.06687031e-05 6.05764812e-05 6.03876719e-05\n",
      " 5.95541478e-05 5.92001440e-05 5.86548617e-05 5.85607490e-05\n",
      " 5.82817532e-05 5.75467595e-05 5.70250054e-05 5.69855701e-05\n",
      " 5.63793887e-05 5.60724123e-05 5.57319043e-05 5.55865778e-05\n",
      " 5.51845809e-05 5.51090641e-05 5.46315099e-05 5.45025126e-05\n",
      " 5.40613753e-05 5.36504389e-05 5.33603989e-05 5.32104003e-05\n",
      " 5.26592201e-05 5.23443971e-05 5.21305966e-05 5.20877787e-05\n",
      " 5.17497687e-05 5.14399994e-05 5.11563083e-05 5.07020857e-05\n",
      " 5.05371919e-05 5.00303619e-05 4.98667286e-05 4.92962066e-05\n",
      " 4.92542329e-05 4.91384077e-05 4.85691868e-05 4.84013613e-05\n",
      " 4.80958970e-05 4.79373175e-05 4.77221466e-05 4.74129122e-05\n",
      " 4.72085455e-05 4.70026900e-05 4.68010034e-05 4.63646047e-05\n",
      " 4.59582132e-05 4.57092275e-05 4.56565074e-05 4.51069925e-05\n",
      " 4.49152081e-05 4.47852538e-05 4.45786813e-05 4.43518533e-05\n",
      " 4.40360357e-05 4.39672322e-05 4.35801854e-05 4.33630919e-05\n",
      " 4.32220298e-05 4.29202054e-05 4.26334353e-05 4.23430887e-05\n",
      " 4.22757828e-05 4.21586537e-05 4.18603680e-05 4.16210965e-05\n",
      " 4.14249783e-05 4.12887474e-05 4.09954750e-05 4.08590677e-05\n",
      " 4.06286750e-05 4.04014769e-05 4.03595524e-05 3.99557253e-05\n",
      " 3.97696944e-05 3.96620395e-05 3.94583986e-05 3.91387300e-05\n",
      " 3.90495178e-05 3.87021520e-05 3.86617425e-05 3.83020402e-05\n",
      " 3.81394853e-05 3.78822627e-05 3.77191816e-05 3.76370210e-05\n",
      " 3.73693208e-05 3.70690200e-05 3.69593125e-05 3.68215203e-05\n",
      " 3.65058676e-05 3.63594251e-05 3.63305310e-05 3.61844672e-05\n",
      " 3.60859976e-05 3.59173626e-05 3.58830939e-05 3.55133506e-05\n",
      " 3.52705750e-05 3.50423680e-05 3.49594614e-05 3.48610206e-05\n",
      " 3.47308120e-05 3.45681935e-05 3.43424352e-05 3.41970914e-05\n",
      " 3.38689171e-05 3.38025508e-05 3.35088765e-05 3.33958181e-05\n",
      " 3.32719353e-05 3.31232840e-05 3.29497630e-05 3.28258339e-05\n",
      " 3.27200901e-05 3.27138770e-05 3.25701580e-05 3.23771478e-05\n",
      " 3.23136148e-05 3.20582714e-05 3.18234841e-05 3.16808984e-05\n",
      " 3.15637288e-05 3.13151160e-05 3.10358918e-05 3.10317690e-05\n",
      " 3.08631138e-05 3.07236216e-05 3.05146378e-05 3.03948373e-05\n",
      " 3.01763271e-05 3.01016632e-05 2.99558337e-05 2.98198282e-05\n",
      " 2.97195892e-05 2.94925241e-05 2.94389020e-05 2.93461626e-05\n",
      " 2.92692234e-05 2.91741392e-05 2.90388017e-05 2.88293726e-05\n",
      " 2.87373589e-05 2.85429696e-05 2.84310706e-05 2.83431710e-05\n",
      " 2.82417206e-05 2.80480598e-05 2.78833019e-05 2.77788679e-05\n",
      " 2.76459444e-05 2.75282226e-05 2.74689974e-05 2.72729948e-05\n",
      " 2.72294281e-05 2.70485636e-05 2.70102761e-05 2.67763822e-05\n",
      " 2.66814773e-05 2.66013289e-05 2.65178990e-05 2.62438846e-05\n",
      " 2.62071583e-05 2.61636321e-05 2.60123382e-05 2.57623175e-05\n",
      " 2.55871890e-05 2.54849117e-05 2.54434382e-05 2.53034487e-05\n",
      " 2.50930714e-05 2.49686306e-05 2.48178716e-05 2.47634486e-05\n",
      " 2.47209718e-05 2.46112903e-05 2.44923571e-05 2.43377443e-05\n",
      " 2.42284212e-05 2.40320530e-05 2.39782183e-05 2.38641553e-05\n",
      " 2.38285782e-05 2.36762262e-05 2.35948650e-05 2.32536140e-05\n",
      " 2.32096035e-05 2.31085376e-05 2.29433214e-05 2.27428520e-05\n",
      " 2.26639829e-05 2.26335623e-05 2.24581028e-05 2.24059479e-05\n",
      " 2.22976527e-05 2.22019773e-05 2.21719744e-05 2.19931352e-05\n",
      " 2.18327169e-05 2.15741398e-05 2.15069162e-05 2.14457337e-05\n",
      " 2.14009498e-05 2.12891361e-05 2.11873358e-05 2.10377001e-05\n",
      " 2.08295084e-05 2.06479992e-05 2.06350628e-05 2.05760703e-05\n",
      " 2.05045693e-05 2.03084193e-05 2.02537130e-05 2.01628312e-05\n",
      " 2.00831063e-05 2.00192191e-05 1.98521541e-05 1.98085382e-05\n",
      " 1.96145219e-05 1.95828291e-05 1.93753675e-05 1.93230015e-05\n",
      " 1.92433446e-05 1.91471055e-05 1.91033523e-05 1.87842215e-05\n",
      " 1.85650218e-05 1.85249331e-05 1.84329714e-05 1.84002696e-05\n",
      " 1.81458789e-05 1.80685305e-05 1.79421483e-05 1.78308781e-05]\n",
      "Gesamte erklärte Varianz: 0.9954752697509338\n",
      "✅ Reduzierter Chunk 11 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_11.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183420, 1024)\n",
      "Gesamte Datenform vor PCA: (183420, 1024)\n",
      "Gesamte Datenform nach PCA: (183420, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32692151e-01 9.59572567e-02 7.89016970e-02 5.08699557e-02\n",
      " 4.81399477e-02 4.30670036e-02 4.11149932e-02 3.69062683e-02\n",
      " 3.41731972e-02 2.98750897e-02 2.76746406e-02 2.53105408e-02\n",
      " 2.11793738e-02 1.88977778e-02 1.79646287e-02 1.74097765e-02\n",
      " 1.66920808e-02 1.39677193e-02 1.24457418e-02 1.13921260e-02\n",
      " 1.05133516e-02 9.66844826e-03 8.70619913e-03 8.40795010e-03\n",
      " 8.14136431e-03 7.26811931e-03 6.50768678e-03 6.47814020e-03\n",
      " 6.02567349e-03 5.66178691e-03 5.21220301e-03 4.64065347e-03\n",
      " 4.62645007e-03 4.41684634e-03 4.21790738e-03 3.84518459e-03\n",
      " 3.70632343e-03 3.51965524e-03 3.35781078e-03 3.17156177e-03\n",
      " 3.01116593e-03 2.80738468e-03 2.63425760e-03 2.54569784e-03\n",
      " 2.42141093e-03 2.30480413e-03 2.20896770e-03 2.18712975e-03\n",
      " 2.10290125e-03 1.99393032e-03 1.89193110e-03 1.79243995e-03\n",
      " 1.68804968e-03 1.66377864e-03 1.60524204e-03 1.49088114e-03\n",
      " 1.47274197e-03 1.41310011e-03 1.37932669e-03 1.29087644e-03\n",
      " 1.22610512e-03 1.17745195e-03 1.13809851e-03 1.09057522e-03\n",
      " 1.07253640e-03 1.00880860e-03 9.84311396e-04 9.70551998e-04\n",
      " 9.45962680e-04 9.03089678e-04 8.99949404e-04 8.70208227e-04\n",
      " 8.29791084e-04 7.97092749e-04 7.65511113e-04 7.60693500e-04\n",
      " 7.41494238e-04 7.25618412e-04 6.78285650e-04 6.66690265e-04\n",
      " 6.57788426e-04 6.37865776e-04 6.27428563e-04 5.93607743e-04\n",
      " 5.82911873e-04 5.68634558e-04 5.55078087e-04 5.27983104e-04\n",
      " 5.24538163e-04 5.12573955e-04 4.96919339e-04 4.78513409e-04\n",
      " 4.62832548e-04 4.54936317e-04 4.49982849e-04 4.43922495e-04\n",
      " 4.30243082e-04 4.19711627e-04 4.15774320e-04 4.09138606e-04\n",
      " 3.96624125e-04 3.92133305e-04 3.78039383e-04 3.71355876e-04\n",
      " 3.64188081e-04 3.59799419e-04 3.55735884e-04 3.46669076e-04\n",
      " 3.37551003e-04 3.32786436e-04 3.28289992e-04 3.23375336e-04\n",
      " 3.19367185e-04 3.11208409e-04 3.08367897e-04 2.97632034e-04\n",
      " 2.88648220e-04 2.85368550e-04 2.82239525e-04 2.76274940e-04\n",
      " 2.73204701e-04 2.71336006e-04 2.62381423e-04 2.58932570e-04\n",
      " 2.51850384e-04 2.47270021e-04 2.46534504e-04 2.42679497e-04\n",
      " 2.40386850e-04 2.39811976e-04 2.31868578e-04 2.29498167e-04\n",
      " 2.25688083e-04 2.23633958e-04 2.18548685e-04 2.16538094e-04\n",
      " 2.13552099e-04 2.09120852e-04 2.05366105e-04 2.02428851e-04\n",
      " 2.00130488e-04 1.98235479e-04 1.92694580e-04 1.91952132e-04\n",
      " 1.89832070e-04 1.88235156e-04 1.86228419e-04 1.83470867e-04\n",
      " 1.79357364e-04 1.76411258e-04 1.74363880e-04 1.74116725e-04\n",
      " 1.72341243e-04 1.68932244e-04 1.66458901e-04 1.64705787e-04\n",
      " 1.63599006e-04 1.62631828e-04 1.59824065e-04 1.57587068e-04\n",
      " 1.57319130e-04 1.53275812e-04 1.52555454e-04 1.51124251e-04\n",
      " 1.49915763e-04 1.47406604e-04 1.45810974e-04 1.44842837e-04\n",
      " 1.43554224e-04 1.42708854e-04 1.40806971e-04 1.39427842e-04\n",
      " 1.39065962e-04 1.36615843e-04 1.34915996e-04 1.33771952e-04\n",
      " 1.32701264e-04 1.31073938e-04 1.28875708e-04 1.28107453e-04\n",
      " 1.27641992e-04 1.25700209e-04 1.23735270e-04 1.21867247e-04\n",
      " 1.21483681e-04 1.20788631e-04 1.18851442e-04 1.17733030e-04\n",
      " 1.17257294e-04 1.16952579e-04 1.15896182e-04 1.14724805e-04\n",
      " 1.14207641e-04 1.11817581e-04 1.11537966e-04 1.11143963e-04\n",
      " 1.10440917e-04 1.08228811e-04 1.07191392e-04 1.06655145e-04\n",
      " 1.06133016e-04 1.04459345e-04 1.03995180e-04 1.03464441e-04\n",
      " 1.02173375e-04 1.01375878e-04 1.01052487e-04 9.98644349e-05\n",
      " 9.78961285e-05 9.77282534e-05 9.68982856e-05 9.55138046e-05\n",
      " 9.47714312e-05 9.39627675e-05 9.36045154e-05 9.32267356e-05\n",
      " 9.23450699e-05 9.17864022e-05 9.12833322e-05 9.07311332e-05\n",
      " 9.02523529e-05 8.89722513e-05 8.88132692e-05 8.72102623e-05\n",
      " 8.68992182e-05 8.64667607e-05 8.58958902e-05 8.54671357e-05\n",
      " 8.44369143e-05 8.39922541e-05 8.32258572e-05 8.28680448e-05\n",
      " 8.20454774e-05 8.13320398e-05 8.09006295e-05 8.04384839e-05\n",
      " 7.96720581e-05 7.90850621e-05 7.88075248e-05 7.83690150e-05\n",
      " 7.80580751e-05 7.73407031e-05 7.67703418e-05 7.64472571e-05\n",
      " 7.61176110e-05 7.56160627e-05 7.41465447e-05 7.40382770e-05\n",
      " 7.33342243e-05 7.28677682e-05 7.23184333e-05 7.20964358e-05\n",
      " 7.19145412e-05 7.13371442e-05 7.08094605e-05 7.00512334e-05\n",
      " 6.98854644e-05 6.89623478e-05 6.87366415e-05 6.81232092e-05\n",
      " 6.71452587e-05 6.70452418e-05 6.60611465e-05 6.58010192e-05\n",
      " 6.56064706e-05 6.48186539e-05 6.45233536e-05 6.43124306e-05\n",
      " 6.39649925e-05 6.38353108e-05 6.35118962e-05 6.32003604e-05\n",
      " 6.27189011e-05 6.23962966e-05 6.18325140e-05 6.16019591e-05\n",
      " 6.06292448e-05 6.04899757e-05 6.01252838e-05 5.95307776e-05\n",
      " 5.92101056e-05 5.88472652e-05 5.86960248e-05 5.85490198e-05\n",
      " 5.80897730e-05 5.76212975e-05 5.71518037e-05 5.69253684e-05\n",
      " 5.67920878e-05 5.64181382e-05 5.57261667e-05 5.54035333e-05\n",
      " 5.48227399e-05 5.43381388e-05 5.41049918e-05 5.38522359e-05\n",
      " 5.37686455e-05 5.34114754e-05 5.31326536e-05 5.29985109e-05\n",
      " 5.28392337e-05 5.24689178e-05 5.22900319e-05 5.21117940e-05\n",
      " 5.14833702e-05 5.11916514e-05 5.09083165e-05 5.06340020e-05\n",
      " 5.03870553e-05 5.00261532e-05 4.97915365e-05 4.95319242e-05\n",
      " 4.91462060e-05 4.90134461e-05 4.87208999e-05 4.83399580e-05\n",
      " 4.79580961e-05 4.77216858e-05 4.75171997e-05 4.73001984e-05\n",
      " 4.70152984e-05 4.67558482e-05 4.65421594e-05 4.63432076e-05\n",
      " 4.59772920e-05 4.55925400e-05 4.55113103e-05 4.52007320e-05\n",
      " 4.47797076e-05 4.46524646e-05 4.45223634e-05 4.42709933e-05\n",
      " 4.37596088e-05 4.36435618e-05 4.33055116e-05 4.30631503e-05\n",
      " 4.29076369e-05 4.26200898e-05 4.25579655e-05 4.22088553e-05\n",
      " 4.20116277e-05 4.19267355e-05 4.16205661e-05 4.14862700e-05\n",
      " 4.12493216e-05 4.11743147e-05 4.06517777e-05 4.05364018e-05\n",
      " 4.02946654e-05 4.02482212e-05 4.00341274e-05 3.98678723e-05\n",
      " 3.97462216e-05 3.93388810e-05 3.92282960e-05 3.89512332e-05\n",
      " 3.86427754e-05 3.83862500e-05 3.83506805e-05 3.82847606e-05\n",
      " 3.81674349e-05 3.79538879e-05 3.78301773e-05 3.75024551e-05\n",
      " 3.72906150e-05 3.72111905e-05 3.70223902e-05 3.67709536e-05\n",
      " 3.64041701e-05 3.63671309e-05 3.61837869e-05 3.59145728e-05\n",
      " 3.58057756e-05 3.56908742e-05 3.55463774e-05 3.54015161e-05\n",
      " 3.51077462e-05 3.50157574e-05 3.48601630e-05 3.48504917e-05\n",
      " 3.45189044e-05 3.43072148e-05 3.42253572e-05 3.40697079e-05\n",
      " 3.39531951e-05 3.37321090e-05 3.36168084e-05 3.34371124e-05\n",
      " 3.32227872e-05 3.31379615e-05 3.29094114e-05 3.28660192e-05\n",
      " 3.28247245e-05 3.24735863e-05 3.23983046e-05 3.22347082e-05\n",
      " 3.20623634e-05 3.19022588e-05 3.17913758e-05 3.15595538e-05\n",
      " 3.13723447e-05 3.13021575e-05 3.11843602e-05 3.10035794e-05\n",
      " 3.09277567e-05 3.08043239e-05 3.06751079e-05 3.03777506e-05\n",
      " 3.01973893e-05 3.01049723e-05 2.99993777e-05 2.97973999e-05\n",
      " 2.96081483e-05 2.95614176e-05 2.94303849e-05 2.92708879e-05\n",
      " 2.90653458e-05 2.90184560e-05 2.87581437e-05 2.87422437e-05\n",
      " 2.85827640e-05 2.84917242e-05 2.83330516e-05 2.81563702e-05\n",
      " 2.81068709e-05 2.80081819e-05 2.79131844e-05 2.77252058e-05\n",
      " 2.75509689e-05 2.74745416e-05 2.73879222e-05 2.73003336e-05\n",
      " 2.70438921e-05 2.69526005e-05 2.67930746e-05 2.66369132e-05\n",
      " 2.65680829e-05 2.64122339e-05 2.62569057e-05 2.61951690e-05\n",
      " 2.61412579e-05 2.59295133e-05 2.58603705e-05 2.57847503e-05\n",
      " 2.56818115e-05 2.55645466e-05 2.53253473e-05 2.52610966e-05\n",
      " 2.50939534e-05 2.49653797e-05 2.48771952e-05 2.47395548e-05\n",
      " 2.45768870e-05 2.45493312e-05 2.44323960e-05 2.43068831e-05\n",
      " 2.41056647e-05 2.39823606e-05 2.38443311e-05 2.37538640e-05\n",
      " 2.37029616e-05 2.34367490e-05 2.33406781e-05 2.32361207e-05\n",
      " 2.31994027e-05 2.29991794e-05 2.28814863e-05 2.27880466e-05\n",
      " 2.26581913e-05 2.25806965e-05 2.23633510e-05 2.23100734e-05\n",
      " 2.22547563e-05 2.21297656e-05 2.20573263e-05 2.18666022e-05\n",
      " 2.17503715e-05 2.16837572e-05 2.14457933e-05 2.13989498e-05\n",
      " 2.13579429e-05 2.12636252e-05 2.11461100e-05 2.10899568e-05\n",
      " 2.09581849e-05 2.07687150e-05 2.06755530e-05 2.05448949e-05\n",
      " 2.04551164e-05 2.03440540e-05 2.02496163e-05 2.00932755e-05\n",
      " 2.00173660e-05 1.99028509e-05 1.98224601e-05 1.97071132e-05\n",
      " 1.95916202e-05 1.94168033e-05 1.93565088e-05 1.92640383e-05\n",
      " 1.91052428e-05 1.90548294e-05 1.87538587e-05 1.87287909e-05\n",
      " 1.85891572e-05 1.85103344e-05 1.84123615e-05 1.83551466e-05\n",
      " 1.81431083e-05 1.80689794e-05 1.79035257e-05 1.77776627e-05]\n",
      "Gesamte erklärte Varianz: 0.9954912029975503\n",
      "✅ Reduzierter Chunk 12 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_12.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183521, 1024)\n",
      "Gesamte Datenform vor PCA: (183521, 1024)\n",
      "Gesamte Datenform nach PCA: (183521, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32354085e-01 9.60670487e-02 7.87056387e-02 5.12422657e-02\n",
      " 4.81518005e-02 4.33986475e-02 4.12215252e-02 3.68138750e-02\n",
      " 3.46001908e-02 2.96304348e-02 2.76740760e-02 2.51660098e-02\n",
      " 2.08722633e-02 1.89306991e-02 1.82991279e-02 1.72739453e-02\n",
      " 1.66298925e-02 1.39209349e-02 1.24229351e-02 1.13325456e-02\n",
      " 1.04476695e-02 9.64170292e-03 8.74739151e-03 8.39558268e-03\n",
      " 8.18646950e-03 7.26938623e-03 6.59558061e-03 6.44047594e-03\n",
      " 6.05072698e-03 5.61197719e-03 5.19151386e-03 4.60786554e-03\n",
      " 4.60437509e-03 4.36207609e-03 4.22574128e-03 3.80406297e-03\n",
      " 3.70295572e-03 3.51924769e-03 3.41941320e-03 3.20626578e-03\n",
      " 2.99334950e-03 2.78269334e-03 2.63174255e-03 2.52277149e-03\n",
      " 2.40501854e-03 2.30450007e-03 2.23686069e-03 2.18258806e-03\n",
      " 2.11331885e-03 1.98943807e-03 1.88981063e-03 1.77626361e-03\n",
      " 1.69422365e-03 1.66544583e-03 1.60280766e-03 1.48149361e-03\n",
      " 1.43883990e-03 1.40211085e-03 1.37871826e-03 1.29956350e-03\n",
      " 1.22334115e-03 1.17736609e-03 1.14560889e-03 1.09071516e-03\n",
      " 1.08184493e-03 1.00519281e-03 9.93562494e-04 9.62347776e-04\n",
      " 9.52344991e-04 9.07296155e-04 9.03940903e-04 8.61400171e-04\n",
      " 8.31798164e-04 7.91872218e-04 7.63941030e-04 7.63220149e-04\n",
      " 7.40476629e-04 7.23280822e-04 6.70508063e-04 6.66986400e-04\n",
      " 6.61194302e-04 6.34534391e-04 6.27286687e-04 5.94171583e-04\n",
      " 5.87120941e-04 5.74573704e-04 5.51491343e-04 5.32425663e-04\n",
      " 5.29848336e-04 5.13678820e-04 5.01952986e-04 4.80304840e-04\n",
      " 4.64595755e-04 4.54349650e-04 4.52058426e-04 4.45370609e-04\n",
      " 4.30287023e-04 4.22009416e-04 4.19592388e-04 4.10679189e-04\n",
      " 4.00595652e-04 3.85803492e-04 3.81325570e-04 3.74566865e-04\n",
      " 3.63450091e-04 3.60871630e-04 3.53676526e-04 3.42858796e-04\n",
      " 3.36431083e-04 3.34925192e-04 3.30240299e-04 3.20808213e-04\n",
      " 3.17587168e-04 3.12832173e-04 3.07302123e-04 2.99489739e-04\n",
      " 2.88836707e-04 2.84538114e-04 2.81285699e-04 2.78774770e-04\n",
      " 2.76254904e-04 2.71640946e-04 2.65365499e-04 2.58156688e-04\n",
      " 2.54645467e-04 2.50121636e-04 2.49485025e-04 2.43797918e-04\n",
      " 2.40246553e-04 2.39542526e-04 2.33266754e-04 2.28446496e-04\n",
      " 2.26719835e-04 2.21686540e-04 2.20104482e-04 2.17506042e-04\n",
      " 2.11216726e-04 2.10006893e-04 2.07048752e-04 2.04401139e-04\n",
      " 2.02307370e-04 1.98250661e-04 1.95091453e-04 1.92723447e-04\n",
      " 1.90494696e-04 1.88577698e-04 1.86575354e-04 1.81290001e-04\n",
      " 1.79497973e-04 1.77513640e-04 1.74777022e-04 1.72822392e-04\n",
      " 1.72151101e-04 1.70659888e-04 1.65695294e-04 1.64969320e-04\n",
      " 1.63070936e-04 1.61989070e-04 1.61628213e-04 1.57762443e-04\n",
      " 1.56696899e-04 1.54570684e-04 1.53400624e-04 1.50681936e-04\n",
      " 1.49575044e-04 1.48772683e-04 1.47080992e-04 1.45343451e-04\n",
      " 1.44861802e-04 1.44138838e-04 1.41706715e-04 1.41515348e-04\n",
      " 1.38794993e-04 1.36522220e-04 1.34642715e-04 1.33996763e-04\n",
      " 1.33280188e-04 1.30460168e-04 1.30018096e-04 1.28865782e-04\n",
      " 1.27492061e-04 1.25967568e-04 1.23916236e-04 1.22544773e-04\n",
      " 1.21318432e-04 1.19548977e-04 1.19314895e-04 1.18032934e-04\n",
      " 1.17281691e-04 1.16409957e-04 1.16389317e-04 1.14951769e-04\n",
      " 1.14194749e-04 1.12985066e-04 1.12702853e-04 1.10944394e-04\n",
      " 1.09278448e-04 1.08782238e-04 1.07896393e-04 1.06781687e-04\n",
      " 1.06581951e-04 1.05639779e-04 1.03769015e-04 1.03254503e-04\n",
      " 1.02833835e-04 1.01132826e-04 1.00417003e-04 9.96948028e-05\n",
      " 9.94615780e-05 9.87540436e-05 9.78449320e-05 9.57939528e-05\n",
      " 9.49168419e-05 9.45906362e-05 9.38928080e-05 9.35129205e-05\n",
      " 9.32087662e-05 9.18950938e-05 9.16603641e-05 9.10844499e-05\n",
      " 8.95893245e-05 8.92206364e-05 8.88243405e-05 8.85320686e-05\n",
      " 8.74193541e-05 8.66076279e-05 8.56899388e-05 8.54385055e-05\n",
      " 8.53047034e-05 8.39509158e-05 8.34272538e-05 8.29600285e-05\n",
      " 8.23636255e-05 8.20181291e-05 8.16564442e-05 8.06900624e-05\n",
      " 8.03546136e-05 7.92887686e-05 7.89761758e-05 7.88218848e-05\n",
      " 7.83174730e-05 7.73302725e-05 7.70705570e-05 7.61691200e-05\n",
      " 7.60643899e-05 7.50990210e-05 7.48641929e-05 7.42556645e-05\n",
      " 7.39520023e-05 7.34010392e-05 7.28223353e-05 7.20770472e-05\n",
      " 7.18753137e-05 7.08597704e-05 7.05619307e-05 6.95046400e-05\n",
      " 6.93580816e-05 6.89534687e-05 6.85973632e-05 6.81080402e-05\n",
      " 6.73623121e-05 6.70104839e-05 6.67215457e-05 6.63391348e-05\n",
      " 6.60229882e-05 6.51146696e-05 6.50212199e-05 6.47352046e-05\n",
      " 6.40806401e-05 6.39758637e-05 6.36500168e-05 6.32545312e-05\n",
      " 6.29432870e-05 6.22190374e-05 6.18902330e-05 6.15706948e-05\n",
      " 6.14279505e-05 6.07398458e-05 6.01616454e-05 6.01016260e-05\n",
      " 5.92991603e-05 5.90461064e-05 5.84835389e-05 5.83424499e-05\n",
      " 5.80090904e-05 5.72995419e-05 5.71160111e-05 5.64689649e-05\n",
      " 5.63696753e-05 5.61046060e-05 5.57719759e-05 5.55070629e-05\n",
      " 5.52808818e-05 5.46909900e-05 5.45540683e-05 5.41730753e-05\n",
      " 5.40055766e-05 5.36000087e-05 5.32798975e-05 5.31079769e-05\n",
      " 5.28549579e-05 5.26892476e-05 5.22750269e-05 5.19254212e-05\n",
      " 5.16851699e-05 5.11867774e-05 5.09453976e-05 5.08498643e-05\n",
      " 5.06863765e-05 5.04707350e-05 4.98682953e-05 4.94567313e-05\n",
      " 4.93645317e-05 4.90401550e-05 4.86102436e-05 4.82177966e-05\n",
      " 4.81865454e-05 4.79352945e-05 4.75911379e-05 4.73361944e-05\n",
      " 4.71348024e-05 4.69003013e-05 4.64222732e-05 4.61754904e-05\n",
      " 4.59818077e-05 4.57554733e-05 4.54204354e-05 4.50299099e-05\n",
      " 4.48554283e-05 4.46457626e-05 4.44109924e-05 4.41994429e-05\n",
      " 4.38845552e-05 4.37306028e-05 4.34207273e-05 4.32874982e-05\n",
      " 4.27302843e-05 4.26125346e-05 4.24193294e-05 4.22563105e-05\n",
      " 4.20585532e-05 4.19038340e-05 4.14799130e-05 4.14117820e-05\n",
      " 4.10412186e-05 4.10126501e-05 4.07061923e-05 4.06066191e-05\n",
      " 4.02794932e-05 4.02291480e-05 4.00703745e-05 3.98511879e-05\n",
      " 3.97038568e-05 3.95093899e-05 3.92685078e-05 3.89890471e-05\n",
      " 3.87891252e-05 3.86710022e-05 3.84541944e-05 3.83707663e-05\n",
      " 3.82139287e-05 3.78858333e-05 3.77294501e-05 3.74906545e-05\n",
      " 3.73394889e-05 3.72493643e-05 3.71485583e-05 3.67570577e-05\n",
      " 3.65462026e-05 3.63744296e-05 3.62099232e-05 3.60154621e-05\n",
      " 3.59419022e-05 3.57544298e-05 3.55700250e-05 3.54958053e-05\n",
      " 3.52117491e-05 3.51258872e-05 3.50083488e-05 3.47366206e-05\n",
      " 3.45506386e-05 3.44937186e-05 3.43824136e-05 3.43003977e-05\n",
      " 3.38141016e-05 3.37980029e-05 3.36125302e-05 3.32931049e-05\n",
      " 3.31799275e-05 3.30917592e-05 3.30031857e-05 3.28949627e-05\n",
      " 3.28636334e-05 3.25696194e-05 3.24010731e-05 3.23082630e-05\n",
      " 3.21128671e-05 3.20624902e-05 3.17057712e-05 3.16125907e-05\n",
      " 3.13903684e-05 3.12541672e-05 3.11019685e-05 3.08461742e-05\n",
      " 3.08230173e-05 3.07720442e-05 3.05160820e-05 3.03973686e-05\n",
      " 3.02952083e-05 3.01492604e-05 3.00744995e-05 2.97595048e-05\n",
      " 2.96630888e-05 2.95875784e-05 2.94031331e-05 2.92782586e-05\n",
      " 2.91137956e-05 2.89641001e-05 2.87553490e-05 2.86865408e-05\n",
      " 2.86650334e-05 2.84052223e-05 2.81928740e-05 2.81726688e-05\n",
      " 2.79989483e-05 2.77890193e-05 2.77276195e-05 2.76629178e-05\n",
      " 2.76277743e-05 2.74471894e-05 2.73178496e-05 2.72762267e-05\n",
      " 2.71889873e-05 2.70213063e-05 2.69854973e-05 2.67965722e-05\n",
      " 2.66833774e-05 2.64798266e-05 2.62390776e-05 2.62128242e-05\n",
      " 2.61177887e-05 2.60150004e-05 2.58092994e-05 2.56770773e-05\n",
      " 2.55620912e-05 2.55050236e-05 2.53399529e-05 2.52766258e-05\n",
      " 2.51778455e-05 2.50432649e-05 2.48918273e-05 2.48093513e-05\n",
      " 2.47980420e-05 2.46164326e-05 2.43496878e-05 2.42892112e-05\n",
      " 2.41153575e-05 2.40697830e-05 2.39227311e-05 2.37154616e-05\n",
      " 2.36306849e-05 2.35763477e-05 2.35028572e-05 2.33302522e-05\n",
      " 2.32450733e-05 2.31134821e-05 2.27481321e-05 2.26552756e-05\n",
      " 2.26191005e-05 2.23525336e-05 2.22770738e-05 2.21844677e-05\n",
      " 2.21264654e-05 2.19313531e-05 2.18311230e-05 2.17024243e-05\n",
      " 2.16322039e-05 2.14953371e-05 2.13607290e-05 2.12744113e-05\n",
      " 2.12222604e-05 2.11330937e-05 2.10043717e-05 2.09340920e-05\n",
      " 2.08687811e-05 2.06931058e-05 2.05351960e-05 2.05283534e-05\n",
      " 2.04262943e-05 2.03064523e-05 2.01757003e-05 2.01108409e-05\n",
      " 2.00672169e-05 1.99133094e-05 1.97198770e-05 1.97032284e-05\n",
      " 1.96573255e-05 1.94976085e-05 1.94265460e-05 1.93224829e-05\n",
      " 1.92027378e-05 1.91102504e-05 1.90157690e-05 1.89541551e-05\n",
      " 1.86264735e-05 1.85788703e-05 1.84615634e-05 1.83873899e-05\n",
      " 1.83260509e-05 1.81005803e-05 1.79304669e-05 1.78126724e-05]\n",
      "Gesamte erklärte Varianz: 0.9954939695480092\n",
      "✅ Reduzierter Chunk 13 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_13.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183466, 1024)\n",
      "Gesamte Datenform vor PCA: (183466, 1024)\n",
      "Gesamte Datenform nach PCA: (183466, 512)\n",
      "Erklärte Varianz pro Komponente: [1.32325704e-01 9.64117087e-02 7.89321619e-02 5.07786797e-02\n",
      " 4.81346867e-02 4.32456134e-02 4.13545615e-02 3.70482792e-02\n",
      " 3.45135810e-02 2.96092510e-02 2.74793609e-02 2.52722063e-02\n",
      " 2.11740709e-02 1.88823509e-02 1.80531200e-02 1.73357939e-02\n",
      " 1.65506626e-02 1.38821224e-02 1.22483026e-02 1.13610827e-02\n",
      " 1.03581853e-02 9.59821209e-03 8.82083797e-03 8.40572115e-03\n",
      " 8.20264904e-03 7.16814570e-03 6.65875016e-03 6.48111169e-03\n",
      " 6.01704876e-03 5.58765278e-03 5.21008650e-03 4.62074621e-03\n",
      " 4.54816173e-03 4.39441715e-03 4.18384367e-03 3.84742218e-03\n",
      " 3.73990117e-03 3.52465253e-03 3.34702258e-03 3.16672676e-03\n",
      " 3.01355023e-03 2.77378151e-03 2.65564834e-03 2.52428030e-03\n",
      " 2.40529801e-03 2.30112498e-03 2.22556451e-03 2.17699909e-03\n",
      " 2.10691884e-03 1.98329675e-03 1.91130533e-03 1.79835854e-03\n",
      " 1.70022934e-03 1.67210522e-03 1.60256303e-03 1.49595789e-03\n",
      " 1.46138816e-03 1.40810396e-03 1.37353311e-03 1.30947179e-03\n",
      " 1.21983639e-03 1.16612051e-03 1.15783195e-03 1.09706445e-03\n",
      " 1.08260550e-03 1.01168326e-03 9.90224719e-04 9.69305349e-04\n",
      " 9.57177972e-04 9.05188200e-04 9.03726309e-04 8.64960679e-04\n",
      " 8.35227745e-04 8.05427136e-04 7.73985498e-04 7.60262317e-04\n",
      " 7.43716216e-04 7.30719861e-04 6.75539034e-04 6.70291139e-04\n",
      " 6.64189078e-04 6.36501365e-04 6.20825447e-04 5.91645292e-04\n",
      " 5.90306622e-04 5.72496930e-04 5.43085517e-04 5.30683880e-04\n",
      " 5.26051070e-04 5.15750387e-04 4.99341300e-04 4.77003041e-04\n",
      " 4.64579602e-04 4.61431735e-04 4.48726999e-04 4.45308204e-04\n",
      " 4.32053187e-04 4.20041764e-04 4.17373587e-04 4.12781187e-04\n",
      " 4.02525219e-04 3.91206507e-04 3.83046679e-04 3.72808740e-04\n",
      " 3.67723365e-04 3.64798610e-04 3.58112037e-04 3.47604119e-04\n",
      " 3.40165122e-04 3.37001772e-04 3.31656277e-04 3.22314349e-04\n",
      " 3.17969293e-04 3.11988138e-04 3.07794377e-04 2.98438077e-04\n",
      " 2.89037224e-04 2.83666548e-04 2.80780305e-04 2.78387107e-04\n",
      " 2.76414535e-04 2.71926516e-04 2.63111470e-04 2.58459936e-04\n",
      " 2.54286472e-04 2.49678427e-04 2.48555577e-04 2.44614830e-04\n",
      " 2.41548107e-04 2.37660778e-04 2.32239485e-04 2.29502454e-04\n",
      " 2.26600288e-04 2.24674675e-04 2.20729045e-04 2.16582197e-04\n",
      " 2.10298892e-04 2.08975543e-04 2.06126354e-04 2.04559086e-04\n",
      " 2.01870080e-04 1.96742674e-04 1.95672085e-04 1.93048056e-04\n",
      " 1.91633090e-04 1.88979032e-04 1.87117529e-04 1.83953392e-04\n",
      " 1.80475382e-04 1.78000450e-04 1.75687053e-04 1.74106859e-04\n",
      " 1.71955254e-04 1.71113354e-04 1.66615464e-04 1.64833532e-04\n",
      " 1.63842918e-04 1.61670945e-04 1.61038664e-04 1.58736308e-04\n",
      " 1.57598322e-04 1.53762997e-04 1.52740837e-04 1.51656604e-04\n",
      " 1.50952413e-04 1.49777639e-04 1.46694033e-04 1.45672093e-04\n",
      " 1.44933186e-04 1.43735788e-04 1.42557820e-04 1.40481654e-04\n",
      " 1.39745662e-04 1.37579765e-04 1.35094892e-04 1.34479830e-04\n",
      " 1.32308471e-04 1.31141728e-04 1.29476595e-04 1.29220710e-04\n",
      " 1.26680916e-04 1.25827976e-04 1.24764769e-04 1.22888003e-04\n",
      " 1.22302694e-04 1.20918972e-04 1.20337204e-04 1.18587095e-04\n",
      " 1.17599015e-04 1.17336223e-04 1.15798510e-04 1.15117359e-04\n",
      " 1.14288224e-04 1.13163812e-04 1.12414270e-04 1.10312563e-04\n",
      " 1.09750410e-04 1.09157937e-04 1.08219536e-04 1.07165518e-04\n",
      " 1.06242231e-04 1.05732800e-04 1.04364586e-04 1.03886643e-04\n",
      " 1.03102233e-04 1.02284207e-04 1.01259050e-04 1.00646291e-04\n",
      " 9.91664276e-05 9.82071279e-05 9.78553680e-05 9.64982511e-05\n",
      " 9.55599260e-05 9.46854214e-05 9.37039494e-05 9.31884662e-05\n",
      " 9.24510400e-05 9.21275453e-05 9.17777064e-05 9.08467180e-05\n",
      " 9.03768871e-05 8.92251824e-05 8.86018348e-05 8.83511581e-05\n",
      " 8.74494415e-05 8.70273054e-05 8.66213993e-05 8.57782207e-05\n",
      " 8.55945354e-05 8.45684178e-05 8.42651570e-05 8.39268556e-05\n",
      " 8.28430912e-05 8.20965635e-05 8.11490616e-05 8.08109569e-05\n",
      " 7.99401902e-05 7.95661597e-05 7.91448221e-05 7.86599589e-05\n",
      " 7.85135129e-05 7.76390778e-05 7.75008885e-05 7.68651124e-05\n",
      " 7.56636067e-05 7.50658164e-05 7.48698471e-05 7.45427997e-05\n",
      " 7.38558688e-05 7.33204815e-05 7.29942846e-05 7.27137516e-05\n",
      " 7.20965431e-05 7.15721783e-05 7.10008419e-05 7.08877874e-05\n",
      " 7.01178203e-05 6.95454714e-05 6.93096070e-05 6.84658845e-05\n",
      " 6.79623786e-05 6.74620260e-05 6.66414942e-05 6.64015275e-05\n",
      " 6.58930455e-05 6.53132557e-05 6.46786542e-05 6.44230072e-05\n",
      " 6.41332801e-05 6.39085655e-05 6.37597009e-05 6.32560966e-05\n",
      " 6.31999946e-05 6.28694466e-05 6.21618188e-05 6.17894952e-05\n",
      " 6.15671992e-05 6.09860439e-05 6.07454696e-05 6.00598984e-05\n",
      " 5.99402073e-05 5.93474624e-05 5.87996003e-05 5.81989805e-05\n",
      " 5.81767040e-05 5.78293995e-05 5.72602098e-05 5.67446804e-05\n",
      " 5.65545667e-05 5.61068271e-05 5.58397583e-05 5.57434951e-05\n",
      " 5.51771753e-05 5.49060794e-05 5.47000481e-05 5.42217001e-05\n",
      " 5.39618351e-05 5.36612127e-05 5.31820605e-05 5.31235399e-05\n",
      " 5.26801457e-05 5.24236308e-05 5.22608563e-05 5.20470196e-05\n",
      " 5.18388013e-05 5.12916045e-05 5.10633084e-05 5.06774337e-05\n",
      " 5.02809358e-05 5.01472134e-05 5.00044704e-05 4.98546106e-05\n",
      " 4.96878958e-05 4.88836634e-05 4.88339551e-05 4.85805588e-05\n",
      " 4.83153706e-05 4.80222933e-05 4.76607231e-05 4.73290121e-05\n",
      " 4.72676246e-05 4.67823072e-05 4.67099203e-05 4.64451312e-05\n",
      " 4.60287234e-05 4.59457218e-05 4.56667106e-05 4.52477713e-05\n",
      " 4.52228188e-05 4.47235685e-05 4.45556039e-05 4.45069371e-05\n",
      " 4.39577876e-05 4.38416462e-05 4.36587853e-05 4.33806073e-05\n",
      " 4.31424458e-05 4.30393839e-05 4.29200949e-05 4.27200812e-05\n",
      " 4.21458663e-05 4.20989786e-05 4.18798101e-05 4.17746709e-05\n",
      " 4.16282046e-05 4.12924173e-05 4.09835556e-05 4.09106508e-05\n",
      " 4.08157415e-05 4.04884800e-05 4.03532763e-05 4.00718436e-05\n",
      " 3.98529903e-05 3.95550180e-05 3.93777379e-05 3.91509346e-05\n",
      " 3.91247698e-05 3.88788897e-05 3.85494324e-05 3.82228566e-05\n",
      " 3.79508783e-05 3.78625160e-05 3.77379442e-05 3.76831210e-05\n",
      " 3.74492818e-05 3.72381125e-05 3.70880877e-05 3.66989468e-05\n",
      " 3.66207564e-05 3.64520455e-05 3.63424048e-05 3.62300244e-05\n",
      " 3.60175879e-05 3.58950789e-05 3.56261355e-05 3.54580205e-05\n",
      " 3.53057796e-05 3.52105752e-05 3.50739598e-05 3.50420668e-05\n",
      " 3.48318667e-05 3.45241506e-05 3.44030100e-05 3.42612073e-05\n",
      " 3.39281684e-05 3.37621538e-05 3.36144290e-05 3.34786380e-05\n",
      " 3.33267646e-05 3.32978804e-05 3.31351408e-05 3.30328918e-05\n",
      " 3.29631086e-05 3.26570531e-05 3.24260288e-05 3.22229253e-05\n",
      " 3.21613034e-05 3.19066979e-05 3.18231930e-05 3.16839535e-05\n",
      " 3.14289112e-05 3.13758816e-05 3.12708900e-05 3.09006031e-05\n",
      " 3.08339531e-05 3.07267339e-05 3.05483833e-05 3.04890180e-05\n",
      " 3.04475780e-05 3.01642589e-05 3.00048983e-05 2.98425058e-05\n",
      " 2.96894694e-05 2.94398486e-05 2.94265435e-05 2.93201748e-05\n",
      " 2.92304614e-05 2.91146932e-05 2.89153218e-05 2.88159919e-05\n",
      " 2.86764689e-05 2.85254894e-05 2.83469884e-05 2.82381462e-05\n",
      " 2.81248429e-05 2.80048827e-05 2.79084892e-05 2.77221972e-05\n",
      " 2.76405641e-05 2.76178335e-05 2.75832586e-05 2.74664085e-05\n",
      " 2.70804991e-05 2.70208300e-05 2.68321079e-05 2.67600131e-05\n",
      " 2.66537428e-05 2.65300159e-05 2.63954053e-05 2.62773921e-05\n",
      " 2.61519669e-05 2.60426994e-05 2.59471363e-05 2.58091177e-05\n",
      " 2.56246165e-05 2.56082823e-05 2.54886346e-05 2.52942710e-05\n",
      " 2.51789136e-05 2.51224558e-05 2.51112886e-05 2.49189444e-05\n",
      " 2.48130820e-05 2.46096863e-05 2.44783796e-05 2.42948331e-05\n",
      " 2.42492083e-05 2.41530665e-05 2.40435358e-05 2.39481216e-05\n",
      " 2.38047870e-05 2.36540014e-05 2.35726388e-05 2.34637807e-05\n",
      " 2.33242244e-05 2.30001149e-05 2.29084472e-05 2.28597543e-05\n",
      " 2.28067248e-05 2.26394300e-05 2.25576884e-05 2.24563956e-05\n",
      " 2.24298490e-05 2.21765337e-05 2.21567554e-05 2.20001750e-05\n",
      " 2.19479555e-05 2.18013258e-05 2.16947401e-05 2.14910161e-05\n",
      " 2.13611674e-05 2.12847027e-05 2.12058875e-05 2.11584632e-05\n",
      " 2.09501565e-05 2.08688489e-05 2.07784065e-05 2.06436700e-05\n",
      " 2.05351995e-05 2.04831594e-05 2.03689549e-05 2.02683290e-05\n",
      " 2.02206342e-05 2.00906177e-05 1.99396151e-05 1.98864742e-05\n",
      " 1.96976754e-05 1.96149964e-05 1.95628926e-05 1.94801790e-05\n",
      " 1.93800897e-05 1.91805967e-05 1.91272272e-05 1.90102830e-05\n",
      " 1.87877614e-05 1.87552421e-05 1.85770550e-05 1.84633842e-05\n",
      " 1.83525487e-05 1.83290369e-05 1.81334640e-05 1.79713131e-05]\n",
      "Gesamte erklärte Varianz: 0.9954795816272249\n",
      "✅ Reduzierter Chunk 14 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_14.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (183351, 1024)\n",
      "Gesamte Datenform vor PCA: (183351, 1024)\n",
      "Gesamte Datenform nach PCA: (183351, 512)\n",
      "Erklärte Varianz pro Komponente: [1.31750139e-01 9.58323143e-02 7.86854533e-02 5.12899857e-02\n",
      " 4.81776747e-02 4.33935882e-02 4.13863208e-02 3.68588700e-02\n",
      " 3.47365084e-02 2.97435207e-02 2.76450501e-02 2.55926550e-02\n",
      " 2.11292639e-02 1.86774978e-02 1.79329762e-02 1.75665311e-02\n",
      " 1.64655756e-02 1.39053672e-02 1.24863171e-02 1.13458979e-02\n",
      " 1.05284904e-02 9.50415616e-03 8.65890017e-03 8.37674358e-03\n",
      " 8.13367079e-03 7.26379589e-03 6.68027516e-03 6.42337511e-03\n",
      " 6.02129028e-03 5.59726186e-03 5.23893794e-03 4.63897868e-03\n",
      " 4.58982084e-03 4.38874676e-03 4.21292997e-03 3.88451192e-03\n",
      " 3.71672555e-03 3.50877376e-03 3.36031379e-03 3.17111927e-03\n",
      " 2.99628652e-03 2.78706316e-03 2.65973955e-03 2.52826780e-03\n",
      " 2.41639108e-03 2.30679479e-03 2.24500645e-03 2.18472925e-03\n",
      " 2.10480647e-03 1.99022833e-03 1.90191392e-03 1.77424390e-03\n",
      " 1.69800270e-03 1.67298943e-03 1.60194225e-03 1.49903509e-03\n",
      " 1.45195716e-03 1.39709624e-03 1.38018273e-03 1.30295740e-03\n",
      " 1.24766047e-03 1.16068363e-03 1.14299853e-03 1.10269388e-03\n",
      " 1.07758202e-03 1.01172367e-03 9.93477958e-04 9.73331559e-04\n",
      " 9.49285296e-04 9.02014900e-04 8.96843099e-04 8.66284688e-04\n",
      " 8.36323921e-04 7.98647109e-04 7.70649975e-04 7.60069111e-04\n",
      " 7.36763284e-04 7.29641088e-04 6.74580266e-04 6.65863382e-04\n",
      " 6.62273263e-04 6.42501456e-04 6.27941117e-04 5.92290887e-04\n",
      " 5.88333999e-04 5.73793516e-04 5.55981731e-04 5.35684579e-04\n",
      " 5.30556843e-04 5.20386628e-04 4.98518301e-04 4.74316379e-04\n",
      " 4.71694283e-04 4.54054173e-04 4.53054768e-04 4.48562629e-04\n",
      " 4.31985752e-04 4.22307464e-04 4.19799602e-04 4.06511847e-04\n",
      " 4.02960700e-04 3.91953451e-04 3.81211398e-04 3.76156446e-04\n",
      " 3.63378785e-04 3.61094714e-04 3.57298276e-04 3.43983652e-04\n",
      " 3.42896489e-04 3.35542308e-04 3.33904760e-04 3.25305928e-04\n",
      " 3.18348533e-04 3.13407190e-04 3.08728542e-04 2.98313729e-04\n",
      " 2.90646425e-04 2.88499083e-04 2.82326558e-04 2.79767619e-04\n",
      " 2.75428409e-04 2.71800359e-04 2.61202971e-04 2.57086420e-04\n",
      " 2.54530929e-04 2.51289366e-04 2.50217963e-04 2.44798070e-04\n",
      " 2.39265031e-04 2.35395113e-04 2.30710286e-04 2.30002760e-04\n",
      " 2.27596958e-04 2.24864609e-04 2.20292651e-04 2.15657257e-04\n",
      " 2.10171684e-04 2.08770911e-04 2.07051158e-04 2.03922926e-04\n",
      " 2.02660548e-04 1.98557373e-04 1.95986261e-04 1.94639364e-04\n",
      " 1.90294322e-04 1.88368644e-04 1.86559363e-04 1.82203096e-04\n",
      " 1.80601870e-04 1.76737878e-04 1.74673029e-04 1.74193125e-04\n",
      " 1.73143940e-04 1.71960583e-04 1.66855284e-04 1.65950973e-04\n",
      " 1.63641746e-04 1.61513359e-04 1.59816356e-04 1.58659116e-04\n",
      " 1.56888877e-04 1.55396203e-04 1.54892671e-04 1.51042170e-04\n",
      " 1.48585431e-04 1.47669224e-04 1.46877770e-04 1.45947921e-04\n",
      " 1.44113554e-04 1.43475734e-04 1.42268332e-04 1.40415901e-04\n",
      " 1.40031705e-04 1.37822213e-04 1.35760697e-04 1.34526437e-04\n",
      " 1.32599312e-04 1.30458335e-04 1.30175297e-04 1.28169879e-04\n",
      " 1.26584425e-04 1.25947994e-04 1.25547297e-04 1.23076221e-04\n",
      " 1.21561330e-04 1.20078156e-04 1.19337478e-04 1.18418436e-04\n",
      " 1.17542233e-04 1.16802538e-04 1.16068906e-04 1.15500655e-04\n",
      " 1.14999506e-04 1.13986064e-04 1.12676729e-04 1.11853615e-04\n",
      " 1.10700946e-04 1.09658819e-04 1.08288838e-04 1.07625757e-04\n",
      " 1.07132419e-04 1.05759742e-04 1.04665867e-04 1.04129761e-04\n",
      " 1.03530763e-04 1.01438236e-04 1.01133929e-04 1.00643322e-04\n",
      " 9.94343805e-05 9.83892334e-05 9.72466545e-05 9.60914570e-05\n",
      " 9.55900418e-05 9.46085899e-05 9.36469715e-05 9.32457885e-05\n",
      " 9.27951606e-05 9.17919717e-05 9.12493564e-05 9.09358794e-05\n",
      " 9.03341511e-05 8.97797387e-05 8.89203774e-05 8.75256387e-05\n",
      " 8.73712984e-05 8.67526238e-05 8.62024523e-05 8.52904408e-05\n",
      " 8.45367328e-05 8.39058850e-05 8.34164637e-05 8.30974903e-05\n",
      " 8.27557326e-05 8.22995099e-05 8.16447669e-05 8.07679385e-05\n",
      " 8.02782337e-05 7.94345923e-05 7.91583523e-05 7.82869509e-05\n",
      " 7.79819615e-05 7.75776022e-05 7.69601194e-05 7.67252879e-05\n",
      " 7.56706001e-05 7.49435818e-05 7.46505169e-05 7.39868871e-05\n",
      " 7.34817228e-05 7.30966473e-05 7.29772760e-05 7.24001429e-05\n",
      " 7.18199261e-05 7.15200513e-05 7.09697757e-05 7.05097460e-05\n",
      " 6.97370087e-05 6.93527548e-05 6.89254260e-05 6.85661839e-05\n",
      " 6.75986987e-05 6.75573191e-05 6.67301207e-05 6.63888838e-05\n",
      " 6.62334673e-05 6.52801514e-05 6.50515512e-05 6.44965486e-05\n",
      " 6.43250420e-05 6.41048831e-05 6.38114942e-05 6.32768284e-05\n",
      " 6.28564831e-05 6.23671255e-05 6.21989225e-05 6.16365200e-05\n",
      " 6.10425389e-05 6.07870465e-05 6.03794876e-05 5.98325561e-05\n",
      " 5.92784503e-05 5.91878862e-05 5.87060905e-05 5.86549793e-05\n",
      " 5.82902059e-05 5.74415830e-05 5.71701509e-05 5.69530122e-05\n",
      " 5.64606113e-05 5.60904398e-05 5.58976533e-05 5.54089900e-05\n",
      " 5.51193097e-05 5.48683013e-05 5.46375661e-05 5.43564603e-05\n",
      " 5.38916399e-05 5.36609742e-05 5.33531093e-05 5.31498101e-05\n",
      " 5.29931150e-05 5.25610767e-05 5.22085633e-05 5.20150478e-05\n",
      " 5.16336867e-05 5.15819564e-05 5.10504266e-05 5.09233197e-05\n",
      " 5.07266683e-05 5.06479128e-05 4.97481092e-05 4.93967241e-05\n",
      " 4.93199763e-05 4.92288683e-05 4.88724900e-05 4.86469851e-05\n",
      " 4.83877523e-05 4.81101382e-05 4.78309446e-05 4.76786321e-05\n",
      " 4.71461129e-05 4.67617838e-05 4.65832706e-05 4.62033849e-05\n",
      " 4.58095007e-05 4.57466184e-05 4.55144773e-05 4.53263916e-05\n",
      " 4.48584846e-05 4.47197108e-05 4.44282746e-05 4.42164211e-05\n",
      " 4.38728814e-05 4.37289323e-05 4.35467597e-05 4.33229848e-05\n",
      " 4.31393223e-05 4.29029403e-05 4.28879639e-05 4.23967779e-05\n",
      " 4.22194422e-05 4.20897925e-05 4.17939909e-05 4.15166603e-05\n",
      " 4.14587972e-05 4.10405000e-05 4.08254731e-05 4.06445993e-05\n",
      " 4.04551483e-05 4.03767949e-05 4.01229457e-05 3.99553648e-05\n",
      " 3.97890277e-05 3.94487078e-05 3.91785023e-05 3.88400714e-05\n",
      " 3.87022320e-05 3.86274051e-05 3.85778705e-05 3.82174220e-05\n",
      " 3.79383383e-05 3.79081714e-05 3.76684944e-05 3.76471392e-05\n",
      " 3.73966024e-05 3.72829502e-05 3.70731507e-05 3.68184366e-05\n",
      " 3.67351451e-05 3.64878222e-05 3.64070475e-05 3.62460189e-05\n",
      " 3.60244715e-05 3.58388187e-05 3.57815023e-05 3.55703461e-05\n",
      " 3.52399400e-05 3.50879371e-05 3.49435598e-05 3.47550547e-05\n",
      " 3.46465780e-05 3.44584547e-05 3.43088790e-05 3.40620710e-05\n",
      " 3.40253055e-05 3.38364388e-05 3.37553517e-05 3.34127406e-05\n",
      " 3.32052467e-05 3.30564318e-05 3.27962790e-05 3.27756674e-05\n",
      " 3.25579182e-05 3.24832765e-05 3.24679443e-05 3.21037639e-05\n",
      " 3.18759129e-05 3.16647017e-05 3.15544863e-05 3.15007079e-05\n",
      " 3.13019071e-05 3.12500900e-05 3.10309871e-05 3.09167998e-05\n",
      " 3.08285268e-05 3.06533694e-05 3.05810101e-05 3.04858694e-05\n",
      " 3.02772155e-05 3.01508608e-05 3.01081128e-05 2.98698996e-05\n",
      " 2.97370649e-05 2.94688410e-05 2.94088834e-05 2.93209576e-05\n",
      " 2.91993153e-05 2.91027890e-05 2.89204399e-05 2.87657814e-05\n",
      " 2.86111662e-05 2.85304581e-05 2.84274604e-05 2.82274157e-05\n",
      " 2.81608305e-05 2.79413341e-05 2.78799301e-05 2.75963914e-05\n",
      " 2.75770526e-05 2.74833525e-05 2.73833692e-05 2.73239266e-05\n",
      " 2.70886669e-05 2.70739625e-05 2.69398722e-05 2.66588503e-05\n",
      " 2.66265080e-05 2.64908904e-05 2.64211751e-05 2.61173748e-05\n",
      " 2.60202843e-05 2.59266393e-05 2.58802145e-05 2.56698567e-05\n",
      " 2.56176722e-05 2.55113159e-05 2.53984854e-05 2.53551705e-05\n",
      " 2.51750894e-05 2.50232919e-05 2.48940125e-05 2.48077501e-05\n",
      " 2.46677063e-05 2.45992987e-05 2.45691810e-05 2.43335945e-05\n",
      " 2.41247858e-05 2.40621856e-05 2.39861913e-05 2.38627715e-05\n",
      " 2.37064712e-05 2.36279327e-05 2.35138611e-05 2.34437539e-05\n",
      " 2.32624159e-05 2.31678177e-05 2.30071796e-05 2.29116382e-05\n",
      " 2.27064789e-05 2.26160826e-05 2.24137467e-05 2.23753221e-05\n",
      " 2.22156994e-05 2.20588799e-05 2.19570162e-05 2.18768635e-05\n",
      " 2.17221544e-05 2.15655273e-05 2.14123470e-05 2.13997615e-05\n",
      " 2.13214444e-05 2.12200638e-05 2.10713009e-05 2.09004365e-05\n",
      " 2.07924284e-05 2.07860337e-05 2.06868546e-05 2.06115764e-05\n",
      " 2.04064908e-05 2.03244534e-05 2.01820346e-05 2.01061532e-05\n",
      " 2.00155283e-05 1.98954627e-05 1.98520885e-05 1.97886971e-05\n",
      " 1.96781403e-05 1.94438802e-05 1.93932058e-05 1.92651660e-05\n",
      " 1.91956373e-05 1.90128471e-05 1.89609765e-05 1.88593949e-05\n",
      " 1.86802670e-05 1.85150307e-05 1.84411772e-05 1.83280762e-05\n",
      " 1.82688982e-05 1.80881272e-05 1.79025107e-05 1.78322791e-05]\n",
      "Gesamte erklärte Varianz: 0.9954975688253049\n",
      "✅ Reduzierter Chunk 15 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_15.npz\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (169162, 1024)\n",
      "Gesamte Datenform vor PCA: (169162, 1024)\n",
      "Gesamte Datenform nach PCA: (169162, 512)\n",
      "Erklärte Varianz pro Komponente: [1.31851935e-01 9.58103290e-02 7.87258445e-02 5.13084395e-02\n",
      " 4.84427289e-02 4.34256914e-02 4.14184475e-02 3.67690491e-02\n",
      " 3.43403618e-02 2.97577147e-02 2.75810720e-02 2.50075954e-02\n",
      " 2.13684900e-02 1.88729712e-02 1.81244411e-02 1.73101408e-02\n",
      " 1.66698245e-02 1.37047031e-02 1.22795198e-02 1.14299646e-02\n",
      " 1.05451754e-02 9.66905842e-03 8.71758623e-03 8.38588440e-03\n",
      " 8.09595185e-03 7.16753979e-03 6.60450660e-03 6.46170938e-03\n",
      " 6.07378897e-03 5.62266233e-03 5.28007144e-03 4.63372807e-03\n",
      " 4.56441323e-03 4.34003900e-03 4.18364341e-03 3.86635035e-03\n",
      " 3.73012334e-03 3.52320475e-03 3.41835849e-03 3.18452071e-03\n",
      " 3.01683990e-03 2.76443986e-03 2.61979128e-03 2.54683893e-03\n",
      " 2.39258872e-03 2.30664508e-03 2.21520072e-03 2.19530669e-03\n",
      " 2.09105547e-03 1.98760561e-03 1.88775854e-03 1.78331186e-03\n",
      " 1.70014838e-03 1.67363207e-03 1.62857608e-03 1.50552429e-03\n",
      " 1.43913602e-03 1.41761438e-03 1.38547754e-03 1.28955868e-03\n",
      " 1.23927270e-03 1.16382236e-03 1.14964318e-03 1.10457535e-03\n",
      " 1.09163845e-03 1.01277376e-03 1.00137124e-03 9.75918886e-04\n",
      " 9.52594652e-04 9.15479579e-04 9.06291573e-04 8.72680353e-04\n",
      " 8.36139054e-04 8.06887431e-04 7.72376115e-04 7.65215934e-04\n",
      " 7.38385740e-04 7.35808876e-04 6.78020689e-04 6.72871075e-04\n",
      " 6.59320462e-04 6.35357029e-04 6.30375443e-04 5.94012708e-04\n",
      " 5.92046745e-04 5.71374585e-04 5.50072708e-04 5.30051737e-04\n",
      " 5.26864975e-04 5.17788101e-04 5.02907814e-04 4.78820205e-04\n",
      " 4.69103630e-04 4.57594891e-04 4.54040715e-04 4.46106121e-04\n",
      " 4.35522464e-04 4.24039865e-04 4.14347674e-04 4.12491389e-04\n",
      " 4.07683951e-04 3.90969706e-04 3.83313619e-04 3.75685028e-04\n",
      " 3.66420132e-04 3.62469417e-04 3.56257724e-04 3.44896743e-04\n",
      " 3.39472208e-04 3.36998148e-04 3.30766536e-04 3.26292872e-04\n",
      " 3.22739399e-04 3.13992811e-04 3.09685344e-04 2.95491292e-04\n",
      " 2.91802453e-04 2.85321434e-04 2.82644124e-04 2.79390557e-04\n",
      " 2.76551842e-04 2.73506968e-04 2.65787764e-04 2.56785196e-04\n",
      " 2.55055747e-04 2.51664431e-04 2.49243554e-04 2.48137675e-04\n",
      " 2.43543446e-04 2.40040646e-04 2.32218263e-04 2.31053005e-04\n",
      " 2.28763128e-04 2.25297606e-04 2.23064224e-04 2.18837258e-04\n",
      " 2.13566452e-04 2.09065030e-04 2.07729448e-04 2.05184406e-04\n",
      " 2.03438902e-04 2.00322331e-04 1.95680237e-04 1.95029675e-04\n",
      " 1.90825676e-04 1.89272169e-04 1.87901166e-04 1.85085304e-04\n",
      " 1.81549780e-04 1.78049577e-04 1.77036229e-04 1.75591183e-04\n",
      " 1.74946942e-04 1.69361729e-04 1.66902068e-04 1.65190643e-04\n",
      " 1.63517312e-04 1.62092787e-04 1.59692480e-04 1.59056467e-04\n",
      " 1.57554927e-04 1.54928467e-04 1.53914956e-04 1.51530127e-04\n",
      " 1.50743546e-04 1.49475610e-04 1.48367209e-04 1.46107975e-04\n",
      " 1.44657937e-04 1.43905737e-04 1.42137066e-04 1.40588777e-04\n",
      " 1.39268322e-04 1.37445251e-04 1.36620175e-04 1.35372985e-04\n",
      " 1.32846168e-04 1.31622233e-04 1.31172285e-04 1.30142267e-04\n",
      " 1.28210422e-04 1.26899587e-04 1.26349884e-04 1.23646798e-04\n",
      " 1.21886530e-04 1.20780990e-04 1.19829692e-04 1.18832851e-04\n",
      " 1.18119760e-04 1.16911053e-04 1.16197059e-04 1.15480344e-04\n",
      " 1.14349654e-04 1.13764052e-04 1.12930635e-04 1.11935387e-04\n",
      " 1.10489952e-04 1.08959399e-04 1.08112474e-04 1.07304971e-04\n",
      " 1.06144717e-04 1.05180030e-04 1.04759490e-04 1.03859839e-04\n",
      " 1.03686542e-04 1.02649155e-04 1.02095313e-04 1.00440890e-04\n",
      " 9.98463074e-05 9.95043185e-05 9.82509695e-05 9.69672474e-05\n",
      " 9.63359570e-05 9.57497029e-05 9.46163292e-05 9.34706443e-05\n",
      " 9.30204896e-05 9.19461000e-05 9.14305955e-05 9.11521062e-05\n",
      " 9.07193049e-05 8.99312880e-05 8.90907779e-05 8.85141506e-05\n",
      " 8.76389839e-05 8.75042893e-05 8.70753199e-05 8.61958697e-05\n",
      " 8.59730469e-05 8.46738089e-05 8.43390825e-05 8.39817846e-05\n",
      " 8.31607541e-05 8.23156846e-05 8.12941143e-05 8.06249123e-05\n",
      " 8.02712645e-05 7.98348069e-05 7.93804502e-05 7.86290341e-05\n",
      " 7.85779145e-05 7.77430363e-05 7.73772092e-05 7.68804629e-05\n",
      " 7.60974381e-05 7.55807482e-05 7.51277713e-05 7.45260514e-05\n",
      " 7.40164986e-05 7.32011375e-05 7.29779260e-05 7.28499294e-05\n",
      " 7.24895460e-05 7.19447406e-05 7.14713873e-05 7.07856848e-05\n",
      " 7.03669381e-05 6.94177981e-05 6.92821189e-05 6.87795840e-05\n",
      " 6.80084313e-05 6.74711644e-05 6.70443085e-05 6.67143548e-05\n",
      " 6.65145487e-05 6.61118008e-05 6.55677355e-05 6.50319236e-05\n",
      " 6.44879398e-05 6.41441572e-05 6.39333822e-05 6.35369372e-05\n",
      " 6.27430061e-05 6.25886815e-05 6.21590787e-05 6.15154146e-05\n",
      " 6.12860945e-05 6.10244507e-05 6.05498744e-05 6.03145085e-05\n",
      " 6.02541258e-05 5.93832363e-05 5.90525738e-05 5.89795727e-05\n",
      " 5.87202933e-05 5.84685461e-05 5.74002587e-05 5.70572789e-05\n",
      " 5.68670182e-05 5.63581176e-05 5.62372017e-05 5.58146042e-05\n",
      " 5.54953748e-05 5.53100185e-05 5.48286752e-05 5.47090198e-05\n",
      " 5.40883661e-05 5.39231415e-05 5.37642198e-05 5.33085084e-05\n",
      " 5.30597590e-05 5.27720756e-05 5.24377318e-05 5.23142006e-05\n",
      " 5.21521368e-05 5.15763938e-05 5.14927279e-05 5.11344768e-05\n",
      " 5.07754480e-05 5.06551780e-05 5.00770268e-05 4.98103532e-05\n",
      " 4.96665993e-05 4.93990132e-05 4.88671586e-05 4.88367415e-05\n",
      " 4.85765402e-05 4.81394554e-05 4.79402012e-05 4.75196859e-05\n",
      " 4.73801309e-05 4.72756653e-05 4.70423314e-05 4.68184801e-05\n",
      " 4.65153216e-05 4.62061864e-05 4.58741128e-05 4.56776652e-05\n",
      " 4.53954757e-05 4.50068137e-05 4.48446214e-05 4.47381897e-05\n",
      " 4.44394245e-05 4.41799726e-05 4.39125025e-05 4.38415302e-05\n",
      " 4.33757561e-05 4.31215943e-05 4.28909980e-05 4.28741400e-05\n",
      " 4.26702135e-05 4.24004010e-05 4.22956250e-05 4.20762673e-05\n",
      " 4.16630207e-05 4.14594298e-05 4.11739320e-05 4.10346529e-05\n",
      " 4.09686603e-05 4.07705632e-05 4.05115190e-05 4.03633029e-05\n",
      " 3.99661210e-05 3.97093283e-05 3.94587915e-05 3.93090733e-05\n",
      " 3.89419667e-05 3.88919572e-05 3.85521006e-05 3.85046468e-05\n",
      " 3.81966561e-05 3.80930026e-05 3.78865424e-05 3.76575986e-05\n",
      " 3.75244406e-05 3.73022544e-05 3.72231855e-05 3.71422101e-05\n",
      " 3.67696974e-05 3.65555326e-05 3.64719200e-05 3.64253159e-05\n",
      " 3.62531142e-05 3.60182594e-05 3.58238406e-05 3.56457844e-05\n",
      " 3.55514348e-05 3.53128359e-05 3.52025942e-05 3.51701137e-05\n",
      " 3.50473038e-05 3.46769799e-05 3.45131256e-05 3.44631569e-05\n",
      " 3.42207794e-05 3.41513217e-05 3.39184675e-05 3.38389440e-05\n",
      " 3.35192944e-05 3.33480460e-05 3.31630879e-05 3.30826079e-05\n",
      " 3.29202462e-05 3.28703527e-05 3.27210987e-05 3.24362313e-05\n",
      " 3.24066608e-05 3.21498493e-05 3.19637466e-05 3.18132633e-05\n",
      " 3.16531720e-05 3.14319579e-05 3.13262599e-05 3.12394080e-05\n",
      " 3.12002828e-05 3.10886959e-05 3.08852711e-05 3.07600654e-05\n",
      " 3.05448814e-05 3.03816606e-05 3.02055422e-05 3.01129174e-05\n",
      " 2.99456921e-05 2.97197242e-05 2.93987952e-05 2.93643235e-05\n",
      " 2.92427585e-05 2.92175969e-05 2.90707888e-05 2.89391610e-05\n",
      " 2.88409638e-05 2.87843503e-05 2.85678493e-05 2.83028973e-05\n",
      " 2.82391687e-05 2.80557220e-05 2.80401152e-05 2.78742791e-05\n",
      " 2.77785875e-05 2.76572106e-05 2.75484364e-05 2.74970001e-05\n",
      " 2.74228230e-05 2.73031395e-05 2.70496111e-05 2.68670393e-05\n",
      " 2.68028780e-05 2.66429748e-05 2.65116261e-05 2.63886029e-05\n",
      " 2.63411051e-05 2.61548284e-05 2.59414177e-05 2.58980388e-05\n",
      " 2.57567607e-05 2.56140588e-05 2.55387310e-05 2.53606010e-05\n",
      " 2.53322033e-05 2.50303650e-05 2.49854120e-05 2.48955717e-05\n",
      " 2.48478152e-05 2.47904820e-05 2.46744250e-05 2.45805239e-05\n",
      " 2.44866133e-05 2.42705811e-05 2.41941071e-05 2.40969150e-05\n",
      " 2.38396221e-05 2.36325582e-05 2.35330723e-05 2.34329294e-05\n",
      " 2.33228444e-05 2.31821636e-05 2.31524552e-05 2.28696056e-05\n",
      " 2.27987854e-05 2.26046740e-05 2.25244794e-05 2.23163462e-05\n",
      " 2.22493564e-05 2.20726438e-05 2.20203264e-05 2.19991614e-05\n",
      " 2.18854185e-05 2.17913026e-05 2.16630113e-05 2.16451592e-05\n",
      " 2.14571171e-05 2.13475010e-05 2.11965316e-05 2.11339241e-05\n",
      " 2.09663649e-05 2.09410591e-05 2.08402341e-05 2.07316073e-05\n",
      " 2.06232549e-05 2.05594667e-05 2.03222366e-05 2.02201699e-05\n",
      " 2.01138354e-05 1.99938116e-05 1.99135119e-05 1.98710643e-05\n",
      " 1.97230928e-05 1.96392466e-05 1.94236220e-05 1.93367435e-05\n",
      " 1.91746609e-05 1.90828090e-05 1.90659495e-05 1.89671753e-05\n",
      " 1.87566919e-05 1.85853901e-05 1.85344618e-05 1.82840724e-05\n",
      " 1.82454081e-05 1.80829054e-05 1.79784429e-05 1.78160750e-05]\n",
      "Gesamte erklärte Varianz: 0.9954514246391517\n",
      "✅ Reduzierter Chunk 16 gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_16.npz\n",
      "✅ Letzter Chunk verarbeitet mit Shape: (169162, 1024)\n",
      "🔍 Verfügbare Keys: 1896\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (19260, 1024)\n",
      "Gesamte Datenform vor PCA: (19260, 1024)\n",
      "Gesamte Datenform nach PCA: (19260, 512)\n",
      "Erklärte Varianz pro Komponente: [1.31897613e-01 7.99662396e-02 6.14870414e-02 4.83874567e-02\n",
      " 3.94621342e-02 3.75995897e-02 3.34450454e-02 3.22444737e-02\n",
      " 3.13749500e-02 2.82428395e-02 2.43660994e-02 2.34021489e-02\n",
      " 2.23069582e-02 2.15274431e-02 2.08573900e-02 1.88519191e-02\n",
      " 1.72246341e-02 1.66460723e-02 1.54465074e-02 1.38671454e-02\n",
      " 1.32891294e-02 1.09172855e-02 1.05937021e-02 9.78527032e-03\n",
      " 9.17710084e-03 8.01663194e-03 7.48661673e-03 7.04238517e-03\n",
      " 6.58061216e-03 6.30849833e-03 5.82394190e-03 5.74040273e-03\n",
      " 5.38321678e-03 5.16394200e-03 4.89518791e-03 4.63305367e-03\n",
      " 4.40225750e-03 3.97609966e-03 3.78842209e-03 3.65631958e-03\n",
      " 3.59185319e-03 3.53199034e-03 3.34509904e-03 3.24182236e-03\n",
      " 3.09229549e-03 3.01266741e-03 2.89045949e-03 2.77017104e-03\n",
      " 2.65369541e-03 2.57657049e-03 2.43473798e-03 2.35267542e-03\n",
      " 2.13860185e-03 1.99758029e-03 1.95324514e-03 1.88977178e-03\n",
      " 1.81458856e-03 1.81033276e-03 1.72303861e-03 1.61736051e-03\n",
      " 1.56303518e-03 1.53290736e-03 1.44602952e-03 1.42465485e-03\n",
      " 1.38557714e-03 1.30425254e-03 1.25715253e-03 1.22947455e-03\n",
      " 1.21738785e-03 1.17902167e-03 1.12606678e-03 1.11052243e-03\n",
      " 1.07357162e-03 1.02272537e-03 1.01229595e-03 9.83470585e-04\n",
      " 9.08822578e-04 8.99414299e-04 8.77321581e-04 8.74937861e-04\n",
      " 8.44643393e-04 8.25002673e-04 7.96737149e-04 7.71773281e-04\n",
      " 7.65628007e-04 7.45919358e-04 7.31846842e-04 7.18117633e-04\n",
      " 7.06559280e-04 6.81329402e-04 6.74186274e-04 6.36334065e-04\n",
      " 6.34089054e-04 6.26915600e-04 6.16036763e-04 6.04099710e-04\n",
      " 5.83521149e-04 5.67558454e-04 5.52119687e-04 5.49594813e-04\n",
      " 5.45695832e-04 5.37215034e-04 5.22879825e-04 5.12868981e-04\n",
      " 5.00483962e-04 4.88866179e-04 4.81647847e-04 4.74315108e-04\n",
      " 4.64716577e-04 4.53573884e-04 4.41001903e-04 4.39001771e-04\n",
      " 4.27575782e-04 4.22225305e-04 4.18430835e-04 4.08603461e-04\n",
      " 3.99990124e-04 3.97518190e-04 3.90824018e-04 3.83331440e-04\n",
      " 3.82511673e-04 3.76127311e-04 3.71812464e-04 3.69046204e-04\n",
      " 3.58721620e-04 3.51151393e-04 3.48054338e-04 3.43921478e-04\n",
      " 3.40080849e-04 3.34116135e-04 3.30976618e-04 3.21973843e-04\n",
      " 3.20593943e-04 3.17975413e-04 3.15428071e-04 3.05714144e-04\n",
      " 2.99689535e-04 2.98933650e-04 2.94497062e-04 2.92434677e-04\n",
      " 2.88164854e-04 2.86615657e-04 2.85591086e-04 2.81530287e-04\n",
      " 2.79535889e-04 2.71000608e-04 2.66246410e-04 2.63322523e-04\n",
      " 2.59666296e-04 2.56563071e-04 2.54340615e-04 2.53660197e-04\n",
      " 2.50543264e-04 2.47106334e-04 2.42277340e-04 2.38994704e-04\n",
      " 2.37549568e-04 2.37386208e-04 2.34702486e-04 2.30195918e-04\n",
      " 2.26989519e-04 2.22614006e-04 2.22398477e-04 2.18441812e-04\n",
      " 2.16009706e-04 2.14294865e-04 2.13329724e-04 2.10425409e-04\n",
      " 2.08806450e-04 2.08400757e-04 2.02486437e-04 2.01775489e-04\n",
      " 1.98557027e-04 1.98161491e-04 1.95381945e-04 1.94465087e-04\n",
      " 1.92393374e-04 1.88700127e-04 1.88531994e-04 1.86980455e-04\n",
      " 1.85368102e-04 1.83243901e-04 1.82402888e-04 1.81028299e-04\n",
      " 1.77985479e-04 1.76572430e-04 1.74861620e-04 1.73781184e-04\n",
      " 1.73315726e-04 1.70862011e-04 1.70086205e-04 1.69652863e-04\n",
      " 1.67975755e-04 1.66283819e-04 1.65283273e-04 1.64463752e-04\n",
      " 1.59985968e-04 1.59728661e-04 1.58789640e-04 1.57834467e-04\n",
      " 1.57019982e-04 1.56075010e-04 1.54525274e-04 1.52687455e-04\n",
      " 1.52296241e-04 1.50753447e-04 1.49630927e-04 1.46884588e-04\n",
      " 1.45597704e-04 1.44941470e-04 1.44602818e-04 1.43540805e-04\n",
      " 1.42255318e-04 1.40971024e-04 1.40088712e-04 1.39307289e-04\n",
      " 1.37223091e-04 1.35507711e-04 1.34585775e-04 1.34016882e-04\n",
      " 1.33118854e-04 1.32878602e-04 1.31823137e-04 1.30740271e-04\n",
      " 1.29973501e-04 1.29441949e-04 1.28161206e-04 1.26972998e-04\n",
      " 1.26530038e-04 1.25276449e-04 1.24357510e-04 1.23494945e-04\n",
      " 1.22777172e-04 1.22031444e-04 1.21036705e-04 1.20453464e-04\n",
      " 1.19003613e-04 1.17687021e-04 1.16779745e-04 1.16477924e-04\n",
      " 1.15367955e-04 1.15316703e-04 1.14123504e-04 1.13714173e-04\n",
      " 1.13072696e-04 1.11572248e-04 1.11088732e-04 1.10293840e-04\n",
      " 1.10093002e-04 1.09094566e-04 1.08868204e-04 1.07688997e-04\n",
      " 1.06693115e-04 1.05739637e-04 1.05365703e-04 1.04594634e-04\n",
      " 1.04076629e-04 1.03604987e-04 1.03112943e-04 1.02146951e-04\n",
      " 1.01975522e-04 1.00872981e-04 1.00485835e-04 9.93687572e-05\n",
      " 9.90305343e-05 9.82864221e-05 9.79579418e-05 9.75229705e-05\n",
      " 9.70012043e-05 9.63024795e-05 9.54309071e-05 9.51191978e-05\n",
      " 9.43620471e-05 9.34995187e-05 9.29477683e-05 9.27385845e-05\n",
      " 9.21084429e-05 9.12209362e-05 9.03245818e-05 9.00341547e-05\n",
      " 8.93231627e-05 8.92816170e-05 8.86645139e-05 8.80262887e-05\n",
      " 8.76473714e-05 8.67663985e-05 8.62348825e-05 8.57215709e-05\n",
      " 8.53342717e-05 8.46473631e-05 8.43102607e-05 8.38675769e-05\n",
      " 8.32984151e-05 8.26839387e-05 8.23790397e-05 8.21766007e-05\n",
      " 8.15785752e-05 8.10455313e-05 8.03108778e-05 8.01822825e-05\n",
      " 7.95056476e-05 7.87611934e-05 7.86161618e-05 7.82872012e-05\n",
      " 7.78696485e-05 7.73127176e-05 7.66165831e-05 7.62144555e-05\n",
      " 7.60135445e-05 7.53531131e-05 7.51375992e-05 7.47267113e-05\n",
      " 7.45143043e-05 7.38452436e-05 7.34923524e-05 7.31982364e-05\n",
      " 7.30539978e-05 7.27777297e-05 7.20058015e-05 7.17601506e-05\n",
      " 7.11192115e-05 7.10149761e-05 7.06068313e-05 7.03664700e-05\n",
      " 6.99341472e-05 6.96034986e-05 6.92928006e-05 6.87585416e-05\n",
      " 6.83034596e-05 6.81878664e-05 6.74790863e-05 6.73157338e-05\n",
      " 6.72897877e-05 6.68828579e-05 6.67719942e-05 6.59497819e-05\n",
      " 6.54461692e-05 6.52330928e-05 6.51148221e-05 6.48218629e-05\n",
      " 6.46374392e-05 6.41372244e-05 6.40807848e-05 6.37500780e-05\n",
      " 6.29308561e-05 6.28483394e-05 6.26433175e-05 6.25799294e-05\n",
      " 6.21393046e-05 6.18442718e-05 6.14400342e-05 6.12648219e-05\n",
      " 6.11278301e-05 6.06970279e-05 6.03014814e-05 6.02672771e-05\n",
      " 5.95974088e-05 5.91392018e-05 5.86601891e-05 5.84277441e-05\n",
      " 5.82577086e-05 5.81994900e-05 5.79429106e-05 5.77680403e-05\n",
      " 5.74903279e-05 5.72858189e-05 5.70031880e-05 5.67091010e-05\n",
      " 5.64527400e-05 5.62672358e-05 5.58817810e-05 5.57789790e-05\n",
      " 5.54668622e-05 5.52718957e-05 5.47459749e-05 5.44445465e-05\n",
      " 5.43796705e-05 5.41521367e-05 5.39371686e-05 5.38933818e-05\n",
      " 5.35572835e-05 5.31977385e-05 5.29571844e-05 5.27998454e-05\n",
      " 5.21426919e-05 5.20685608e-05 5.19451351e-05 5.15692373e-05\n",
      " 5.13976884e-05 5.12585066e-05 5.11465587e-05 5.08296871e-05\n",
      " 5.05629105e-05 5.03733136e-05 5.02131152e-05 4.99993221e-05\n",
      " 4.96942921e-05 4.93925254e-05 4.90660605e-05 4.88761798e-05\n",
      " 4.88166806e-05 4.85687706e-05 4.83850927e-05 4.81255265e-05\n",
      " 4.77577669e-05 4.72990178e-05 4.71782660e-05 4.69687875e-05\n",
      " 4.66624915e-05 4.64689183e-05 4.64182667e-05 4.60953561e-05\n",
      " 4.60751071e-05 4.55143963e-05 4.53265602e-05 4.51504675e-05\n",
      " 4.49636427e-05 4.48507417e-05 4.45600817e-05 4.43113458e-05\n",
      " 4.41136253e-05 4.39630676e-05 4.36289374e-05 4.34098474e-05\n",
      " 4.32663110e-05 4.30857217e-05 4.30080108e-05 4.27817176e-05\n",
      " 4.25308826e-05 4.24981772e-05 4.21986333e-05 4.18462696e-05\n",
      " 4.16680959e-05 4.15024224e-05 4.14288079e-05 4.13771486e-05\n",
      " 4.10061402e-05 4.08184751e-05 4.04817220e-05 4.03296763e-05\n",
      " 4.02894693e-05 3.99430573e-05 3.96796167e-05 3.95049683e-05\n",
      " 3.93787777e-05 3.91680624e-05 3.90714995e-05 3.89257075e-05\n",
      " 3.87142682e-05 3.85641069e-05 3.84148152e-05 3.82738181e-05\n",
      " 3.79580815e-05 3.78389414e-05 3.76028838e-05 3.73522962e-05\n",
      " 3.73106377e-05 3.71511342e-05 3.69333611e-05 3.68379733e-05\n",
      " 3.67531575e-05 3.65728556e-05 3.62502433e-05 3.61726852e-05\n",
      " 3.59464830e-05 3.58833349e-05 3.55359662e-05 3.55158772e-05\n",
      " 3.53736832e-05 3.51439812e-05 3.51201743e-05 3.49531038e-05\n",
      " 3.46715169e-05 3.45057961e-05 3.43584252e-05 3.40356273e-05\n",
      " 3.38647660e-05 3.37995116e-05 3.35298901e-05 3.33366115e-05\n",
      " 3.31476731e-05 3.30228686e-05 3.28947644e-05 3.28128881e-05\n",
      " 3.27154739e-05 3.25492329e-05 3.24149587e-05 3.23074455e-05\n",
      " 3.20982945e-05 3.17784907e-05 3.13945493e-05 3.12985212e-05\n",
      " 3.11837721e-05 3.10757387e-05 3.09490315e-05 3.07509727e-05\n",
      " 3.02883764e-05 3.01174077e-05 2.99299281e-05 2.97847546e-05\n",
      " 2.96629787e-05 2.93639750e-05 2.92508284e-05 2.91035940e-05\n",
      " 2.89619329e-05 2.87903185e-05 2.87354978e-05 2.84939142e-05\n",
      " 2.83562349e-05 2.82294695e-05 2.78391908e-05 2.76961000e-05]\n",
      "Gesamte erklärte Varianz: 0.9928954839706421\n",
      "✅ Reduzierter Chunk 0 gespeichert unter: ../../data/embeddings/beta/allele/Epitope_beta_embeddings_reduced_chunk_0.npz\n",
      "✅ Letzter Chunk verarbeitet mit Shape: (19260, 1024)\n"
     ]
    }
   ],
   "source": [
    "# === Lade, verarbeite und speichere TCR-Embeddings ===\n",
    "load_and_process_embeddings_in_chunks(\n",
    "    file_path='../../data/embeddings/beta/allele/TRB_beta_embeddings.npz',\n",
    "    output_path='../../data/embeddings/beta/allele/pca/TRB_beta_embeddings_reduced',\n",
    "    chunk_size=100_000,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# === Lade, verarbeite und speichere Epitope-Embeddings ===\n",
    "load_and_process_embeddings_in_chunks(\n",
    "    file_path='../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz',\n",
    "    output_path='../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced',\n",
    "    chunk_size=100_000,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_0.npz mit Shape: (183794, 512)\n",
      "Chunk 1 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_1.npz mit Shape: (183495, 512)\n",
      "Chunk 2 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_10.npz mit Shape: (183435, 512)\n",
      "Chunk 3 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_11.npz mit Shape: (183548, 512)\n",
      "Chunk 4 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_12.npz mit Shape: (183420, 512)\n",
      "Chunk 5 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_13.npz mit Shape: (183521, 512)\n",
      "Chunk 6 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_14.npz mit Shape: (183466, 512)\n",
      "Chunk 7 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_15.npz mit Shape: (183351, 512)\n",
      "Chunk 8 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_16.npz mit Shape: (169162, 512)\n",
      "Chunk 9 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_2.npz mit Shape: (183992, 512)\n",
      "Chunk 10 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_3.npz mit Shape: (183726, 512)\n",
      "Chunk 11 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_4.npz mit Shape: (183396, 512)\n",
      "Chunk 12 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_5.npz mit Shape: (183618, 512)\n",
      "Chunk 13 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_6.npz mit Shape: (183526, 512)\n",
      "Chunk 14 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_7.npz mit Shape: (183399, 512)\n",
      "Chunk 15 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_8.npz mit Shape: (183892, 512)\n",
      "Chunk 16 geladen von ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_chunk_9.npz mit Shape: (184013, 512)\n",
      "Finale reduzierte Embeddings gespeichert unter: ../../data/embeddings/beta/allele/TRB_beta_embeddings_reduced_final.npz\n",
      "Chunk 0 geladen von ../../data/embeddings/beta/allele/Epitope_beta_embeddings_reduced_chunk_0.npz mit Shape: (19260, 512)\n",
      "Finale reduzierte Embeddings gespeichert unter: ../../data/embeddings/beta/allele/Epitope_beta_embeddings_reduced_final.npz\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def combine_reduced_chunks(output_file_path, chunk_base_path):\n",
    "    \"\"\"\n",
    "    Kombiniert alle reduzierten Chunks in eine finale Datei.\n",
    "    \"\"\"\n",
    "    # Automatisches Auffinden aller Chunk-Dateien\n",
    "    chunk_files = sorted(glob.glob(f\"{chunk_base_path}_chunk_*.npz\"))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(f\"Keine Chunk-Dateien gefunden unter: {chunk_base_path}_chunk_*.npz\")\n",
    "        return\n",
    "\n",
    "    combined_chunks = []\n",
    "\n",
    "    for i, chunk_path in enumerate(chunk_files):\n",
    "        chunk = np.load(chunk_path)['embeddings']\n",
    "        combined_chunks.append(chunk)\n",
    "        print(f\"Chunk {i} geladen von {chunk_path} mit Shape: {chunk.shape}\")\n",
    "\n",
    "    # Zusammenfügen und speichern\n",
    "    final_combined = np.vstack(combined_chunks)\n",
    "    np.savez_compressed(output_file_path, embeddings=final_combined)\n",
    "    print(f\"Finale reduzierte Embeddings gespeichert unter: {output_file_path}\")\n",
    "\n",
    "# === TRB Chunks zusammenführen ===\n",
    "combine_reduced_chunks(\n",
    "    output_file_path='../../data/embeddings/beta/allele/pca/TRB_beta_embeddings_reduced_final.npz',\n",
    "    chunk_base_path='../../data/embeddings/beta/allele/pca/TRB_beta_embeddings_reduced'\n",
    ")\n",
    "\n",
    "# === Epitope Chunks zusammenführen ===\n",
    "combine_reduced_chunks(\n",
    "    output_file_path='../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz',\n",
    "    chunk_base_path='../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finale Shape: (3106754, 512)\n",
      "Erste 5 Embeddings:\n",
      "[[ 1.6094408e+00  3.7290834e-02  2.7662563e-01 ...  1.2580658e-03\n",
      "   1.2690697e-03  1.0749387e-02]\n",
      " [ 2.0140364e+00  5.7252681e-01  2.7287084e-01 ... -1.4161194e-02\n",
      "  -4.2453250e-03 -1.3262349e-03]\n",
      " [-5.1855892e-02  6.1241273e-02 -1.5140444e+00 ... -9.9948049e-03\n",
      "   7.8896916e-04  1.2582984e-02]\n",
      " [-6.4428532e-01  1.3521373e+00  1.1184948e-01 ... -2.1086155e-02\n",
      "   2.8722933e-03  6.9138501e-03]\n",
      " [-7.1419412e-01  1.2662249e+00  1.8346758e-01 ... -1.6341988e-03\n",
      "   1.1619872e-02  1.5161378e-02]]\n",
      "Finale Shape: (19260, 512)\n",
      "Erste 5 Embeddings:\n",
      "[[ 1.6602614   0.68618757  0.35977575 ... -0.00368146  0.01186421\n",
      "   0.00406068]\n",
      " [-0.3451092   0.74639124  0.5987011  ...  0.00566218  0.00303877\n",
      "  -0.01863602]\n",
      " [-0.6257281   0.9881257  -0.6240752  ... -0.00652463 -0.00681908\n",
      "  -0.01094823]\n",
      " [-0.51982516  0.48401675  0.7906777  ...  0.00614688 -0.00629554\n",
      "  -0.00789059]\n",
      " [-0.35892686  0.6379304   0.68366134 ...  0.00621066  0.00234119\n",
      "   0.00591103]]\n"
     ]
    }
   ],
   "source": [
    "def check_final_embeddings(file_path):\n",
    "    data = np.load(file_path, allow_pickle=True)['embeddings']\n",
    "    print(f\"Finale Shape: {data.shape}\")\n",
    "    print(f\"Erste 5 Embeddings:\\n{data[:5]}\")\n",
    "\n",
    "# Überprüfe TRB\n",
    "check_final_embeddings('../../data/embeddings/beta/allele/pca/TRB_beta_embeddings_reduced_final.npz')\n",
    "\n",
    "# Überprüfe Epitope\n",
    "check_final_embeddings('../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beta and paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding embeddings using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 2.75 MiB is free. Including non-PyTorch memory, this process has 14.56 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 64.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# epi_embeddings = load_embeddings_to_gpu(paired_all_epi_path)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# tra_embeddings = load_embeddings_to_gpu(paired_all_tra_path)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# trb_embeddings = load_embeddings_to_gpu(paired_all_trb_path)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m beta_epi_embeddings \u001b[38;5;241m=\u001b[39m load_embeddings_to_gpu(beta_all_epi_path)\n\u001b[0;32m---> 27\u001b[0m beta_trb_embeddings \u001b[38;5;241m=\u001b[39m load_embeddings_to_gpu(beta_all_trb_path)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Find max sequence length\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# max_len = max(max(e.shape[0] for e in epi_embeddings.values()), \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#               max(e.shape[0] for e in tra_embeddings.values()), \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#               max(e.shape[0] for e in trb_embeddings.values()))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m beta_max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmax\u001b[39m(e\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m beta_epi_embeddings\u001b[38;5;241m.\u001b[39mvalues()), \n\u001b[1;32m     35\u001b[0m                     \u001b[38;5;28mmax\u001b[39m(e\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m beta_trb_embeddings\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mload_embeddings_to_gpu\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_embeddings_to_gpu\u001b[39m(path):\n\u001b[1;32m     19\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: torch\u001b[38;5;241m.\u001b[39mtensor(data[key], device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 2.75 MiB is free. Including non-PyTorch memory, this process has 14.56 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 64.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to embeddings files\n",
    "# paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "# paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "# paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "\n",
    "beta_all_epi_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "beta_all_trb_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "\n",
    "# Load NPZ files into GPU\n",
    "def load_embeddings_to_gpu(path):\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    return {key: torch.tensor(data[key], device=device) for key in data}\n",
    "\n",
    "# epi_embeddings = load_embeddings_to_gpu(paired_all_epi_path)\n",
    "# tra_embeddings = load_embeddings_to_gpu(paired_all_tra_path)\n",
    "# trb_embeddings = load_embeddings_to_gpu(paired_all_trb_path)\n",
    "\n",
    "beta_epi_embeddings = load_embeddings_to_gpu(beta_all_epi_path)\n",
    "beta_trb_embeddings = load_embeddings_to_gpu(beta_all_trb_path)\n",
    "\n",
    "# Find max sequence length\n",
    "# max_len = max(max(e.shape[0] for e in epi_embeddings.values()), \n",
    "#               max(e.shape[0] for e in tra_embeddings.values()), \n",
    "#               max(e.shape[0] for e in trb_embeddings.values()))\n",
    "\n",
    "beta_max_len = max(max(e.shape[0] for e in beta_epi_embeddings.values()), \n",
    "                    max(e.shape[0] for e in beta_trb_embeddings.values()))\n",
    "\n",
    "# Function to pad embeddings on GPU\n",
    "def pad_embeddings(embeddings, max_len):\n",
    "    return pad_sequence(\n",
    "        [torch.nn.functional.pad(e, (0, 0, 0, max_len - e.shape[0])) for e in embeddings.values()],\n",
    "        batch_first=True, padding_value=0.0\n",
    "    )\n",
    "\n",
    "# Pad embeddings (all computations on GPU)\n",
    "# padded_epi = pad_embeddings(epi_embeddings, max_len)\n",
    "# padded_tra = pad_embeddings(tra_embeddings, max_len)\n",
    "# padded_trb = pad_embeddings(trb_embeddings, max_len)\n",
    "\n",
    "padded_beta_epi = pad_embeddings(beta_epi_embeddings, beta_max_len)\n",
    "# padded_beta_trb = pad_embeddings(beta_trb_embeddings, beta_max_len)\n",
    "\n",
    "# Move back to CPU before saving\n",
    "# padded_epi = padded_epi.cpu().numpy()\n",
    "# padded_tra = padded_tra.cpu().numpy()\n",
    "# padded_trb = padded_trb.cpu().numpy()\n",
    "\n",
    "padded_beta_epi = padded_beta_epi.cpu().numpy()\n",
    "# padded_beta_trb = padded_beta_trb.cpu().numpy()\n",
    "\n",
    "# Save padded embeddings\n",
    "# padd_paired_all_epi_path = '../../data/embeddings/paired/allele/padded_Epitope_paired_embeddings.npz'\n",
    "# padd_paired_all_tra_path = '../../data/embeddings/paired/allele/padded_TRA_paired_embeddings.npz'\n",
    "# padd_paired_all_trb_path = '../../data/embeddings/paired/allele/padded_TRB_paired_embeddings.npz'\n",
    "\n",
    "padd_beta_all_epi_path = '../../data/embeddings/beta/allele/padded_Epitope_beta_embeddings.npz'\n",
    "# padd_beta_all_trb_path = '../../data/embeddings/beta/allele/padded_TRB_beta_embeddings.npz'\n",
    "\n",
    "# np.savez(padd_paired_all_epi_path, **{key: padded_epi[i] for i, key in enumerate(epi_embeddings)})\n",
    "# np.savez(padd_paired_all_tra_path, **{key: padded_tra[i] for i, key in enumerate(tra_embeddings)})\n",
    "# np.savez(padd_paired_all_trb_path, **{key: padded_trb[i] for i, key in enumerate(trb_embeddings)})\n",
    "\n",
    "np.savez(padd_beta_all_epi_path, **{key: padded_beta_epi[i] for i, key in enumerate(beta_epi_embeddings)})\n",
    "# np.savez(padd_beta_all_trb_path, **{key: padded_beta_trb[i] for i, key in enumerate(beta_trb_embeddings)})\n",
    "\n",
    "print(\"Padded embedding saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### short inspection into padded embeddings in directory prov.\n",
    "#### Just to check the Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 1896\n",
      "\n",
      "Key: NLTTRTQL\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: FIYIFHTL\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: YMHHMELPT\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: ILLDWAANI\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: SMWALVISV\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: LYALVYFLQ\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: WLPTGTLLV\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: YVDDVVLGA\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: GTSGSPIVAR\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "\n",
      "Key: LTGHMLDMY\n",
      "Shape: (43, 1024)\n",
      "Size: 44032\n",
      "Data Type: float32\n",
      "Number of keys in the NPZ file: 10000\n",
      "\n",
      "Key: CASSSTASRNTGELFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSLVTGEQYF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSAHRGGYGYTF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSLGRTGGNIQYF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CSARGQEGQYISYEQYF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSGKQGCDTEAFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASHQTGGRDTEAFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CSASSPRLTSNQPQHF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSIIDGINLSYNEQFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n",
      "\n",
      "Key: CASSLVGGELFF\n",
      "Shape: (38, 1024)\n",
      "Size: 38912\n",
      "Data Type: float32\n"
     ]
    }
   ],
   "source": [
    "paddedepiembpath = '../../data/embeddings/beta/gene/prov/padded_epitope_embeddings_batch_0.npz'\n",
    "paddedtcrembpath = '../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_0.npz'\n",
    "paths = [paddedepiembpath, paddedtcrembpath]\n",
    "for path in paths:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    # Inspect the shape and size of each stored array\n",
    "    for key in data.files[:10]:\n",
    "        array = data[key]\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        print(f\"Shape: {array.shape}\")\n",
    "        print(f\"Size: {array.size}\")\n",
    "        print(f\"Data Type: {array.dtype}\")\n",
    "        # print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove all files in folder 'prov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in ../../data/embeddings/beta/gene/prov have been removed.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define the directory path\n",
    "# directory_path = '../../data/embeddings/beta/gene/prov'\n",
    "\n",
    "# # Check if the directory exists\n",
    "# if os.path.exists(directory_path):\n",
    "#     # Iterate over all files in the directory and remove them\n",
    "#     for filename in os.listdir(directory_path):\n",
    "#         file_path = os.path.join(directory_path, filename)\n",
    "#         try:\n",
    "#             if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "#                 os.unlink(file_path)  # Remove the file or symbolic link\n",
    "#             elif os.path.isdir(file_path):\n",
    "#                 shutil.rmtree(file_path)  # Remove the subdirectory\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "#     print(f\"All files in {directory_path} have been removed.\")\n",
    "# else:\n",
    "#     print(f\"The directory {directory_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding all embeddings and saving in batches\n",
    "\n",
    "to same length (max(max(len(epitope)), max(len(tcr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185630/2300908741.py:8: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(train_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the training and validation data\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "validation_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "train_data = pd.read_csv(train_path, sep='\\t')\n",
    "validation_data = pd.read_csv(validation_path, sep='\\t')\n",
    "\n",
    "# Load the embeddings\n",
    "tcr_embeddings_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "epitope_embeddings_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "\n",
    "tcr_embeddings = np.load(tcr_embeddings_path, allow_pickle=True)\n",
    "epitope_embeddings = np.load(epitope_embeddings_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_0.npz\n",
      "Saved batch 2/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_1.npz\n",
      "Saved batch 3/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_2.npz\n",
      "Saved batch 4/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_3.npz\n",
      "Saved batch 5/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_4.npz\n",
      "Saved batch 6/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_5.npz\n",
      "Saved batch 7/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_6.npz\n",
      "Saved batch 8/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_7.npz\n",
      "Saved batch 9/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_8.npz\n",
      "Saved batch 10/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_9.npz\n",
      "Saved batch 11/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_10.npz\n",
      "Saved batch 12/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_11.npz\n",
      "Saved batch 13/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_12.npz\n",
      "Saved batch 14/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_13.npz\n",
      "Saved batch 15/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_14.npz\n",
      "Saved batch 16/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_15.npz\n",
      "Saved batch 17/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_16.npz\n",
      "Saved batch 18/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_17.npz\n",
      "Saved batch 19/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_18.npz\n",
      "Saved batch 20/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_19.npz\n",
      "Saved batch 21/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_20.npz\n",
      "Saved batch 22/22 to ../../data/embeddings/beta/gene/prov/padded_tcr_embeddings_batch_21.npz\n",
      "All batches saved successfully!\n",
      "Saved batch 1/1 to ../../data/embeddings/beta/gene/prov/padded_epitope_embeddings_batch_0.npz\n",
      "All batches saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def pad_embeddings_in_batches(embeddings_dict, max_length, batch_size, save_path):\n",
    "    \"\"\"\n",
    "    Pad embeddings in batches and save them incrementally to disk.\n",
    "    \"\"\"\n",
    "    keys = list(embeddings_dict.keys())\n",
    "    num_batches = (len(keys) + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "\n",
    "    # Create a directory to save the batches\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_keys = keys[i * batch_size : (i + 1) * batch_size]\n",
    "        padded_batch = {}\n",
    "\n",
    "        for key in batch_keys:\n",
    "            embedding = embeddings_dict[key]\n",
    "            padded_embedding = np.zeros((max_length, embedding.shape[1]), dtype=embedding.dtype)\n",
    "            padded_embedding[:embedding.shape[0], :] = embedding\n",
    "            padded_batch[key] = padded_embedding\n",
    "\n",
    "        # Save the batch to disk\n",
    "        batch_save_path = f\"{save_path}_batch_{i}.npz\"\n",
    "        np.savez_compressed(batch_save_path, **padded_batch)\n",
    "        print(f\"Saved batch {i + 1}/{num_batches} to {batch_save_path}\")\n",
    "\n",
    "    print(\"All batches saved successfully!\")\n",
    "\n",
    "# Define batch size (adjust based on memory availability)\n",
    "batch_size = 10000  # Process 10000 embeddings at a time\n",
    "\n",
    "# Determine the maximum length for TCR and Epitope embeddings\n",
    "max_tcr_length = max([embedding.shape[0] for embedding in tcr_embeddings.values()])\n",
    "max_epitope_length = max([embedding.shape[0] for embedding in epitope_embeddings.values()])\n",
    "\n",
    "# Calculate the global max_length\n",
    "max_length = max(max_tcr_length, max_epitope_length)\n",
    "\n",
    "# Pad and save TCR embeddings in batches using the global max_length\n",
    "pad_embeddings_in_batches(tcr_embeddings, max_length, batch_size, '../../data/embeddings/beta/allele/padded/padded_tcr_embeddings')\n",
    "\n",
    "# Pad and save Epitope embeddings in batches using the global max_length\n",
    "pad_embeddings_in_batches(epitope_embeddings, max_length, batch_size, '../../data/embeddings/beta/allele/padded/padded_epitope_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding on PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spaltennamen in der Trainingsdatei:\n",
      "Index(['TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class',\n",
      "       'TRBC', 'Epitope', 'TRBV', 'TRBJ', 'TRB_CDR3', 'MHC', 'TCR_name',\n",
      "       'Binding', 'task'],\n",
      "      dtype='object')\n",
      "\n",
      "Beispieldaten:\n",
      "   TCR_name\\tTRAV\\tTRAJ\\tTRA_CDR3\\tTRBV\\tTRBJ\\tTRB_CDR3\\tTRB_leader\\tTRAC\\tTRBC\\tLinker\\tLink_order\\tTRA_5_prime_seq\\tTRA_3_prime_seq\\tTRB_5_prime_seq\\tTRB_3_prime_seq\\tEpitope\\tMHC\\tMHC class  \\\n",
      "0                                                NaN                                                                                                                                               \n",
      "1                                                NaN                                                                                                                                               \n",
      "2                                                NaN                                                                                                                                               \n",
      "3                                                NaN                                                                                                                                               \n",
      "4                                                NaN                                                                                                                                               \n",
      "\n",
      "  TRBC      Epitope     TRBV     TRBJ         TRB_CDR3          MHC TCR_name  \\\n",
      "0  NaN    QASQEVKNW   TRBV27  TRBJ2-5    CASRTQRWETQYF     HLA-B*57      295   \n",
      "1  NaN  RAFSPEVIPMF    TRBV2  TRBJ2-2      CASRGGSGELF  HLA-B*57:01      469   \n",
      "2  NaN  KAFSPEVIPMF  TRBV6-5  TRBJ1-1    CASRKGQGDWEAF  HLA-B*57:01      470   \n",
      "3  NaN  RAFSPEVIPMF  TRBV7-9  TRBJ2-7  CASSGFRDRVNEQYF  HLA-B*57:01      565   \n",
      "4  NaN   EAAGIGILTV   TRBV19  TRBJ1-6  CASQSGFLSNSPLHF     HLA-A*02      697   \n",
      "\n",
      "   Binding  task  \n",
      "0        1   NaN  \n",
      "1        1   NaN  \n",
      "2        1   NaN  \n",
      "3        1   NaN  \n",
      "4        1   NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Daten mit low_memory=False, um Typkonflikte zu vermeiden\n",
    "train_data = pd.read_csv('../../data/splitted_datasets/allele/beta/train.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "# Spaltennamen ausgeben\n",
    "print(\"Spaltennamen in der Trainingsdatei:\")\n",
    "print(train_data.columns)\n",
    "\n",
    "# Zeige die ersten paar Zeilen, um die Datenstruktur zu verstehen\n",
    "print(\"\\nBeispieldaten:\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 1/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_0.npz\n",
      "✅ Saved batch 2/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_1.npz\n",
      "✅ Saved batch 3/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_2.npz\n",
      "✅ Saved batch 4/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_3.npz\n",
      "✅ Saved batch 5/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_4.npz\n",
      "✅ Saved batch 6/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_5.npz\n",
      "✅ Saved batch 7/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_6.npz\n",
      "✅ Saved batch 8/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_7.npz\n",
      "✅ Saved batch 9/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_8.npz\n",
      "✅ Saved batch 10/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_9.npz\n",
      "✅ Saved batch 11/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_10.npz\n",
      "✅ Saved batch 12/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_11.npz\n",
      "✅ Saved batch 13/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_12.npz\n",
      "✅ Saved batch 14/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_13.npz\n",
      "✅ Saved batch 15/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_14.npz\n",
      "✅ Saved batch 16/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_15.npz\n",
      "✅ Saved batch 17/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_16.npz\n",
      "✅ Saved batch 18/18 to ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_batch_17.npz\n",
      "✅ All batches saved successfully!\n",
      "✅ Saved batch 1/1 to ../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_batch_0.npz\n",
      "✅ All batches saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === Lade die Trainings- und Validierungsdaten ===\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "validation_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "train_data = pd.read_csv(train_path, sep='\\t', low_memory=False)\n",
    "validation_data = pd.read_csv(validation_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# === Lade die reduzierten Embeddings ===\n",
    "tcr_embeddings_path = '../../data/embeddings/beta/allele/pca/TRB_beta_embeddings_reduced_final.npz'\n",
    "epitope_embeddings_path = '../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz'\n",
    "\n",
    "# Lade die Embeddings und extrahiere das Array\n",
    "tcr_embeddings = np.load(tcr_embeddings_path, allow_pickle=True)['embeddings']\n",
    "epitope_embeddings = np.load(epitope_embeddings_path, allow_pickle=True)['embeddings']\n",
    "\n",
    "# === Entferne NaN-Werte und Duplikate aus den Keys ===\n",
    "tcr_keys = train_data['TRB_CDR3'].dropna().tolist()\n",
    "epitope_keys = train_data['Epitope'].dropna().tolist()\n",
    "\n",
    "# === Sicherstellen, dass die Anzahl der Keys mit den Embeddings übereinstimmt ===\n",
    "tcr_keys = tcr_keys[:tcr_embeddings.shape[0]]\n",
    "epitope_keys = epitope_keys[:epitope_embeddings.shape[0]]\n",
    "\n",
    "# === Erstelle die Dictionaries ===\n",
    "tcr_embeddings_dict = {key: tcr_embeddings[i] for i, key in enumerate(tcr_keys)}\n",
    "epitope_embeddings_dict = {key: epitope_embeddings[i] for i, key in enumerate(epitope_keys)}\n",
    "\n",
    "# === Funktion zum Padden und Speichern in Batches ===\n",
    "def pad_embeddings_in_batches(embeddings_dict, max_length, batch_size, save_path):\n",
    "    keys = list(embeddings_dict.keys())\n",
    "    num_batches = (len(keys) + batch_size - 1) // batch_size  # Anzahl der Batches\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_keys = keys[i * batch_size : (i + 1) * batch_size]\n",
    "        padded_batch = {}\n",
    "\n",
    "        for key in batch_keys:\n",
    "            embedding = embeddings_dict[key]\n",
    "            # Padding auf max_length\n",
    "            padded_embedding = np.zeros((max_length, embedding.shape[0]), dtype=embedding.dtype)\n",
    "            padded_embedding[:embedding.shape[0], :] = embedding\n",
    "            padded_batch[key] = padded_embedding\n",
    "\n",
    "        batch_save_path = f\"{save_path}_batch_{i}.npz\"\n",
    "        np.savez_compressed(batch_save_path, **padded_batch)\n",
    "        print(f\"✅ Saved batch {i + 1}/{num_batches} to {batch_save_path}\")\n",
    "\n",
    "    print(\"✅ All batches saved successfully!\")\n",
    "\n",
    "# === Maximaler Padding-Wert bestimmen ===\n",
    "max_tcr_length = max([embedding.shape[0] for embedding in tcr_embeddings_dict.values()])\n",
    "max_epitope_length = max([embedding.shape[0] for embedding in epitope_embeddings_dict.values()])\n",
    "max_length = max(max_tcr_length, max_epitope_length)\n",
    "\n",
    "# === TCR Embeddings padden und speichern ===\n",
    "pad_embeddings_in_batches(\n",
    "    tcr_embeddings_dict, max_length, batch_size=10000,\n",
    "    save_path='../../data/embeddings/beta/allele/padded/padded_tcr_embeddings'\n",
    ")\n",
    "\n",
    "# === Epitope Embeddings padden und speichern ===\n",
    "pad_embeddings_in_batches(\n",
    "    epitope_embeddings_dict, max_length, batch_size=10000,\n",
    "    save_path='../../data/embeddings/beta/allele/padded/padded_epitope_embeddings'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def combine_batches_to_hdf5(batch_base_path, output_path):\n",
    "    batch_files = sorted(glob.glob(f\"{batch_base_path}_batch_*.npz\"))\n",
    "\n",
    "    if not batch_files:\n",
    "        print(\"❌ Keine Batch-Dateien gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Erstelle eine HDF5-Datei\n",
    "    with h5py.File(output_path, 'w') as hdf5_file:\n",
    "        for i, batch_file in enumerate(batch_files):\n",
    "            batch = np.load(batch_file, allow_pickle=True)\n",
    "\n",
    "            for key in batch.files:\n",
    "                # Direktes Schreiben in die HDF5-Datei\n",
    "                if key not in hdf5_file:\n",
    "                    hdf5_file.create_dataset(key, data=batch[key], compression=\"gzip\")\n",
    "                else:\n",
    "                    print(f\"⚠️ Duplikat-Key übersprungen: {key}\")\n",
    "\n",
    "            print(f\"🔄 Batch {i+1}/{len(batch_files)} geladen und gespeichert: {batch_file}\")\n",
    "\n",
    "    print(f\"✅ Finale gepaddete Embeddings gespeichert unter: {output_path}\")\n",
    "\n",
    "# === TCR Batches zusammenführen ===\n",
    "combine_batches_to_hdf5(\n",
    "    batch_base_path='../../data/embeddings/beta/allele/padded/padded_tcr_embeddings',\n",
    "    output_path='../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "# === Epitope Batches zusammenführen ===\n",
    "combine_batches_to_hdf5(\n",
    "    batch_base_path='../../data/embeddings/beta/allele/padded/padded_epitope_embeddings',\n",
    "    output_path='../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HDF5-Datei geladen: ../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_final.h5\n",
      "Anzahl Keys: 177220\n",
      "Beispiel-Keys: ['C*EFVGLAGGCTDTQYF', 'C*GTRPSISVPAA**RETGELFF', 'C*GTRPSISVPAACIYYLDHP*QFF', 'C*GVGAG*DEQYF', 'C*HRVPTNYGYTF']\n",
      "✅ HDF5-Datei geladen: ../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5\n",
      "Anzahl Keys: 987\n",
      "Beispiel-Keys: ['AAFKRSCLK', 'AAGIGILTV', 'AARGPHGGAASGL', 'ACDPHSGHFV', 'AEGSRGGSQA']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def check_hdf5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as hdf5_file:\n",
    "        keys = list(hdf5_file.keys())\n",
    "        print(f\"✅ HDF5-Datei geladen: {file_path}\")\n",
    "        print(f\"Anzahl Keys: {len(keys)}\")\n",
    "        print(f\"Beispiel-Keys: {keys[:5]}\")\n",
    "\n",
    "# Überprüfe TCR\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_final.h5')\n",
    "\n",
    "# Überprüfe Epitope\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlende TCR-Keys: 0\n",
      "Fehlende Epitope-Keys: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "validation_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep='\\t', low_memory=False)\n",
    "valid_df = pd.read_csv(validation_path, sep='\\t', low_memory=False)\n",
    "\n",
    "def check_missing_keys(h5_file_path, keys_to_check):\n",
    "    with h5py.File(h5_file_path, 'r') as h5_file:\n",
    "        missing_keys = [key for key in keys_to_check if key not in h5_file]\n",
    "    return missing_keys\n",
    "\n",
    "# Lade TCR-Keys aus dem Trainingsdatensatz\n",
    "tcr_keys = set(train_df['TRB_CDR3'].unique())\n",
    "epitope_keys = set(train_df['Epitope'].unique())\n",
    "\n",
    "# Überprüfe TCR-Keys\n",
    "missing_tcr_keys = check_missing_keys('../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_final.h5', tcr_keys)\n",
    "print(f\"Fehlende TCR-Keys: {len(missing_tcr_keys)}\")\n",
    "\n",
    "# Überprüfe Epitope-Keys\n",
    "missing_epitope_keys = check_missing_keys('../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5', epitope_keys)\n",
    "print(f\"Fehlende Epitope-Keys: {len(missing_epitope_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Anzahl Epitope-Keys aus Trainings- und Validierungsdaten: 1752\n",
      "\n",
      "📁 Datei: /home/ubuntu/data/embeddings/beta/allele/Epitope_beta_embeddings.npz\n",
      "✅ Vorhandene Keys: 1752\n",
      "❌ Fehlende Keys: 0\n",
      "Beispiel vorhandene Keys: ['LPVNVAFEL', 'RIFTIGTVTLK', 'RYRIGNYKL', 'LLFALHFSA', 'KMVAVFYTT']\n",
      "Beispiel fehlende Keys: []\n",
      "\n",
      "📁 Datei: /home/ubuntu/data/embeddings/beta/allele/padded/padded_epitope_embeddings_batch_0.npz\n",
      "✅ Vorhandene Keys: 838\n",
      "❌ Fehlende Keys: 914\n",
      "Beispiel vorhandene Keys: ['RIFTIGTVTLK', 'RYRIGNYKL', 'CYTWNQMNL', 'LLFALHFSA', 'FLASKIGRLV']\n",
      "Beispiel fehlende Keys: ['ILIVMFPFL', 'LPVNVAFEL', 'LLFNILGGWV', 'QLCDVMFYL', 'VQMAPISAM']\n",
      "\n",
      "📁 Datei: /home/ubuntu/data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.npz\n",
      "✅ Vorhandene Keys: 838\n",
      "❌ Fehlende Keys: 914\n",
      "Beispiel vorhandene Keys: ['RIFTIGTVTLK', 'RYRIGNYKL', 'CYTWNQMNL', 'LLFALHFSA', 'FLASKIGRLV']\n",
      "Beispiel fehlende Keys: ['ILIVMFPFL', 'LPVNVAFEL', 'LLFNILGGWV', 'QLCDVMFYL', 'VQMAPISAM']\n",
      "\n",
      "📁 Datei: /home/ubuntu/data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5\n",
      "✅ Vorhandene Keys: 1752\n",
      "❌ Fehlende Keys: 0\n",
      "Beispiel vorhandene Keys: ['LPVNVAFEL', 'RIFTIGTVTLK', 'RYRIGNYKL', 'LLFALHFSA', 'KMVAVFYTT']\n",
      "Beispiel fehlende Keys: []\n",
      "\n",
      "📁 Datei: /home/ubuntu/data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_chunk_0.npz\n",
      "✅ Vorhandene Keys: 0\n",
      "❌ Fehlende Keys: 1752\n",
      "Beispiel vorhandene Keys: []\n",
      "Beispiel fehlende Keys: ['ILIVMFPFL', 'RIFTIGTVTLK', 'RYRIGNYKL', 'LPVNVAFEL', 'LLFALHFSA']\n",
      "\n",
      "📁 Datei: /home/ubuntu/data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz\n",
      "✅ Vorhandene Keys: 0\n",
      "❌ Fehlende Keys: 1752\n",
      "Beispiel vorhandene Keys: []\n",
      "Beispiel fehlende Keys: ['ILIVMFPFL', 'RIFTIGTVTLK', 'RYRIGNYKL', 'LPVNVAFEL', 'LLFALHFSA']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "\n",
    "# === Lade die Trainings- und Validierungsdaten ===\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "validation_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep='\\t', low_memory=False)\n",
    "valid_df = pd.read_csv(validation_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# === Extrahiere die Epitope-Keys ===\n",
    "epitope_keys = set(train_df['Epitope'].unique()).union(set(valid_df['Epitope'].unique()))\n",
    "print(f\"🔍 Anzahl Epitope-Keys aus Trainings- und Validierungsdaten: {len(epitope_keys)}\")\n",
    "\n",
    "# === Liste aller Dateien, die überprüft werden sollen ===\n",
    "embedding_files = [\n",
    "    '/home/ubuntu/data/embeddings/beta/allele/Epitope_beta_embeddings.npz',\n",
    "    '/home/ubuntu/data/embeddings/beta/allele/padded/padded_epitope_embeddings_batch_0.npz',\n",
    "    '/home/ubuntu/data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.npz',\n",
    "    '/home/ubuntu/data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5',\n",
    "    '/home/ubuntu/data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_chunk_0.npz',\n",
    "    '/home/ubuntu/data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz',\n",
    "]\n",
    "\n",
    "# === Überprüfen, in welcher Datei welche Keys enthalten sind ===\n",
    "def check_keys_in_npz(file_path, keys_to_check):\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    file_keys = set(data.files)\n",
    "    present = keys_to_check.intersection(file_keys)\n",
    "    missing = keys_to_check - file_keys\n",
    "    return present, missing\n",
    "\n",
    "def check_keys_in_h5(file_path, keys_to_check):\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        file_keys = set(h5_file.keys())\n",
    "        present = keys_to_check.intersection(file_keys)\n",
    "        missing = keys_to_check - file_keys\n",
    "    return present, missing\n",
    "\n",
    "# === Ergebnisse speichern ===\n",
    "results = {}\n",
    "\n",
    "for file_path in embedding_files:\n",
    "    if file_path.endswith('.h5'):\n",
    "        present, missing = check_keys_in_h5(file_path, epitope_keys)\n",
    "    else:\n",
    "        present, missing = check_keys_in_npz(file_path, epitope_keys)\n",
    "    \n",
    "    results[file_path] = {\n",
    "        \"present\": len(present),\n",
    "        \"missing\": len(missing),\n",
    "        \"example_present\": list(present)[:5],\n",
    "        \"example_missing\": list(missing)[:5]\n",
    "    }\n",
    "\n",
    "# === Ergebnisse ausgeben ===\n",
    "for file_path, result in results.items():\n",
    "    print(f\"\\n📁 Datei: {file_path}\")\n",
    "    print(f\"✅ Vorhandene Keys: {result['present']}\")\n",
    "    print(f\"❌ Fehlende Keys: {result['missing']}\")\n",
    "    print(f\"Beispiel vorhandene Keys: {result['example_present']}\")\n",
    "    print(f\"Beispiel fehlende Keys: {result['example_missing']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again for Epitope cause there was a bug somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Verfügbare Keys: 1896\n",
      "Lade und puffer alle Batches...\n",
      "Batch 0 geladen mit Shape: (19260, 1024)\n",
      "Gesamte Datenform vor PCA: (19260, 1024)\n",
      "Gesamte Datenform nach PCA: (19260, 512)\n",
      "Erklärte Varianz pro Komponente: [1.31897643e-01 7.99662918e-02 6.14871122e-02 4.83874790e-02\n",
      " 3.94621268e-02 3.75995897e-02 3.34450267e-02 3.22444253e-02\n",
      " 3.13749351e-02 2.82428376e-02 2.43661292e-02 2.34021526e-02\n",
      " 2.23069619e-02 2.15274468e-02 2.08574086e-02 1.88519396e-02\n",
      " 1.72246229e-02 1.66460983e-02 1.54465213e-02 1.38671612e-02\n",
      " 1.32891508e-02 1.09172873e-02 1.05937039e-02 9.78527311e-03\n",
      " 9.17710271e-03 8.01662821e-03 7.48661580e-03 7.04238284e-03\n",
      " 6.58061029e-03 6.30849786e-03 5.82394609e-03 5.74040739e-03\n",
      " 5.38321864e-03 5.16394619e-03 4.89519443e-03 4.63304995e-03\n",
      " 4.40226030e-03 3.97609826e-03 3.78842070e-03 3.65631864e-03\n",
      " 3.59185180e-03 3.53199104e-03 3.34509811e-03 3.24182143e-03\n",
      " 3.09229316e-03 3.01266788e-03 2.89045949e-03 2.77017243e-03\n",
      " 2.65369611e-03 2.57656886e-03 2.43473728e-03 2.35267496e-03\n",
      " 2.13860185e-03 1.99758005e-03 1.95324444e-03 1.88977143e-03\n",
      " 1.81458914e-03 1.81033218e-03 1.72303861e-03 1.61735970e-03\n",
      " 1.56303553e-03 1.53290876e-03 1.44602964e-03 1.42465532e-03\n",
      " 1.38557714e-03 1.30425231e-03 1.25715288e-03 1.22947618e-03\n",
      " 1.21738762e-03 1.17902225e-03 1.12606701e-03 1.11052289e-03\n",
      " 1.07357325e-03 1.02272537e-03 1.01229595e-03 9.83470352e-04\n",
      " 9.08822229e-04 8.99413950e-04 8.77322280e-04 8.74938385e-04\n",
      " 8.44643218e-04 8.25002091e-04 7.96737149e-04 7.71772931e-04\n",
      " 7.65628123e-04 7.45919358e-04 7.31846842e-04 7.18117866e-04\n",
      " 7.06559804e-04 6.81329111e-04 6.74186274e-04 6.36334182e-04\n",
      " 6.34089520e-04 6.26916066e-04 6.16036297e-04 6.04099361e-04\n",
      " 5.83520276e-04 5.67559036e-04 5.52120153e-04 5.49594348e-04\n",
      " 5.45695948e-04 5.37215150e-04 5.22879534e-04 5.12869156e-04\n",
      " 5.00483497e-04 4.88866237e-04 4.81647468e-04 4.74315108e-04\n",
      " 4.64716722e-04 4.53573884e-04 4.41001874e-04 4.39001917e-04\n",
      " 4.27575695e-04 4.22225276e-04 4.18430835e-04 4.08603461e-04\n",
      " 3.99990269e-04 3.97518394e-04 3.90824047e-04 3.83331353e-04\n",
      " 3.82511673e-04 3.76127398e-04 3.71812464e-04 3.69046087e-04\n",
      " 3.58721620e-04 3.51151335e-04 3.48054309e-04 3.43921478e-04\n",
      " 3.40080849e-04 3.34116019e-04 3.30976560e-04 3.21973843e-04\n",
      " 3.20593856e-04 3.17975500e-04 3.15428042e-04 3.05714144e-04\n",
      " 2.99689535e-04 2.98933563e-04 2.94497208e-04 2.92434677e-04\n",
      " 2.88164796e-04 2.86615570e-04 2.85591086e-04 2.81530287e-04\n",
      " 2.79535976e-04 2.71000608e-04 2.66246352e-04 2.63322436e-04\n",
      " 2.59666209e-04 2.56563275e-04 2.54340586e-04 2.53660168e-04\n",
      " 2.50543380e-04 2.47106334e-04 2.42277383e-04 2.38994762e-04\n",
      " 2.37549510e-04 2.37386266e-04 2.34702602e-04 2.30196005e-04\n",
      " 2.26989563e-04 2.22614093e-04 2.22398477e-04 2.18441870e-04\n",
      " 2.16009619e-04 2.14294865e-04 2.13329593e-04 2.10425409e-04\n",
      " 2.08806494e-04 2.08400845e-04 2.02486495e-04 2.01775387e-04\n",
      " 1.98557187e-04 1.98161448e-04 1.95381901e-04 1.94465130e-04\n",
      " 1.92393447e-04 1.88700127e-04 1.88532096e-04 1.86980367e-04\n",
      " 1.85368102e-04 1.83243974e-04 1.82402859e-04 1.81028328e-04\n",
      " 1.77985406e-04 1.76572503e-04 1.74861634e-04 1.73781300e-04\n",
      " 1.73315799e-04 1.70861982e-04 1.70086205e-04 1.69652863e-04\n",
      " 1.67975755e-04 1.66283848e-04 1.65283360e-04 1.64463840e-04\n",
      " 1.59985895e-04 1.59728705e-04 1.58789670e-04 1.57834555e-04\n",
      " 1.57020026e-04 1.56075053e-04 1.54525187e-04 1.52687484e-04\n",
      " 1.52296285e-04 1.50753447e-04 1.49630927e-04 1.46884515e-04\n",
      " 1.45597805e-04 1.44941543e-04 1.44602935e-04 1.43540819e-04\n",
      " 1.42255332e-04 1.40971082e-04 1.40088785e-04 1.39307173e-04\n",
      " 1.37223047e-04 1.35507769e-04 1.34585760e-04 1.34016867e-04\n",
      " 1.33118810e-04 1.32878529e-04 1.31823181e-04 1.30740402e-04\n",
      " 1.29973589e-04 1.29441833e-04 1.28161075e-04 1.26973217e-04\n",
      " 1.26530096e-04 1.25276420e-04 1.24357437e-04 1.23494829e-04\n",
      " 1.22777070e-04 1.22031444e-04 1.21036945e-04 1.20453806e-04\n",
      " 1.19003569e-04 1.17686992e-04 1.16779593e-04 1.16477626e-04\n",
      " 1.15367955e-04 1.15316769e-04 1.14123643e-04 1.13713912e-04\n",
      " 1.13072892e-04 1.11572277e-04 1.11088200e-04 1.10293637e-04\n",
      " 1.10093315e-04 1.09094581e-04 1.08867775e-04 1.07689419e-04\n",
      " 1.06692998e-04 1.05740022e-04 1.05365856e-04 1.04594896e-04\n",
      " 1.04076542e-04 1.03605518e-04 1.03113241e-04 1.02147082e-04\n",
      " 1.01976126e-04 1.00872450e-04 1.00485951e-04 9.93692447e-05\n",
      " 9.90305489e-05 9.82862912e-05 9.79591059e-05 9.75234361e-05\n",
      " 9.70005494e-05 9.63028579e-05 9.54317729e-05 9.51192123e-05\n",
      " 9.43616760e-05 9.35010248e-05 9.29480448e-05 9.27405708e-05\n",
      " 9.21078172e-05 9.12213363e-05 9.03240070e-05 9.00326777e-05\n",
      " 8.93224642e-05 8.92820535e-05 8.86672206e-05 8.80280422e-05\n",
      " 8.76468184e-05 8.67671406e-05 8.62360030e-05 8.57221530e-05\n",
      " 8.53346937e-05 8.46468902e-05 8.43080998e-05 8.38686683e-05\n",
      " 8.32982842e-05 8.26795876e-05 8.23797454e-05 8.21752474e-05\n",
      " 8.15767780e-05 8.10468264e-05 8.03117146e-05 8.01819406e-05\n",
      " 7.95050946e-05 7.87603349e-05 7.86177843e-05 7.82904608e-05\n",
      " 7.78697504e-05 7.73109350e-05 7.66119265e-05 7.62171621e-05\n",
      " 7.60102339e-05 7.53494678e-05 7.51395419e-05 7.47238810e-05\n",
      " 7.45137004e-05 7.38408053e-05 7.34870991e-05 7.31986001e-05\n",
      " 7.30621105e-05 7.27773731e-05 7.20162134e-05 7.17584189e-05\n",
      " 7.11214161e-05 7.10212189e-05 7.06073479e-05 7.03552796e-05\n",
      " 6.99324155e-05 6.95975468e-05 6.92975373e-05 6.87583888e-05\n",
      " 6.83091130e-05 6.81774254e-05 6.74864496e-05 6.73193717e-05\n",
      " 6.72850656e-05 6.68837019e-05 6.67636632e-05 6.59562647e-05\n",
      " 6.54454925e-05 6.52205999e-05 6.51068112e-05 6.48229179e-05\n",
      " 6.46456174e-05 6.41410079e-05 6.40898506e-05 6.37389676e-05\n",
      " 6.29710557e-05 6.28447451e-05 6.26554320e-05 6.25660323e-05\n",
      " 6.21327272e-05 6.18289268e-05 6.14137898e-05 6.12571166e-05\n",
      " 6.11017604e-05 6.06816138e-05 6.02860164e-05 6.02411601e-05\n",
      " 5.95983947e-05 5.91281059e-05 5.86509195e-05 5.84536720e-05\n",
      " 5.82407120e-05 5.82088142e-05 5.79371663e-05 5.77300998e-05\n",
      " 5.75040285e-05 5.72752870e-05 5.70110569e-05 5.67259885e-05\n",
      " 5.64489201e-05 5.63026842e-05 5.58897882e-05 5.57222847e-05\n",
      " 5.54206417e-05 5.52824604e-05 5.47610180e-05 5.44379400e-05\n",
      " 5.43795140e-05 5.41369809e-05 5.39608445e-05 5.38033855e-05\n",
      " 5.35686377e-05 5.31674668e-05 5.29608769e-05 5.28115997e-05\n",
      " 5.21430957e-05 5.20794601e-05 5.19105888e-05 5.16210603e-05\n",
      " 5.14323074e-05 5.13376617e-05 5.11322251e-05 5.07984914e-05\n",
      " 5.05789394e-05 5.03196679e-05 5.02418698e-05 4.98985428e-05\n",
      " 4.97849724e-05 4.93506013e-05 4.90741731e-05 4.89352969e-05\n",
      " 4.88622863e-05 4.85433411e-05 4.83216172e-05 4.81135430e-05\n",
      " 4.78011352e-05 4.72892025e-05 4.71208368e-05 4.70503401e-05\n",
      " 4.67899954e-05 4.65512858e-05 4.63203614e-05 4.60670781e-05\n",
      " 4.59132825e-05 4.55903173e-05 4.51884443e-05 4.51395317e-05\n",
      " 4.49590443e-05 4.48690298e-05 4.42995515e-05 4.42009332e-05\n",
      " 4.40904296e-05 4.38021125e-05 4.36880327e-05 4.34393878e-05\n",
      " 4.32760353e-05 4.30663677e-05 4.30061737e-05 4.29273168e-05\n",
      " 4.25006547e-05 4.23572783e-05 4.20833967e-05 4.19311473e-05\n",
      " 4.17097326e-05 4.15029790e-05 4.13529415e-05 4.11118745e-05\n",
      " 4.10785797e-05 4.10635985e-05 4.05089522e-05 4.02338846e-05\n",
      " 4.01484394e-05 3.99863857e-05 3.97748190e-05 3.97630429e-05\n",
      " 3.95619281e-05 3.91952562e-05 3.89977540e-05 3.89455417e-05\n",
      " 3.87866312e-05 3.86165993e-05 3.85109925e-05 3.82421676e-05\n",
      " 3.81083191e-05 3.79308694e-05 3.76797616e-05 3.76460812e-05\n",
      " 3.74965311e-05 3.73575276e-05 3.69912668e-05 3.69181689e-05\n",
      " 3.67806824e-05 3.66604218e-05 3.66481800e-05 3.63933723e-05\n",
      " 3.62104511e-05 3.59971928e-05 3.58786383e-05 3.55300290e-05\n",
      " 3.54059921e-05 3.51992530e-05 3.49746915e-05 3.48507492e-05\n",
      " 3.47422538e-05 3.46379820e-05 3.44386717e-05 3.42558524e-05\n",
      " 3.41459054e-05 3.39687831e-05 3.38633035e-05 3.36861740e-05\n",
      " 3.34338256e-05 3.33174648e-05 3.29791910e-05 3.27026282e-05\n",
      " 3.25846086e-05 3.23985805e-05 3.22858978e-05 3.20912404e-05\n",
      " 3.18789425e-05 3.18106540e-05 3.16276710e-05 3.13514356e-05\n",
      " 3.12913435e-05 3.10352552e-05 3.07539012e-05 3.05269023e-05\n",
      " 3.04257610e-05 3.03515208e-05 3.01800592e-05 3.00621141e-05\n",
      " 2.96903054e-05 2.95796126e-05 2.93809135e-05 2.93041358e-05\n",
      " 2.91852339e-05 2.90409680e-05 2.88125975e-05 2.87092935e-05\n",
      " 2.85540500e-05 2.82214660e-05 2.80990116e-05 2.79113301e-05]\n",
      "Gesamte erklärte Varianz: 0.9929021000862122\n",
      "✅ Reduzierter Chunk 0 gespeichert unter: ../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_chunk_0.npz\n",
      "✅ Letzter Chunk verarbeitet mit Shape: (19260, 1024)\n",
      "Chunk 0 geladen von ../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_chunk_0.npz mit Shape: (19260, 512)\n",
      "✅ Finale reduzierte Epitope gespeichert unter: ../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from data_scripts.pca_analysis import perform_pca_on_embeddings\n",
    "import glob\n",
    "\n",
    "def process_chunk(chunk, chunk_index, output_path, device='cuda', n_components=512):\n",
    "    \"\"\"\n",
    "    Führt PCA auf einem Chunk durch und speichert das reduzierte Ergebnis.\n",
    "    \"\"\"\n",
    "    # Auf CUDA verschieben\n",
    "    chunk_tensor = torch.tensor(chunk, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Zurück auf CPU und in NumPy konvertieren für PCA\n",
    "    chunk_cpu = chunk_tensor.cpu().numpy()\n",
    "\n",
    "    # PCA durchführen\n",
    "    pca_df = perform_pca_on_embeddings([chunk_cpu], n_components=n_components)\n",
    "\n",
    "    # Speichern des reduzierten Chunks\n",
    "    chunk_output_path = f\"{output_path}_chunk_{chunk_index}.npz\"\n",
    "    np.savez_compressed(chunk_output_path, embeddings=pca_df.values)\n",
    "    print(f\"✅ Reduzierter Chunk {chunk_index} gespeichert unter: {chunk_output_path}\")\n",
    "\n",
    "def load_and_process_embeddings_in_chunks(file_path, output_path, chunk_size=100_000, device='cuda'):\n",
    "    \"\"\"\n",
    "    Lädt Embeddings in Blöcken, führt PCA durch und speichert sie sofort.\n",
    "    \"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    print(f\"🔍 Verfügbare Keys: {len(data.files)}\")\n",
    "\n",
    "    current_chunk = []\n",
    "    chunk_index = 0\n",
    "\n",
    "    for idx, key in enumerate(data.files):\n",
    "        current_chunk.append(data[key])\n",
    "\n",
    "        # Wenn Chunk voll, dann verarbeiten und speichern\n",
    "        if len(current_chunk) * 8 >= chunk_size:  # Jeder Key enthält 8 Samples\n",
    "            combined_chunk = np.concatenate(current_chunk, axis=0)\n",
    "            process_chunk(combined_chunk, chunk_index, output_path, device)\n",
    "            current_chunk = []  # Speicher freigeben\n",
    "            chunk_index += 1\n",
    "\n",
    "    # Verarbeite den letzten, unvollständigen Chunk\n",
    "    if current_chunk:\n",
    "        combined_chunk = np.concatenate(current_chunk, axis=0)\n",
    "        process_chunk(combined_chunk, chunk_index, output_path, device)\n",
    "        print(f\"✅ Letzter Chunk verarbeitet mit Shape: {combined_chunk.shape}\")\n",
    "\n",
    "# === Epitope-Embeddings neu verarbeiten ===\n",
    "load_and_process_embeddings_in_chunks(\n",
    "    file_path='../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz',\n",
    "    output_path='../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced',\n",
    "    chunk_size=100_000,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# === Epitope-Chunks zusammenführen ===\n",
    "def combine_reduced_chunks(output_file_path, chunk_base_path):\n",
    "    \"\"\"\n",
    "    Kombiniert alle reduzierten Chunks in eine finale Datei.\n",
    "    \"\"\"\n",
    "    chunk_files = sorted(glob.glob(f\"{chunk_base_path}_chunk_*.npz\"))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(f\"Keine Chunk-Dateien gefunden unter: {chunk_base_path}_chunk_*.npz\")\n",
    "        return\n",
    "\n",
    "    combined_chunks = []\n",
    "\n",
    "    for i, chunk_path in enumerate(chunk_files):\n",
    "        chunk = np.load(chunk_path)['embeddings']\n",
    "        combined_chunks.append(chunk)\n",
    "        print(f\"Chunk {i} geladen von {chunk_path} mit Shape: {chunk.shape}\")\n",
    "\n",
    "    # Zusammenfügen und speichern\n",
    "    final_combined = np.vstack(combined_chunks)\n",
    "    np.savez_compressed(output_file_path, embeddings=final_combined)\n",
    "    print(f\"✅ Finale reduzierte Epitope gespeichert unter: {output_file_path}\")\n",
    "\n",
    "combine_reduced_chunks(\n",
    "    output_file_path='../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz',\n",
    "    chunk_base_path='../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Original-Keys: 1896\n",
      "Anzahl Reduzierte Embeddings: 19260\n",
      "✅ Gepaddetes Epitop gespeichert: YLDELIKNT\n",
      "✅ Gepaddetes Epitop gespeichert: IQPGQTFSV\n",
      "✅ Gepaddetes Epitop gespeichert: FLPSDFFPSV\n",
      "✅ Gepaddetes Epitop gespeichert: YVVPGSPCL\n",
      "✅ Gepaddetes Epitop gespeichert: LVMPFSIVYI\n",
      "✅ Gepaddetes Epitop gespeichert: VPHVGEIPVAYRKVLL\n",
      "✅ Gepaddetes Epitop gespeichert: LMVLHSPSL\n",
      "✅ Gepaddetes Epitop gespeichert: RLNEVAKNL\n",
      "✅ Gepaddetes Epitop gespeichert: SEYKGPITDVFYKENSY\n",
      "✅ Gepaddetes Epitop gespeichert: FLKGIGWIPI\n",
      "✅ Gepaddetes Epitop gespeichert: RCMLQCPSV\n",
      "✅ Gepaddetes Epitop gespeichert: SLPFGWLIV\n",
      "✅ Gepaddetes Epitop gespeichert: CPACYNSEV\n",
      "✅ Gepaddetes Epitop gespeichert: YMGVVYEM\n",
      "✅ Gepaddetes Epitop gespeichert: ILCETCLIV\n",
      "✅ Gepaddetes Epitop gespeichert: TRLALIAPK\n",
      "✅ Gepaddetes Epitop gespeichert: RPDTRYVLM\n",
      "✅ Gepaddetes Epitop gespeichert: GTITVEELK\n",
      "✅ Gepaddetes Epitop gespeichert: KQIYKTPPIKDF\n",
      "✅ Gepaddetes Epitop gespeichert: LSDRVVFVL\n",
      "✅ Gepaddetes Epitop gespeichert: LPDDFMGCV\n",
      "✅ Gepaddetes Epitop gespeichert: TQWSLFFFL\n",
      "✅ Gepaddetes Epitop gespeichert: FIYIFHTL\n",
      "✅ Gepaddetes Epitop gespeichert: NLDYIINLI\n",
      "✅ Gepaddetes Epitop gespeichert: YMHHMELPT\n",
      "✅ Gepaddetes Epitop gespeichert: FVVPYMIYLL\n",
      "✅ Gepaddetes Epitop gespeichert: YVVDDPCPI\n",
      "✅ Gepaddetes Epitop gespeichert: FLKEKGGL\n",
      "✅ Gepaddetes Epitop gespeichert: NEGVKAAW\n",
      "✅ Gepaddetes Epitop gespeichert: NIIFLHEDLTV\n",
      "✅ Gepaddetes Epitop gespeichert: EENLLDFVRF\n",
      "✅ Gepaddetes Epitop gespeichert: FVGPLDEDF\n",
      "✅ Gepaddetes Epitop gespeichert: LSITPEIAPYF\n",
      "✅ Gepaddetes Epitop gespeichert: MIELSLIDFYLCFLAFLLFLVLIML\n",
      "✅ Gepaddetes Epitop gespeichert: IIKNLSKSLTENKY\n",
      "✅ Gepaddetes Epitop gespeichert: FLFMYLVMV\n",
      "✅ Gepaddetes Epitop gespeichert: KLDGFMGRI\n",
      "✅ Gepaddetes Epitop gespeichert: RLGPVQNEV\n",
      "✅ Gepaddetes Epitop gespeichert: REGVFVSNGTHW\n",
      "✅ Gepaddetes Epitop gespeichert: NMLSTVLGV\n",
      "✅ Gepaddetes Epitop gespeichert: APHGVVFLH\n",
      "✅ Gepaddetes Epitop gespeichert: WTAGAAAYYVGY\n",
      "✅ Gepaddetes Epitop gespeichert: WTAGAAAYY\n",
      "✅ Gepaddetes Epitop gespeichert: ITLATCELY\n",
      "✅ Gepaddetes Epitop gespeichert: MSAFAMMFV\n",
      "✅ Gepaddetes Epitop gespeichert: LMWLSYFIA\n",
      "✅ Gepaddetes Epitop gespeichert: AEHSLQVAY\n",
      "✅ Gepaddetes Epitop gespeichert: DIAGIGLKTV\n",
      "✅ Gepaddetes Epitop gespeichert: GQARVAYQV\n",
      "✅ Gepaddetes Epitop gespeichert: NLAQTDLATV\n",
      "✅ Gepaddetes Epitop gespeichert: YLAVFDKNL\n",
      "✅ Gepaddetes Epitop gespeichert: NPANNVAIV\n",
      "✅ Gepaddetes Epitop gespeichert: ALWGPDPAA\n",
      "✅ Gepaddetes Epitop gespeichert: YMLDLQPETT\n",
      "✅ Gepaddetes Epitop gespeichert: FLYNPLTRV\n",
      "✅ Gepaddetes Epitop gespeichert: SLFWNEPAI\n",
      "✅ Gepaddetes Epitop gespeichert: FPLCANGQV\n",
      "✅ Gepaddetes Epitop gespeichert: YVVDDPCPIHFY\n",
      "✅ Gepaddetes Epitop gespeichert: LPRWYFYYL\n",
      "✅ Gepaddetes Epitop gespeichert: QVPLRPMTYK\n",
      "✅ Gepaddetes Epitop gespeichert: RMYSPVSIL\n",
      "✅ Gepaddetes Epitop gespeichert: LTISPEIPSYF\n",
      "✅ Gepaddetes Epitop gespeichert: RMEQVDWTV\n",
      "✅ Gepaddetes Epitop gespeichert: APRGPHGAAASGL\n",
      "✅ Gepaddetes Epitop gespeichert: APLLSAGIF\n",
      "✅ Gepaddetes Epitop gespeichert: ALSPVIPLI\n",
      "✅ Gepaddetes Epitop gespeichert: FLPDLNANA\n",
      "✅ Gepaddetes Epitop gespeichert: YLEAGPVTA\n",
      "✅ Gepaddetes Epitop gespeichert: FVCNLLLLFVTVYSHLLLV\n",
      "✅ Gepaddetes Epitop gespeichert: LPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERD\n",
      "✅ Gepaddetes Epitop gespeichert: EIDPKLDNY\n",
      "✅ Gepaddetes Epitop gespeichert: RLFARTRSM\n",
      "✅ Gepaddetes Epitop gespeichert: MCDVRQLLF\n",
      "✅ Gepaddetes Epitop gespeichert: VMAPRTLIL\n",
      "✅ Gepaddetes Epitop gespeichert: FVFPLNSII\n",
      "✅ Gepaddetes Epitop gespeichert: NLVAMVATV\n",
      "✅ Gepaddetes Epitop gespeichert: WIFGTTLDSK\n",
      "✅ Gepaddetes Epitop gespeichert: IPLTTAAKL\n",
      "✅ Gepaddetes Epitop gespeichert: ELGHPYLWVL\n",
      "✅ Gepaddetes Epitop gespeichert: GQFLTPNSH\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGYPRYV\n",
      "✅ Gepaddetes Epitop gespeichert: GRIAFFLKY\n",
      "✅ Gepaddetes Epitop gespeichert: LMCQPILLL\n",
      "✅ Gepaddetes Epitop gespeichert: YPLHEQHGM\n",
      "✅ Gepaddetes Epitop gespeichert: NLWNTFTRL\n",
      "✅ Gepaddetes Epitop gespeichert: ILITQVLGL\n",
      "✅ Gepaddetes Epitop gespeichert: VSFIEFVGW\n",
      "✅ Gepaddetes Epitop gespeichert: AIILASFSA\n",
      "✅ Gepaddetes Epitop gespeichert: GDCEEEEFEPSTQYEY\n",
      "✅ Gepaddetes Epitop gespeichert: FIDCYLLAI\n",
      "✅ Gepaddetes Epitop gespeichert: NLVPQVATV\n",
      "✅ Gepaddetes Epitop gespeichert: VVDSYYSLL\n",
      "✅ Gepaddetes Epitop gespeichert: CTFEYVSQPFLM\n",
      "✅ Gepaddetes Epitop gespeichert: TYLPSAWNFF\n",
      "✅ Gepaddetes Epitop gespeichert: TLNAWVKVV\n",
      "✅ Gepaddetes Epitop gespeichert: IPSINVHHY\n",
      "✅ Gepaddetes Epitop gespeichert: LMGHFAWWT\n",
      "✅ Gepaddetes Epitop gespeichert: GFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYT\n",
      "✅ Gepaddetes Epitop gespeichert: TSAMQTMLF\n",
      "✅ Gepaddetes Epitop gespeichert: FPNITNLCP\n",
      "✅ Gepaddetes Epitop gespeichert: KLDDKDPNF\n",
      "✅ Gepaddetes Epitop gespeichert: KVKEVCPNV\n",
      "✅ Gepaddetes Epitop gespeichert: LLAILPYYV\n",
      "✅ Gepaddetes Epitop gespeichert: VFITLCFTLK\n",
      "✅ Gepaddetes Epitop gespeichert: EALPFFLFF\n",
      "✅ Gepaddetes Epitop gespeichert: VLNYFEPYL\n",
      "✅ Gepaddetes Epitop gespeichert: LLFNILGGWV\n",
      "✅ Gepaddetes Epitop gespeichert: KVSIWNLDY\n",
      "✅ Gepaddetes Epitop gespeichert: TLEYMDWLV\n",
      "✅ Gepaddetes Epitop gespeichert: FLVWDHNPIG\n",
      "✅ Gepaddetes Epitop gespeichert: TRSMWSFNPETNILLNVPLHGTILTRPLLESELVIGAVILR\n",
      "✅ Gepaddetes Epitop gespeichert: RAQAPPPSW\n",
      "✅ Gepaddetes Epitop gespeichert: CLAYYFMRF\n",
      "✅ Gepaddetes Epitop gespeichert: KQLLFVLEV\n",
      "✅ Gepaddetes Epitop gespeichert: NVILLNKHI\n",
      "✅ Gepaddetes Epitop gespeichert: STSAFVETV\n",
      "✅ Gepaddetes Epitop gespeichert: KAYNVTQAF\n",
      "✅ Gepaddetes Epitop gespeichert: SLYNTVATLY\n",
      "✅ Gepaddetes Epitop gespeichert: ELTSMKYFV\n",
      "✅ Gepaddetes Epitop gespeichert: AMFWSVPTV\n",
      "✅ Gepaddetes Epitop gespeichert: TPGPGVRYPL\n",
      "✅ Gepaddetes Epitop gespeichert: SMWSFNPET\n",
      "✅ Gepaddetes Epitop gespeichert: TLLANVTAV\n",
      "✅ Gepaddetes Epitop gespeichert: QAWQPGVAM\n",
      "✅ Gepaddetes Epitop gespeichert: TDDNALAYY\n",
      "✅ Gepaddetes Epitop gespeichert: CCIEVINNT\n",
      "✅ Gepaddetes Epitop gespeichert: LQAGNATEV\n",
      "✅ Gepaddetes Epitop gespeichert: SLLWMNPFV\n",
      "✅ Gepaddetes Epitop gespeichert: APGQTGVIA\n",
      "✅ Gepaddetes Epitop gespeichert: ILTGLNYEA\n",
      "✅ Gepaddetes Epitop gespeichert: VLWAHGFEL\n",
      "✅ Gepaddetes Epitop gespeichert: HRRGSRSYV\n",
      "✅ Gepaddetes Epitop gespeichert: PLIELCVDEAGSKSPIQYIDIGNYTVSCLPFTINCQE\n",
      "✅ Gepaddetes Epitop gespeichert: RLHDLVLPL\n",
      "✅ Gepaddetes Epitop gespeichert: KQLPFFYYS\n",
      "✅ Gepaddetes Epitop gespeichert: ALWGPDPAAA\n",
      "✅ Gepaddetes Epitop gespeichert: LAITPEIAPYF\n",
      "✅ Gepaddetes Epitop gespeichert: GLIAIVMVT\n",
      "✅ Gepaddetes Epitop gespeichert: KLNEEIAII\n",
      "✅ Gepaddetes Epitop gespeichert: AVGSHVYSV\n",
      "✅ Gepaddetes Epitop gespeichert: YFTSDYYQL\n",
      "✅ Gepaddetes Epitop gespeichert: KLSYGIATV\n",
      "✅ Gepaddetes Epitop gespeichert: SSDNIALLV\n",
      "✅ Gepaddetes Epitop gespeichert: FLPFAMGII\n",
      "✅ Gepaddetes Epitop gespeichert: RLSFKELLV\n",
      "✅ Gepaddetes Epitop gespeichert: AMAVLYLAL\n",
      "✅ Gepaddetes Epitop gespeichert: TLREIRRYQK\n",
      "✅ Gepaddetes Epitop gespeichert: FSDVTWFHA\n",
      "✅ Gepaddetes Epitop gespeichert: AEIRASANLAATK\n",
      "✅ Gepaddetes Epitop gespeichert: NSVTSSIVITSGDGTTSPISEHDYQIGGYTEKWESGVK\n",
      "✅ Gepaddetes Epitop gespeichert: MVNTVAGAMK\n",
      "✅ Gepaddetes Epitop gespeichert: LLSIIFFPA\n",
      "✅ Gepaddetes Epitop gespeichert: ALTPVVVTL\n",
      "✅ Gepaddetes Epitop gespeichert: TSDLATNNLVVMAY\n",
      "✅ Gepaddetes Epitop gespeichert: FVSNGTHWF\n",
      "✅ Gepaddetes Epitop gespeichert: FLMASISSS\n",
      "✅ Gepaddetes Epitop gespeichert: RLRPGGKKR\n",
      "✅ Gepaddetes Epitop gespeichert: MLFTMLRKL\n",
      "✅ Gepaddetes Epitop gespeichert: SVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQL\n",
      "✅ Gepaddetes Epitop gespeichert: FSVVGNICY\n",
      "✅ Gepaddetes Epitop gespeichert: APRGPHAGAASGL\n",
      "✅ Gepaddetes Epitop gespeichert: APRGPAGGAASGL\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGLALIEV\n",
      "✅ Gepaddetes Epitop gespeichert: TFTYASALW\n",
      "✅ Gepaddetes Epitop gespeichert: VVFLHVTYV\n",
      "✅ Gepaddetes Epitop gespeichert: NVFAFPFTI\n",
      "✅ Gepaddetes Epitop gespeichert: AMAGSLVFL\n",
      "✅ Gepaddetes Epitop gespeichert: QWNLVIGFL\n",
      "✅ Gepaddetes Epitop gespeichert: FMSAYSGVL\n",
      "✅ Gepaddetes Epitop gespeichert: FLEIYTVTV\n",
      "✅ Gepaddetes Epitop gespeichert: MPASWVMRI\n",
      "✅ Gepaddetes Epitop gespeichert: FLCWHTNCY\n",
      "✅ Gepaddetes Epitop gespeichert: YQYTFPDF\n",
      "✅ Gepaddetes Epitop gespeichert: YMPTTIIAA\n",
      "✅ Gepaddetes Epitop gespeichert: ELSLIDFYL\n",
      "✅ Gepaddetes Epitop gespeichert: KVLEYVIKV\n",
      "✅ Gepaddetes Epitop gespeichert: MQLFFSYFA\n",
      "✅ Gepaddetes Epitop gespeichert: QYNKYVEVF\n",
      "✅ Gepaddetes Epitop gespeichert: TLMNVITLV\n",
      "✅ Gepaddetes Epitop gespeichert: LPAADLDDF\n",
      "✅ Gepaddetes Epitop gespeichert: AYRDLQTRK\n",
      "✅ Gepaddetes Epitop gespeichert: LPRRSGAAGA\n",
      "✅ Gepaddetes Epitop gespeichert: QYSPVQATF\n",
      "✅ Gepaddetes Epitop gespeichert: IIFLHEDLTV\n",
      "✅ Gepaddetes Epitop gespeichert: KLSHQLVLL\n",
      "✅ Gepaddetes Epitop gespeichert: RLRAEAQVK\n",
      "✅ Gepaddetes Epitop gespeichert: RLYYDSMSY\n",
      "✅ Gepaddetes Epitop gespeichert: CTSICSLY\n",
      "✅ Gepaddetes Epitop gespeichert: HPVTKYIM\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGYPVYV\n",
      "✅ Gepaddetes Epitop gespeichert: GTIRPEIREMW\n",
      "✅ Gepaddetes Epitop gespeichert: ALYGFVPVL\n",
      "✅ Gepaddetes Epitop gespeichert: IPIQASLSF\n",
      "✅ Gepaddetes Epitop gespeichert: CLEASFNYL\n",
      "✅ Gepaddetes Epitop gespeichert: DLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYE\n",
      "✅ Gepaddetes Epitop gespeichert: RSFVPEIGMGF\n",
      "✅ Gepaddetes Epitop gespeichert: YLEPGPVAA\n",
      "✅ Gepaddetes Epitop gespeichert: VLNPFIYML\n",
      "✅ Gepaddetes Epitop gespeichert: RKHFSMMILS\n",
      "✅ Gepaddetes Epitop gespeichert: FIDIKRGVY\n",
      "✅ Gepaddetes Epitop gespeichert: HSYFTSDYY\n",
      "✅ Gepaddetes Epitop gespeichert: ALLAVFQSA\n",
      "✅ Gepaddetes Epitop gespeichert: IVLPEDKSW\n",
      "✅ Gepaddetes Epitop gespeichert: FRLIARLL\n",
      "✅ Gepaddetes Epitop gespeichert: AVLQSGFRK\n",
      "✅ Gepaddetes Epitop gespeichert: RYPLTFGWCF\n",
      "✅ Gepaddetes Epitop gespeichert: HLLLVAAGL\n",
      "✅ Gepaddetes Epitop gespeichert: IYLDLPWYL\n",
      "✅ Gepaddetes Epitop gespeichert: TQMNLKYAI\n",
      "✅ Gepaddetes Epitop gespeichert: KLQCVDLHV\n",
      "✅ Gepaddetes Epitop gespeichert: VQELYSPIFLIV\n",
      "✅ Gepaddetes Epitop gespeichert: NLVPMVAAV\n",
      "✅ Gepaddetes Epitop gespeichert: VLAWLYAAV\n",
      "✅ Gepaddetes Epitop gespeichert: NMMVTNNTF\n",
      "✅ Gepaddetes Epitop gespeichert: FNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKL\n",
      "✅ Gepaddetes Epitop gespeichert: IIGWMWIPV\n",
      "✅ Gepaddetes Epitop gespeichert: TSTLSEQVAW\n",
      "✅ Gepaddetes Epitop gespeichert: ALLLQLFTL\n",
      "✅ Gepaddetes Epitop gespeichert: AVIKTLQPV\n",
      "✅ Gepaddetes Epitop gespeichert: NSAIGKIQY\n",
      "✅ Gepaddetes Epitop gespeichert: KLSHQPVLL\n",
      "✅ Gepaddetes Epitop gespeichert: MMISAGFSL\n",
      "✅ Gepaddetes Epitop gespeichert: LLDEKGPEV\n",
      "✅ Gepaddetes Epitop gespeichert: LPEPLPQGQGTAY\n",
      "✅ Gepaddetes Epitop gespeichert: GTSGSPIIDK\n",
      "✅ Gepaddetes Epitop gespeichert: LLSAGIFGA\n",
      "✅ Gepaddetes Epitop gespeichert: MMWDRGLGMM\n",
      "✅ Gepaddetes Epitop gespeichert: ELAAIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: QLCDVMFYL\n",
      "✅ Gepaddetes Epitop gespeichert: IFLWLLWPV\n",
      "✅ Gepaddetes Epitop gespeichert: RSVASQSIIAYTMSL\n",
      "✅ Gepaddetes Epitop gespeichert: AVFDRKSDAK\n",
      "✅ Gepaddetes Epitop gespeichert: LEVVSYAPFTL\n",
      "✅ Gepaddetes Epitop gespeichert: SMQNCVLKL\n",
      "✅ Gepaddetes Epitop gespeichert: SLIDLQELGKYEQYIKW\n",
      "✅ Gepaddetes Epitop gespeichert: TLSKPSPMPV\n",
      "✅ Gepaddetes Epitop gespeichert: YLGGMSYYC\n",
      "✅ Gepaddetes Epitop gespeichert: YVWKSYVHV\n",
      "✅ Gepaddetes Epitop gespeichert: YLRPRTFLL\n",
      "✅ Gepaddetes Epitop gespeichert: CLLGTYTQDV\n",
      "✅ Gepaddetes Epitop gespeichert: LIIPCIHLI\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGILTI\n",
      "✅ Gepaddetes Epitop gespeichert: RTMEVCYNT\n",
      "✅ Gepaddetes Epitop gespeichert: YLNDHLEPWI\n",
      "✅ Gepaddetes Epitop gespeichert: YINVFAFPF\n",
      "✅ Gepaddetes Epitop gespeichert: TVYEKLKPV\n",
      "✅ Gepaddetes Epitop gespeichert: LIIPFIHLI\n",
      "✅ Gepaddetes Epitop gespeichert: FKDNVILLN\n",
      "✅ Gepaddetes Epitop gespeichert: GVYALIAGA\n",
      "✅ Gepaddetes Epitop gespeichert: CLGSLIYST\n",
      "✅ Gepaddetes Epitop gespeichert: FLPFFSNVT\n",
      "✅ Gepaddetes Epitop gespeichert: LIDFYLCFL\n",
      "✅ Gepaddetes Epitop gespeichert: LDYIINLII\n",
      "✅ Gepaddetes Epitop gespeichert: VEQCCTSICSL\n",
      "✅ Gepaddetes Epitop gespeichert: ESITGSLGPLL\n",
      "✅ Gepaddetes Epitop gespeichert: APRGPHGGAASGL\n",
      "✅ Gepaddetes Epitop gespeichert: FTISVTTEIL\n",
      "✅ Gepaddetes Epitop gespeichert: AMDEFIEQY\n",
      "✅ Gepaddetes Epitop gespeichert: LLTNMFTPL\n",
      "✅ Gepaddetes Epitop gespeichert: STQWSLFFF\n",
      "✅ Gepaddetes Epitop gespeichert: GCQEICPAC\n",
      "✅ Gepaddetes Epitop gespeichert: QLRTLGLNAV\n",
      "✅ Gepaddetes Epitop gespeichert: MGYINVFAFPFTIYSLLLCRMN\n",
      "✅ Gepaddetes Epitop gespeichert: KLIEYTDFA\n",
      "✅ Gepaddetes Epitop gespeichert: NTQGYFPDW\n",
      "✅ Gepaddetes Epitop gespeichert: MMFTSDLAT\n",
      "✅ Gepaddetes Epitop gespeichert: AARAVFLAL\n",
      "✅ Gepaddetes Epitop gespeichert: STDVTIATY\n",
      "✅ Gepaddetes Epitop gespeichert: AYSSAGASI\n",
      "✅ Gepaddetes Epitop gespeichert: KLGGALQAK\n",
      "✅ Gepaddetes Epitop gespeichert: FFSYFAVHF\n",
      "✅ Gepaddetes Epitop gespeichert: YMDDVVLGA\n",
      "✅ Gepaddetes Epitop gespeichert: ATDALMTGY\n",
      "✅ Gepaddetes Epitop gespeichert: MVWGPDPLYV\n",
      "✅ Gepaddetes Epitop gespeichert: TEILPVSMTK\n",
      "✅ Gepaddetes Epitop gespeichert: TLRGRAYGL\n",
      "✅ Gepaddetes Epitop gespeichert: WLSLLVPFV\n",
      "✅ Gepaddetes Epitop gespeichert: TQNVLYENQK\n",
      "✅ Gepaddetes Epitop gespeichert: FKDQVILLN\n",
      "✅ Gepaddetes Epitop gespeichert: EMLFSHGLVK\n",
      "✅ Gepaddetes Epitop gespeichert: WLDNFELCL\n",
      "✅ Gepaddetes Epitop gespeichert: YLQPRTFLL\n",
      "✅ Gepaddetes Epitop gespeichert: GMGPLLAMV\n",
      "✅ Gepaddetes Epitop gespeichert: KPASRELKV\n",
      "✅ Gepaddetes Epitop gespeichert: KLTPLCVTL\n",
      "✅ Gepaddetes Epitop gespeichert: IPIQASLPF\n",
      "✅ Gepaddetes Epitop gespeichert: FLSDHLYLV\n",
      "✅ Gepaddetes Epitop gespeichert: ISDEFSSNV\n",
      "✅ Gepaddetes Epitop gespeichert: TPINLVRDL\n",
      "✅ Gepaddetes Epitop gespeichert: ISPRTLNAW\n",
      "✅ Gepaddetes Epitop gespeichert: SVYAGILSYGV\n",
      "✅ Gepaddetes Epitop gespeichert: ACDPHSGHFV\n",
      "✅ Gepaddetes Epitop gespeichert: LDDKDPNFK\n",
      "✅ Gepaddetes Epitop gespeichert: TEDEHFEFY\n",
      "✅ Gepaddetes Epitop gespeichert: WLIVGVALL\n",
      "✅ Gepaddetes Epitop gespeichert: FMYSDFHFI\n",
      "✅ Gepaddetes Epitop gespeichert: VAAGLEAPFLYLYALVYFLQSINFVRIIMRLWLCWKCR\n",
      "✅ Gepaddetes Epitop gespeichert: YQFGPDFPIA\n",
      "✅ Gepaddetes Epitop gespeichert: WLIRETQPITK\n",
      "✅ Gepaddetes Epitop gespeichert: AHIQWMVMF\n",
      "✅ Gepaddetes Epitop gespeichert: FVLAAVYRI\n",
      "✅ Gepaddetes Epitop gespeichert: ALRANSAVK\n",
      "✅ Gepaddetes Epitop gespeichert: ALGIGAVPI\n",
      "✅ Gepaddetes Epitop gespeichert: KAALDLSHF\n",
      "✅ Gepaddetes Epitop gespeichert: VTIAEILLI\n",
      "✅ Gepaddetes Epitop gespeichert: YIDIGNYTV\n",
      "✅ Gepaddetes Epitop gespeichert: LMWLIINLV\n",
      "✅ Gepaddetes Epitop gespeichert: ILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASA\n",
      "✅ Gepaddetes Epitop gespeichert: KMVSLLSVL\n",
      "✅ Gepaddetes Epitop gespeichert: AMMFTSDLA\n",
      "✅ Gepaddetes Epitop gespeichert: TVLSFCAFAV\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGLTV\n",
      "✅ Gepaddetes Epitop gespeichert: ILEGIGILSV\n",
      "✅ Gepaddetes Epitop gespeichert: RSRNSSRNL\n",
      "✅ Gepaddetes Epitop gespeichert: VYFLQSINF\n",
      "✅ Gepaddetes Epitop gespeichert: VLYENQKLI\n",
      "✅ Gepaddetes Epitop gespeichert: ILNVDIFTL\n",
      "✅ Gepaddetes Epitop gespeichert: VTEHDTLLY\n",
      "✅ Gepaddetes Epitop gespeichert: FAFRWVLGIAY\n",
      "✅ Gepaddetes Epitop gespeichert: LFFSDVTWF\n",
      "✅ Gepaddetes Epitop gespeichert: DRVMLIAPR\n",
      "✅ Gepaddetes Epitop gespeichert: MLAWIFLPI\n",
      "✅ Gepaddetes Epitop gespeichert: RSGGFSFGK\n",
      "✅ Gepaddetes Epitop gespeichert: KYIKTWRPRYF\n",
      "✅ Gepaddetes Epitop gespeichert: KISEMHPAL\n",
      "✅ Gepaddetes Epitop gespeichert: GLCTLVAML\n",
      "✅ Gepaddetes Epitop gespeichert: ALNLGETFV\n",
      "✅ Gepaddetes Epitop gespeichert: LLLNMLIVL\n",
      "✅ Gepaddetes Epitop gespeichert: MPNMLRIMA\n",
      "✅ Gepaddetes Epitop gespeichert: FLKEQGGL\n",
      "✅ Gepaddetes Epitop gespeichert: QYIKWPWYI\n",
      "✅ Gepaddetes Epitop gespeichert: ASQRVAGDSGFAAY\n",
      "✅ Gepaddetes Epitop gespeichert: NPTIQKNVL\n",
      "✅ Gepaddetes Epitop gespeichert: KRKEPLKI\n",
      "✅ Gepaddetes Epitop gespeichert: ATDALMTGF\n",
      "✅ Gepaddetes Epitop gespeichert: SLAPLSPRA\n",
      "✅ Gepaddetes Epitop gespeichert: FTDTCFANK\n",
      "✅ Gepaddetes Epitop gespeichert: FGEVFNATRFASVY\n",
      "✅ Gepaddetes Epitop gespeichert: FAFPFTIYS\n",
      "✅ Gepaddetes Epitop gespeichert: IKLDDKDPN\n",
      "✅ Gepaddetes Epitop gespeichert: VTDVTQLYL\n",
      "✅ Gepaddetes Epitop gespeichert: IVEQCCTSIC\n",
      "✅ Gepaddetes Epitop gespeichert: QIKVRVDMV\n",
      "✅ Gepaddetes Epitop gespeichert: LRVMMLAPF\n",
      "✅ Gepaddetes Epitop gespeichert: GLALYYPSA\n",
      "✅ Gepaddetes Epitop gespeichert: FLRGRAYVL\n",
      "✅ Gepaddetes Epitop gespeichert: TINCQEPKLGSLVVRCSFYEDFLEYHDVRVVLDFI\n",
      "✅ Gepaddetes Epitop gespeichert: FLNVFFPLL\n",
      "✅ Gepaddetes Epitop gespeichert: MPYGYVLNEF\n",
      "✅ Gepaddetes Epitop gespeichert: RVCTNYMPY\n",
      "✅ Gepaddetes Epitop gespeichert: RLLPLLALL\n",
      "✅ Gepaddetes Epitop gespeichert: VLQVRDVLV\n",
      "✅ Gepaddetes Epitop gespeichert: FPQSAPHGVVF\n",
      "✅ Gepaddetes Epitop gespeichert: RVGARKSAPL\n",
      "✅ Gepaddetes Epitop gespeichert: SSCMGGMNWR\n",
      "✅ Gepaddetes Epitop gespeichert: YTNDKACPL\n",
      "✅ Gepaddetes Epitop gespeichert: ARNLVPMVATVQGQN\n",
      "✅ Gepaddetes Epitop gespeichert: GPEPLPQGQLTAY\n",
      "✅ Gepaddetes Epitop gespeichert: LLLDRLNQL\n",
      "✅ Gepaddetes Epitop gespeichert: KQWLVWLFL\n",
      "✅ Gepaddetes Epitop gespeichert: SLEGGGLGY\n",
      "✅ Gepaddetes Epitop gespeichert: CMTSCCSCLK\n",
      "✅ Gepaddetes Epitop gespeichert: KFPRGQGVPI\n",
      "✅ Gepaddetes Epitop gespeichert: TSTLAEQVAW\n",
      "✅ Gepaddetes Epitop gespeichert: RARSVASQSI\n",
      "✅ Gepaddetes Epitop gespeichert: FLTENLLLY\n",
      "✅ Gepaddetes Epitop gespeichert: KLMNIQQKL\n",
      "✅ Gepaddetes Epitop gespeichert: VLFAAFSSA\n",
      "✅ Gepaddetes Epitop gespeichert: FQLNQSFEI\n",
      "✅ Gepaddetes Epitop gespeichert: LLYGFVNYV\n",
      "✅ Gepaddetes Epitop gespeichert: RQFGPDWIVA\n",
      "✅ Gepaddetes Epitop gespeichert: TDDNALSYY\n",
      "✅ Gepaddetes Epitop gespeichert: TAMDIFVTV\n",
      "✅ Gepaddetes Epitop gespeichert: TYASALWEI\n",
      "✅ Gepaddetes Epitop gespeichert: TPRDLGACI\n",
      "✅ Gepaddetes Epitop gespeichert: LSDAFVSV\n",
      "✅ Gepaddetes Epitop gespeichert: TVYDPLQPELDSFK\n",
      "✅ Gepaddetes Epitop gespeichert: VYIGDPAQL\n",
      "✅ Gepaddetes Epitop gespeichert: RLSDFSEQL\n",
      "✅ Gepaddetes Epitop gespeichert: MLSDTLKNL\n",
      "✅ Gepaddetes Epitop gespeichert: LQNNELSPV\n",
      "✅ Gepaddetes Epitop gespeichert: CTELKLNDY\n",
      "✅ Gepaddetes Epitop gespeichert: SLLWMNLFV\n",
      "✅ Gepaddetes Epitop gespeichert: LLFALHFSA\n",
      "✅ Gepaddetes Epitop gespeichert: RTLNAWVKV\n",
      "✅ Gepaddetes Epitop gespeichert: YLLAIFSGL\n",
      "✅ Gepaddetes Epitop gespeichert: HFISNSWLM\n",
      "✅ Gepaddetes Epitop gespeichert: TYHGAIKLD\n",
      "✅ Gepaddetes Epitop gespeichert: KVDPIGHVY\n",
      "✅ Gepaddetes Epitop gespeichert: KLLEIAPNC\n",
      "✅ Gepaddetes Epitop gespeichert: LSSTASALGK\n",
      "✅ Gepaddetes Epitop gespeichert: TMSFSHLFYL\n",
      "✅ Gepaddetes Epitop gespeichert: VIYMGTLSY\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGIITV\n",
      "✅ Gepaddetes Epitop gespeichert: LLLGIGILV\n",
      "✅ Gepaddetes Epitop gespeichert: TSTLTEQIAW\n",
      "✅ Gepaddetes Epitop gespeichert: FLGIITTVA\n",
      "✅ Gepaddetes Epitop gespeichert: GDAALALLLLDRLNQL\n",
      "✅ Gepaddetes Epitop gespeichert: GTHWFVTQR\n",
      "✅ Gepaddetes Epitop gespeichert: LTITPEIRPYF\n",
      "✅ Gepaddetes Epitop gespeichert: SMWALVISV\n",
      "✅ Gepaddetes Epitop gespeichert: PQFKDNVIL\n",
      "✅ Gepaddetes Epitop gespeichert: YLEPGPVTA\n",
      "✅ Gepaddetes Epitop gespeichert: SYMIMEIE\n",
      "✅ Gepaddetes Epitop gespeichert: QCCTSICSL\n",
      "✅ Gepaddetes Epitop gespeichert: FLDEFMEGV\n",
      "✅ Gepaddetes Epitop gespeichert: RIMTWLDMV\n",
      "✅ Gepaddetes Epitop gespeichert: VLLSMQGAV\n",
      "✅ Gepaddetes Epitop gespeichert: FLPFFSNVTWFHAI\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGMPPCL\n",
      "✅ Gepaddetes Epitop gespeichert: FPPTSFGPL\n",
      "✅ Gepaddetes Epitop gespeichert: RYSIFFDYM\n",
      "✅ Gepaddetes Epitop gespeichert: ILNAMIAKI\n",
      "✅ Gepaddetes Epitop gespeichert: RLGPYLEFL\n",
      "✅ Gepaddetes Epitop gespeichert: FEDLRVSSF\n",
      "✅ Gepaddetes Epitop gespeichert: TSTLAEQIAW\n",
      "✅ Gepaddetes Epitop gespeichert: NLAPMVATV\n",
      "✅ Gepaddetes Epitop gespeichert: AIILASFSAST\n",
      "✅ Gepaddetes Epitop gespeichert: FQAQHIAMA\n",
      "✅ Gepaddetes Epitop gespeichert: APRGAHGGAASGL\n",
      "✅ Gepaddetes Epitop gespeichert: IVGVALLAVF\n",
      "✅ Gepaddetes Epitop gespeichert: TTLNDFNLV\n",
      "✅ Gepaddetes Epitop gespeichert: LLSVCLGSL\n",
      "✅ Gepaddetes Epitop gespeichert: EYYELFVNI\n",
      "✅ Gepaddetes Epitop gespeichert: RNPANNAAIVL\n",
      "✅ Gepaddetes Epitop gespeichert: YANRNRFLY\n",
      "✅ Gepaddetes Epitop gespeichert: LMNVLTLVY\n",
      "✅ Gepaddetes Epitop gespeichert: SRVMLLNPK\n",
      "✅ Gepaddetes Epitop gespeichert: RLLQCTQQAV\n",
      "✅ Gepaddetes Epitop gespeichert: IPLTTSSKL\n",
      "✅ Gepaddetes Epitop gespeichert: FPRPWLHGL\n",
      "✅ Gepaddetes Epitop gespeichert: LLDTVLVNV\n",
      "✅ Gepaddetes Epitop gespeichert: AIKLDDKDP\n",
      "✅ Gepaddetes Epitop gespeichert: YLSFYTAEQL\n",
      "✅ Gepaddetes Epitop gespeichert: YIDDVVLGA\n",
      "✅ Gepaddetes Epitop gespeichert: LLEWLAMAV\n",
      "✅ Gepaddetes Epitop gespeichert: FGDHPGHSY\n",
      "✅ Gepaddetes Epitop gespeichert: RAKFKQLL\n",
      "✅ Gepaddetes Epitop gespeichert: LLFAAFSSA\n",
      "✅ Gepaddetes Epitop gespeichert: TYIKWPWWVW\n",
      "✅ Gepaddetes Epitop gespeichert: AVLDMCASL\n",
      "✅ Gepaddetes Epitop gespeichert: VLMGGVPGVE\n",
      "✅ Gepaddetes Epitop gespeichert: SPIFRIVAA\n",
      "✅ Gepaddetes Epitop gespeichert: KDPQFKDNV\n",
      "✅ Gepaddetes Epitop gespeichert: QINDMILSL\n",
      "✅ Gepaddetes Epitop gespeichert: GQMWLLAPR\n",
      "✅ Gepaddetes Epitop gespeichert: VTLAILTAL\n",
      "✅ Gepaddetes Epitop gespeichert: YLLFMIGYA\n",
      "✅ Gepaddetes Epitop gespeichert: GRLMLIAPK\n",
      "✅ Gepaddetes Epitop gespeichert: YILEETSVM\n",
      "✅ Gepaddetes Epitop gespeichert: LVFLFVAAI\n",
      "✅ Gepaddetes Epitop gespeichert: LLIASVTWL\n",
      "✅ Gepaddetes Epitop gespeichert: IAMAVLYLA\n",
      "✅ Gepaddetes Epitop gespeichert: SQASSRSSSR\n",
      "✅ Gepaddetes Epitop gespeichert: FMGVTYEM\n",
      "✅ Gepaddetes Epitop gespeichert: SSSTCMMCY\n",
      "✅ Gepaddetes Epitop gespeichert: LLLDDFVEII\n",
      "✅ Gepaddetes Epitop gespeichert: IMIEFCPGG\n",
      "✅ Gepaddetes Epitop gespeichert: FPDLNGDVV\n",
      "✅ Gepaddetes Epitop gespeichert: HIGIGLLSL\n",
      "✅ Gepaddetes Epitop gespeichert: MLAKALRKV\n",
      "✅ Gepaddetes Epitop gespeichert: HLRMAGHSL\n",
      "✅ Gepaddetes Epitop gespeichert: LMLWASSSI\n",
      "✅ Gepaddetes Epitop gespeichert: ILGGQVVHTV\n",
      "✅ Gepaddetes Epitop gespeichert: WLTYHGAIK\n",
      "✅ Gepaddetes Epitop gespeichert: NYNYLYRLF\n",
      "✅ Gepaddetes Epitop gespeichert: GMEVTPSGTWLTY\n",
      "✅ Gepaddetes Epitop gespeichert: LPPSYTNSF\n",
      "✅ Gepaddetes Epitop gespeichert: VAANIVLTV\n",
      "✅ Gepaddetes Epitop gespeichert: IPIGAGICASY\n",
      "✅ Gepaddetes Epitop gespeichert: YVDNSSLTI\n",
      "✅ Gepaddetes Epitop gespeichert: YIDIGDYTV\n",
      "✅ Gepaddetes Epitop gespeichert: VYSTGSNVFQTR\n",
      "✅ Gepaddetes Epitop gespeichert: IAGLGLLTV\n",
      "✅ Gepaddetes Epitop gespeichert: SEHDYQIGGYTEKW\n",
      "✅ Gepaddetes Epitop gespeichert: FVDGVPFVV\n",
      "✅ Gepaddetes Epitop gespeichert: ITEEVGHTDLMAAY\n",
      "✅ Gepaddetes Epitop gespeichert: LMNVLTLFY\n",
      "✅ Gepaddetes Epitop gespeichert: VEWLGRCIL\n",
      "✅ Gepaddetes Epitop gespeichert: FVTVYSHLL\n",
      "✅ Gepaddetes Epitop gespeichert: FMLLTQARL\n",
      "✅ Gepaddetes Epitop gespeichert: LMDYWQGQL\n",
      "✅ Gepaddetes Epitop gespeichert: GRIAFSLKY\n",
      "✅ Gepaddetes Epitop gespeichert: LVFPLVMGV\n",
      "✅ Gepaddetes Epitop gespeichert: LWLLWPVTL\n",
      "✅ Gepaddetes Epitop gespeichert: GVYDYLVST\n",
      "✅ Gepaddetes Epitop gespeichert: ALHGGWTTK\n",
      "✅ Gepaddetes Epitop gespeichert: HSKKKCDEI\n",
      "✅ Gepaddetes Epitop gespeichert: NRLMMIAPR\n",
      "✅ Gepaddetes Epitop gespeichert: SYMIMEIEL\n",
      "✅ Gepaddetes Epitop gespeichert: SRVLHKLEV\n",
      "✅ Gepaddetes Epitop gespeichert: LVVDFSQFSR\n",
      "✅ Gepaddetes Epitop gespeichert: NLTTRTQL\n",
      "✅ Gepaddetes Epitop gespeichert: FMCVEYCPI\n",
      "✅ Gepaddetes Epitop gespeichert: VLFGLGFAI\n",
      "✅ Gepaddetes Epitop gespeichert: NLVPMVASV\n",
      "✅ Gepaddetes Epitop gespeichert: FLIYLDVSV\n",
      "✅ Gepaddetes Epitop gespeichert: VMAPRTLVL\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGALTV\n",
      "✅ Gepaddetes Epitop gespeichert: IIWFLLLSV\n",
      "✅ Gepaddetes Epitop gespeichert: PVTLACFVL\n",
      "✅ Gepaddetes Epitop gespeichert: FLEYHDVRV\n",
      "✅ Gepaddetes Epitop gespeichert: QIKVRVKMV\n",
      "✅ Gepaddetes Epitop gespeichert: TYDTVHRHL\n",
      "✅ Gepaddetes Epitop gespeichert: KLKDCVMYA\n",
      "✅ Gepaddetes Epitop gespeichert: LPPANTNSF\n",
      "✅ Gepaddetes Epitop gespeichert: LTSHTVMPL\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: ATSPASASK\n",
      "✅ Gepaddetes Epitop gespeichert: YLCSGSSYFV\n",
      "✅ Gepaddetes Epitop gespeichert: RKTVRARSRTPSCRSRSHTPSRRRR\n",
      "✅ Gepaddetes Epitop gespeichert: TLYSLTLLY\n",
      "✅ Gepaddetes Epitop gespeichert: MLNIPSINV\n",
      "✅ Gepaddetes Epitop gespeichert: FLWLLWPVTLACFVLAAV\n",
      "✅ Gepaddetes Epitop gespeichert: TIVEVQPQL\n",
      "✅ Gepaddetes Epitop gespeichert: FIASNGVKLV\n",
      "✅ Gepaddetes Epitop gespeichert: VENPHLMGWD\n",
      "✅ Gepaddetes Epitop gespeichert: LMIERFVSL\n",
      "✅ Gepaddetes Epitop gespeichert: RTQSPRRR\n",
      "✅ Gepaddetes Epitop gespeichert: TLIGDCATV\n",
      "✅ Gepaddetes Epitop gespeichert: YLFDESGEF\n",
      "✅ Gepaddetes Epitop gespeichert: FLPGVYSVI\n",
      "✅ Gepaddetes Epitop gespeichert: STQDLFLPFF\n",
      "✅ Gepaddetes Epitop gespeichert: IMLCCMTSC\n",
      "✅ Gepaddetes Epitop gespeichert: ITSGWTFGA\n",
      "✅ Gepaddetes Epitop gespeichert: DQVILLNKH\n",
      "✅ Gepaddetes Epitop gespeichert: QSSPVQATF\n",
      "✅ Gepaddetes Epitop gespeichert: NLVPTVATV\n",
      "✅ Gepaddetes Epitop gespeichert: LIFNMLYWI\n",
      "✅ Gepaddetes Epitop gespeichert: QYIKWPWYIW\n",
      "✅ Gepaddetes Epitop gespeichert: NSFRPEVAMKY\n",
      "✅ Gepaddetes Epitop gespeichert: GIGIGSGQL\n",
      "✅ Gepaddetes Epitop gespeichert: GMGPLLATV\n",
      "✅ Gepaddetes Epitop gespeichert: ALWMRLLPL\n",
      "✅ Gepaddetes Epitop gespeichert: STLPETAVVRR\n",
      "✅ Gepaddetes Epitop gespeichert: LLAGTLAGV\n",
      "✅ Gepaddetes Epitop gespeichert: ALAGIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: FQPTNGVGY\n",
      "✅ Gepaddetes Epitop gespeichert: YQKVGMQKY\n",
      "✅ Gepaddetes Epitop gespeichert: CRVLCCYVL\n",
      "✅ Gepaddetes Epitop gespeichert: YLQDSLATT\n",
      "✅ Gepaddetes Epitop gespeichert: CGSHLVEALYL\n",
      "✅ Gepaddetes Epitop gespeichert: TEKSNIIRGW\n",
      "✅ Gepaddetes Epitop gespeichert: TLVPQEHYV\n",
      "✅ Gepaddetes Epitop gespeichert: ERGFFYTPK\n",
      "✅ Gepaddetes Epitop gespeichert: KSKRTPMGF\n",
      "✅ Gepaddetes Epitop gespeichert: FTEQPIDLVPNQPY\n",
      "✅ Gepaddetes Epitop gespeichert: FITESKPSV\n",
      "✅ Gepaddetes Epitop gespeichert: KLSGLGINAV\n",
      "✅ Gepaddetes Epitop gespeichert: SLLLCRMNSRNYIA\n",
      "✅ Gepaddetes Epitop gespeichert: YYTSNPTTF\n",
      "✅ Gepaddetes Epitop gespeichert: LPGCDGGSL\n",
      "✅ Gepaddetes Epitop gespeichert: LMDMHNGQL\n",
      "✅ Gepaddetes Epitop gespeichert: KIILFLPCI\n",
      "✅ Gepaddetes Epitop gespeichert: RLDKVEAEV\n",
      "✅ Gepaddetes Epitop gespeichert: YEDFLEYHDVRVVL\n",
      "✅ Gepaddetes Epitop gespeichert: VTDNTFTLK\n",
      "✅ Gepaddetes Epitop gespeichert: YVFHLYLQY\n",
      "✅ Gepaddetes Epitop gespeichert: IVDTVSALV\n",
      "✅ Gepaddetes Epitop gespeichert: GRKLFGTHF\n",
      "✅ Gepaddetes Epitop gespeichert: LLMPILTLT\n",
      "✅ Gepaddetes Epitop gespeichert: YLYDRLLRI\n",
      "✅ Gepaddetes Epitop gespeichert: KLKSLGLNAV\n",
      "✅ Gepaddetes Epitop gespeichert: ALYGSVPVL\n",
      "✅ Gepaddetes Epitop gespeichert: ALAWVFVPI\n",
      "✅ Gepaddetes Epitop gespeichert: LSGSPELRMIF\n",
      "✅ Gepaddetes Epitop gespeichert: SACVLAAEC\n",
      "✅ Gepaddetes Epitop gespeichert: ITDQVPFSV\n",
      "✅ Gepaddetes Epitop gespeichert: KLNIKLLGV\n",
      "✅ Gepaddetes Epitop gespeichert: YIFLGNLAL\n",
      "✅ Gepaddetes Epitop gespeichert: FVNEFYAYL\n",
      "✅ Gepaddetes Epitop gespeichert: YLDAYNMMI\n",
      "✅ Gepaddetes Epitop gespeichert: TWLTYTGAI\n",
      "✅ Gepaddetes Epitop gespeichert: WVLALFDEV\n",
      "✅ Gepaddetes Epitop gespeichert: FEDLRVLSF\n",
      "✅ Gepaddetes Epitop gespeichert: RLITGRLQSL\n",
      "✅ Gepaddetes Epitop gespeichert: TPQDLNTML\n",
      "✅ Gepaddetes Epitop gespeichert: FVMMSAPPA\n",
      "✅ Gepaddetes Epitop gespeichert: GLTVLPPLL\n",
      "✅ Gepaddetes Epitop gespeichert: MQVESDDYI\n",
      "✅ Gepaddetes Epitop gespeichert: RPHERNGFTV\n",
      "✅ Gepaddetes Epitop gespeichert: VVGAVGVGK\n",
      "✅ Gepaddetes Epitop gespeichert: KLEQLDWTV\n",
      "✅ Gepaddetes Epitop gespeichert: CFLLILPNC\n",
      "✅ Gepaddetes Epitop gespeichert: APSASAFFG\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGLLTV\n",
      "✅ Gepaddetes Epitop gespeichert: NLNESLIDL\n",
      "✅ Gepaddetes Epitop gespeichert: FTYASALWEI\n",
      "✅ Gepaddetes Epitop gespeichert: YTNSFTRGV\n",
      "✅ Gepaddetes Epitop gespeichert: LYPEFIASI\n",
      "✅ Gepaddetes Epitop gespeichert: AELAKNVSLDNVL\n",
      "✅ Gepaddetes Epitop gespeichert: KLANPLPYT\n",
      "✅ Gepaddetes Epitop gespeichert: FLDGSCGSV\n",
      "✅ Gepaddetes Epitop gespeichert: EIQEFCPNT\n",
      "✅ Gepaddetes Epitop gespeichert: SPFHPLADNKFAL\n",
      "✅ Gepaddetes Epitop gespeichert: EIYKRWII\n",
      "✅ Gepaddetes Epitop gespeichert: FIAGLIAIV\n",
      "✅ Gepaddetes Epitop gespeichert: SMDNSPNLA\n",
      "✅ Gepaddetes Epitop gespeichert: YLWPPVQLA\n",
      "✅ Gepaddetes Epitop gespeichert: PMDSTVKNY\n",
      "✅ Gepaddetes Epitop gespeichert: KQLIKVTLV\n",
      "✅ Gepaddetes Epitop gespeichert: NRDVDTDFVNEFYAY\n",
      "✅ Gepaddetes Epitop gespeichert: LEPLVDLPI\n",
      "✅ Gepaddetes Epitop gespeichert: LQFAYANRNRFLY\n",
      "✅ Gepaddetes Epitop gespeichert: VSDGTHWFV\n",
      "✅ Gepaddetes Epitop gespeichert: ALIAPVHAV\n",
      "✅ Gepaddetes Epitop gespeichert: LQIPFAMQMAY\n",
      "✅ Gepaddetes Epitop gespeichert: VYQYTFPDFL\n",
      "✅ Gepaddetes Epitop gespeichert: QKRPIPIKYKAM\n",
      "✅ Gepaddetes Epitop gespeichert: IMNDMPIYM\n",
      "✅ Gepaddetes Epitop gespeichert: FPLDVGSIVAY\n",
      "✅ Gepaddetes Epitop gespeichert: SVAALTNNV\n",
      "✅ Gepaddetes Epitop gespeichert: LFTRFFYVL\n",
      "✅ Gepaddetes Epitop gespeichert: SGGGETALA\n",
      "✅ Gepaddetes Epitop gespeichert: GMDYHNGHL\n",
      "✅ Gepaddetes Epitop gespeichert: YTAYVFIPI\n",
      "✅ Gepaddetes Epitop gespeichert: YLLPAIVHI\n",
      "✅ Gepaddetes Epitop gespeichert: VYQYTFPDF\n",
      "✅ Gepaddetes Epitop gespeichert: FLNGSCGSV\n",
      "✅ Gepaddetes Epitop gespeichert: YYKKDNSYF\n",
      "✅ Gepaddetes Epitop gespeichert: LLLDDFVEI\n",
      "✅ Gepaddetes Epitop gespeichert: CALDPLSETK\n",
      "✅ Gepaddetes Epitop gespeichert: KLLAGDEIV\n",
      "✅ Gepaddetes Epitop gespeichert: KMVTVCPNT\n",
      "✅ Gepaddetes Epitop gespeichert: AIMEKNIVL\n",
      "✅ Gepaddetes Epitop gespeichert: TCDGTTFTY\n",
      "✅ Gepaddetes Epitop gespeichert: KITEHSWNA\n",
      "✅ Gepaddetes Epitop gespeichert: SLDPTTSPV\n",
      "✅ Gepaddetes Epitop gespeichert: KLSREIMPV\n",
      "✅ Gepaddetes Epitop gespeichert: WLPTGTLLV\n",
      "✅ Gepaddetes Epitop gespeichert: ALIHHNTYL\n",
      "✅ Gepaddetes Epitop gespeichert: NLDAVPTGY\n",
      "✅ Gepaddetes Epitop gespeichert: QADVEWKFY\n",
      "✅ Gepaddetes Epitop gespeichert: FEADTFFHFV\n",
      "✅ Gepaddetes Epitop gespeichert: LPEPLPQAQLTAY\n",
      "✅ Gepaddetes Epitop gespeichert: YEQYIKWPWYI\n",
      "✅ Gepaddetes Epitop gespeichert: YVDDVVLGA\n",
      "✅ Gepaddetes Epitop gespeichert: FLPFFSDVT\n",
      "✅ Gepaddetes Epitop gespeichert: TLEELDWCL\n",
      "✅ Gepaddetes Epitop gespeichert: ILGFVFTLT\n",
      "✅ Gepaddetes Epitop gespeichert: YGFQPTNGV\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGILSV\n",
      "✅ Gepaddetes Epitop gespeichert: FLASKIGRLV\n",
      "✅ Gepaddetes Epitop gespeichert: SVIYLYLTF\n",
      "✅ Gepaddetes Epitop gespeichert: RLHDLVFPL\n",
      "✅ Gepaddetes Epitop gespeichert: VLGSLAATV\n",
      "✅ Gepaddetes Epitop gespeichert: RLRPGGRKR\n",
      "✅ Gepaddetes Epitop gespeichert: ELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCL\n",
      "✅ Gepaddetes Epitop gespeichert: ARVMLVAPR\n",
      "✅ Gepaddetes Epitop gespeichert: FLTVFSHFM\n",
      "✅ Gepaddetes Epitop gespeichert: SFHSLHLLF\n",
      "✅ Gepaddetes Epitop gespeichert: QEVFAQVKQIYK\n",
      "✅ Gepaddetes Epitop gespeichert: YVDDSSLTI\n",
      "✅ Gepaddetes Epitop gespeichert: VQYLGMLPV\n",
      "✅ Gepaddetes Epitop gespeichert: SRIMLLAPK\n",
      "✅ Gepaddetes Epitop gespeichert: GFFYTPKT\n",
      "✅ Gepaddetes Epitop gespeichert: RFLYIIKLI\n",
      "✅ Gepaddetes Epitop gespeichert: KSCTPEVREYF\n",
      "✅ Gepaddetes Epitop gespeichert: LTNDNTSRY\n",
      "✅ Gepaddetes Epitop gespeichert: PTDSYITTY\n",
      "✅ Gepaddetes Epitop gespeichert: AAGIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: FLPDLNVNA\n",
      "✅ Gepaddetes Epitop gespeichert: HYPYRLWHY\n",
      "✅ Gepaddetes Epitop gespeichert: FMIGYASAL\n",
      "✅ Gepaddetes Epitop gespeichert: ASFNYLKSPNFSK\n",
      "✅ Gepaddetes Epitop gespeichert: FLPRVFSAV\n",
      "✅ Gepaddetes Epitop gespeichert: GRVGDLSPR\n",
      "✅ Gepaddetes Epitop gespeichert: ELRRKMMYM\n",
      "✅ Gepaddetes Epitop gespeichert: VSFGLQFQF\n",
      "✅ Gepaddetes Epitop gespeichert: LLNKHIDAY\n",
      "✅ Gepaddetes Epitop gespeichert: ALLSDLQDL\n",
      "✅ Gepaddetes Epitop gespeichert: HQNPVTGLLL\n",
      "✅ Gepaddetes Epitop gespeichert: FYGKTILWF\n",
      "✅ Gepaddetes Epitop gespeichert: KSWMESEFRVY\n",
      "✅ Gepaddetes Epitop gespeichert: ILLIIMRTFKVSIWNLDYII\n",
      "✅ Gepaddetes Epitop gespeichert: NTLQCIMLV\n",
      "✅ Gepaddetes Epitop gespeichert: FLAFVVFLL\n",
      "✅ Gepaddetes Epitop gespeichert: YLVALGINAV\n",
      "✅ Gepaddetes Epitop gespeichert: EIAGIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: KLNDLCFTNVYADSFVIR\n",
      "✅ Gepaddetes Epitop gespeichert: NYNYLYRLFRK\n",
      "✅ Gepaddetes Epitop gespeichert: VLSFCAFAV\n",
      "✅ Gepaddetes Epitop gespeichert: ISDYDYYRY\n",
      "✅ Gepaddetes Epitop gespeichert: HSKRKCDEL\n",
      "✅ Gepaddetes Epitop gespeichert: VLLGVKLSGV\n",
      "✅ Gepaddetes Epitop gespeichert: HTTDPSFLGRY\n",
      "✅ Gepaddetes Epitop gespeichert: FIASFRLFA\n",
      "✅ Gepaddetes Epitop gespeichert: SLIYSTAAL\n",
      "✅ Gepaddetes Epitop gespeichert: APRGPHGGAAAGL\n",
      "✅ Gepaddetes Epitop gespeichert: TVDLQSHWAA\n",
      "✅ Gepaddetes Epitop gespeichert: ITFDNLKTL\n",
      "✅ Gepaddetes Epitop gespeichert: SLPITVYYA\n",
      "✅ Gepaddetes Epitop gespeichert: CHIPVEPNT\n",
      "✅ Gepaddetes Epitop gespeichert: RQLLFVVEV\n",
      "✅ Gepaddetes Epitop gespeichert: RILLVAASY\n",
      "✅ Gepaddetes Epitop gespeichert: YTSEILSPI\n",
      "✅ Gepaddetes Epitop gespeichert: RIMASLVLA\n",
      "✅ Gepaddetes Epitop gespeichert: APISAMVRM\n",
      "✅ Gepaddetes Epitop gespeichert: RVDFCGKGY\n",
      "✅ Gepaddetes Epitop gespeichert: KLVALGINAV\n",
      "✅ Gepaddetes Epitop gespeichert: TYHWLLLTI\n",
      "✅ Gepaddetes Epitop gespeichert: SNEKQEILGTVSWNL\n",
      "✅ Gepaddetes Epitop gespeichert: NLSALGIFST\n",
      "✅ Gepaddetes Epitop gespeichert: LITLATCELYHYQECV\n",
      "✅ Gepaddetes Epitop gespeichert: KLLKSIAAT\n",
      "✅ Gepaddetes Epitop gespeichert: MCDIRQLLF\n",
      "✅ Gepaddetes Epitop gespeichert: AMQTMLFTM\n",
      "✅ Gepaddetes Epitop gespeichert: YSFKPELKEIF\n",
      "✅ Gepaddetes Epitop gespeichert: YTDFSSEII\n",
      "✅ Gepaddetes Epitop gespeichert: SANNCTFEY\n",
      "✅ Gepaddetes Epitop gespeichert: CTDDNALAY\n",
      "✅ Gepaddetes Epitop gespeichert: FPTKDVAL\n",
      "✅ Gepaddetes Epitop gespeichert: YYVGYLQPRTFLL\n",
      "✅ Gepaddetes Epitop gespeichert: FTISVTTEI\n",
      "✅ Gepaddetes Epitop gespeichert: FSASTSAFV\n",
      "✅ Gepaddetes Epitop gespeichert: FVGEFFTDV\n",
      "✅ Gepaddetes Epitop gespeichert: VAFNTLLFL\n",
      "✅ Gepaddetes Epitop gespeichert: KLPLDINPV\n",
      "✅ Gepaddetes Epitop gespeichert: APATVCGPK\n",
      "✅ Gepaddetes Epitop gespeichert: ARMILMTHF\n",
      "✅ Gepaddetes Epitop gespeichert: IWDYVNKELA\n",
      "✅ Gepaddetes Epitop gespeichert: YVNELVVFV\n",
      "✅ Gepaddetes Epitop gespeichert: LPEPLPQGQLTGY\n",
      "✅ Gepaddetes Epitop gespeichert: DNVILLNKH\n",
      "✅ Gepaddetes Epitop gespeichert: HQLGGVVEV\n",
      "✅ Gepaddetes Epitop gespeichert: FRNEGIHL\n",
      "✅ Gepaddetes Epitop gespeichert: LLWPVTLAC\n",
      "✅ Gepaddetes Epitop gespeichert: RPIPIKYKA\n",
      "✅ Gepaddetes Epitop gespeichert: IVEQCCTSI\n",
      "✅ Gepaddetes Epitop gespeichert: FAMQMAYRF\n",
      "✅ Gepaddetes Epitop gespeichert: NPANNASIV\n",
      "✅ Gepaddetes Epitop gespeichert: LPSYAAFAT\n",
      "✅ Gepaddetes Epitop gespeichert: MPLKAPKEV\n",
      "✅ Gepaddetes Epitop gespeichert: QLIPCMDVV\n",
      "✅ Gepaddetes Epitop gespeichert: YLSHLPLTCKF\n",
      "✅ Gepaddetes Epitop gespeichert: ALWEIQQVV\n",
      "✅ Gepaddetes Epitop gespeichert: GSFCTQLNR\n",
      "✅ Gepaddetes Epitop gespeichert: KIYSKHTPI\n",
      "✅ Gepaddetes Epitop gespeichert: NYIAQVDVVNFNL\n",
      "✅ Gepaddetes Epitop gespeichert: LTDEMIAQY\n",
      "✅ Gepaddetes Epitop gespeichert: CYTWNQMNL\n",
      "✅ Gepaddetes Epitop gespeichert: MFLARGIVF\n",
      "✅ Gepaddetes Epitop gespeichert: YTMADLVYAL\n",
      "✅ Gepaddetes Epitop gespeichert: NLVPMVATV\n",
      "✅ Gepaddetes Epitop gespeichert: TLVFLFVAA\n",
      "✅ Gepaddetes Epitop gespeichert: KTSVDCTMYI\n",
      "✅ Gepaddetes Epitop gespeichert: APQPAPENAY\n",
      "✅ Gepaddetes Epitop gespeichert: LAAVYRINWI\n",
      "✅ Gepaddetes Epitop gespeichert: SLLVLVQST\n",
      "✅ Gepaddetes Epitop gespeichert: FLKKDAPYI\n",
      "✅ Gepaddetes Epitop gespeichert: ALSGVFCGV\n",
      "✅ Gepaddetes Epitop gespeichert: RARSVSPKLFIR\n",
      "✅ Gepaddetes Epitop gespeichert: KQIYKTPPI\n",
      "✅ Gepaddetes Epitop gespeichert: VMDEFIERY\n",
      "✅ Gepaddetes Epitop gespeichert: AIFYLITPV\n",
      "✅ Gepaddetes Epitop gespeichert: ILIVMFPFL\n",
      "✅ Gepaddetes Epitop gespeichert: QLLVSLWGL\n",
      "✅ Gepaddetes Epitop gespeichert: FLVLLPLVS\n",
      "✅ Gepaddetes Epitop gespeichert: KVYPIILRL\n",
      "✅ Gepaddetes Epitop gespeichert: SEETGTLIV\n",
      "✅ Gepaddetes Epitop gespeichert: NSVKPEIRPVW\n",
      "✅ Gepaddetes Epitop gespeichert: IMLEALERV\n",
      "✅ Gepaddetes Epitop gespeichert: SALRPEIRLLW\n",
      "✅ Gepaddetes Epitop gespeichert: YHSIEWA\n",
      "✅ Gepaddetes Epitop gespeichert: YQYTFPDFLY\n",
      "✅ Gepaddetes Epitop gespeichert: KIFGSLAFL\n",
      "✅ Gepaddetes Epitop gespeichert: EMMLFDWKV\n",
      "✅ Gepaddetes Epitop gespeichert: RSRRCLRL\n",
      "✅ Gepaddetes Epitop gespeichert: IPTITQMNL\n",
      "✅ Gepaddetes Epitop gespeichert: FLTYLDVSV\n",
      "✅ Gepaddetes Epitop gespeichert: LQLGFSTGV\n",
      "✅ Gepaddetes Epitop gespeichert: TPRVTGGGAM\n",
      "✅ Gepaddetes Epitop gespeichert: IPYNSVTSSI\n",
      "✅ Gepaddetes Epitop gespeichert: MLFSHGLVK\n",
      "✅ Gepaddetes Epitop gespeichert: VMVELVAEL\n",
      "✅ Gepaddetes Epitop gespeichert: LLWNGPMAV\n",
      "✅ Gepaddetes Epitop gespeichert: NYIKWPWWVW\n",
      "✅ Gepaddetes Epitop gespeichert: VLCNSQTSL\n",
      "✅ Gepaddetes Epitop gespeichert: YMFLASPSV\n",
      "✅ Gepaddetes Epitop gespeichert: NYSGVVTTVMF\n",
      "✅ Gepaddetes Epitop gespeichert: CSSCFERLYF\n",
      "✅ Gepaddetes Epitop gespeichert: GSEHSLAEY\n",
      "✅ Gepaddetes Epitop gespeichert: ILKEPVHGV\n",
      "✅ Gepaddetes Epitop gespeichert: GTSGSPIVAR\n",
      "✅ Gepaddetes Epitop gespeichert: KLGASPLHV\n",
      "✅ Gepaddetes Epitop gespeichert: LLFALYFSA\n",
      "✅ Gepaddetes Epitop gespeichert: ALNTPKDHI\n",
      "✅ Gepaddetes Epitop gespeichert: MPYFFTLLL\n",
      "✅ Gepaddetes Epitop gespeichert: RLPAKAPLLGCG\n",
      "✅ Gepaddetes Epitop gespeichert: YHSIEWAI\n",
      "✅ Gepaddetes Epitop gespeichert: GLVEVEKGV\n",
      "✅ Gepaddetes Epitop gespeichert: GMSRIGMEVTPSGTW\n",
      "✅ Gepaddetes Epitop gespeichert: YVTAMDIFV\n",
      "✅ Gepaddetes Epitop gespeichert: VMCGGSLYV\n",
      "✅ Gepaddetes Epitop gespeichert: FLCWHTSCY\n",
      "✅ Gepaddetes Epitop gespeichert: DYKHYTPSF\n",
      "✅ Gepaddetes Epitop gespeichert: LPPIVAKEI\n",
      "✅ Gepaddetes Epitop gespeichert: TLGVYDYLVST\n",
      "✅ Gepaddetes Epitop gespeichert: QRRPQGLPNNTASWFTALTQHGKEDLKFPRGQGVPINT\n",
      "✅ Gepaddetes Epitop gespeichert: NTNSSPDDQIGYY\n",
      "✅ Gepaddetes Epitop gespeichert: GARGVGKSAL\n",
      "✅ Gepaddetes Epitop gespeichert: YFLQSINFV\n",
      "✅ Gepaddetes Epitop gespeichert: YSVLYNSASFSTFK\n",
      "✅ Gepaddetes Epitop gespeichert: HTQGYFPDW\n",
      "✅ Gepaddetes Epitop gespeichert: LLQYGSFCT\n",
      "✅ Gepaddetes Epitop gespeichert: HSDKFTDGV\n",
      "✅ Gepaddetes Epitop gespeichert: KLYGLDWAEL\n",
      "✅ Gepaddetes Epitop gespeichert: FLEGETLPT\n",
      "✅ Gepaddetes Epitop gespeichert: LLIRWQHFL\n",
      "✅ Gepaddetes Epitop gespeichert: TQLNRALTGI\n",
      "✅ Gepaddetes Epitop gespeichert: GSLAPEIRMYW\n",
      "✅ Gepaddetes Epitop gespeichert: IKLDDKDPQ\n",
      "✅ Gepaddetes Epitop gespeichert: FLIGCNYLG\n",
      "✅ Gepaddetes Epitop gespeichert: RMAAISNTV\n",
      "✅ Gepaddetes Epitop gespeichert: YPLECIKDL\n",
      "✅ Gepaddetes Epitop gespeichert: NQKLIANAF\n",
      "✅ Gepaddetes Epitop gespeichert: RMWDFDIFL\n",
      "✅ Gepaddetes Epitop gespeichert: YYRRATRRIR\n",
      "✅ Gepaddetes Epitop gespeichert: HIMPDTAEI\n",
      "✅ Gepaddetes Epitop gespeichert: ILIVMVPFL\n",
      "✅ Gepaddetes Epitop gespeichert: EWFLAYILF\n",
      "✅ Gepaddetes Epitop gespeichert: TLMNVLTLV\n",
      "✅ Gepaddetes Epitop gespeichert: SPRWYFYYL\n",
      "✅ Gepaddetes Epitop gespeichert: IMLIIFWFSL\n",
      "✅ Gepaddetes Epitop gespeichert: KIVALGINAV\n",
      "✅ Gepaddetes Epitop gespeichert: TIHDIILECV\n",
      "✅ Gepaddetes Epitop gespeichert: GLDNHTILL\n",
      "✅ Gepaddetes Epitop gespeichert: NLQSNHDLY\n",
      "✅ Gepaddetes Epitop gespeichert: SEPVLKGVKL\n",
      "✅ Gepaddetes Epitop gespeichert: KQFDTYNLW\n",
      "✅ Gepaddetes Epitop gespeichert: AYAQKIFKI\n",
      "✅ Gepaddetes Epitop gespeichert: RLIKHYPGI\n",
      "✅ Gepaddetes Epitop gespeichert: AVCPWTWLR\n",
      "✅ Gepaddetes Epitop gespeichert: FMLYLVTLM\n",
      "✅ Gepaddetes Epitop gespeichert: YMVLASPSV\n",
      "✅ Gepaddetes Epitop gespeichert: ALASCMGLI\n",
      "✅ Gepaddetes Epitop gespeichert: RMDYNNMQM\n",
      "✅ Gepaddetes Epitop gespeichert: GLMAIAWFI\n",
      "✅ Gepaddetes Epitop gespeichert: VYGIRLEHF\n",
      "✅ Gepaddetes Epitop gespeichert: LNKHIDAYK\n",
      "✅ Gepaddetes Epitop gespeichert: YTMADLVYA\n",
      "✅ Gepaddetes Epitop gespeichert: YLQQNWWTL\n",
      "✅ Gepaddetes Epitop gespeichert: LKKRWQLALSKGVHFVCNLLLLFVTVYSHLLLVAAGLE\n",
      "✅ Gepaddetes Epitop gespeichert: QLSSLGLNAV\n",
      "✅ Gepaddetes Epitop gespeichert: WLWEPLRVV\n",
      "✅ Gepaddetes Epitop gespeichert: GLEAPFLYLYALVYFLQSINFVRIIMR\n",
      "✅ Gepaddetes Epitop gespeichert: YLGGPDFPTI\n",
      "✅ Gepaddetes Epitop gespeichert: KLFIRQEEV\n",
      "✅ Gepaddetes Epitop gespeichert: KPADNSLKI\n",
      "✅ Gepaddetes Epitop gespeichert: YLCLQNNFI\n",
      "✅ Gepaddetes Epitop gespeichert: TMMDLVYAM\n",
      "✅ Gepaddetes Epitop gespeichert: SVLNDILSRL\n",
      "✅ Gepaddetes Epitop gespeichert: ILLDWAANI\n",
      "✅ Gepaddetes Epitop gespeichert: MDLFMRIFTIGTVTLKQGEIKDATPSDFVRATATIPIQ\n",
      "✅ Gepaddetes Epitop gespeichert: GLYDGMEHL\n",
      "✅ Gepaddetes Epitop gespeichert: LLIDLTSFLL\n",
      "✅ Gepaddetes Epitop gespeichert: RLFLGLAIK\n",
      "✅ Gepaddetes Epitop gespeichert: RPHERNGFTVL\n",
      "✅ Gepaddetes Epitop gespeichert: THLMVLCCV\n",
      "✅ Gepaddetes Epitop gespeichert: LASYFFWGL\n",
      "✅ Gepaddetes Epitop gespeichert: YVLTWIVGA\n",
      "✅ Gepaddetes Epitop gespeichert: FLFTFFASI\n",
      "✅ Gepaddetes Epitop gespeichert: KLWAQCVQL\n",
      "✅ Gepaddetes Epitop gespeichert: IPLTEEAEL\n",
      "✅ Gepaddetes Epitop gespeichert: IIFWFSLEL\n",
      "✅ Gepaddetes Epitop gespeichert: IVAGGIVAI\n",
      "✅ Gepaddetes Epitop gespeichert: ISQWLTNIF\n",
      "✅ Gepaddetes Epitop gespeichert: DKDPNFKDQ\n",
      "✅ Gepaddetes Epitop gespeichert: VMPFSIVYI\n",
      "✅ Gepaddetes Epitop gespeichert: TMCDIRQLL\n",
      "✅ Gepaddetes Epitop gespeichert: VFLVLLPLV\n",
      "✅ Gepaddetes Epitop gespeichert: VLNPFIYTL\n",
      "✅ Gepaddetes Epitop gespeichert: LGAENSVAYSNN\n",
      "✅ Gepaddetes Epitop gespeichert: NQKLIANQF\n",
      "✅ Gepaddetes Epitop gespeichert: FQDYIKSYL\n",
      "✅ Gepaddetes Epitop gespeichert: IFFITGNTL\n",
      "✅ Gepaddetes Epitop gespeichert: APTKVTFGV\n",
      "✅ Gepaddetes Epitop gespeichert: EILDITPCSF\n",
      "✅ Gepaddetes Epitop gespeichert: GRMPLVAPK\n",
      "✅ Gepaddetes Epitop gespeichert: FLRGRAFGL\n",
      "✅ Gepaddetes Epitop gespeichert: RYGSFSVTL\n",
      "✅ Gepaddetes Epitop gespeichert: ILEGIGILAV\n",
      "✅ Gepaddetes Epitop gespeichert: FTPLVPFWI\n",
      "✅ Gepaddetes Epitop gespeichert: AIKTSPKANK\n",
      "✅ Gepaddetes Epitop gespeichert: RQFDITSVSV\n",
      "✅ Gepaddetes Epitop gespeichert: LYALVYFLQ\n",
      "✅ Gepaddetes Epitop gespeichert: SLCAFNPNPHL\n",
      "✅ Gepaddetes Epitop gespeichert: KRPIPIKYKAM\n",
      "✅ Gepaddetes Epitop gespeichert: AEVQIDRLI\n",
      "✅ Gepaddetes Epitop gespeichert: MLICCCCTL\n",
      "✅ Gepaddetes Epitop gespeichert: QLCTQHQPY\n",
      "✅ Gepaddetes Epitop gespeichert: GTSGSPIVNR\n",
      "✅ Gepaddetes Epitop gespeichert: QVVSDIDYV\n",
      "✅ Gepaddetes Epitop gespeichert: VPYFNMVYM\n",
      "✅ Gepaddetes Epitop gespeichert: HTDFSSEIIGY\n",
      "✅ Gepaddetes Epitop gespeichert: KLQFTSLEI\n",
      "✅ Gepaddetes Epitop gespeichert: HPVGDADYFEY\n",
      "✅ Gepaddetes Epitop gespeichert: GPRLGVRAT\n",
      "✅ Gepaddetes Epitop gespeichert: EEYLQAFTY\n",
      "✅ Gepaddetes Epitop gespeichert: RLPAKAPL\n",
      "✅ Gepaddetes Epitop gespeichert: GLLHHAPSL\n",
      "✅ Gepaddetes Epitop gespeichert: YMLTLHTKL\n",
      "✅ Gepaddetes Epitop gespeichert: LPVNVAFEL\n",
      "✅ Gepaddetes Epitop gespeichert: LPKEITVAT\n",
      "✅ Gepaddetes Epitop gespeichert: ALLADKFPV\n",
      "✅ Gepaddetes Epitop gespeichert: VYSTGSNVF\n",
      "✅ Gepaddetes Epitop gespeichert: FADDLNQLTGY\n",
      "✅ Gepaddetes Epitop gespeichert: LTISPEIPPYF\n",
      "✅ Gepaddetes Epitop gespeichert: RLLCALTSL\n",
      "✅ Gepaddetes Epitop gespeichert: KLPDDFMGC\n",
      "✅ Gepaddetes Epitop gespeichert: MMDFFNAQM\n",
      "✅ Gepaddetes Epitop gespeichert: RISNCVADY\n",
      "✅ Gepaddetes Epitop gespeichert: YTVELGTEV\n",
      "✅ Gepaddetes Epitop gespeichert: RYLALYNKY\n",
      "✅ Gepaddetes Epitop gespeichert: SLSSTASAL\n",
      "✅ Gepaddetes Epitop gespeichert: ITLKKRWQLAL\n",
      "✅ Gepaddetes Epitop gespeichert: DLAGIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: TTIQTIVEV\n",
      "✅ Gepaddetes Epitop gespeichert: YVFCTVNAL\n",
      "✅ Gepaddetes Epitop gespeichert: SLLMPILTL\n",
      "✅ Gepaddetes Epitop gespeichert: SISAFNLDVV\n",
      "✅ Gepaddetes Epitop gespeichert: FSKQLQQSM\n",
      "✅ Gepaddetes Epitop gespeichert: NMEYMTWDV\n",
      "✅ Gepaddetes Epitop gespeichert: KLMNIQQQL\n",
      "✅ Gepaddetes Epitop gespeichert: QELIRQGTDYKHW\n",
      "✅ Gepaddetes Epitop gespeichert: LPKGIMMNV\n",
      "✅ Gepaddetes Epitop gespeichert: CVETMCNEY\n",
      "✅ Gepaddetes Epitop gespeichert: HIVEISTPV\n",
      "✅ Gepaddetes Epitop gespeichert: GDAALALLL\n",
      "✅ Gepaddetes Epitop gespeichert: LVLEVDPNI\n",
      "✅ Gepaddetes Epitop gespeichert: GSKLTFDY\n",
      "✅ Gepaddetes Epitop gespeichert: TYLPSAWNF\n",
      "✅ Gepaddetes Epitop gespeichert: FLNVLFPLL\n",
      "✅ Gepaddetes Epitop gespeichert: NYMPYFFTL\n",
      "✅ Gepaddetes Epitop gespeichert: FGDVGSTLF\n",
      "✅ Gepaddetes Epitop gespeichert: EMQLTPFFI\n",
      "✅ Gepaddetes Epitop gespeichert: NLNCCSVPV\n",
      "✅ Gepaddetes Epitop gespeichert: GRPMFYSVK\n",
      "✅ Gepaddetes Epitop gespeichert: AVEEVSLRK\n",
      "✅ Gepaddetes Epitop gespeichert: FVYVFTTHL\n",
      "✅ Gepaddetes Epitop gespeichert: TLLFLMSFT\n",
      "✅ Gepaddetes Epitop gespeichert: ELKRKMIYM\n",
      "✅ Gepaddetes Epitop gespeichert: ASFSPELRMAW\n",
      "✅ Gepaddetes Epitop gespeichert: FLLPSLATV\n",
      "✅ Gepaddetes Epitop gespeichert: SPNLAWPLI\n",
      "✅ Gepaddetes Epitop gespeichert: NLSDRVVFV\n",
      "✅ Gepaddetes Epitop gespeichert: GPAESAAGL\n",
      "✅ Gepaddetes Epitop gespeichert: LQAENVTGL\n",
      "✅ Gepaddetes Epitop gespeichert: SFLPGVYSV\n",
      "✅ Gepaddetes Epitop gespeichert: SLFNTVATL\n",
      "✅ Gepaddetes Epitop gespeichert: VAQFPSQLLTW\n",
      "✅ Gepaddetes Epitop gespeichert: NGVEGFNCY\n",
      "✅ Gepaddetes Epitop gespeichert: STDTVVEHV\n",
      "✅ Gepaddetes Epitop gespeichert: GEIYKRWII\n",
      "✅ Gepaddetes Epitop gespeichert: DLFMRIFTI\n",
      "✅ Gepaddetes Epitop gespeichert: EVLPFFLFF\n",
      "✅ Gepaddetes Epitop gespeichert: RLQSLQTYV\n",
      "✅ Gepaddetes Epitop gespeichert: VYALIAGATL\n",
      "✅ Gepaddetes Epitop gespeichert: TTSPISEHDY\n",
      "✅ Gepaddetes Epitop gespeichert: NPANNAAIV\n",
      "✅ Gepaddetes Epitop gespeichert: KLFEFLVHGV\n",
      "✅ Gepaddetes Epitop gespeichert: SLYNTIATL\n",
      "✅ Gepaddetes Epitop gespeichert: VLQLPQGTTLPKGFYA\n",
      "✅ Gepaddetes Epitop gespeichert: RLNQLESKM\n",
      "✅ Gepaddetes Epitop gespeichert: LSLRPEIPLFF\n",
      "✅ Gepaddetes Epitop gespeichert: KPILWRGLK\n",
      "✅ Gepaddetes Epitop gespeichert: TSAMHTMLF\n",
      "✅ Gepaddetes Epitop gespeichert: KSLTPEVRGYW\n",
      "✅ Gepaddetes Epitop gespeichert: MLLKHDVSL\n",
      "✅ Gepaddetes Epitop gespeichert: QPYRVVVLS\n",
      "✅ Gepaddetes Epitop gespeichert: HPVAEADYFEY\n",
      "✅ Gepaddetes Epitop gespeichert: RTAPHGHEL\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGYAVYV\n",
      "✅ Gepaddetes Epitop gespeichert: EFFWDANDIY\n",
      "✅ Gepaddetes Epitop gespeichert: YFFWGLWAL\n",
      "✅ Gepaddetes Epitop gespeichert: QPRAPIRPI\n",
      "✅ Gepaddetes Epitop gespeichert: ALWMRLLPLLA\n",
      "✅ Gepaddetes Epitop gespeichert: LTITPEIAPFF\n",
      "✅ Gepaddetes Epitop gespeichert: YLLFMIGYT\n",
      "✅ Gepaddetes Epitop gespeichert: NLLKDCPAV\n",
      "✅ Gepaddetes Epitop gespeichert: WLMWLIINL\n",
      "✅ Gepaddetes Epitop gespeichert: YYQLYSTQL\n",
      "✅ Gepaddetes Epitop gespeichert: HTMLCMCCK\n",
      "✅ Gepaddetes Epitop gespeichert: ALPETTADI\n",
      "✅ Gepaddetes Epitop gespeichert: FLRDGWEIV\n",
      "✅ Gepaddetes Epitop gespeichert: YTDFATSAC\n",
      "✅ Gepaddetes Epitop gespeichert: FLALCADSI\n",
      "✅ Gepaddetes Epitop gespeichert: SLPGVFCGV\n",
      "✅ Gepaddetes Epitop gespeichert: KPYIKWDLL\n",
      "✅ Gepaddetes Epitop gespeichert: KLVAMGINAV\n",
      "✅ Gepaddetes Epitop gespeichert: YLYALVYFL\n",
      "✅ Gepaddetes Epitop gespeichert: TLNDLNETL\n",
      "✅ Gepaddetes Epitop gespeichert: LMMPFSIVYI\n",
      "✅ Gepaddetes Epitop gespeichert: SPRRARSVA\n",
      "✅ Gepaddetes Epitop gespeichert: NLDESLIDL\n",
      "✅ Gepaddetes Epitop gespeichert: VLNGTVHPV\n",
      "✅ Gepaddetes Epitop gespeichert: GTITSGWTF\n",
      "✅ Gepaddetes Epitop gespeichert: DLSPRWYFYYLGTGPEAGLPYGANKDGIIWVATEGALN\n",
      "✅ Gepaddetes Epitop gespeichert: EEAAGIGIL\n",
      "✅ Gepaddetes Epitop gespeichert: ALNNIINNA\n",
      "✅ Gepaddetes Epitop gespeichert: SLLKLGKLPL\n",
      "✅ Gepaddetes Epitop gespeichert: IGAEHVNNSY\n",
      "✅ Gepaddetes Epitop gespeichert: FVNLEQHVV\n",
      "✅ Gepaddetes Epitop gespeichert: YLNELVVFV\n",
      "✅ Gepaddetes Epitop gespeichert: TMETIDWKV\n",
      "✅ Gepaddetes Epitop gespeichert: LPEPLPQGGLTAY\n",
      "✅ Gepaddetes Epitop gespeichert: KLLAGVEIV\n",
      "✅ Gepaddetes Epitop gespeichert: NTLTLAVPY\n",
      "✅ Gepaddetes Epitop gespeichert: YADVFHLYL\n",
      "✅ Gepaddetes Epitop gespeichert: LLAILTYYV\n",
      "✅ Gepaddetes Epitop gespeichert: TLYSLTPLY\n",
      "✅ Gepaddetes Epitop gespeichert: LPEPLGQGQLTAY\n",
      "✅ Gepaddetes Epitop gespeichert: GVFCGVDAV\n",
      "✅ Gepaddetes Epitop gespeichert: ILDTAGKEEY\n",
      "✅ Gepaddetes Epitop gespeichert: MTLHGFMMY\n",
      "✅ Gepaddetes Epitop gespeichert: LSEFCRVLCCYVLEE\n",
      "✅ Gepaddetes Epitop gespeichert: GLMWLSYFV\n",
      "✅ Gepaddetes Epitop gespeichert: VMVNHLHKY\n",
      "✅ Gepaddetes Epitop gespeichert: AFLLFLVLI\n",
      "✅ Gepaddetes Epitop gespeichert: AMAVMLLLL\n",
      "✅ Gepaddetes Epitop gespeichert: ELYSPIFLI\n",
      "✅ Gepaddetes Epitop gespeichert: LSDDAVVCFNSTY\n",
      "✅ Gepaddetes Epitop gespeichert: KLSELHAYI\n",
      "✅ Gepaddetes Epitop gespeichert: RPPIFIRRL\n",
      "✅ Gepaddetes Epitop gespeichert: PTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNF\n",
      "✅ Gepaddetes Epitop gespeichert: SLFNTIAVL\n",
      "✅ Gepaddetes Epitop gespeichert: ILTGLNYEV\n",
      "✅ Gepaddetes Epitop gespeichert: GVYDGREHTV\n",
      "✅ Gepaddetes Epitop gespeichert: GNYTVSCLPFTI\n",
      "✅ Gepaddetes Epitop gespeichert: IYSKHTPINL\n",
      "✅ Gepaddetes Epitop gespeichert: ALALLLLDR\n",
      "✅ Gepaddetes Epitop gespeichert: LRQMLQAPH\n",
      "✅ Gepaddetes Epitop gespeichert: LPEPLPQGQLGAY\n",
      "✅ Gepaddetes Epitop gespeichert: YLLPRRGPRL\n",
      "✅ Gepaddetes Epitop gespeichert: YLCSGSSYF\n",
      "✅ Gepaddetes Epitop gespeichert: RLYLDAYNM\n",
      "✅ Gepaddetes Epitop gespeichert: EELKKLLEQWNLVIGFLF\n",
      "✅ Gepaddetes Epitop gespeichert: IQYIDIGNY\n",
      "✅ Gepaddetes Epitop gespeichert: RYRIGNYKL\n",
      "✅ Gepaddetes Epitop gespeichert: ATSRTLSYY\n",
      "✅ Gepaddetes Epitop gespeichert: YVYSRVKNL\n",
      "✅ Gepaddetes Epitop gespeichert: WTDNNCYLA\n",
      "✅ Gepaddetes Epitop gespeichert: TMMDLCFAL\n",
      "✅ Gepaddetes Epitop gespeichert: AEGSRGGSQA\n",
      "✅ Gepaddetes Epitop gespeichert: LAVFPNIHM\n",
      "✅ Gepaddetes Epitop gespeichert: FLYENAFLP\n",
      "✅ Gepaddetes Epitop gespeichert: AMDLGIHKV\n",
      "✅ Gepaddetes Epitop gespeichert: GLGIGALVL\n",
      "✅ Gepaddetes Epitop gespeichert: VVTGVLVYL\n",
      "✅ Gepaddetes Epitop gespeichert: LSPRWYFYY\n",
      "✅ Gepaddetes Epitop gespeichert: IPTNFTISV\n",
      "✅ Gepaddetes Epitop gespeichert: RPGGKKHYM\n",
      "✅ Gepaddetes Epitop gespeichert: LPEPLPQGQLTAY\n",
      "✅ Gepaddetes Epitop gespeichert: VLEEVDWLI\n",
      "✅ Gepaddetes Epitop gespeichert: ATRGATVVI\n",
      "✅ Gepaddetes Epitop gespeichert: NLIDSYFVV\n",
      "✅ Gepaddetes Epitop gespeichert: FLCLFLLPS\n",
      "✅ Gepaddetes Epitop gespeichert: ILFTRFFYV\n",
      "✅ Gepaddetes Epitop gespeichert: QQQQGQTVTK\n",
      "✅ Gepaddetes Epitop gespeichert: FISTCACEI\n",
      "✅ Gepaddetes Epitop gespeichert: AQALNTLVKQL\n",
      "✅ Gepaddetes Epitop gespeichert: RPIPIKYKAM\n",
      "✅ Gepaddetes Epitop gespeichert: AARGPHGGAASGL\n",
      "✅ Gepaddetes Epitop gespeichert: KYTKAFLV\n",
      "✅ Gepaddetes Epitop gespeichert: LITGRLQSL\n",
      "✅ Gepaddetes Epitop gespeichert: FVVFLLVTL\n",
      "✅ Gepaddetes Epitop gespeichert: RLFRKSNLK\n",
      "✅ Gepaddetes Epitop gespeichert: RPSTAEKYIF\n",
      "✅ Gepaddetes Epitop gespeichert: GLSPNLNRFL\n",
      "✅ Gepaddetes Epitop gespeichert: GIIAMSAFA\n",
      "✅ Gepaddetes Epitop gespeichert: SIWNLDYII\n",
      "✅ Gepaddetes Epitop gespeichert: LDDKDPQFK\n",
      "✅ Gepaddetes Epitop gespeichert: VMHANYIFW\n",
      "✅ Gepaddetes Epitop gespeichert: MLSILALVRV\n",
      "✅ Gepaddetes Epitop gespeichert: KMGVTYEM\n",
      "✅ Gepaddetes Epitop gespeichert: SLDLTTSPV\n",
      "✅ Gepaddetes Epitop gespeichert: LLDRLNQLE\n",
      "✅ Gepaddetes Epitop gespeichert: LPEGLPQGQLTAY\n",
      "✅ Gepaddetes Epitop gespeichert: KLGEFYNQM\n",
      "✅ Gepaddetes Epitop gespeichert: KPVETSNSF\n",
      "✅ Gepaddetes Epitop gespeichert: FLAHIQWMV\n",
      "✅ Gepaddetes Epitop gespeichert: FLASPSVPL\n",
      "✅ Gepaddetes Epitop gespeichert: TVYSHLLLV\n",
      "✅ Gepaddetes Epitop gespeichert: AAFKRSCLK\n",
      "✅ Gepaddetes Epitop gespeichert: VYSSANNCTFEY\n",
      "✅ Gepaddetes Epitop gespeichert: FSFPLDFLV\n",
      "✅ Gepaddetes Epitop gespeichert: VMYASAVVL\n",
      "✅ Gepaddetes Epitop gespeichert: FIFSYVVAV\n",
      "✅ Gepaddetes Epitop gespeichert: VLASPSVPL\n",
      "✅ Gepaddetes Epitop gespeichert: NPLLYDANYFLCW\n",
      "✅ Gepaddetes Epitop gespeichert: RYCNLEGPPI\n",
      "✅ Gepaddetes Epitop gespeichert: APARLERRHSA\n",
      "✅ Gepaddetes Epitop gespeichert: SMWALIISV\n",
      "✅ Gepaddetes Epitop gespeichert: FLHVTYVPA\n",
      "✅ Gepaddetes Epitop gespeichert: SEISMDNSPNL\n",
      "✅ Gepaddetes Epitop gespeichert: ALYYPSARI\n",
      "✅ Gepaddetes Epitop gespeichert: IILVAVPHV\n",
      "✅ Gepaddetes Epitop gespeichert: SVLENFTIFL\n",
      "✅ Gepaddetes Epitop gespeichert: GPGHKARVL\n",
      "✅ Gepaddetes Epitop gespeichert: GTSGAPIVNR\n",
      "✅ Gepaddetes Epitop gespeichert: KIILFLALI\n",
      "✅ Gepaddetes Epitop gespeichert: SLQTPFPWPPM\n",
      "✅ Gepaddetes Epitop gespeichert: FLKEMGGL\n",
      "✅ Gepaddetes Epitop gespeichert: ERSSYWARRR\n",
      "✅ Gepaddetes Epitop gespeichert: MLWCKDGHV\n",
      "✅ Gepaddetes Epitop gespeichert: LLDFVRFMGV\n",
      "✅ Gepaddetes Epitop gespeichert: WSDPLKLTVK\n",
      "✅ Gepaddetes Epitop gespeichert: TMAAISNTV\n",
      "✅ Gepaddetes Epitop gespeichert: QLRSLGLNAV\n",
      "✅ Gepaddetes Epitop gespeichert: RILGAGCFV\n",
      "✅ Gepaddetes Epitop gespeichert: NSSTCMMCY\n",
      "✅ Gepaddetes Epitop gespeichert: FPFTSFGPL\n",
      "✅ Gepaddetes Epitop gespeichert: TVAYFNMVY\n",
      "✅ Gepaddetes Epitop gespeichert: RSDTRYVLM\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGMTPCL\n",
      "✅ Gepaddetes Epitop gespeichert: CVNGSCFTV\n",
      "✅ Gepaddetes Epitop gespeichert: SLLSVLLSM\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGYPVAV\n",
      "✅ Gepaddetes Epitop gespeichert: GRQPVPLWR\n",
      "✅ Gepaddetes Epitop gespeichert: KPNELSRVL\n",
      "✅ Gepaddetes Epitop gespeichert: LLLGIGILVL\n",
      "✅ Gepaddetes Epitop gespeichert: AVILRGHLR\n",
      "✅ Gepaddetes Epitop gespeichert: HMTEVVRHC\n",
      "✅ Gepaddetes Epitop gespeichert: NVLTLVYKV\n",
      "✅ Gepaddetes Epitop gespeichert: SLLMWITQA\n",
      "✅ Gepaddetes Epitop gespeichert: ALSPVIPHI\n",
      "✅ Gepaddetes Epitop gespeichert: LMVPHSPSL\n",
      "✅ Gepaddetes Epitop gespeichert: LWFHISCLTF\n",
      "✅ Gepaddetes Epitop gespeichert: SLLRSLENV\n",
      "✅ Gepaddetes Epitop gespeichert: WMESEFRVY\n",
      "✅ Gepaddetes Epitop gespeichert: ALWGFFPVL\n",
      "✅ Gepaddetes Epitop gespeichert: FYSKWYIRV\n",
      "✅ Gepaddetes Epitop gespeichert: SLLMWITQC\n",
      "✅ Gepaddetes Epitop gespeichert: FLAEHEYGL\n",
      "✅ Gepaddetes Epitop gespeichert: LLAIKVPNV\n",
      "✅ Gepaddetes Epitop gespeichert: RTIKVFTTV\n",
      "✅ Gepaddetes Epitop gespeichert: KRWIILGLNK\n",
      "✅ Gepaddetes Epitop gespeichert: NFSKLINII\n",
      "✅ Gepaddetes Epitop gespeichert: RFPLTFGWCF\n",
      "✅ Gepaddetes Epitop gespeichert: TMADLVYAL\n",
      "✅ Gepaddetes Epitop gespeichert: AYILFTRFFYV\n",
      "✅ Gepaddetes Epitop gespeichert: QLLFVVEVV\n",
      "✅ Gepaddetes Epitop gespeichert: LLGATCMFV\n",
      "✅ Gepaddetes Epitop gespeichert: LTGHMLDMY\n",
      "✅ Gepaddetes Epitop gespeichert: CFVECAPVC\n",
      "✅ Gepaddetes Epitop gespeichert: GVPINTNSSPDDQIGYYRRATRRIRGGDGKMKDLSPRW\n",
      "✅ Gepaddetes Epitop gespeichert: YLYDRLLRV\n",
      "✅ Gepaddetes Epitop gespeichert: APKEIIFLEGETL\n",
      "✅ Gepaddetes Epitop gespeichert: FLGIYTVTVV\n",
      "✅ Gepaddetes Epitop gespeichert: MLDLQPETT\n",
      "✅ Gepaddetes Epitop gespeichert: IMDQVPFSV\n",
      "✅ Gepaddetes Epitop gespeichert: KVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYV\n",
      "✅ Gepaddetes Epitop gespeichert: SSGDATTAY\n",
      "✅ Gepaddetes Epitop gespeichert: RLFARTRSMW\n",
      "✅ Gepaddetes Epitop gespeichert: HPVGEADYFEY\n",
      "✅ Gepaddetes Epitop gespeichert: ALIHHNTHL\n",
      "✅ Gepaddetes Epitop gespeichert: TAMDLFVTV\n",
      "✅ Gepaddetes Epitop gespeichert: VLEETSVML\n",
      "✅ Gepaddetes Epitop gespeichert: DYTEISFML\n",
      "✅ Gepaddetes Epitop gespeichert: NLVGMVATV\n",
      "✅ Gepaddetes Epitop gespeichert: FASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQ\n",
      "✅ Gepaddetes Epitop gespeichert: HVTYVPAQEKNF\n",
      "✅ Gepaddetes Epitop gespeichert: RLFPGLAIK\n",
      "✅ Gepaddetes Epitop gespeichert: ILRKGGRTI\n",
      "✅ Gepaddetes Epitop gespeichert: RVVVLSFEL\n",
      "✅ Gepaddetes Epitop gespeichert: EFTVSGNIL\n",
      "✅ Gepaddetes Epitop gespeichert: TLFTMVGTV\n",
      "✅ Gepaddetes Epitop gespeichert: MPYIFTLLL\n",
      "✅ Gepaddetes Epitop gespeichert: WSYDSTLLAY\n",
      "✅ Gepaddetes Epitop gespeichert: FVFKNIDGY\n",
      "✅ Gepaddetes Epitop gespeichert: KLNTITAVV\n",
      "✅ Gepaddetes Epitop gespeichert: KAANIVLTV\n",
      "✅ Gepaddetes Epitop gespeichert: MPLSAPTLL\n",
      "✅ Gepaddetes Epitop gespeichert: RAFSPEVIPMF\n",
      "✅ Gepaddetes Epitop gespeichert: VPHHVVATV\n",
      "✅ Gepaddetes Epitop gespeichert: GTDLEGNFY\n",
      "✅ Gepaddetes Epitop gespeichert: PTDNYITTY\n",
      "✅ Gepaddetes Epitop gespeichert: SLAPLSPRV\n",
      "✅ Gepaddetes Epitop gespeichert: MEVDPIGHLY\n",
      "✅ Gepaddetes Epitop gespeichert: RYPLTFGW\n",
      "✅ Gepaddetes Epitop gespeichert: GRVMLVAPK\n",
      "✅ Gepaddetes Epitop gespeichert: SLTYSTAAL\n",
      "✅ Gepaddetes Epitop gespeichert: LLDQLIEEV\n",
      "✅ Gepaddetes Epitop gespeichert: KMQRMLLEK\n",
      "✅ Gepaddetes Epitop gespeichert: VVMSWAPPV\n",
      "✅ Gepaddetes Epitop gespeichert: RLWEPLRVV\n",
      "✅ Gepaddetes Epitop gespeichert: TLNDFNLVA\n",
      "✅ Gepaddetes Epitop gespeichert: LSFKELLVY\n",
      "✅ Gepaddetes Epitop gespeichert: ALGIGIYSL\n",
      "✅ Gepaddetes Epitop gespeichert: TLSEQLDFI\n",
      "✅ Gepaddetes Epitop gespeichert: SSSKLWAQY\n",
      "✅ Gepaddetes Epitop gespeichert: CTELKLSDY\n",
      "✅ Gepaddetes Epitop gespeichert: VYFFLPDHL\n",
      "✅ Gepaddetes Epitop gespeichert: ALRGMGVNAV\n",
      "✅ Gepaddetes Epitop gespeichert: DSFKEELDKY\n",
      "✅ Gepaddetes Epitop gespeichert: SEIISFKSL\n",
      "✅ Gepaddetes Epitop gespeichert: LLYDANYFL\n",
      "✅ Gepaddetes Epitop gespeichert: GVAMPNLYK\n",
      "✅ Gepaddetes Epitop gespeichert: LLGWVFAQV\n",
      "✅ Gepaddetes Epitop gespeichert: LIGIGIAPL\n",
      "✅ Gepaddetes Epitop gespeichert: NLDSKVGGNY\n",
      "✅ Gepaddetes Epitop gespeichert: TSNEVAVLY\n",
      "✅ Gepaddetes Epitop gespeichert: STECSNLLL\n",
      "✅ Gepaddetes Epitop gespeichert: QLMCQPILLL\n",
      "✅ Gepaddetes Epitop gespeichert: QLTSLGLNAV\n",
      "✅ Gepaddetes Epitop gespeichert: ALRKVPTDNYITTY\n",
      "✅ Gepaddetes Epitop gespeichert: NMIHFAPNV\n",
      "✅ Gepaddetes Epitop gespeichert: EHEGSGPEL\n",
      "✅ Gepaddetes Epitop gespeichert: LLMPILTL\n",
      "✅ Gepaddetes Epitop gespeichert: IPTDFTISV\n",
      "✅ Gepaddetes Epitop gespeichert: SSRGTSPAR\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIAILTV\n",
      "✅ Gepaddetes Epitop gespeichert: ILGWIFVPI\n",
      "✅ Gepaddetes Epitop gespeichert: KAAVDLSHF\n",
      "✅ Gepaddetes Epitop gespeichert: LMFWASSSI\n",
      "✅ Gepaddetes Epitop gespeichert: HPKVSSEVHI\n",
      "✅ Gepaddetes Epitop gespeichert: QYDPVAALF\n",
      "✅ Gepaddetes Epitop gespeichert: CVMYASAVV\n",
      "✅ Gepaddetes Epitop gespeichert: FMSEAKCWT\n",
      "✅ Gepaddetes Epitop gespeichert: LLAMKVPNV\n",
      "✅ Gepaddetes Epitop gespeichert: GILGLVFTL\n",
      "✅ Gepaddetes Epitop gespeichert: FLLPAPYWAR\n",
      "✅ Gepaddetes Epitop gespeichert: VLVCVLVAL\n",
      "✅ Gepaddetes Epitop gespeichert: MLGERLFPL\n",
      "✅ Gepaddetes Epitop gespeichert: LLAMTVPNV\n",
      "✅ Gepaddetes Epitop gespeichert: SLVALGINAV\n",
      "✅ Gepaddetes Epitop gespeichert: SVTTEILPV\n",
      "✅ Gepaddetes Epitop gespeichert: APRGPHGGAASGA\n",
      "✅ Gepaddetes Epitop gespeichert: IVTDFSVIK\n",
      "✅ Gepaddetes Epitop gespeichert: YFPLQSYGF\n",
      "✅ Gepaddetes Epitop gespeichert: ALQIPFAMQM\n",
      "✅ Gepaddetes Epitop gespeichert: LPNNTASWF\n",
      "✅ Gepaddetes Epitop gespeichert: IMMNVAKYT\n",
      "✅ Gepaddetes Epitop gespeichert: GTSGSPIINR\n",
      "✅ Gepaddetes Epitop gespeichert: KRKGGIGGY\n",
      "✅ Gepaddetes Epitop gespeichert: YASAVVLLI\n",
      "✅ Gepaddetes Epitop gespeichert: STDTGVEHVTFFIYNK\n",
      "✅ Gepaddetes Epitop gespeichert: VQMAPISAM\n",
      "✅ Gepaddetes Epitop gespeichert: LSYYKLGASQRVAGDSGFAAYSRYRIGNYKLNTDHSS\n",
      "✅ Gepaddetes Epitop gespeichert: FIDQYYSSI\n",
      "✅ Gepaddetes Epitop gespeichert: WPWYIWLGF\n",
      "✅ Gepaddetes Epitop gespeichert: LLHGFSFYL\n",
      "✅ Gepaddetes Epitop gespeichert: KLVVLGINAV\n",
      "✅ Gepaddetes Epitop gespeichert: KTWGQYWQV\n",
      "✅ Gepaddetes Epitop gespeichert: LPQGFSAL\n",
      "✅ Gepaddetes Epitop gespeichert: KPVPEVKIL\n",
      "✅ Gepaddetes Epitop gespeichert: YLNSTNVTI\n",
      "✅ Gepaddetes Epitop gespeichert: RAMPNMLRI\n",
      "✅ Gepaddetes Epitop gespeichert: APYIVGDVV\n",
      "✅ Gepaddetes Epitop gespeichert: ELYHYQECV\n",
      "✅ Gepaddetes Epitop gespeichert: NSTRFASVY\n",
      "✅ Gepaddetes Epitop gespeichert: ILHCANFNV\n",
      "✅ Gepaddetes Epitop gespeichert: CLAVHECFV\n",
      "✅ Gepaddetes Epitop gespeichert: SMLGIGIVPV\n",
      "✅ Gepaddetes Epitop gespeichert: LLTILTSLL\n",
      "✅ Gepaddetes Epitop gespeichert: WICLLQFAY\n",
      "✅ Gepaddetes Epitop gespeichert: VLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIG\n",
      "✅ Gepaddetes Epitop gespeichert: KLFAAETLK\n",
      "✅ Gepaddetes Epitop gespeichert: CEVSNPVSSNW\n",
      "✅ Gepaddetes Epitop gespeichert: MLYQHLLPL\n",
      "✅ Gepaddetes Epitop gespeichert: MLMLAVIVA\n",
      "✅ Gepaddetes Epitop gespeichert: TLIVNSVLL\n",
      "✅ Gepaddetes Epitop gespeichert: FTSDYYQLY\n",
      "✅ Gepaddetes Epitop gespeichert: ILTSLLVLV\n",
      "✅ Gepaddetes Epitop gespeichert: YQDVDCTEV\n",
      "✅ Gepaddetes Epitop gespeichert: ILNAMITKI\n",
      "✅ Gepaddetes Epitop gespeichert: GTIRPEIPDYF\n",
      "✅ Gepaddetes Epitop gespeichert: MMPFSIVYI\n",
      "✅ Gepaddetes Epitop gespeichert: WESGVKDCVVLHSYFTSDYYQLYSTQLSTDTGVEHVTF\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGKPVYV\n",
      "✅ Gepaddetes Epitop gespeichert: MFHLVDFQVTIAEILLIIMRTFKVSIWNLDY\n",
      "✅ Gepaddetes Epitop gespeichert: KLSALGINAV\n",
      "✅ Gepaddetes Epitop gespeichert: KLINIIIWF\n",
      "✅ Gepaddetes Epitop gespeichert: RLARLALVL\n",
      "✅ Gepaddetes Epitop gespeichert: FIMGISILL\n",
      "✅ Gepaddetes Epitop gespeichert: YRINWITGGIAIAMACLVGLMWLSYFIASFRLFARTRSMWS\n",
      "✅ Gepaddetes Epitop gespeichert: HSRKKCDEL\n",
      "✅ Gepaddetes Epitop gespeichert: ESDPIVAQY\n",
      "✅ Gepaddetes Epitop gespeichert: NMMWFQGQL\n",
      "✅ Gepaddetes Epitop gespeichert: LPDDFTGCV\n",
      "✅ Gepaddetes Epitop gespeichert: RLPAKAPLL\n",
      "✅ Gepaddetes Epitop gespeichert: ALVGAIPSI\n",
      "✅ Gepaddetes Epitop gespeichert: ILDQVPFSV\n",
      "✅ Gepaddetes Epitop gespeichert: LLSLFSLWL\n",
      "✅ Gepaddetes Epitop gespeichert: YPDKVFRSSV\n",
      "✅ Gepaddetes Epitop gespeichert: VVYRGTTTY\n",
      "✅ Gepaddetes Epitop gespeichert: SPLPSYAAF\n",
      "✅ Gepaddetes Epitop gespeichert: IPVAIKTSPK\n",
      "✅ Gepaddetes Epitop gespeichert: NLHPDSATL\n",
      "✅ Gepaddetes Epitop gespeichert: IDLLLQRGPQYSEHPTFTSQYRI\n",
      "✅ Gepaddetes Epitop gespeichert: NILDAIAEI\n",
      "✅ Gepaddetes Epitop gespeichert: MASGGGETA\n",
      "✅ Gepaddetes Epitop gespeichert: SWISDIRAGTAPLCRNHIKSSCSLI\n",
      "✅ Gepaddetes Epitop gespeichert: ITNFKSVLY\n",
      "✅ Gepaddetes Epitop gespeichert: FPQGEAREL\n",
      "✅ Gepaddetes Epitop gespeichert: YLRGRAYGL\n",
      "✅ Gepaddetes Epitop gespeichert: LWMRLLPLLAL\n",
      "✅ Gepaddetes Epitop gespeichert: FAFACPDGV\n",
      "✅ Gepaddetes Epitop gespeichert: NLFNRYLAL\n",
      "✅ Gepaddetes Epitop gespeichert: KMKDLSPRW\n",
      "✅ Gepaddetes Epitop gespeichert: RVAGDSGFAAY\n",
      "✅ Gepaddetes Epitop gespeichert: FLGIYTVTV\n",
      "✅ Gepaddetes Epitop gespeichert: VYAWNRKRI\n",
      "✅ Gepaddetes Epitop gespeichert: ERYLKDQQL\n",
      "✅ Gepaddetes Epitop gespeichert: MVMCGGSLYV\n",
      "✅ Gepaddetes Epitop gespeichert: TSTLQEQIGW\n",
      "✅ Gepaddetes Epitop gespeichert: MRTHPGRPFF\n",
      "✅ Gepaddetes Epitop gespeichert: EEYLKAWTF\n",
      "✅ Gepaddetes Epitop gespeichert: LYPELIASI\n",
      "✅ Gepaddetes Epitop gespeichert: AYSNNSIAIPTNFTISV\n",
      "✅ Gepaddetes Epitop gespeichert: STEGGGLAY\n",
      "✅ Gepaddetes Epitop gespeichert: WPVTLACFV\n",
      "✅ Gepaddetes Epitop gespeichert: VPHVGEIPV\n",
      "✅ Gepaddetes Epitop gespeichert: SSGVPEVRMMF\n",
      "✅ Gepaddetes Epitop gespeichert: LLQCTQQAV\n",
      "✅ Gepaddetes Epitop gespeichert: SPYNSQNAV\n",
      "✅ Gepaddetes Epitop gespeichert: QLLKGVQKK\n",
      "✅ Gepaddetes Epitop gespeichert: MVTNNTFTL\n",
      "✅ Gepaddetes Epitop gespeichert: TFYLTNDVSFL\n",
      "✅ Gepaddetes Epitop gespeichert: FLYALALLL\n",
      "✅ Gepaddetes Epitop gespeichert: YYVKWPWYVW\n",
      "✅ Gepaddetes Epitop gespeichert: YLEGSVRVV\n",
      "✅ Gepaddetes Epitop gespeichert: RWAQALYDF\n",
      "✅ Gepaddetes Epitop gespeichert: RIFTIGTVTLK\n",
      "✅ Gepaddetes Epitop gespeichert: QVILLNKHI\n",
      "✅ Gepaddetes Epitop gespeichert: AIGIGAYLV\n",
      "✅ Gepaddetes Epitop gespeichert: LVGLMWLSY\n",
      "✅ Gepaddetes Epitop gespeichert: LACFVLAAV\n",
      "✅ Gepaddetes Epitop gespeichert: QECVRGTTVL\n",
      "✅ Gepaddetes Epitop gespeichert: LLMGTLGIVC\n",
      "✅ Gepaddetes Epitop gespeichert: GAIKLDDKD\n",
      "✅ Gepaddetes Epitop gespeichert: FPLKLRGTA\n",
      "✅ Gepaddetes Epitop gespeichert: QPGQTFSVL\n",
      "✅ Gepaddetes Epitop gespeichert: MGRRAGAEPAPRPCLGRRCSAPAAASVA\n",
      "✅ Gepaddetes Epitop gespeichert: YMPYIFTLL\n",
      "✅ Gepaddetes Epitop gespeichert: AMAGSPVFL\n",
      "✅ Gepaddetes Epitop gespeichert: TDLGQNLLY\n",
      "✅ Gepaddetes Epitop gespeichert: VLLGLWYLQTA\n",
      "✅ Gepaddetes Epitop gespeichert: KPLPEVKIL\n",
      "✅ Gepaddetes Epitop gespeichert: NLVPVVATV\n",
      "✅ Gepaddetes Epitop gespeichert: KINAWIKVV\n",
      "✅ Gepaddetes Epitop gespeichert: FLIASVTWL\n",
      "✅ Gepaddetes Epitop gespeichert: SYFIASFRLFA\n",
      "✅ Gepaddetes Epitop gespeichert: YLDELIRNT\n",
      "✅ Gepaddetes Epitop gespeichert: YEGNSPFHPL\n",
      "✅ Gepaddetes Epitop gespeichert: MADSNGTITVEELKKLLEQWNLVIGFLFLTWICLLQFAYAN\n",
      "✅ Gepaddetes Epitop gespeichert: KLTESLHKV\n",
      "✅ Gepaddetes Epitop gespeichert: AYKTFPPTEPK\n",
      "✅ Gepaddetes Epitop gespeichert: YILKYSVFL\n",
      "✅ Gepaddetes Epitop gespeichert: VLLGVKLFGV\n",
      "✅ Gepaddetes Epitop gespeichert: SLENVAFNV\n",
      "✅ Gepaddetes Epitop gespeichert: AALPILFQV\n",
      "✅ Gepaddetes Epitop gespeichert: CLLWSFQTSA\n",
      "✅ Gepaddetes Epitop gespeichert: MLNPNYEDL\n",
      "✅ Gepaddetes Epitop gespeichert: SSTFNVPMEKLK\n",
      "✅ Gepaddetes Epitop gespeichert: SVFNICQAV\n",
      "✅ Gepaddetes Epitop gespeichert: SIVAYTMSL\n",
      "✅ Gepaddetes Epitop gespeichert: TSNQVAVLY\n",
      "✅ Gepaddetes Epitop gespeichert: KSALDLSHF\n",
      "✅ Gepaddetes Epitop gespeichert: RQFGPDFPTI\n",
      "✅ Gepaddetes Epitop gespeichert: SMWSFNPETNIL\n",
      "✅ Gepaddetes Epitop gespeichert: GLPWNVVRI\n",
      "✅ Gepaddetes Epitop gespeichert: FLGKIWPSHK\n",
      "✅ Gepaddetes Epitop gespeichert: SVASTITGV\n",
      "✅ Gepaddetes Epitop gespeichert: GPKKSTNLV\n",
      "✅ Gepaddetes Epitop gespeichert: TSTLTEQVAW\n",
      "✅ Gepaddetes Epitop gespeichert: YMPYFFTLL\n",
      "✅ Gepaddetes Epitop gespeichert: GTTLEQQYNK\n",
      "✅ Gepaddetes Epitop gespeichert: RLCAYCCNI\n",
      "✅ Gepaddetes Epitop gespeichert: FMIDVQQWG\n",
      "✅ Gepaddetes Epitop gespeichert: YLVYQMLKV\n",
      "✅ Gepaddetes Epitop gespeichert: EVDPIGHLY\n",
      "✅ Gepaddetes Epitop gespeichert: LSDLQDLKW\n",
      "✅ Gepaddetes Epitop gespeichert: LLSYFGTPT\n",
      "✅ Gepaddetes Epitop gespeichert: QSAPHGVVFL\n",
      "✅ Gepaddetes Epitop gespeichert: LLSIISFPA\n",
      "✅ Gepaddetes Epitop gespeichert: RMFPNAPYL\n",
      "✅ Gepaddetes Epitop gespeichert: GILGFVFTL\n",
      "✅ Gepaddetes Epitop gespeichert: FLWLLWPVT\n",
      "✅ Gepaddetes Epitop gespeichert: DRLNQLESK\n",
      "✅ Gepaddetes Epitop gespeichert: LLGIGILVL\n",
      "✅ Gepaddetes Epitop gespeichert: EYLVSFGVW\n",
      "✅ Gepaddetes Epitop gespeichert: GPGMKARVL\n",
      "✅ Gepaddetes Epitop gespeichert: FVCDNIKFA\n",
      "✅ Gepaddetes Epitop gespeichert: LIVAAIVFI\n",
      "✅ Gepaddetes Epitop gespeichert: ILLNKHIDA\n",
      "✅ Gepaddetes Epitop gespeichert: FIDTKRGVY\n",
      "✅ Gepaddetes Epitop gespeichert: HLRIAGHHLGR\n",
      "✅ Gepaddetes Epitop gespeichert: KLMPVCVET\n",
      "✅ Gepaddetes Epitop gespeichert: KLAKPLPYT\n",
      "✅ Gepaddetes Epitop gespeichert: MYVKWPWYVW\n",
      "✅ Gepaddetes Epitop gespeichert: GKTELLAPF\n",
      "✅ Gepaddetes Epitop gespeichert: FMPDFDLHL\n",
      "✅ Gepaddetes Epitop gespeichert: SVLENFTILL\n",
      "✅ Gepaddetes Epitop gespeichert: SLYNTVATL\n",
      "✅ Gepaddetes Epitop gespeichert: GEEDGAGGHSL\n",
      "✅ Gepaddetes Epitop gespeichert: ALVYFLQSI\n",
      "✅ Gepaddetes Epitop gespeichert: YLQDSMATT\n",
      "✅ Gepaddetes Epitop gespeichert: DPNFKDQVI\n",
      "✅ Gepaddetes Epitop gespeichert: DEEDAIAAY\n",
      "✅ Gepaddetes Epitop gespeichert: KLVSSFLEM\n",
      "✅ Gepaddetes Epitop gespeichert: LTLTPEIAPYF\n",
      "✅ Gepaddetes Epitop gespeichert: GQVMVVAPR\n",
      "✅ Gepaddetes Epitop gespeichert: IITTDNTFV\n",
      "✅ Gepaddetes Epitop gespeichert: FLLNKEMYL\n",
      "✅ Gepaddetes Epitop gespeichert: FWITIAYII\n",
      "✅ Gepaddetes Epitop gespeichert: AVGSYVYSV\n",
      "✅ Gepaddetes Epitop gespeichert: VLSFELLHA\n",
      "✅ Gepaddetes Epitop gespeichert: PTDQSYYIV\n",
      "✅ Gepaddetes Epitop gespeichert: QYIKWPWYVW\n",
      "✅ Gepaddetes Epitop gespeichert: IVCPICSQK\n",
      "✅ Gepaddetes Epitop gespeichert: YYRYNLPTM\n",
      "✅ Gepaddetes Epitop gespeichert: QELIRQGTDY\n",
      "✅ Gepaddetes Epitop gespeichert: KYTSFPWL\n",
      "✅ Gepaddetes Epitop gespeichert: NYGLGLLQF\n",
      "✅ Gepaddetes Epitop gespeichert: LLEDLDWDV\n",
      "✅ Gepaddetes Epitop gespeichert: YLSNIIPAL\n",
      "✅ Gepaddetes Epitop gespeichert: SLLMWITQV\n",
      "✅ Gepaddetes Epitop gespeichert: VLPPLLTDEMIAQYT\n",
      "✅ Gepaddetes Epitop gespeichert: SETKCTLKSFTVEK\n",
      "✅ Gepaddetes Epitop gespeichert: NMLRIMASL\n",
      "✅ Gepaddetes Epitop gespeichert: NSLKPEIPDYF\n",
      "✅ Gepaddetes Epitop gespeichert: ALDYIHHVGYV\n",
      "✅ Gepaddetes Epitop gespeichert: LPSYAALAT\n",
      "✅ Gepaddetes Epitop gespeichert: CINGVCWTV\n",
      "✅ Gepaddetes Epitop gespeichert: WSDPLKLTV\n",
      "✅ Gepaddetes Epitop gespeichert: RLVPYLEFL\n",
      "✅ Gepaddetes Epitop gespeichert: YVTAMDLFV\n",
      "✅ Gepaddetes Epitop gespeichert: YLEPGAVTA\n",
      "✅ Gepaddetes Epitop gespeichert: TMMDLVYAL\n",
      "✅ Gepaddetes Epitop gespeichert: RQWGPDPAAV\n",
      "✅ Gepaddetes Epitop gespeichert: RSLAPEVRGYW\n",
      "✅ Gepaddetes Epitop gespeichert: RLDAMNGQL\n",
      "✅ Gepaddetes Epitop gespeichert: FRYMNSQGL\n",
      "✅ Gepaddetes Epitop gespeichert: RMYSPVSI\n",
      "✅ Gepaddetes Epitop gespeichert: CLGGLLTMV\n",
      "✅ Gepaddetes Epitop gespeichert: ARHIHEGASL\n",
      "✅ Gepaddetes Epitop gespeichert: KLVALVINAV\n",
      "✅ Gepaddetes Epitop gespeichert: ILLLDQALV\n",
      "✅ Gepaddetes Epitop gespeichert: ILLDWAANV\n",
      "✅ Gepaddetes Epitop gespeichert: WPLVSSQCV\n",
      "✅ Gepaddetes Epitop gespeichert: VSALSRAAEK\n",
      "✅ Gepaddetes Epitop gespeichert: RIAAWMATY\n",
      "✅ Gepaddetes Epitop gespeichert: LWMRLLPLL\n",
      "✅ Gepaddetes Epitop gespeichert: ALSKGVHFV\n",
      "✅ Gepaddetes Epitop gespeichert: GLGIGIASM\n",
      "✅ Gepaddetes Epitop gespeichert: VLDLFQGQL\n",
      "✅ Gepaddetes Epitop gespeichert: IPARARVEC\n",
      "✅ Gepaddetes Epitop gespeichert: KPAPRELKV\n",
      "✅ Gepaddetes Epitop gespeichert: FSNVTWFHA\n",
      "✅ Gepaddetes Epitop gespeichert: YIFFASFYY\n",
      "✅ Gepaddetes Epitop gespeichert: YVVTWIVGA\n",
      "✅ Gepaddetes Epitop gespeichert: ATEGALNTPKDHI\n",
      "✅ Gepaddetes Epitop gespeichert: KLVNKFLAL\n",
      "✅ Gepaddetes Epitop gespeichert: NTFSSTFNV\n",
      "✅ Gepaddetes Epitop gespeichert: NLFNRYPAL\n",
      "✅ Gepaddetes Epitop gespeichert: FLDEFMEAV\n",
      "✅ Gepaddetes Epitop gespeichert: APRGPHGGAASAL\n",
      "✅ Gepaddetes Epitop gespeichert: FLNRFTTTL\n",
      "✅ Gepaddetes Epitop gespeichert: RVRFFFPSL\n",
      "✅ Gepaddetes Epitop gespeichert: YTPSKLIEY\n",
      "✅ Gepaddetes Epitop gespeichert: TVSCLPFTI\n",
      "✅ Gepaddetes Epitop gespeichert: KSLAPEVRDLF\n",
      "✅ Gepaddetes Epitop gespeichert: QLEMELTPV\n",
      "✅ Gepaddetes Epitop gespeichert: SYQTQTNSPRRARSVA\n",
      "✅ Gepaddetes Epitop gespeichert: GLGPGFSSY\n",
      "✅ Gepaddetes Epitop gespeichert: KEIDRLNEV\n",
      "✅ Gepaddetes Epitop gespeichert: QSRGDENRGW\n",
      "✅ Gepaddetes Epitop gespeichert: VQPTESIVRFPNITNLCPF\n",
      "✅ Gepaddetes Epitop gespeichert: FLRGRAYGL\n",
      "✅ Gepaddetes Epitop gespeichert: KASEKIFYV\n",
      "✅ Gepaddetes Epitop gespeichert: QAKWRLQTL\n",
      "✅ Gepaddetes Epitop gespeichert: KDNVILLNK\n",
      "✅ Gepaddetes Epitop gespeichert: MVIGIPVYV\n",
      "✅ Gepaddetes Epitop gespeichert: GGGPGAGSLQP\n",
      "✅ Gepaddetes Epitop gespeichert: ALGIGVYPV\n",
      "✅ Gepaddetes Epitop gespeichert: GLMWLSYFI\n",
      "✅ Gepaddetes Epitop gespeichert: YMRSLKVPA\n",
      "✅ Gepaddetes Epitop gespeichert: MIISHLSLI\n",
      "✅ Gepaddetes Epitop gespeichert: RPQGLSNNT\n",
      "✅ Gepaddetes Epitop gespeichert: GRIMLLAPK\n",
      "✅ Gepaddetes Epitop gespeichert: CLNEYHLFL\n",
      "✅ Gepaddetes Epitop gespeichert: APLLSAGVF\n",
      "✅ Gepaddetes Epitop gespeichert: QDVEIPPNI\n",
      "✅ Gepaddetes Epitop gespeichert: KALDYIHHV\n",
      "✅ Gepaddetes Epitop gespeichert: GLNKIVRMY\n",
      "✅ Gepaddetes Epitop gespeichert: VPMEKFKTL\n",
      "✅ Gepaddetes Epitop gespeichert: LVLSVNPYV\n",
      "✅ Gepaddetes Epitop gespeichert: VEAEVQIDRLITGR\n",
      "✅ Gepaddetes Epitop gespeichert: IIKDYGKQM\n",
      "✅ Gepaddetes Epitop gespeichert: AIAMACLVGLM\n",
      "✅ Gepaddetes Epitop gespeichert: MSDVKCTSV\n",
      "✅ Gepaddetes Epitop gespeichert: KSVNITFEL\n",
      "✅ Gepaddetes Epitop gespeichert: LYDANYFLC\n",
      "✅ Gepaddetes Epitop gespeichert: FLKETGGL\n",
      "✅ Gepaddetes Epitop gespeichert: FVFLVLLPL\n",
      "✅ Gepaddetes Epitop gespeichert: AIMTRCLAV\n",
      "✅ Gepaddetes Epitop gespeichert: KLEFMDWRL\n",
      "✅ Gepaddetes Epitop gespeichert: SSFSPELRMRW\n",
      "✅ Gepaddetes Epitop gespeichert: KLYEGCEVV\n",
      "✅ Gepaddetes Epitop gespeichert: FSAVGDICY\n",
      "✅ Gepaddetes Epitop gespeichert: VLPFNDGVYFASTEK\n",
      "✅ Gepaddetes Epitop gespeichert: YHLMSFPQSA\n",
      "✅ Gepaddetes Epitop gespeichert: FTQRWRVSHV\n",
      "✅ Gepaddetes Epitop gespeichert: KLPDDFTGC\n",
      "✅ Gepaddetes Epitop gespeichert: TPHTVLQAV\n",
      "✅ Gepaddetes Epitop gespeichert: GYQPYRVVVLSF\n",
      "✅ Gepaddetes Epitop gespeichert: NVEYYDIKL\n",
      "✅ Gepaddetes Epitop gespeichert: FLYNLLTRV\n",
      "✅ Gepaddetes Epitop gespeichert: NDMLVCANI\n",
      "✅ Gepaddetes Epitop gespeichert: FLNDLLSVL\n",
      "✅ Gepaddetes Epitop gespeichert: SLSKILDTV\n",
      "✅ Gepaddetes Epitop gespeichert: SEHGFGPSL\n",
      "✅ Gepaddetes Epitop gespeichert: RPPLNRNYV\n",
      "✅ Gepaddetes Epitop gespeichert: TSQWLTNIF\n",
      "✅ Gepaddetes Epitop gespeichert: FRSTRFLHI\n",
      "✅ Gepaddetes Epitop gespeichert: SLINTLNDL\n",
      "✅ Gepaddetes Epitop gespeichert: FQVIQPERV\n",
      "✅ Gepaddetes Epitop gespeichert: SMDWFQGQM\n",
      "✅ Gepaddetes Epitop gespeichert: SVLYYQNNV\n",
      "✅ Gepaddetes Epitop gespeichert: APRAPHGGAASGL\n",
      "✅ Gepaddetes Epitop gespeichert: EYILSLEEL\n",
      "✅ Gepaddetes Epitop gespeichert: TTEELPDEF\n",
      "✅ Gepaddetes Epitop gespeichert: PSGSKLTFDY\n",
      "✅ Gepaddetes Epitop gespeichert: ITADVNYNL\n",
      "✅ Gepaddetes Epitop gespeichert: GSLSPELRPIF\n",
      "✅ Gepaddetes Epitop gespeichert: DRFYKTLRA\n",
      "✅ Gepaddetes Epitop gespeichert: SYKSYSSTF\n",
      "✅ Gepaddetes Epitop gespeichert: VVVGADGVGK\n",
      "✅ Gepaddetes Epitop gespeichert: DLPIGINITRFQTL\n",
      "✅ Gepaddetes Epitop gespeichert: TLSGTWLTY\n",
      "✅ Gepaddetes Epitop gespeichert: KPLEFGATSAAL\n",
      "✅ Gepaddetes Epitop gespeichert: HVYQLRARL\n",
      "✅ Gepaddetes Epitop gespeichert: HTDLMAAYV\n",
      "✅ Gepaddetes Epitop gespeichert: SEVGPEHSLAEY\n",
      "✅ Gepaddetes Epitop gespeichert: TLACFVLAA\n",
      "✅ Gepaddetes Epitop gespeichert: LAMPFATPM\n",
      "✅ Gepaddetes Epitop gespeichert: VQIISCQY\n",
      "✅ Gepaddetes Epitop gespeichert: FGDDTVIEV\n",
      "✅ Gepaddetes Epitop gespeichert: RHDLPPYRVYL\n",
      "✅ Gepaddetes Epitop gespeichert: RTHPGRPFF\n",
      "✅ Gepaddetes Epitop gespeichert: ILPDPSKPS\n",
      "✅ Gepaddetes Epitop gespeichert: KLSELHTYI\n",
      "✅ Gepaddetes Epitop gespeichert: VVLSWAPPV\n",
      "✅ Gepaddetes Epitop gespeichert: QSDNGLDSDY\n",
      "✅ Gepaddetes Epitop gespeichert: TGAIKLDDK\n",
      "✅ Gepaddetes Epitop gespeichert: MYIEMLKSI\n",
      "✅ Gepaddetes Epitop gespeichert: VYAIFIFQL\n",
      "✅ Gepaddetes Epitop gespeichert: GADGVGKSAL\n",
      "✅ Gepaddetes Epitop gespeichert: FCNDPFLGVYY\n",
      "✅ Gepaddetes Epitop gespeichert: MVMCGGSLY\n",
      "✅ Gepaddetes Epitop gespeichert: YTERSEKSY\n",
      "✅ Gepaddetes Epitop gespeichert: KLASHMYCS\n",
      "✅ Gepaddetes Epitop gespeichert: CFAYYFMRF\n",
      "✅ Gepaddetes Epitop gespeichert: KPFERDISTEIY\n",
      "✅ Gepaddetes Epitop gespeichert: LLYGFVNYI\n",
      "✅ Gepaddetes Epitop gespeichert: WLTYTGAIK\n",
      "✅ Gepaddetes Epitop gespeichert: TLATHGLAAV\n",
      "✅ Gepaddetes Epitop gespeichert: LFRVPSYQALLRRVFHQTVSRKVAL\n",
      "✅ Gepaddetes Epitop gespeichert: FITGISILL\n",
      "✅ Gepaddetes Epitop gespeichert: GRVMLRAPK\n",
      "✅ Gepaddetes Epitop gespeichert: LTNDDTSRY\n",
      "✅ Gepaddetes Epitop gespeichert: ALVPMVATV\n",
      "✅ Gepaddetes Epitop gespeichert: FMIGYTSAL\n",
      "✅ Gepaddetes Epitop gespeichert: RTAPHGHVV\n",
      "✅ Gepaddetes Epitop gespeichert: LSITPEISPYF\n",
      "✅ Gepaddetes Epitop gespeichert: IPVAYRKVL\n",
      "✅ Gepaddetes Epitop gespeichert: MIISHLYLI\n",
      "✅ Gepaddetes Epitop gespeichert: FMLLTQARI\n",
      "✅ Gepaddetes Epitop gespeichert: TPAFDKSAF\n",
      "✅ Gepaddetes Epitop gespeichert: HLMSFPQSA\n",
      "✅ Gepaddetes Epitop gespeichert: TMDFYQGQL\n",
      "✅ Gepaddetes Epitop gespeichert: LLAGTLAVV\n",
      "✅ Gepaddetes Epitop gespeichert: WLLWPVTLA\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGILAV\n",
      "✅ Gepaddetes Epitop gespeichert: SELVIGAVIL\n",
      "✅ Gepaddetes Epitop gespeichert: HHLWAYRDLQTRKDIRNAAWHKHGW\n",
      "✅ Gepaddetes Epitop gespeichert: LGYGFVNYI\n",
      "✅ Gepaddetes Epitop gespeichert: YLLEMLWRL\n",
      "✅ Gepaddetes Epitop gespeichert: ALGWVFVPV\n",
      "✅ Gepaddetes Epitop gespeichert: CPACHNSEV\n",
      "✅ Gepaddetes Epitop gespeichert: VMPLSAPTL\n",
      "✅ Gepaddetes Epitop gespeichert: SLLQGSPHV\n",
      "✅ Gepaddetes Epitop gespeichert: MIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGV\n",
      "✅ Gepaddetes Epitop gespeichert: ARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEI\n",
      "✅ Gepaddetes Epitop gespeichert: KLVPSGSKL\n",
      "✅ Gepaddetes Epitop gespeichert: YLAMPFATPMEAELARRSLA\n",
      "✅ Gepaddetes Epitop gespeichert: KLVANNTRL\n",
      "✅ Gepaddetes Epitop gespeichert: TADFDITEL\n",
      "✅ Gepaddetes Epitop gespeichert: ILLGIGIYAL\n",
      "✅ Gepaddetes Epitop gespeichert: AEMEFCPCC\n",
      "✅ Gepaddetes Epitop gespeichert: FAFACPDGVKHVYQL\n",
      "✅ Gepaddetes Epitop gespeichert: VVVGAVGVGK\n",
      "✅ Gepaddetes Epitop gespeichert: FLGRYMSAL\n",
      "✅ Gepaddetes Epitop gespeichert: KSFVPELKPAF\n",
      "✅ Gepaddetes Epitop gespeichert: WMVMFTPLV\n",
      "✅ Gepaddetes Epitop gespeichert: HLVEALYL\n",
      "✅ Gepaddetes Epitop gespeichert: LLFLMSFTV\n",
      "✅ Gepaddetes Epitop gespeichert: FLFMYLVTV\n",
      "✅ Gepaddetes Epitop gespeichert: VLENFTIFLV\n",
      "✅ Gepaddetes Epitop gespeichert: WMRLLPLL\n",
      "✅ Gepaddetes Epitop gespeichert: YTDINGNLH\n",
      "✅ Gepaddetes Epitop gespeichert: DYIIRSHKV\n",
      "✅ Gepaddetes Epitop gespeichert: KSVDITFEL\n",
      "✅ Gepaddetes Epitop gespeichert: KQLSSNFGA\n",
      "✅ Gepaddetes Epitop gespeichert: SMGVTYEM\n",
      "✅ Gepaddetes Epitop gespeichert: GLFKDCSKV\n",
      "✅ Gepaddetes Epitop gespeichert: CQARVTYKI\n",
      "✅ Gepaddetes Epitop gespeichert: HTQGYFPD\n",
      "✅ Gepaddetes Epitop gespeichert: VLNYFKPYL\n",
      "✅ Gepaddetes Epitop gespeichert: SHWAAQVI\n",
      "✅ Gepaddetes Epitop gespeichert: VLKNRYLVL\n",
      "✅ Gepaddetes Epitop gespeichert: LMASISSFL\n",
      "✅ Gepaddetes Epitop gespeichert: RLNQLESKV\n",
      "✅ Gepaddetes Epitop gespeichert: AGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVAS\n",
      "✅ Gepaddetes Epitop gespeichert: KAFSPEVIPMF\n",
      "✅ Gepaddetes Epitop gespeichert: LRFLRNGLT\n",
      "✅ Gepaddetes Epitop gespeichert: KAYDVTQAF\n",
      "✅ Gepaddetes Epitop gespeichert: TVLSFCAFA\n",
      "✅ Gepaddetes Epitop gespeichert: SLVKPSFYV\n",
      "✅ Gepaddetes Epitop gespeichert: DTDFVNEFY\n",
      "✅ Gepaddetes Epitop gespeichert: RQIAPGQTGKIADYNYKL\n",
      "✅ Gepaddetes Epitop gespeichert: MLLKQDVSL\n",
      "✅ Gepaddetes Epitop gespeichert: FPLTSFGPL\n",
      "✅ Gepaddetes Epitop gespeichert: TLGVLVPHV\n",
      "✅ Gepaddetes Epitop gespeichert: TCDGTTFIY\n",
      "✅ Gepaddetes Epitop gespeichert: LTITPEIAPYF\n",
      "✅ Gepaddetes Epitop gespeichert: FLTVFSPFM\n",
      "✅ Gepaddetes Epitop gespeichert: FRCPRRFCF\n",
      "✅ Gepaddetes Epitop gespeichert: KRWIIMGLNK\n",
      "✅ Gepaddetes Epitop gespeichert: FVSEETGTL\n",
      "✅ Gepaddetes Epitop gespeichert: MDLFMRIFTI\n",
      "✅ Gepaddetes Epitop gespeichert: RPRGEVRFL\n",
      "✅ Gepaddetes Epitop gespeichert: LMFALAKGRRA\n",
      "✅ Gepaddetes Epitop gespeichert: YQVNGYPNM\n",
      "✅ Gepaddetes Epitop gespeichert: VLQAVGACV\n",
      "✅ Gepaddetes Epitop gespeichert: KLNTITPVV\n",
      "✅ Gepaddetes Epitop gespeichert: VTWFHAIHV\n",
      "✅ Gepaddetes Epitop gespeichert: LGLARAFGIPV\n",
      "✅ Gepaddetes Epitop gespeichert: GGETALALL\n",
      "✅ Gepaddetes Epitop gespeichert: RRWNFIYIF\n",
      "✅ Gepaddetes Epitop gespeichert: ILGLPTQTV\n",
      "✅ Gepaddetes Epitop gespeichert: VLDGFIPGT\n",
      "✅ Gepaddetes Epitop gespeichert: MLIGIPVYV\n",
      "✅ Gepaddetes Epitop gespeichert: GRVMLLAPLR\n",
      "✅ Gepaddetes Epitop gespeichert: HGAIKLDDK\n",
      "✅ Gepaddetes Epitop gespeichert: LYALVYFLH\n",
      "✅ Gepaddetes Epitop gespeichert: APAGPHGGAASGL\n",
      "✅ Gepaddetes Epitop gespeichert: KLDDKDPQF\n",
      "✅ Gepaddetes Epitop gespeichert: LPPVYTNSF\n",
      "✅ Gepaddetes Epitop gespeichert: LLAWHFVAV\n",
      "✅ Gepaddetes Epitop gespeichert: AAASATLAL\n",
      "✅ Gepaddetes Epitop gespeichert: VLSTFISAA\n",
      "✅ Gepaddetes Epitop gespeichert: EEHVQIHTI\n",
      "✅ Gepaddetes Epitop gespeichert: KPYSGTAYNAL\n",
      "✅ Gepaddetes Epitop gespeichert: KCYGVSPTK\n",
      "✅ Gepaddetes Epitop gespeichert: RARLVSPKL\n",
      "✅ Gepaddetes Epitop gespeichert: SVLLFLAFV\n",
      "✅ Gepaddetes Epitop gespeichert: KLPLDIDPV\n",
      "✅ Gepaddetes Epitop gespeichert: RVRAMTIYKQ\n",
      "✅ Gepaddetes Epitop gespeichert: KMVAVFYTT\n",
      "✅ Gepaddetes Epitop gespeichert: LLLTILTSL\n",
      "✅ Gepaddetes Epitop gespeichert: SVCAGILSYGV\n",
      "✅ Gepaddetes Epitop gespeichert: MFDAYVNTF\n",
      "✅ Gepaddetes Epitop gespeichert: NLNRCSVPV\n",
      "✅ Gepaddetes Epitop gespeichert: ILAYCNKTV\n",
      "✅ Gepaddetes Epitop gespeichert: VLFHRAFLV\n",
      "✅ Gepaddetes Epitop gespeichert: IFNGECPNF\n",
      "✅ Gepaddetes Epitop gespeichert: TLWCSPIKV\n",
      "✅ Gepaddetes Epitop gespeichert: MMPFSIVYIV\n",
      "✅ Gepaddetes Epitop gespeichert: KMFDAYVNT\n",
      "✅ Gepaddetes Epitop gespeichert: NMINFAPNI\n",
      "✅ Gepaddetes Epitop gespeichert: LPPAYTNSF\n",
      "✅ Gepaddetes Epitop gespeichert: VQYLGMFPV\n",
      "✅ Gepaddetes Epitop gespeichert: KTAYSHLSTSK\n",
      "✅ Gepaddetes Epitop gespeichert: KTVGIYPNA\n",
      "✅ Gepaddetes Epitop gespeichert: ALGIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: YLEPGPATA\n",
      "✅ Gepaddetes Epitop gespeichert: MLWGYLQYV\n",
      "✅ Gepaddetes Epitop gespeichert: VVDKYFDCY\n",
      "✅ Gepaddetes Epitop gespeichert: LLALHRSYL\n",
      "✅ Gepaddetes Epitop gespeichert: ILAKFLHWL\n",
      "✅ Gepaddetes Epitop gespeichert: KLWASPLHV\n",
      "✅ Gepaddetes Epitop gespeichert: FMLYLLTLM\n",
      "✅ Gepaddetes Epitop gespeichert: ALSYTPAEV\n",
      "✅ Gepaddetes Epitop gespeichert: ASFRPELAEFW\n",
      "✅ Gepaddetes Epitop gespeichert: VLLFLAFVV\n",
      "✅ Gepaddetes Epitop gespeichert: ETALALLLL\n",
      "✅ Gepaddetes Epitop gespeichert: YQDVNCTEV\n",
      "✅ Gepaddetes Epitop gespeichert: VLENFTILLV\n",
      "✅ Gepaddetes Epitop gespeichert: KPRQKRTAT\n",
      "✅ Gepaddetes Epitop gespeichert: TVCSSCFERLY\n",
      "✅ Gepaddetes Epitop gespeichert: ELGGIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: YLKLTDNVYIK\n",
      "✅ Gepaddetes Epitop gespeichert: VPLTEDAEL\n",
      "✅ Gepaddetes Epitop gespeichert: GLNDNLLEI\n",
      "✅ Gepaddetes Epitop gespeichert: YAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFT\n",
      "✅ Gepaddetes Epitop gespeichert: LGIGVITI\n",
      "✅ Gepaddetes Epitop gespeichert: LTALRLCAY\n",
      "✅ Gepaddetes Epitop gespeichert: FVRATATIPI\n",
      "✅ Gepaddetes Epitop gespeichert: VQVMLNNPK\n",
      "✅ Gepaddetes Epitop gespeichert: RLGEVRHPV\n",
      "✅ Gepaddetes Epitop gespeichert: KVDGVDVEL\n",
      "✅ Gepaddetes Epitop gespeichert: IPRRNVATL\n",
      "✅ Gepaddetes Epitop gespeichert: AIMDKNIIL\n",
      "✅ Gepaddetes Epitop gespeichert: TLKNTVCTV\n",
      "✅ Gepaddetes Epitop gespeichert: FILDAVQRV\n",
      "✅ Gepaddetes Epitop gespeichert: CISSCNPNL\n",
      "✅ Gepaddetes Epitop gespeichert: RLQSLQIYV\n",
      "✅ Gepaddetes Epitop gespeichert: PTDQSFYIV\n",
      "✅ Gepaddetes Epitop gespeichert: ALLPGLPAA\n",
      "✅ Gepaddetes Epitop gespeichert: AVGVGKSAL\n",
      "✅ Gepaddetes Epitop gespeichert: FSTQFAFACPDGVKHVYQLRARSVSPKLFIRQEEV\n",
      "✅ Gepaddetes Epitop gespeichert: VLLSVLQQL\n",
      "✅ Gepaddetes Epitop gespeichert: LYKKHTETF\n",
      "✅ Gepaddetes Epitop gespeichert: VINGDRWFL\n",
      "✅ Gepaddetes Epitop gespeichert: SSCFERLY\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGILTA\n",
      "✅ Gepaddetes Epitop gespeichert: TAFTIPSI\n",
      "✅ Gepaddetes Epitop gespeichert: FIYQYYSSI\n",
      "✅ Gepaddetes Epitop gespeichert: LIIMRTFKV\n",
      "✅ Gepaddetes Epitop gespeichert: YMGVSYEM\n",
      "✅ Gepaddetes Epitop gespeichert: KDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQY\n",
      "✅ Gepaddetes Epitop gespeichert: VYYPDKVFRSSVLH\n",
      "✅ Gepaddetes Epitop gespeichert: TTLPVNVAF\n",
      "✅ Gepaddetes Epitop gespeichert: CLTPVYSFL\n",
      "✅ Gepaddetes Epitop gespeichert: RYPLTLGWCF\n",
      "✅ Gepaddetes Epitop gespeichert: RLVDPQIQL\n",
      "✅ Gepaddetes Epitop gespeichert: MIAQYTSAL\n",
      "✅ Gepaddetes Epitop gespeichert: GAVECCPNC\n",
      "✅ Gepaddetes Epitop gespeichert: FVAAIFYLI\n",
      "✅ Gepaddetes Epitop gespeichert: LPLKMLNI\n",
      "✅ Gepaddetes Epitop gespeichert: NNILIATCV\n",
      "✅ Gepaddetes Epitop gespeichert: KLNVGDYFV\n",
      "✅ Gepaddetes Epitop gespeichert: VEQCCTSIC\n",
      "✅ Gepaddetes Epitop gespeichert: WLTNIFGTV\n",
      "✅ Gepaddetes Epitop gespeichert: SAYGEPRKL\n",
      "✅ Gepaddetes Epitop gespeichert: KLGIGFAKA\n",
      "✅ Gepaddetes Epitop gespeichert: DATYQRTRALVR\n",
      "✅ Gepaddetes Epitop gespeichert: PPWQAGILARNLVPMVATVQGQN\n",
      "✅ Gepaddetes Epitop gespeichert: LLLEWLAMA\n",
      "✅ Gepaddetes Epitop gespeichert: LTYHGAIKL\n",
      "✅ Gepaddetes Epitop gespeichert: MTLHGHMMY\n",
      "✅ Gepaddetes Epitop gespeichert: ALDPHSGHFV\n",
      "✅ Gepaddetes Epitop gespeichert: IVNSVLLFL\n",
      "✅ Gepaddetes Epitop gespeichert: APRITFGGL\n",
      "✅ Gepaddetes Epitop gespeichert: KWPWYIWLG\n",
      "✅ Gepaddetes Epitop gespeichert: LSPRWYFYYL\n",
      "✅ Gepaddetes Epitop gespeichert: YTACSHAAV\n",
      "✅ Gepaddetes Epitop gespeichert: LIGIGLNLV\n",
      "✅ Gepaddetes Epitop gespeichert: VFLVLWPLV\n",
      "✅ Gepaddetes Epitop gespeichert: NLQREEYTV\n",
      "✅ Gepaddetes Epitop gespeichert: MLDMYSVML\n",
      "✅ Gepaddetes Epitop gespeichert: TYGPVFMCL\n",
      "✅ Gepaddetes Epitop gespeichert: MLGEQLFPL\n",
      "✅ Gepaddetes Epitop gespeichert: RLPGVLPRA\n",
      "✅ Gepaddetes Epitop gespeichert: ILNVDVFTL\n",
      "✅ Gepaddetes Epitop gespeichert: RIRGGDGKMK\n",
      "✅ Gepaddetes Epitop gespeichert: KLPDDFTGCV\n",
      "✅ Gepaddetes Epitop gespeichert: TILTSLLVL\n",
      "✅ Gepaddetes Epitop gespeichert: YTVSCLPFT\n",
      "✅ Gepaddetes Epitop gespeichert: VLHDDLLEA\n",
      "✅ Gepaddetes Epitop gespeichert: LHMEFCSCC\n",
      "✅ Gepaddetes Epitop gespeichert: TVATSRTLSYYK\n",
      "✅ Gepaddetes Epitop gespeichert: KTFPPTEPK\n",
      "✅ Gepaddetes Epitop gespeichert: TSTLAEQMAW\n",
      "✅ Gepaddetes Epitop gespeichert: RGFFYTPKT\n",
      "✅ Gepaddetes Epitop gespeichert: GRVPLRAPK\n",
      "✅ Gepaddetes Epitop gespeichert: QASQEVKNW\n",
      "✅ Gepaddetes Epitop gespeichert: EAAGIGILTV\n",
      "✅ Gepaddetes Epitop gespeichert: SYLDLPWYL\n",
      "✅ Gepaddetes Epitop gespeichert: MGYINVFAFPFTIYSL\n",
      "✅ Gepaddetes Epitop gespeichert: YHGAIKLDD\n",
      "✅ Gepaddetes Epitop gespeichert: RIRGGDGKM\n",
      "✅ Gepaddetes Epitop gespeichert: KLILWRGLK\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGFPVYV\n",
      "✅ Gepaddetes Epitop gespeichert: AQFAPSASAFFGMSR\n",
      "✅ Gepaddetes Epitop gespeichert: AVASKILGL\n",
      "✅ Gepaddetes Epitop gespeichert: EPLPQGQLTAY\n",
      "✅ Gepaddetes Epitop gespeichert: CPSQEPMSIYVY\n",
      "✅ Gepaddetes Epitop gespeichert: LLGIGIYAL\n",
      "✅ Gepaddetes Epitop gespeichert: TSLEVVSY\n",
      "✅ Gepaddetes Epitop gespeichert: MALWMRLLPLL\n",
      "✅ Gepaddetes Epitop gespeichert: SMMILSDDA\n",
      "✅ Gepaddetes Epitop gespeichert: LLFNKVTLA\n",
      "✅ Gepaddetes Epitop gespeichert: MIDVQQWGF\n",
      "✅ Gepaddetes Epitop gespeichert: GLTSFFIAI\n",
      "✅ Gepaddetes Epitop gespeichert: YLDSTDVTI\n",
      "✅ Gepaddetes Epitop gespeichert: LLFLVLIML\n",
      "✅ Gepaddetes Epitop gespeichert: HLVDFQVTI\n",
      "✅ Gepaddetes Epitop gespeichert: DKDPQFKDN\n",
      "✅ Gepaddetes Epitop gespeichert: WSMATYYLF\n",
      "✅ Gepaddetes Epitop gespeichert: SVYGDTLEK\n",
      "✅ Gepaddetes Epitop gespeichert: YLNTLTLAV\n",
      "✅ Gepaddetes Epitop gespeichert: FLNKVVSTT\n",
      "✅ Gepaddetes Epitop gespeichert: TALALLLLD\n",
      "✅ Gepaddetes Epitop gespeichert: EWSMATYYL\n",
      "✅ Gepaddetes Epitop gespeichert: SIIGRLLEV\n",
      "✅ Gepaddetes Epitop gespeichert: FEDLRLLSF\n",
      "✅ Gepaddetes Epitop gespeichert: YSEHPTFTSQY\n",
      "✅ Gepaddetes Epitop gespeichert: WLDMVDTSL\n",
      "✅ Gepaddetes Epitop gespeichert: SSNVANYQK\n",
      "✅ Gepaddetes Epitop gespeichert: TLDSKTQSL\n",
      "✅ Gepaddetes Epitop gespeichert: SLLQGSPHL\n",
      "✅ Gepaddetes Epitop gespeichert: FLCLFLLPSLATV\n",
      "✅ Gepaddetes Epitop gespeichert: VLNDILSRL\n",
      "✅ Gepaddetes Epitop gespeichert: MMILSDDAV\n",
      "✅ Gepaddetes Epitop gespeichert: ALSYTPVEV\n",
      "✅ Gepaddetes Epitop gespeichert: HPVGQADYFEY\n",
      "✅ Gepaddetes Epitop gespeichert: FIDSYICQV\n",
      "✅ Gepaddetes Epitop gespeichert: SQLGGLHLL\n",
      "✅ Gepaddetes Epitop gespeichert: SRVMLLAPR\n",
      "✅ Gepaddetes Epitop gespeichert: SWMESEFRV\n",
      "✅ Gepaddetes Epitop gespeichert: LFRVPSYQALLRGVFHQTVSRKVAL\n",
      "✅ Gepaddetes Epitop gespeichert: QMAPISAMV\n",
      "✅ Gepaddetes Epitop gespeichert: YTDIATSAC\n",
      "✅ Gepaddetes Epitop gespeichert: LITGRLQSLQTYV\n",
      "✅ Gepaddetes Epitop gespeichert: PYMFLSEWI\n",
      "✅ Gepaddetes Epitop gespeichert: RLANECAQV\n",
      "✅ Gepaddetes Epitop gespeichert: QTNPVTLQY\n",
      "✅ Gepaddetes Epitop gespeichert: RLRPGGKKK\n",
      "✅ Gepaddetes Epitop gespeichert: QTDPVTLQY\n",
      "✅ Gepaddetes Epitop gespeichert: LLFGPVYV\n",
      "✅ Gepaddetes Epitop gespeichert: WTYRPEVRGVW\n",
      "✅ Gepaddetes Epitop gespeichert: ATVVIGTSK\n",
      "✅ Gepaddetes Epitop gespeichert: ILPDPSKPSK\n",
      "✅ Gepaddetes Epitop gespeichert: MYTEMLKSI\n",
      "✅ Gepaddetes Epitop gespeichert: ELAGIGIATV\n",
      "✅ Gepaddetes Epitop gespeichert: RTYVPEIKHRW\n",
      "✅ Gepaddetes Epitop gespeichert: VTNNTFTLK\n",
      "✅ Gepaddetes Epitop gespeichert: DDKDPQFKD\n",
      "✅ Gepaddetes Epitop gespeichert: FPNIHMETC\n",
      "✅ Gepaddetes Epitop gespeichert: YLHAYAKAL\n",
      "✅ Gepaddetes Epitop gespeichert: ALWGPDPAAAF\n",
      "✅ Gepaddetes Epitop gespeichert: RVIGMPPPV\n",
      "✅ Gepaddetes Epitop gespeichert: KVAELVHFL\n",
      "✅ Gepaddetes Epitop gespeichert: YLVALGVNAV\n",
      "✅ Gepaddetes Epitop gespeichert: TQGYFPDWQNY\n",
      "✅ Gepaddetes Epitop gespeichert: RFDNPVLPF\n",
      "✅ Gepaddetes Epitop gespeichert: VSDGGPNLY\n",
      "✅ Gepaddetes Epitop gespeichert: AVYDGREHTV\n",
      "✅ Gepaddetes Epitop gespeichert: VLLAPLLSA\n",
      "✅ Gepaddetes Epitop gespeichert: AQWGPDPAAA\n",
      "✅ Gepaddetes Epitop gespeichert: HSKKKCDEL\n",
      "✅ Gepaddetes Epitop gespeichert: VLHSYFTSDYYQLY\n",
      "✅ Gepaddetes Epitop gespeichert: SILDAVQRV\n",
      "✅ Gepaddetes Epitop gespeichert: SRVMLLAPK\n",
      "✅ Gepaddetes Epitop gespeichert: TRVMLISPK\n",
      "✅ Gepaddetes Epitop gespeichert: LCWKCRSKNPLLYDANYFLCWHTNCYDYCIPYNSVTSS\n",
      "✅ Gepaddetes Epitop gespeichert: VPGLPGTIL\n",
      "✅ Gepaddetes Epitop gespeichert: ALLETLSLLL\n",
      "✅ Gepaddetes Epitop gespeichert: LLDDFVEII\n",
      "✅ Gepaddetes Epitop gespeichert: ATSRMLSYY\n",
      "✅ Gepaddetes Epitop gespeichert: APHGVVFLHVTYV\n",
      "✅ Gepaddetes Epitop gespeichert: FLNDLLSVM\n",
      "✅ Gepaddetes Epitop gespeichert: ESEERPPTPY\n",
      "✅ Gepaddetes Epitop gespeichert: WLDGVTPSL\n",
      "✅ Gepaddetes Epitop gespeichert: AMDEFIERY\n",
      "✅ Gepaddetes Epitop gespeichert: TLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNEN\n",
      "✅ Gepaddetes Epitop gespeichert: ALLQVTLLL\n",
      "✅ Gepaddetes Epitop gespeichert: ALEPGPVTA\n",
      "✅ Gepaddetes Epitop gespeichert: SLFNTVATLY\n",
      "✅ Gepaddetes Epitop gespeichert: LDRLNQLES\n",
      "✅ Gepaddetes Epitop gespeichert: TTDPSFLGRY\n",
      "✅ Gepaddetes Epitop gespeichert: RLMKHYPGI\n",
      "✅ Gepaddetes Epitop gespeichert: TPWCSPIKV\n",
      "✅ Gepaddetes Epitop gespeichert: YYRARAGEAANF\n",
      "✅ Gepaddetes Epitop gespeichert: VMPFSIVYIV\n",
      "✅ Gepaddetes Epitop gespeichert: SIIAYTMSL\n",
      "✅ Gepaddetes Epitop gespeichert: HIAKSPFEV\n",
      "✅ Gepaddetes Epitop gespeichert: FDVMVMPNL\n",
      "✅ Gepaddetes Epitop gespeichert: FYWFFSNYL\n",
      "✅ Gepaddetes Epitop gespeichert: RVSTLRVSL\n",
      "✅ Gepaddetes Epitop gespeichert: LVIGMPPPV\n",
      "✅ Gepaddetes Epitop gespeichert: RTIKGTYHW\n",
      "✅ Gepaddetes Epitop gespeichert: LVQSTQWSL\n",
      "✅ Gepaddetes Epitop gespeichert: GILEFVFTL\n",
      "✅ Gepaddetes Epitop gespeichert: EDVPSGKLFMHVTLG\n",
      "✅ Gepaddetes Epitop gespeichert: ATIPIQASLPFGWLIVGVALLAVFQSASKIITLKKRWQ\n",
      "✅ Gepaddetes Epitop gespeichert: TMSSSHLFYL\n",
      "✅ Gepaddetes Epitop gespeichert: TSDPTTFHL\n",
      "✅ Gepaddetes Epitop gespeichert: GLLDEDFYA\n",
      "✅ Gepaddetes Epitop gespeichert: YVLDHLIVV\n",
      "✅ Gepaddetes Epitop gespeichert: KLFEFLVYGV\n",
      "✅ Gepaddetes Epitop gespeichert: FLCMKALLL\n",
      "✅ Gepaddetes Epitop gespeichert: LLWNTVWNM\n",
      "✅ Gepaddetes Epitop gespeichert: IPVAYRNVL\n",
      "✅ Gepaddetes Epitop gespeichert: MEVTPSGTWL\n",
      "✅ Gepaddetes Epitop gespeichert: LLVDLAEEL\n",
      "✅ Gepaddetes Epitop gespeichert: KMDYFSGQL\n",
      "✅ Gepaddetes Epitop gespeichert: KMGIGWKPL\n",
      "✅ Gepaddetes Epitop gespeichert: FTVLCLTPV\n",
      "✅ Gepaddetes Epitop gespeichert: GTGPEAGLPY\n",
      "✅ Gepaddetes Epitop gespeichert: VEQCCTSI\n",
      "✅ Gepaddetes Epitop gespeichert: KMVAVFYNT\n",
      "✅ Gepaddetes Epitop gespeichert: YLEPGPVTV\n",
      "✅ Finale gepaddete Epitope-Embeddings gespeichert unter: ../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "# === Lade die originalen Epitope-Embeddings ===\n",
    "original_epitope_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "original_epitopes = np.load(original_epitope_path, allow_pickle=True)\n",
    "\n",
    "# Lade die reduzierten Embeddings (nach PCA)\n",
    "reduced_epitope_path = '../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz'\n",
    "reduced_epitopes = np.load(reduced_epitope_path, allow_pickle=True)['embeddings']\n",
    "\n",
    "# Prüfen, ob die Anzahl der Keys mit den reduzierten Embeddings übereinstimmt\n",
    "original_keys = list(original_epitopes.keys())\n",
    "print(f\"Anzahl Original-Keys: {len(original_keys)}\")\n",
    "print(f\"Anzahl Reduzierte Embeddings: {reduced_epitopes.shape[0]}\")\n",
    "\n",
    "# Dictionary erstellen, das Keys mit den reduzierten Embeddings verknüpft\n",
    "epitope_embeddings_dict = {}\n",
    "for i, key in enumerate(original_keys):\n",
    "    if i < reduced_epitopes.shape[0]:\n",
    "        epitope_embeddings_dict[key] = reduced_epitopes[i]\n",
    "    else:\n",
    "        print(f\"⚠️ Kein reduziertes Embedding für Key: {key}\")\n",
    "\n",
    "# === Padding und Speichern in HDF5 ===\n",
    "max_length = 512\n",
    "embedding_dim = 512\n",
    "\n",
    "h5_output_path = '../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5'\n",
    "\n",
    "with h5py.File(h5_output_path, 'w') as hdf5_file:\n",
    "    for key, embedding in epitope_embeddings_dict.items():\n",
    "        # Sicherstellen, dass die Dimensionen stimmen\n",
    "        if embedding.shape[-1] != embedding_dim:\n",
    "            print(f\"⚠️ Falsche Dimension für Key {key}: {embedding.shape}, wird übersprungen.\")\n",
    "            continue\n",
    "\n",
    "        padded = np.zeros((max_length, embedding_dim), dtype=embedding.dtype)\n",
    "        padded[:embedding.shape[0], :] = embedding\n",
    "        hdf5_file.create_dataset(key, data=padded, compression=\"gzip\")\n",
    "        print(f\"✅ Gepaddetes Epitop gespeichert: {key}\")\n",
    "\n",
    "print(f\"✅ Finale gepaddete Epitope-Embeddings gespeichert unter: {h5_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fehlende Epitope-Keys nach finalem Fix: 0\n"
     ]
    }
   ],
   "source": [
    "def check_missing_keys(h5_file_path, keys_to_check):\n",
    "    with h5py.File(h5_file_path, 'r') as h5_file:\n",
    "        missing_keys = [key for key in keys_to_check if key not in h5_file]\n",
    "    return missing_keys\n",
    "\n",
    "# Lade alle Epitope-Keys aus den Trainingsdaten\n",
    "epitope_keys = set(train_df['Epitope'].dropna().unique())\n",
    "\n",
    "# Überprüfe Epitope-Keys\n",
    "missing_epitope_keys = check_missing_keys(h5_output_path, epitope_keys)\n",
    "print(f\"✅ Fehlende Epitope-Keys nach finalem Fix: {len(missing_epitope_keys)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking dimensions of subset padded emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if the dimensions in the version2 were padded correctly for both epitopes and tcrs\n",
    "padd_beta_all_epi_path = './dummy_data/subset_padded_epitope_embeddings.npz'\n",
    "padd_beta_all_trb_path = './dummy_data/subset_padded_tcr_embeddings.npz'\n",
    "\n",
    "paths = [padd_beta_all_epi_path, padd_beta_all_trb_path]\n",
    "for path in paths:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    # Inspect the shape and size of each stored array\n",
    "    for key in data.files[:5]:\n",
    "        array = data[key]\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        print(f\"Shape: {array.shape}\")\n",
    "        print(f\"Size: {array.size}\")\n",
    "        print(f\"Data Type: {array.dtype}\")\n",
    "        # print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_padded_embeddings(save_path_pattern):\n",
    "    \"\"\"\n",
    "    Load padded embeddings saved in batches.\n",
    "    \"\"\"\n",
    "    padded_embeddings = {}\n",
    "    i = 0\n",
    "\n",
    "    while True:\n",
    "        batch_path = f\"{save_path_pattern}_batch_{i}.npz\"\n",
    "        if not os.path.exists(batch_path):\n",
    "            break  # Stop if no more batches exist\n",
    "\n",
    "        batch = np.load(batch_path, allow_pickle=True)\n",
    "        for key in batch.files:\n",
    "            padded_embeddings[key] = batch[key]\n",
    "        \n",
    "        # if i == 3: # prov\n",
    "        #     break\n",
    "        i += 1\n",
    "\n",
    "    print(f\"Loaded {len(padded_embeddings)} embeddings from {i} batches.\")\n",
    "    return padded_embeddings\n",
    "\n",
    "# Load TCR embeddings\n",
    "padded_tcr_embeddings = load_padded_embeddings('../../data/embeddings/beta/allele/padded/padded_tcr_embeddings')\n",
    "\n",
    "# Load Epitope embeddings\n",
    "padded_epitope_embeddings = load_padded_embeddings('../../data/embeddings/beta/allele/padded/padded_epitope_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating dummy/subsets to try the model and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling subsets\n",
      "Saving subsets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from tqdm import tqdm  # Für eine Fortschrittsanzeige\n",
    "\n",
    "# === File paths ===\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "subset_dir = './dummy_data'\n",
    "os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "# === Load data ===\n",
    "train_df = pd.read_csv(train_path, sep='\\t', low_memory=False)\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t', low_memory=False)\n",
    "subset_size = 10000\n",
    "\n",
    "# === Randomly sample rows ===\n",
    "print('Sampling subsets')\n",
    "subset_train = train_df.sample(n=subset_size, random_state=42)\n",
    "subset_valid = valid_df.sample(n=subset_size, random_state=42)\n",
    "subset_test = test_df.sample(n=subset_size, random_state=42)\n",
    "# === Save subsets ===\n",
    "print('Saving subsets')\n",
    "subset_train.to_csv(os.path.join(subset_dir, 'subset_train.tsv'), sep='\\t', index=False)\n",
    "subset_valid.to_csv(os.path.join(subset_dir, 'subset_validation.tsv'), sep='\\t', index=False)\n",
    "subset_test.to_csv(os.path.join(subset_dir, 'subset_test.tsv'), sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_0.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_1.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_2.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_3.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_4.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_5.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_6.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_7.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_8.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_9.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_10.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_11.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_12.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_13.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_14.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_15.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_16.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_17.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_18.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_19.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_20.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_21.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_22.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_23.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_24.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_25.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_26.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_27.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_28.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_29.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_30.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_31.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_32.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_33.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_34.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train/batch_35.npz...\n",
      "Saved embeddings to ./dummy_data/subset_tcr_emb_train.npz\n",
      "All items in the subsample were successfully matched.\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_epitope_padded_batches/batch_0.npz...\n",
      "Saved embeddings to ./dummy_data/subset_epi_emb_train.npz\n",
      "Warning: 297 items were not matched to any embeddings.\n",
      "Unmatched items saved to ./dummy_data/unmatched_epi_train.csv\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_0.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_1.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_2.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_3.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_4.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_5.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_6.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_7.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_8.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_9.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_10.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_11.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_12.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_13.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches/batch_14.npz...\n",
      "Saved embeddings to ./dummy_data/subset_tcr_emb_val.npz\n",
      "All items in the subsample were successfully matched.\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/valid_epitope_padded_batches/batch_0.npz...\n",
      "Saved embeddings to ./dummy_data/subset_epi_emb_val.npz\n",
      "All items in the subsample were successfully matched.\n",
      "Embedding files created successfully with unmatched items check.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the subsamples\n",
    "train_subsample = pd.read_csv('./dummy_data/subset_train.tsv', sep='\\t')\n",
    "val_subsample = pd.read_csv('./dummy_data/subset_validation.tsv', sep='\\t')\n",
    "\n",
    "# Function to process embeddings in chunks and save as key-value pairs\n",
    "def process_and_save_embeddings_with_check(subsample, base_path, num_batches, output_file, column_name, unmatched_output_file=None):\n",
    "    embeddings_dict = {}\n",
    "    unmatched_items = set(subsample[column_name])  # Start with all items as unmatched\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_path = os.path.join(base_path, f'batch_{i}.npz')\n",
    "        if os.path.exists(batch_path):\n",
    "            print(f\"Processing {batch_path}...\")\n",
    "            batch_data = np.load(batch_path, allow_pickle=True)\n",
    "            batch_dict = dict(batch_data)\n",
    "            \n",
    "            # Match embeddings for the subsample\n",
    "            for item in subsample[column_name]:\n",
    "                if item in batch_dict:\n",
    "                    embeddings_dict[item] = batch_dict[item]\n",
    "                    unmatched_items.discard(item)  # Remove matched item from unmatched set\n",
    "            \n",
    "            # Free memory\n",
    "            del batch_data, batch_dict\n",
    "        else:\n",
    "            print(f\"Warning: {batch_path} does not exist.\")\n",
    "    \n",
    "    # Save the embeddings as a .npz file\n",
    "    np.savez(output_file, **embeddings_dict)\n",
    "    print(f\"Saved embeddings to {output_file}\")\n",
    "    \n",
    "    # Check and report unmatched items\n",
    "    if unmatched_items:\n",
    "        print(f\"Warning: {len(unmatched_items)} items were not matched to any embeddings.\")\n",
    "        if unmatched_output_file:\n",
    "            # Save unmatched items to a file\n",
    "            pd.DataFrame({column_name: list(unmatched_items)}).to_csv(unmatched_output_file, index=False)\n",
    "            print(f\"Unmatched items saved to {unmatched_output_file}\")\n",
    "    else:\n",
    "        print(\"All items in the subsample were successfully matched.\")\n",
    "\n",
    "# Example Usage:\n",
    "# TCR Train with check for unmatched items\n",
    "process_and_save_embeddings_with_check(\n",
    "    train_subsample,\n",
    "    '../../data/embeddings/beta/allele/padded_pca/tcr_padded_batches_train',\n",
    "    36,\n",
    "    './dummy_data/subset_tcr_emb_train.npz',\n",
    "    'TRB_CDR3',\n",
    "    unmatched_output_file='./dummy_data/unmatched_tcr_train.csv'\n",
    ")\n",
    "\n",
    "# Epitope Train with check for unmatched items\n",
    "process_and_save_embeddings_with_check(\n",
    "    train_subsample,\n",
    "    '../../data/embeddings/beta/allele/padded_pca/test_epitope_padded_batches',\n",
    "    1,\n",
    "    './dummy_data/subset_epi_emb_train.npz',\n",
    "    'Epitope',\n",
    "    unmatched_output_file='./dummy_data/unmatched_epi_train.csv'\n",
    ")\n",
    "\n",
    "# TCR Validation with check for unmatched items\n",
    "process_and_save_embeddings_with_check(\n",
    "    val_subsample,\n",
    "    '../../data/embeddings/beta/allele/padded_pca/valid_tcr_padded_batches',\n",
    "    15,\n",
    "    './dummy_data/subset_tcr_emb_val.npz',\n",
    "    'TRB_CDR3',\n",
    "    unmatched_output_file='./dummy_data/unmatched_tcr_val.csv'\n",
    ")\n",
    "\n",
    "# Epitope Validation with check for unmatched items\n",
    "process_and_save_embeddings_with_check(\n",
    "    val_subsample,\n",
    "    '../../data/embeddings/beta/allele/padded_pca/valid_epitope_padded_batches',\n",
    "    1,\n",
    "    './dummy_data/subset_epi_emb_val.npz',\n",
    "    'Epitope',\n",
    "    unmatched_output_file='./dummy_data/unmatched_epi_val.csv'\n",
    ")\n",
    "\n",
    "print(\"Embedding files created successfully with unmatched items check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing mismatches from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered train subsample saved to ./dummy_data/filtered_subset_train.tsv\n"
     ]
    }
   ],
   "source": [
    "# Load the unmatched items\n",
    "unmatched_items_file = './dummy_data/unmatched_epi_train.csv'\n",
    "unmatched_items = pd.read_csv(unmatched_items_file)\n",
    "\n",
    "# Remove unmatched items from the subsample train file\n",
    "filtered_train_subsample = train_subsample[~train_subsample['Epitope'].isin(unmatched_items['Epitope'])]\n",
    "\n",
    "# Save the filtered train subsample back to a file\n",
    "filtered_train_subsample_file = './dummy_data/filtered_subset_train.tsv'\n",
    "filtered_train_subsample.to_csv(filtered_train_subsample_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Filtered train subsample saved to {filtered_train_subsample_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove from valid the repeated pairs from train in validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 37455\n",
      "Validation dataset length: 50000\n",
      "Number of matching rows: 2844\n",
      "Filtered validation dataset saved to ./dummy_data/filtered_subset_validation.tsv\n",
      "Filtered validation dataset length: 47156\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "train_path = './dummy_data/filtered_subset_train.tsv'\n",
    "valid_path = './dummy_data/subset_validation.tsv'\n",
    "output_path = './dummy_data/filtered_subset_validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t')\n",
    "\n",
    "print(f\"Train dataset length: {len(train_df)}\")\n",
    "print(f\"Validation dataset length: {len(valid_df)}\")\n",
    "\n",
    "# Select only the relevant columns\n",
    "target_columns = ['Epitope', 'TRB_CDR3']\n",
    "\n",
    "# Convert train_df pairs into a set for fast lookup\n",
    "train_pairs = set(map(tuple, train_df[target_columns].values))\n",
    "\n",
    "# Identify rows in valid_df that match train_pairs\n",
    "matching_mask = valid_df[target_columns].apply(tuple, axis=1).isin(train_pairs)\n",
    "\n",
    "# Count how many rows in valid_df have the same (Epitope, TRB_CDR3) pair as in train_df\n",
    "matching_rows = matching_mask.sum()\n",
    "print(f\"Number of matching rows: {matching_rows}\")\n",
    "\n",
    "# Remove matching rows from valid_df\n",
    "filtered_valid_df = valid_df[~matching_mask]\n",
    "\n",
    "# Save the filtered DataFrame to a new file\n",
    "filtered_valid_df.to_csv(output_path, sep='\\t', index=False)\n",
    "print(f\"Filtered validation dataset saved to {output_path}\")\n",
    "print(f\"Filtered validation dataset length: {len(filtered_valid_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create subset of TEST dataset and respective embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test data:  54630\n",
      "Sampling subsets\n",
      "Length of subset_test data:  30000\n",
      "Saving subsets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from tqdm import tqdm  # Für eine Fortschrittsanzeige\n",
    "\n",
    "# === File paths ===\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "\n",
    "subset_dir = './dummy_data'\n",
    "# os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "# === Load data ===\n",
    "test_df = pd.read_csv(test_path, sep='\\t', low_memory=False)\n",
    "print(\"Length of test data: \",len(test_df))\n",
    "subset_size = 30000\n",
    "\n",
    "# === Randomly sample rows ===\n",
    "print('Sampling subsets')\n",
    "subset_test = test_df.sample(n=subset_size, random_state=42)\n",
    "print(\"Length of subset_test data: \",len(subset_test))\n",
    "# === Save subsets ===\n",
    "print('Saving subset')\n",
    "subset_test.to_csv(os.path.join(subset_dir, 'subset_test.tsv'), sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings of TEST subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_0.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_1.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_2.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_3.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_4.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_5.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_6.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_7.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_8.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_9.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_10.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_11.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_12.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_13.npz...\n",
      "Processing ../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches/batch_14.npz...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll items in the subsample were successfully matched.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Example Usage:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# TCR Test with check for unmatched items\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m process_and_save_embeddings_with_check(\n\u001b[1;32m     48\u001b[0m     test_subsample,\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dummy_data/subset_tcr_emb_test.npz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRB_CDR3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     53\u001b[0m     unmatched_output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dummy_data/unmatched_tcr_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Epitope Test with check for unmatched items\u001b[39;00m\n\u001b[1;32m     57\u001b[0m process_and_save_embeddings_with_check(\n\u001b[1;32m     58\u001b[0m     test_subsample,\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/beta/allele/padded_pca/test_epitope_padded_batches\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     unmatched_output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dummy_data/unmatched_epi_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m )\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mprocess_and_save_embeddings_with_check\u001b[0;34m(subsample, base_path, num_batches, output_file, column_name, unmatched_output_file)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(batch_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m batch_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(batch_data)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Match embeddings for the subsample\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m subsample[column_name]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mread_array(\u001b[38;5;28mbytes\u001b[39m,\n\u001b[1;32m    257\u001b[0m                              allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_pickle,\n\u001b[1;32m    258\u001b[0m                              pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpickle_kwargs,\n\u001b[1;32m    259\u001b[0m                              max_header_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_header_size)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py:832\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    830\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[1;32m    831\u001b[0m             data \u001b[38;5;241m=\u001b[39m _read_bytes(fp, read_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 832\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    833\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n\u001b[1;32m    836\u001b[0m     array\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m shape[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the subsamples\n",
    "test_subsample = pd.read_csv('./dummy_data/subset_test.tsv', sep='\\t')\n",
    "\n",
    "# Function to process embeddings in chunks and save as key-value pairs\n",
    "def process_and_save_embeddings_with_check(subsample, base_path, num_batches, output_file, column_name, unmatched_output_file=None):\n",
    "    embeddings_dict = {}\n",
    "    unmatched_items = set(subsample[column_name])  # Start with all items as unmatched\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_path = os.path.join(base_path, f'batch_{i}.npz')\n",
    "        if os.path.exists(batch_path):\n",
    "            print(f\"Processing {batch_path}...\")\n",
    "            batch_data = np.load(batch_path, allow_pickle=True)\n",
    "            batch_dict = dict(batch_data)\n",
    "            \n",
    "            # Match embeddings for the subsample\n",
    "            for item in subsample[column_name]:\n",
    "                if item in batch_dict:\n",
    "                    embeddings_dict[item] = batch_dict[item]\n",
    "                    unmatched_items.discard(item)  # Remove matched item from unmatched set\n",
    "            \n",
    "            # Free memory\n",
    "            del batch_data, batch_dict\n",
    "        else:\n",
    "            print(f\"Warning: {batch_path} does not exist.\")\n",
    "    \n",
    "    # Save the embeddings as a .npz file\n",
    "    np.savez(output_file, **embeddings_dict)\n",
    "    print(f\"Saved embeddings to {output_file}\")\n",
    "    \n",
    "    # Check and report unmatched items\n",
    "    if unmatched_items:\n",
    "        print(f\"Warning: {len(unmatched_items)} items were not matched to any embeddings.\")\n",
    "        if unmatched_output_file:\n",
    "            # Save unmatched items to a file\n",
    "            pd.DataFrame({column_name: list(unmatched_items)}).to_csv(unmatched_output_file, index=False)\n",
    "            print(f\"Unmatched items saved to {unmatched_output_file}\")\n",
    "    else:\n",
    "        print(\"All items in the subsample were successfully matched.\")\n",
    "\n",
    "# Example Usage:\n",
    "# TCR Test with check for unmatched items\n",
    "process_and_save_embeddings_with_check(\n",
    "    test_subsample,\n",
    "    '../../data/embeddings/beta/allele/padded_pca/test_tcr_padded_batches',\n",
    "    20,\n",
    "    './dummy_data/subset_tcr_emb_test.npz',\n",
    "    'TRB_CDR3',\n",
    "    unmatched_output_file='./dummy_data/unmatched_tcr_test.csv'\n",
    ")\n",
    "\n",
    "# Epitope Test with check for unmatched items\n",
    "process_and_save_embeddings_with_check(\n",
    "    test_subsample,\n",
    "    '../../data/embeddings/beta/allele/padded_pca/test_epitope_padded_batches',\n",
    "    1,\n",
    "    './dummy_data/subset_epi_emb_test.npz',\n",
    "    'Epitope',\n",
    "    unmatched_output_file='./dummy_data/unmatched_epi_test.csv'\n",
    ")\n",
    "\n",
    "print(\"Embedding files created successfully with unmatched items check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binding not Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset - Binding: 24984, Non-Binding: 25016\n",
      "Validation Dataset - Binding: 8287, Non-Binding: 41713\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "t_path = \"./dummy_data/subset_train_filtered.tsv\"\n",
    "v_path = \"./dummy_data/subset_validation_filtered.tsv\"\n",
    "\n",
    "# Function to count binding and non-binding rows\n",
    "def count_binding_rows(file_path):\n",
    "    # Read the TSV file\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Count the number of binding (1) and non-binding (0) rows\n",
    "    binding_count = df[df['Binding'] == 1].shape[0]\n",
    "    non_binding_count = df[df['Binding'] == 0].shape[0]\n",
    "    \n",
    "    return binding_count, non_binding_count\n",
    "\n",
    "# Count for training dataset\n",
    "train_binding, train_non_binding = count_binding_rows(t_path)\n",
    "print(f\"Training Dataset - Binding: {train_binding}, Non-Binding: {train_non_binding}\")\n",
    "\n",
    "# Count for validation dataset\n",
    "val_binding, val_non_binding = count_binding_rows(v_path)\n",
    "print(f\"Validation Dataset - Binding: {val_binding}, Non-Binding: {val_non_binding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the v1_mha and v1_mha_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(configs_path='./configs/v1_mha_config.yaml', train='./dummy_data/subset_train_filtered.tsv', val='./dummy_data/subset_validation_filtered.tsv', tcr_embeddings='./dummy_data/subset_padded_tcr_embeddings.npz', epitope_embeddings='./dummy_data/subset_padded_epitope_embeddings.npz', epochs=3, batch_size=8, learning_rate=None, embed_dim=None, num_heads=None, num_layers=None, max_tcr_length=None, max_epitope_length=None) \n",
      " {'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embed_dim': 128, 'num_heads': 8, 'num_layers': 2, 'max_tcr_length': 43, 'max_epitope_length': 43, 'model_path': 'results/trained_models/v1_mha.pth', 'data_paths': {'train': '../../data/splitted_datasets/allele/beta/train.tsv', 'val': '../../data/splitted_datasets/allele/beta/validation.tsv', 'test': '../../data/splitted_datasets/allele/beta/test.tsv'}, 'embeddings': {'tcr': '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz', 'epitope': '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'}}\n",
      "embed_dim:  128\n",
      "max_tcr_length:  43\n",
      "max_epitope_length:  43\n",
      "Epoch [1/3], Loss: 0.5120, Val AUC: 0.9997\n",
      "Epoch [2/3], Loss: 0.5086, Val AUC: 0.9952\n",
      "Epoch [3/3], Loss: 0.5136, Val AUC: 0.9974\n",
      "Best model saved with AUC: 0.9996612561329208\n"
     ]
    }
   ],
   "source": [
    "# overfit the model \n",
    "# train and validation a 50.000 samples\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/subset_train_filtered.tsv \\\n",
    "    --val ./dummy_data/subset_validation_filtered.tsv \\\n",
    "        --tcr_embeddings ./dummy_data/subset_padded_tcr_embeddings.npz \\\n",
    "            --epitope_embeddings ./dummy_data/subset_padded_epitope_embeddings.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 \n",
    "                        #-- config_path ./configs/v1_mha_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.001\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.5128, Val AUC: 1.0000, Val Accuracy: 0.1657\n",
      "Epoch [2/3], Loss: 0.5092, Val AUC: 0.9999, Val Accuracy: 0.1657\n",
      "Epoch [3/3], Loss: 0.5053, Val AUC: 1.0000, Val Accuracy: 0.1657\n",
      "Best model saved with AUC: 0.9999773631714294\n"
     ]
    }
   ],
   "source": [
    "# check memory\n",
    "# train and validation a 50.000 samples\n",
    "# batch_size = 16\n",
    "# accuracy calculation added\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/subset_train_filtered.tsv \\\n",
    "    --val ./dummy_data/subset_validation_filtered.tsv \\\n",
    "        --tcr_embeddings ./dummy_data/subset_padded_tcr_embeddings.npz \\\n",
    "            --epitope_embeddings ./dummy_data/subset_padded_epitope_embeddings.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/subset_train_filtered.tsv\n",
      "val_path: ./dummy_data/subset_validation_filtered.tsv\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.5151, Val AUC: 0.9467, Val Accuracy: 0.1657\n",
      "Epoch [2/3], Loss: 0.5066, Val AUC: 0.9994, Val Accuracy: 0.1657\n",
      "Epoch [3/3], Loss: 0.5041, Val AUC: 0.9990, Val Accuracy: 0.1657\n",
      "Best model saved with AUC: 0.9994373945324483\n"
     ]
    }
   ],
   "source": [
    "# overfit the model \n",
    "# train and validation a 50.000 samples\n",
    "# \n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/subset_train_filtered.tsv \\\n",
    "    --val ./dummy_data/subset_validation_filtered.tsv \\\n",
    "        --tcr_embeddings ./dummy_data/subset_padded_tcr_embeddings.npz \\\n",
    "            --epitope_embeddings ./dummy_data/subset_padded_epitope_embeddings.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 \n",
    "\n",
    "# 17m 26.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/subset_train_filtered.tsv\n",
      "val_path: ./dummy_data/subset_validation_filtered.tsv\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.5075, Val AUC: 0.9995, Val Accuracy: 0.9939\n",
      "Epoch [2/3], Loss: 0.5034, Val AUC: 0.9999, Val Accuracy: 0.9999\n",
      "Epoch [3/3], Loss: 0.5033, Val AUC: 0.9998, Val Accuracy: 0.9999\n",
      "Best model saved with AUC: 0.999853282975565\n"
     ]
    }
   ],
   "source": [
    "# new USING:\n",
    "# pooled = combined.mean(dim=1)  # Average across all tokens, shape: (B, D)\n",
    "# output = torch.sigmoid(self.output_layer(pooled)).squeeze(1)\n",
    "\n",
    "# subset = 50.000 samples\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/subset_train_filtered.tsv \\\n",
    "    --val ./dummy_data/subset_validation_filtered.tsv \\\n",
    "        --tcr_embeddings ./dummy_data/subset_padded_tcr_embeddings.npz \\\n",
    "            --epitope_embeddings ./dummy_data/subset_padded_epitope_embeddings.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 \n",
    "\n",
    "# 14m 13.5s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_batches.py:58: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(train_path, sep='\\t')\n",
      "Sample 0 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 1 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 2 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 3 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 4 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 0 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 1 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 2 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 3 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Sample 4 - TCR shape: torch.Size([43, 1024]), Epitope shape: torch.Size([43, 1024]), Label: 1.0\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "starting epoch  0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_batches.py\", line 103, in <module>\n",
      "    epoch_loss = 0\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models/morning_stars_v1/beta/v1_mha_batches.py\", line 52, in __getitem__\n",
      "    tcr_embedding = torch.tensor(tcr_batch_data[tcr_id], dtype=torch.float32)\n",
      "                                 ~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py\", line 256, in __getitem__\n",
      "    return format.read_array(bytes,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py\", line 782, in read_array\n",
      "    version = read_magic(fp)\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py\", line 235, in read_magic\n",
      "    magic_str = _read_bytes(fp, MAGIC_LEN, \"magic string\")\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py\", line 966, in _read_bytes\n",
      "    r = fp.read(size - len(data))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 981, in read\n",
      "    data = self._read1(n)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1057, in _read1\n",
      "    data = self._decompressor.decompress(data, n)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# train with all the data (tran/validation) and all batches of embeddings\n",
    "! python models_scripts/v1_mha/train_batches.py \\\n",
    "    --model_path results/trained_models/v1_mha/v1_mha_batches.pth \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 16 \n",
    "\n",
    "# 138m  and still in 'starting epoch 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_batches.py:59: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(train_path, sep='\\t')\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Starting epoch 0\n",
      "Training Epoch 0:   4%|▋               | 724/17516 [11:18<4:22:11,  1.07batch/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_batches.py\", line 106, in <module>\n",
      "    for tcr, epitope, label in tqdm(train_loader, desc=f\"Training Epoch {epoch}\", unit=\"batch\"):\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models/morning_stars_v1/beta/v1_mha_batches.py\", line 51, in __getitem__\n",
      "    tcr_batch_data = np.load(tcr_batch_file, mmap_mode=\"r\")\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py\", line 444, in load\n",
      "    ret = NpzFile(fid, own_fid=own_fid, allow_pickle=allow_pickle,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py\", line 190, in __init__\n",
      "    _zip = zipfile_factory(fid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n",
      "    return zipfile.ZipFile(file, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1341, in __init__\n",
      "    self._RealGetContents()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1480, in _RealGetContents\n",
      "    for zinfo in sorted(self.filelist,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1481, in <lambda>\n",
      "    key=lambda zinfo: zinfo.header_offset,\n",
      "\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# train with all the data (tran/validation) and all batches of embeddings\n",
    "# with progress bar\n",
    "! python models_scripts/v1_mha/train_batches.py \\\n",
    "    --model_path results/trained_models/v1_mha/v1_mha_batches.pth \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 16 \n",
    "# 11m 25.3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/filtered_subset_train.tsv\n",
      "val_path: ./dummy_data/subset_validation.tsv\n",
      "Loading embeddings...\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.5685, Val AUC: 0.9960, Val Accuracy: 0.8640                \n",
      "Epoch [2/3], Loss: 0.5673, Val AUC: 0.9994, Val Accuracy: 0.9847                \n",
      "Epoch [3/3], Loss: 0.5670, Val AUC: 0.9989, Val Accuracy: 0.9987                \n",
      "Best model saved with AUC: 0.9993650521462418\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/filtered_subset_train.tsv \\\n",
    "    --val ./dummy_data/subset_validation.tsv \\\n",
    "        --tcr_train_embeddings ./dummy_data/subset_tcr_emb_train.npz \\\n",
    "            --epitope_train_embeddings ./dummy_data/subset_epi_emb_train.npz \\\n",
    "            --tcr_valid_embeddings ./dummy_data/subset_tcr_emb_val.npz \\\n",
    "                --epitope_valid_embeddings ./dummy_data/subset_epi_emb_val.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 \n",
    "# 10m 51.3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/filtered_subset_train.tsv\n",
      "val_path: ./dummy_data/subset_validation.tsv\n",
      "Loading embeddings...\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.0045, Val AUC: 1.0000, Val Accuracy: 0.9999                \n",
      "Epoch [2/3], Loss: 0.0000, Val AUC: 1.0000, Val Accuracy: 0.9998                \n",
      "Epoch [3/3], Loss: 0.0000, Val AUC: 1.0000, Val Accuracy: 0.9998                \n",
      "Best model saved with AUC: 0.9999997280687686\n"
     ]
    }
   ],
   "source": [
    "# with only sigmoid in train and not model\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/filtered_subset_train.tsv \\\n",
    "    --val ./dummy_data/subset_validation.tsv \\\n",
    "        --tcr_train_embeddings ./dummy_data/subset_tcr_emb_train.npz \\\n",
    "            --epitope_train_embeddings ./dummy_data/subset_epi_emb_train.npz \\\n",
    "            --tcr_valid_embeddings ./dummy_data/subset_tcr_emb_val.npz \\\n",
    "                --epitope_valid_embeddings ./dummy_data/subset_epi_emb_val.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/filtered_subset_train.tsv\n",
      "val_path: ./dummy_data/filtered_subset_validation.tsv\n",
      "Loading embeddings...\n",
      "tcr_train  ./dummy_data/subset_tcr_emb_train.npz\n",
      "epi_train  ./dummy_data/subset_epi_emb_train.npz\n",
      "tcr_valid  ./dummy_data/subset_tcr_emb_val.npz\n",
      "epi_valid  ./dummy_data/subset_epi_emb_val.npz\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.0240, Val AUC: 0.9999, Val Accuracy: 0.9991                \n",
      "Epoch [2/3], Loss: 0.0001, Val AUC: 1.0000, Val Accuracy: 0.9991                \n",
      "Epoch [3/3], Loss: 0.0000, Val AUC: 1.0000, Val Accuracy: 0.9991                \n",
      "Best model saved with AUC: 0.9999830924573252\n"
     ]
    }
   ],
   "source": [
    "# with only sigmoid in train and not in model\n",
    "# doubles in train-validation removed from validation.\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/filtered_subset_train.tsv \\\n",
    "    --val ./dummy_data/filtered_subset_validation.tsv \\\n",
    "        --tcr_train_embeddings ./dummy_data/subset_tcr_emb_train.npz \\\n",
    "            --epitope_train_embeddings ./dummy_data/subset_epi_emb_train.npz \\\n",
    "            --tcr_valid_embeddings ./dummy_data/subset_tcr_emb_val.npz \\\n",
    "                --epitope_valid_embeddings ./dummy_data/subset_epi_emb_val.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/filtered_subset_train.tsv\n",
      "val_path: ./dummy_data/filtered_subset_validation.tsv\n",
      "Loading embeddings...\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.5684, Val AUC: 0.9996, Val Accuracy: 0.1707                \n",
      "Epoch [2/3], Loss: 0.5674, Val AUC: 0.9977, Val Accuracy: 0.9715                \n",
      "Epoch [3/3], Loss: 0.5671, Val AUC: 0.9999, Val Accuracy: 0.9949                \n",
      "Best model saved with AUC: 0.9998654291139732\n"
     ]
    }
   ],
   "source": [
    "# back to sigmoid in train and in model\n",
    "# doubles in train-validation removed from validation.\n",
    "\n",
    "! python models_scripts/v1_mha/train.py --train ./dummy_data/filtered_subset_train.tsv \\\n",
    "    --val ./dummy_data/filtered_subset_validation.tsv \\\n",
    "        --tcr_train_embeddings ./dummy_data/subset_tcr_emb_train.npz \\\n",
    "            --epitope_train_embeddings ./dummy_data/subset_epi_emb_train.npz \\\n",
    "            --tcr_valid_embeddings ./dummy_data/subset_tcr_emb_val.npz \\\n",
    "                --epitope_valid_embeddings ./dummy_data/subset_epi_emb_val.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.001\n",
      "train_path: ./dummy_data/filtered_subset_train.tsv\n",
      "val_path: ./dummy_data/filtered_subset_validation.tsv\n",
      "Loading embeddings...\n",
      "tcr_train  ./dummy_data/subset_tcr_emb_train.npz\n",
      "epi_train  ./dummy_data/subset_epi_emb_train.npz\n",
      "tcr_valid  ./dummy_data/subset_tcr_emb_val.npz\n",
      "epi_valid  ./dummy_data/subset_epi_emb_val.npz\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/3], Loss: 0.0239, Val AUC: 0.9995, Val Accuracy: 0.9987, Val F1: 0.9962, TP: 7987, TN: 39108, FP: 0, FN: 61\n",
      "Epoch [2/3], Loss: 0.0003, Val AUC: 0.9996, Val Accuracy: 0.9988, Val F1: 0.9964, TP: 7990, TN: 39108, FP: 0, FN: 58\n",
      "Epoch [3/3], Loss: 0.0014, Val AUC: 0.9994, Val Accuracy: 0.9973, Val F1: 0.9920, TP: 7921, TN: 39108, FP: 0, FN: 127\n",
      "Best model saved with AUC: 0.9995872386373179\n"
     ]
    }
   ],
   "source": [
    "# with only sigmoid in train and not in model\n",
    "# doubles in train-validation removed from validation.\n",
    "\n",
    "! python models_scripts/v1_mha/O__train.py --train ./dummy_data/filtered_subset_train.tsv \\\n",
    "    --val ./dummy_data/filtered_subset_validation.tsv \\\n",
    "        --tcr_train_embeddings ./dummy_data/subset_tcr_emb_train.npz \\\n",
    "            --epitope_train_embeddings ./dummy_data/subset_epi_emb_train.npz \\\n",
    "            --tcr_valid_embeddings ./dummy_data/subset_tcr_emb_val.npz \\\n",
    "                --epitope_valid_embeddings ./dummy_data/subset_epi_emb_val.npz \\\n",
    "                --epochs 3 \\\n",
    "                    --batch_size 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Dimensions in forward of v1_mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch 1/1 [Training]:   0%|                          | 0/280252 [00:00<?, ?it/s]TCR Input (Raw, Padded): tensor([[ 3.1740e+01,  1.0894e+01,  2.7601e+00,  ...,  3.0147e-02,\n",
      "         -7.6628e-03, -8.9293e-02],\n",
      "        [ 1.3225e+01,  2.7631e+01,  1.5405e-01,  ...,  2.5095e-02,\n",
      "          8.3678e-02,  3.0795e-01],\n",
      "        [-3.5663e-03, -5.6855e+00, -2.5329e+01,  ...,  3.5874e-02,\n",
      "          1.5968e-01,  1.4019e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "Epitope Input (Raw, Padded): tensor([[-2.4699e+01, -5.1315e-02,  2.4659e+00,  ..., -7.4737e-02,\n",
      "         -1.3676e-02,  2.4898e-01],\n",
      "        [-6.9972e+00,  9.4694e-01, -3.6500e+00,  ..., -2.2876e-01,\n",
      "          2.1689e-01,  3.2405e-01],\n",
      "        [ 5.9762e+00,  1.2570e+00, -2.9699e+00,  ...,  1.5582e-01,\n",
      "         -3.3448e-01,  5.1799e-02],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "TCR Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Epitope Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Combined Shape: torch.Size([1, 86, 128])\n",
      "Combined Tensor (Sample): tensor([[ 1.7780, -0.8312, -1.1876,  0.9936,  1.5551,  1.8165, -0.2429,  0.1288,\n",
      "          2.3385, -0.5410,  0.3374, -1.4232,  0.4877, -0.9961,  0.7486, -0.7067,\n",
      "         -1.2496, -2.3882,  2.0794,  1.2598,  1.3388,  0.2312, -3.2034, -1.0548,\n",
      "         -0.3159, -1.3790, -1.2795, -0.4266, -0.3207,  0.3834, -0.1663,  0.1506,\n",
      "          1.3170, -0.5523, -0.2266, -0.1948, -1.0653, -1.0526, -0.3870, -0.0793,\n",
      "         -0.7697,  0.1538, -0.6132,  1.7032,  0.6049,  0.9738,  0.5644, -0.4533,\n",
      "         -0.2463,  1.0284,  2.7208, -0.5159, -0.1958,  0.4438, -0.8369,  1.3394,\n",
      "          0.4688, -2.8249,  1.2845,  3.2641, -4.1160, -0.5375,  2.3244, -0.0746,\n",
      "         -1.0773, -0.8126,  0.6662, -2.0417, -1.5727, -0.7559, -0.3738,  0.0335,\n",
      "          1.0121,  0.7381, -1.0408, -1.1755, -0.3254, -1.1548, -0.1795, -1.2782,\n",
      "         -0.6720,  0.8145, -1.4614, -0.3805, -1.7552,  0.4003, -3.4590,  1.0147,\n",
      "         -1.0985, -0.0121,  1.9124,  1.0343, -1.0765,  1.3804,  0.0102,  0.1361,\n",
      "         -0.4418, -1.2801, -2.5781,  1.9790,  0.6374,  1.9038,  0.0092,  0.8782,\n",
      "         -1.8000,  0.1074, -1.5064, -0.0227, -0.5256,  0.2370, -0.4836,  2.2885,\n",
      "         -0.1372,  0.8483,  0.5639, -0.1523, -1.3972, -0.2195,  0.2883,  0.4250,\n",
      "         -1.1772, -2.1466,  1.1728,  2.1164,  0.0464, -0.6876,  2.5458,  2.7525],\n",
      "        [ 1.1183, -1.8966, -1.6935, -0.9016,  1.7960,  1.3985, -2.0081,  0.0730,\n",
      "         -1.6187,  2.8279, -1.2286, -1.7781,  0.8138,  0.0309, -0.5438, -0.1364,\n",
      "          1.6768, -1.6568,  1.4472,  1.8875,  1.6927,  0.0118,  0.5104,  3.0336,\n",
      "         -0.3520, -0.9782, -0.0704, -0.4740, -1.5119, -0.3029,  0.6768, -0.1103,\n",
      "          0.5799, -0.2813, -0.0677,  0.4736, -3.3908, -1.9478, -3.1659,  1.4616,\n",
      "          1.0363,  1.0540, -0.1750,  1.6139,  0.6674,  0.7671, -1.1428, -0.2825,\n",
      "          0.5406,  1.4101,  0.8115, -0.7718,  0.7900, -2.0002, -0.8104, -0.3017,\n",
      "          1.3434,  0.0967, -0.7612,  2.9251, -1.6464, -3.5328,  0.4723, -1.7859,\n",
      "         -0.9248, -0.9704,  0.1164, -1.0720, -1.0458, -0.8445, -0.4330,  2.0002,\n",
      "         -2.3746, -0.4976,  0.0147, -0.2827,  0.0672,  0.9340, -0.4153, -0.5787,\n",
      "         -0.0054,  0.6333,  1.2631, -0.5119, -1.3786, -1.2629, -1.1696,  2.4930,\n",
      "         -0.1301,  0.0514,  1.7977, -0.8087,  1.5343,  1.3930,  0.0083,  0.8096,\n",
      "          0.4964, -1.3345,  1.3628,  1.6713,  0.6365, -1.4025, -0.1127,  2.0111,\n",
      "         -0.7867, -0.7961,  0.0631, -2.5260,  1.2609,  0.5353,  0.1449,  0.0316,\n",
      "         -0.3165,  0.4516, -2.4446, -0.2135,  1.7025,  1.0305,  1.2863, -0.2867,\n",
      "         -0.6675,  0.0744, -0.3739, -1.3732,  1.5675, -0.4180,  2.2365, -0.7287],\n",
      "        [-0.2233,  0.1642, -0.6252, -2.3757, -1.5933,  0.2299,  2.6432, -0.2139,\n",
      "          0.5931, -0.6482, -0.2288,  0.9426,  1.3410,  2.7667, -1.2132,  0.1918,\n",
      "         -0.4021,  0.1700, -0.9361, -0.8529,  1.2158,  1.1717, -0.5775,  0.1144,\n",
      "          0.4812, -1.4419, -1.9698, -1.4479, -2.9059, -0.1867,  1.1354,  0.2453,\n",
      "         -2.2837, -1.8283, -0.6031,  1.0126, -2.9181, -0.9460,  1.9839,  1.8415,\n",
      "         -0.3150,  0.6868, -2.8506,  1.4591, -0.8420, -2.3580, -1.7711, -0.2195,\n",
      "         -0.3941, -1.8682, -1.0965, -0.9207, -0.7357,  2.5892, -2.0503,  1.5635,\n",
      "          0.5715, -0.7501,  0.3338, -2.0924, -1.1019,  0.7273, -1.2307, -0.0330,\n",
      "         -0.7114, -2.4001,  0.5768,  0.3597, -0.6454,  1.0737,  0.6941, -1.5876,\n",
      "         -0.6575, -1.5526, -0.0805,  2.4314, -0.6332,  1.4888, -0.6731,  0.3428,\n",
      "         -0.2390,  0.1087, -1.2767, -1.6270, -0.9961,  0.1808, -1.4119,  0.1544,\n",
      "          0.7438,  0.4517, -0.4736, -0.1318, -0.5007,  0.3544,  0.1736, -1.2889,\n",
      "          0.5958,  2.1943,  2.1849, -0.8439,  0.4898, -0.0739,  1.0275, -0.3722,\n",
      "          0.5690, -0.6540, -1.6972, -1.9582,  2.4303,  0.0355,  0.5071, -0.2131,\n",
      "          0.2291, -0.0060,  0.2424,  0.4679, -0.2690,  0.0835, -0.5301, -0.7218,\n",
      "         -0.7313,  0.5815,  0.0547, -2.3708,  2.0429, -0.6708,  1.9524, -2.2771],\n",
      "        [ 0.1589,  0.8404, -0.6336, -2.8513,  0.5758, -2.7584, -1.1038,  1.1318,\n",
      "         -2.2459,  1.2784, -1.4458, -1.7990,  0.4925, -0.9043, -1.3523,  0.6446,\n",
      "          1.5557, -2.8503,  1.3982, -0.4682,  0.8969,  1.7713,  1.0568,  0.5507,\n",
      "         -0.6765,  0.9917, -1.1983, -1.0221, -1.2804, -0.8092,  1.6740,  1.6437,\n",
      "         -1.3463, -2.5289,  1.9688,  1.1216,  0.0603,  0.1118, -0.5956, -1.0330,\n",
      "         -0.0657,  1.2668, -0.6251,  0.7483, -0.7624,  0.3365, -0.6990,  1.1953,\n",
      "         -0.1579,  1.2203,  0.0862,  2.8396,  2.3578,  0.6133,  2.0345,  1.7488,\n",
      "          0.4542,  0.8751, -2.2526, -0.5201, -0.3495,  1.5342,  0.4730, -0.2757,\n",
      "         -1.8015, -0.3871,  1.7206, -1.7852, -0.5211,  0.5739,  0.6209,  1.6640,\n",
      "          0.0581, -1.0894,  3.1620, -0.7459,  0.3512,  1.8429,  0.0210, -1.5248,\n",
      "          0.8000, -1.4294,  0.1514, -0.9554, -0.6590, -1.0204, -1.0363,  0.6169,\n",
      "         -1.5596, -1.3847, -0.2641,  0.2849,  0.2078,  0.1248, -0.5801,  1.2008,\n",
      "          0.4800, -2.3169,  1.7174,  1.1499,  0.2122,  0.8843, -0.8615,  0.9231,\n",
      "          0.1396, -0.7787,  0.0230, -0.1799, -1.3980,  1.1184, -0.2539, -0.7846,\n",
      "         -0.9435,  0.9591,  1.5162,  2.0254,  0.1514, -0.0278,  1.1037,  1.1149,\n",
      "         -0.0534,  1.4489, -1.4268, -0.7580, -1.0248,  0.4483, -1.5092,  1.8845],\n",
      "        [ 1.3823, -0.1198,  0.5235, -0.3754, -1.0507, -1.1597, -1.1239, -0.2855,\n",
      "          0.7334, -0.0521,  0.0781,  1.8773, -2.0490,  1.8785,  0.1847, -1.1584,\n",
      "         -3.4276,  0.8603,  0.7071, -1.4600, -0.0555, -0.3647, -0.3278, -0.0852,\n",
      "         -0.6113, -1.2210,  0.6398,  0.3931, -0.2622,  0.4103,  0.2264,  0.6758,\n",
      "         -0.0479, -1.2728,  0.0198,  1.1980,  0.6835,  0.2224,  0.6784,  0.8369,\n",
      "         -0.4347, -1.9410, -0.5230,  0.0304,  0.1928, -1.8993, -0.3113,  1.3549,\n",
      "         -1.4597,  0.5102, -1.1503, -0.0377,  0.3819, -0.4682, -0.2042,  0.4278,\n",
      "          1.0406,  0.3590,  2.2648, -0.7186,  0.1236, -1.2531, -0.9772,  0.6650,\n",
      "         -0.4152,  1.2672,  1.1535,  1.7055, -0.6324, -0.6385,  0.7606, -0.9572,\n",
      "          0.8880, -2.1815,  1.2529, -1.6865, -2.3381,  0.2032, -1.5874,  0.4230,\n",
      "          0.4963,  0.5663,  1.4297, -0.4721,  0.3552,  0.2484, -1.3174, -0.2210,\n",
      "          0.8114,  0.5682, -0.9265, -0.7140, -1.6321, -0.7432,  1.1056,  0.8290,\n",
      "         -0.8908, -0.4807,  1.3608, -0.4809,  0.7098, -1.3122,  0.3477,  0.0639,\n",
      "          0.4698,  0.3161, -1.1070,  0.3565, -0.1873,  0.7454,  0.4567,  0.7062,\n",
      "         -1.2037,  0.6119, -1.2007,  1.8508,  0.1284, -0.5947, -0.7808,  1.2741,\n",
      "          0.4568, -1.0225,  0.0113,  0.2511, -0.6043,  0.5433, -1.1045,  0.8033]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Key Padding Mask Shape: torch.Size([1, 86])\n",
      "Key Padding Mask (Sample): tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False], device='cuda:0')\n",
      "Combined Shape after Transformer Layer 1: torch.Size([1, 86, 128])\n",
      "Combined Shape after Transformer Layer 2: torch.Size([1, 86, 128])\n",
      "Pooled Shape: torch.Size([1, 128])\n",
      "Output Shape: torch.Size([1])\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train_dimensions_check.py --epochs 1 \\\n",
    "    --train \"../../data/splitted_datasets/allele/beta/train.tsv\" \\\n",
    "        --val \"../../data/splitted_datasets/allele/beta/validation.tsv\" \\\n",
    "    --tcr_train_embeddings  \"../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\" \\\n",
    "        --epitope_train_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\" \\\n",
    "            --tcr_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\" \\\n",
    "                    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch 1/1 [Training]:   0%|                          | 0/280252 [00:00<?, ?it/s]TCR Sequence (Raw, Padded): [[ 2.5579268e+01  6.5075474e+00  3.5236814e+00 ...  3.6206666e-02\n",
      "  -1.3864447e-02 -1.3629834e-01]\n",
      " [ 2.9741594e+01  9.8486700e+00  4.3866434e+00 ...  1.1590675e-02\n",
      "   7.8122273e-02  1.2144786e-01]\n",
      " [-9.7072959e-01 -6.1731248e+00 -2.4681810e+01 ...  7.9845518e-02\n",
      "   1.9318277e-01 -2.5544181e-03]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "Epitope Sequence (Raw, Padded): [[-2.8430790e+01 -1.1926186e-01  9.1367823e-01 ...  3.6895454e-02\n",
      "  -1.2947612e-01  3.3648127e-01]\n",
      " [-6.4094186e+00  1.1819981e+00 -6.9121156e+00 ... -1.8408881e-01\n",
      "   1.0397623e-01  2.2796379e-01]\n",
      " [ 3.0136077e+00 -9.5848167e-01 -2.5824420e+00 ... -4.9776942e-02\n",
      "  -2.3388222e-02  8.0410302e-02]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "TCR Input (Raw, Padded): tensor([[ 2.5579e+01,  6.5075e+00,  3.5237e+00,  ...,  3.6207e-02,\n",
      "         -1.3864e-02, -1.3630e-01],\n",
      "        [ 2.9742e+01,  9.8487e+00,  4.3866e+00,  ...,  1.1591e-02,\n",
      "          7.8122e-02,  1.2145e-01],\n",
      "        [-9.7073e-01, -6.1731e+00, -2.4682e+01,  ...,  7.9846e-02,\n",
      "          1.9318e-01, -2.5544e-03],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "Epitope Input (Raw, Padded): tensor([[-2.8431e+01, -1.1926e-01,  9.1368e-01,  ...,  3.6895e-02,\n",
      "         -1.2948e-01,  3.3648e-01],\n",
      "        [-6.4094e+00,  1.1820e+00, -6.9121e+00,  ..., -1.8409e-01,\n",
      "          1.0398e-01,  2.2796e-01],\n",
      "        [ 3.0136e+00, -9.5848e-01, -2.5824e+00,  ..., -4.9777e-02,\n",
      "         -2.3388e-02,  8.0410e-02],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "TCR Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Epitope Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Combined Shape: torch.Size([1, 86, 128])\n",
      "Combined Tensor (Sample): tensor([[-1.4639e-01, -8.9134e-01,  7.4875e-01, -1.2156e+00,  3.0177e+00,\n",
      "         -9.4878e-01,  2.2964e+00,  2.3382e+00,  1.6920e+00,  3.0500e-01,\n",
      "         -1.7430e+00,  3.0214e-01,  6.7009e-01,  6.2940e-01,  8.9357e-02,\n",
      "          7.4990e-01, -6.2890e-01,  1.3643e+00, -7.0880e-01,  1.1949e+00,\n",
      "         -3.3579e-01,  5.7274e-01,  4.8517e-01, -1.4838e+00, -6.0568e-01,\n",
      "         -7.2031e-01, -2.1032e+00, -1.3461e+00, -5.2511e-01,  2.1267e+00,\n",
      "         -4.5901e-01,  5.4698e-01,  8.3430e-01, -3.1673e-01, -1.2799e+00,\n",
      "         -1.8475e+00,  3.6535e-02,  2.3770e-01,  1.0055e+00, -2.2972e-01,\n",
      "         -1.3555e+00, -5.9314e-02,  9.8842e-01,  1.8950e-01,  5.2684e-01,\n",
      "         -8.6188e-02, -8.0428e-01, -2.2743e-01, -1.6735e+00, -1.5261e+00,\n",
      "          1.5294e-01,  7.4858e-01, -1.0697e+00, -1.2505e+00, -1.8893e+00,\n",
      "          1.1101e+00,  8.5708e-03, -2.6011e+00, -2.1935e+00,  1.1585e+00,\n",
      "          5.8775e-01, -1.4110e+00, -1.3303e+00, -1.7742e-01, -1.0046e+00,\n",
      "          5.4656e-01, -1.0146e+00,  6.6728e-01, -2.8974e+00, -4.3994e-01,\n",
      "          1.7190e+00,  1.8861e+00,  6.8915e-01,  3.6647e+00, -1.6888e+00,\n",
      "          1.2389e+00, -2.4757e+00,  3.4978e-01,  1.1163e+00,  4.8050e-01,\n",
      "          1.6486e+00, -5.2204e-01, -1.6226e+00, -9.9036e-03,  2.0604e+00,\n",
      "         -1.4762e+00, -1.2959e+00, -1.4442e+00,  8.7799e-01,  1.1061e+00,\n",
      "          2.1350e+00,  9.2470e-01,  1.1709e+00,  1.8373e+00,  1.4435e+00,\n",
      "          7.2073e-01,  2.4173e+00,  2.0875e-02, -6.8692e-01, -8.1808e-01,\n",
      "          5.2195e-01,  4.8465e-01,  1.2502e+00, -2.8763e-01,  2.8452e-01,\n",
      "          4.5325e-01,  7.7543e-01,  7.0524e-01,  1.1372e+00, -9.9919e-01,\n",
      "         -3.7406e-01, -6.1328e-01, -5.4254e-01,  1.4990e-01, -2.1193e-01,\n",
      "          3.2796e-01, -8.0937e-01,  6.3241e-01,  2.4666e-01, -1.1836e+00,\n",
      "         -1.0477e-01,  1.1357e+00,  5.8458e-01,  1.2969e+00, -3.4313e-02,\n",
      "          1.7506e+00, -2.3772e+00, -3.4098e+00],\n",
      "        [-1.5592e+00, -1.0982e+00, -2.2520e+00,  1.6263e+00,  9.9521e-01,\n",
      "         -2.2370e+00,  3.9215e+00,  1.9539e+00,  4.4351e+00, -1.2230e+00,\n",
      "         -6.7651e-01, -1.5622e+00,  1.3417e+00, -7.2204e-01, -4.7642e-01,\n",
      "          7.6069e-01, -2.6888e+00, -5.6451e-01, -1.4018e+00,  1.9456e+00,\n",
      "          1.2371e+00, -2.8128e-01,  1.2513e+00,  8.7871e-01,  2.1907e+00,\n",
      "         -1.3498e+00, -3.4493e-01, -2.8304e+00,  2.0904e+00,  1.2425e+00,\n",
      "         -9.4607e-01, -5.0875e-01, -1.2436e+00, -4.4325e-01, -4.1088e-01,\n",
      "         -1.4529e+00, -1.1608e+00,  9.7955e-01, -1.1137e+00, -6.4081e-01,\n",
      "         -4.8495e-01,  1.3125e+00, -5.6288e-01, -7.4701e-01, -5.7904e-01,\n",
      "         -6.9171e-01, -1.7010e+00, -1.5206e+00,  1.1097e+00, -1.5407e+00,\n",
      "         -2.0897e-01,  1.4067e+00,  2.4272e+00, -2.2470e+00, -8.7210e-01,\n",
      "         -1.6343e-01, -6.6406e-01, -5.8556e-01,  8.7953e-03,  4.6771e-01,\n",
      "         -8.7255e-01,  1.9505e-01, -2.5744e-01,  1.0094e+00, -7.7492e-01,\n",
      "          1.2110e+00, -1.0414e+00,  1.5893e+00, -5.4884e-01, -3.1084e+00,\n",
      "          1.7478e+00, -2.2229e+00, -7.4350e-01,  9.8160e-01, -3.1929e-01,\n",
      "          4.7186e-01, -1.1548e-01, -7.6611e-01, -6.7497e-01,  3.9500e+00,\n",
      "         -2.2031e+00, -2.1510e+00,  1.5492e+00, -1.6767e-01, -2.4438e+00,\n",
      "         -1.9758e+00,  4.8713e-01, -1.6962e+00, -1.6959e+00, -1.0924e+00,\n",
      "          8.0572e-02,  1.6716e+00,  2.7896e-01,  7.6206e-01, -5.0346e-01,\n",
      "         -2.2420e+00,  1.0133e+00, -9.2795e-01,  2.6032e+00,  2.6809e+00,\n",
      "          1.4823e+00,  3.8291e-01,  5.2471e-02,  1.7652e-02, -1.5312e-01,\n",
      "          1.9076e+00,  2.1865e-01,  1.0461e+00, -3.2458e-01,  1.9615e+00,\n",
      "         -6.5755e-02, -6.6700e-01, -8.5416e-01, -1.0522e+00, -2.2846e+00,\n",
      "         -1.8710e+00, -2.2936e+00,  3.8259e-01,  2.4274e+00,  5.4624e-01,\n",
      "         -7.9362e-02,  1.9305e+00,  2.1560e+00,  1.3388e+00,  2.0725e+00,\n",
      "          2.7419e-01,  1.0592e+00,  6.4360e-01],\n",
      "        [ 1.4713e+00,  1.5539e+00,  4.4401e-01, -1.3109e+00, -2.3171e-01,\n",
      "          9.6328e-01, -1.2081e-01, -1.5666e-01,  3.5793e-01,  7.6442e-02,\n",
      "          2.0931e-01, -2.0928e+00, -4.9402e-01, -1.7071e+00,  1.6640e-01,\n",
      "         -2.3259e-01, -6.6822e-01, -4.3820e-01, -8.4754e-02,  3.6925e-01,\n",
      "          5.5811e-01, -1.1447e+00, -7.7392e-01,  4.5258e-01, -3.2317e-01,\n",
      "         -6.5090e-01, -9.2291e-01,  2.7288e-01, -1.7536e+00, -2.0878e-01,\n",
      "         -1.1804e+00, -8.6607e-01, -2.3122e-02,  3.0003e-01,  1.2987e+00,\n",
      "          9.7409e-01, -8.0116e-01, -5.4758e-01, -5.9833e-01,  5.0307e-01,\n",
      "          3.4384e-01,  3.1196e+00,  4.5077e-01,  3.5333e-01,  1.8129e+00,\n",
      "          7.8113e-01,  7.8424e-01, -4.3203e-01, -1.3500e+00,  3.6676e-02,\n",
      "         -5.4802e-01,  1.1607e+00, -1.2784e+00,  1.0464e+00,  1.2332e+00,\n",
      "         -1.4258e+00,  5.8740e-01,  2.3149e-01, -6.6865e-02,  1.4852e+00,\n",
      "         -1.7090e+00, -1.2342e+00, -2.5473e-01, -9.0363e-01, -1.8862e-01,\n",
      "          4.1461e-02,  2.0912e+00, -1.4523e+00,  7.5140e-01,  2.9661e-01,\n",
      "         -2.7879e+00, -1.0680e+00, -9.8168e-01,  5.5333e-01, -8.8708e-01,\n",
      "         -7.9474e-01,  1.0550e+00, -1.7151e+00, -3.1249e+00,  1.4372e+00,\n",
      "          3.4801e-01, -1.3329e+00,  2.0677e+00,  2.5948e+00,  9.9326e-01,\n",
      "          1.3003e+00,  2.9541e-02,  2.4207e+00, -2.9994e-02, -9.7866e-01,\n",
      "         -2.4816e+00, -3.8090e-02, -1.7521e-01, -2.5388e-01,  1.4238e+00,\n",
      "          1.9573e-01, -5.9175e-01, -1.4325e+00, -8.6289e-01,  1.6189e+00,\n",
      "         -9.6258e-01, -1.1801e+00,  3.4989e-01, -7.9307e-01, -3.5260e-01,\n",
      "         -1.8912e+00,  8.0860e-01,  5.6620e-01, -2.3868e-01,  5.4273e-01,\n",
      "          1.4113e-01, -1.6741e+00, -6.5484e-01, -1.8676e+00,  7.5314e-01,\n",
      "          1.0129e+00, -5.8723e-01,  1.6040e-01,  1.3160e+00, -6.3311e-02,\n",
      "         -7.6065e-01,  9.7548e-01,  6.7840e-01,  1.4658e-01,  1.4278e+00,\n",
      "          4.3254e-01,  4.2927e-01,  1.5952e+00],\n",
      "        [-5.5270e-01,  5.5553e-01,  4.7718e-01,  5.4210e-01,  1.1315e+00,\n",
      "         -2.1502e-01, -8.1620e-01, -3.0520e-01, -2.1429e+00,  3.8039e-01,\n",
      "          2.7931e-01,  1.2734e+00,  7.7913e-01,  1.9699e+00,  2.4460e+00,\n",
      "         -1.6256e+00,  2.2999e+00, -4.5769e-01,  4.2161e-01,  1.2170e+00,\n",
      "         -1.4232e+00,  2.3221e+00,  5.4398e-01,  1.2499e+00, -1.0609e+00,\n",
      "         -5.9845e-01, -5.7492e-01,  7.7008e-01, -1.2662e+00, -1.4338e-01,\n",
      "          1.5290e+00, -3.2089e-02, -7.3497e-02,  1.5746e+00,  1.2274e+00,\n",
      "          6.8637e-01,  5.4162e-01, -1.1143e+00,  8.4980e-01, -1.4521e+00,\n",
      "          4.1456e-01, -6.8059e-01, -1.0462e+00,  4.6277e-02,  1.2223e+00,\n",
      "         -1.4902e+00,  1.7758e-01, -1.5545e+00,  2.6895e-01,  1.9799e+00,\n",
      "         -1.2386e+00, -1.1513e+00,  1.1951e-01,  6.6344e-01,  6.3478e-01,\n",
      "          1.0357e+00, -1.7596e+00, -9.9174e-01, -6.1733e-01, -5.5725e-01,\n",
      "         -6.4404e-01,  6.8705e-01,  3.7418e-01, -5.1053e-01, -1.4708e+00,\n",
      "         -3.2651e-01,  3.1464e+00, -6.7754e-01, -2.0552e-01, -2.0420e+00,\n",
      "         -1.1018e+00, -6.6652e-01, -5.4549e-01, -2.4386e-01, -3.9950e-01,\n",
      "         -9.0182e-01, -4.3040e-01,  6.0974e-01, -2.2482e+00, -1.1253e+00,\n",
      "         -3.4183e+00,  1.3554e+00,  2.1862e+00,  7.4977e-01,  5.9968e-01,\n",
      "          8.8836e-03,  5.7063e-01, -1.2434e+00,  7.3613e-01,  1.3468e-02,\n",
      "         -1.8589e+00, -1.1972e+00,  5.7907e-01, -1.6081e+00,  1.6449e+00,\n",
      "          3.3949e-01,  9.7133e-01, -3.5073e-01,  4.3876e-01, -1.0666e+00,\n",
      "          2.4806e-01,  1.5329e+00,  4.2406e-01, -1.2473e+00, -1.8620e+00,\n",
      "          1.5210e+00,  5.8436e-01, -7.9666e-01,  5.1330e-01, -4.9506e-01,\n",
      "          1.5178e+00,  1.4278e+00, -1.2071e+00, -1.2567e+00, -5.7188e-01,\n",
      "          2.5086e-01,  1.2069e+00, -2.7465e+00, -1.2022e+00, -6.9082e-01,\n",
      "          1.4756e+00, -1.4274e+00, -7.3378e-01,  6.8718e-01,  1.7347e+00,\n",
      "         -9.8976e-02, -3.0256e-01,  1.7110e-01],\n",
      "        [ 1.2894e+00, -1.2394e+00,  5.3563e-01,  1.9649e+00,  3.4810e-01,\n",
      "         -3.2743e-01, -1.4461e+00, -1.1756e+00,  7.7769e-02, -1.1914e+00,\n",
      "          1.5469e+00,  1.3527e-01,  2.6827e+00,  2.8839e+00,  3.4856e+00,\n",
      "          6.1613e-01, -3.5702e-01, -5.4094e-01,  2.7611e-03,  1.4326e+00,\n",
      "          1.1515e+00, -2.0704e+00, -3.4002e-01, -4.8209e-01, -6.5753e-01,\n",
      "          1.2362e+00, -7.6434e-01, -3.0896e+00, -5.7004e-02,  9.6163e-02,\n",
      "         -9.6657e-01,  2.7752e-02,  8.6549e-01, -3.7903e-01,  3.2383e-01,\n",
      "         -3.3282e-01,  3.0497e-01, -2.3759e+00,  5.8186e-01, -1.9863e-01,\n",
      "          2.2701e+00, -2.9015e+00,  9.3031e-01,  4.2830e-01,  7.7281e-01,\n",
      "          1.9264e+00, -5.3181e-01,  9.0214e-01,  1.2243e+00,  5.8830e-01,\n",
      "          1.4089e-02, -3.6677e-01,  9.4272e-01,  1.1910e+00,  1.5712e+00,\n",
      "          8.1163e-01,  3.1166e+00,  4.2973e-01,  2.4352e-01, -1.7484e-01,\n",
      "          9.1033e-01, -1.3067e+00, -5.1400e-02, -5.8697e-02,  7.5668e-01,\n",
      "          1.6850e+00, -1.1553e+00, -1.0231e+00, -2.1399e-01, -1.5629e+00,\n",
      "          1.5722e+00,  6.2613e-01,  4.4122e-01,  1.1567e+00,  1.4163e+00,\n",
      "         -1.3305e+00, -5.4380e-01, -1.7814e+00,  1.2426e+00, -2.1907e+00,\n",
      "         -7.0324e-01,  4.4794e-01,  1.2671e+00, -2.0341e-01, -8.9511e-01,\n",
      "         -8.0482e-01,  1.2076e-01,  1.3371e+00, -2.9636e-01, -5.6416e-01,\n",
      "         -6.1985e-01, -8.9681e-01, -9.7216e-01,  8.3700e-01,  2.4225e+00,\n",
      "          4.2888e-01,  1.1614e-01, -1.1213e+00,  6.7111e-01, -4.5033e+00,\n",
      "          2.2797e+00, -1.4450e+00,  1.2343e+00,  6.8232e-01, -5.7221e-01,\n",
      "          1.7144e+00, -1.7760e+00, -7.1059e-01, -2.6387e+00, -6.9964e-01,\n",
      "          9.7207e-01, -1.1753e+00, -2.4737e+00,  1.1623e+00, -2.1409e+00,\n",
      "          1.7508e+00, -9.2760e-01, -1.4677e+00,  1.4234e-01, -1.3427e+00,\n",
      "         -5.6338e-01,  8.2082e-01, -5.1147e-01, -2.5663e+00, -1.1672e+00,\n",
      "         -2.4268e+00, -1.7103e+00,  2.5383e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Key Padding Mask Shape: torch.Size([1, 86])\n",
      "Key Padding Mask (Sample): tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False], device='cuda:0')\n",
      "Combined Shape after Transformer Layer 1: torch.Size([1, 86, 128])\n",
      "Combined Shape after Transformer Layer 2: torch.Size([1, 86, 128])\n",
      "Pooled Shape: torch.Size([1, 128])\n",
      "Output Shape: torch.Size([1])\n",
      "Key Padding Mask for First Sample: tensor(False, device='cuda:0')\n",
      "Output for First Sample: [0.03788588]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train_dimensions_check.py --epochs 1 \\\n",
    "    --train \"../../data/splitted_datasets/allele/beta/train.tsv\" \\\n",
    "        --val \"../../data/splitted_datasets/allele/beta/validation.tsv\" \\\n",
    "    --tcr_train_embeddings  \"../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\" \\\n",
    "        --epitope_train_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\" \\\n",
    "            --tcr_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\" \\\n",
    "                    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch 1/1 [Training]:   0%|                          | 0/280252 [00:00<?, ?it/s]TCR Sequence (Raw, Padded): [[ 3.2088238e+01  8.3878813e+00  4.0445685e+00 ... -1.9395858e-03\n",
      "  -2.7317656e-02 -4.1515868e-02]\n",
      " [ 2.7061586e+01  1.2071258e+01  3.0378304e+00 ... -4.2684130e-02\n",
      "  -7.1428187e-02 -6.2126148e-02]\n",
      " [-1.0033892e+00 -6.0639811e+00 -2.4794743e+01 ...  3.8143832e-04\n",
      "  -3.8493473e-02 -4.5021929e-02]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "Epitope Sequence (Raw, Padded): [[-27.9988       1.8165553    2.8864703  ...  -0.32495236   0.17416044\n",
      "    0.24314025]\n",
      " [ 10.119896   -22.424664    -1.629031   ...   0.1934768    0.15802856\n",
      "   -0.10534884]\n",
      " [  9.85574    -21.434538    -1.6456085  ...  -0.1876323    0.10715318\n",
      "   -0.21561605]\n",
      " ...\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]]\n",
      "TCR Input (Raw, Padded): tensor([[ 3.2088e+01,  8.3879e+00,  4.0446e+00,  ..., -1.9396e-03,\n",
      "         -2.7318e-02, -4.1516e-02],\n",
      "        [ 2.7062e+01,  1.2071e+01,  3.0378e+00,  ..., -4.2684e-02,\n",
      "         -7.1428e-02, -6.2126e-02],\n",
      "        [-1.0034e+00, -6.0640e+00, -2.4795e+01,  ...,  3.8144e-04,\n",
      "         -3.8493e-02, -4.5022e-02],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "Epitope Input (Raw, Padded): tensor([[-27.9988,   1.8166,   2.8865,  ...,  -0.3250,   0.1742,   0.2431],\n",
      "        [ 10.1199, -22.4247,  -1.6290,  ...,   0.1935,   0.1580,  -0.1053],\n",
      "        [  9.8557, -21.4345,  -1.6456,  ...,  -0.1876,   0.1072,  -0.2156],\n",
      "        ...,\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "       device='cuda:0')\n",
      "TCR Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Epitope Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Combined Shape: torch.Size([1, 86, 128])\n",
      "Combined Tensor (Sample): tensor([[ 1.6651e+00,  3.1519e-01, -5.1641e-01,  1.2708e+00,  5.9881e-01,\n",
      "         -1.8193e+00,  4.9681e-01,  1.0992e+00,  4.3126e-01,  7.8765e-01,\n",
      "          5.0830e-01, -1.3421e+00,  4.5715e-01,  9.2925e-01,  1.4223e-01,\n",
      "         -1.6759e+00,  1.9528e-01,  1.4473e+00,  2.2247e+00, -1.0178e+00,\n",
      "          1.4396e+00, -2.7801e-02, -3.1713e-01,  7.5290e-02,  1.1171e-01,\n",
      "         -3.9599e-01, -3.0051e-01, -1.7428e+00,  4.8555e-02,  1.6987e+00,\n",
      "          1.3264e+00,  1.3759e-02,  7.1027e-01, -1.5905e+00, -1.0022e+00,\n",
      "         -1.2011e+00, -5.3030e-01,  4.5440e-01,  5.7284e-01,  7.1711e-01,\n",
      "         -1.2933e-01,  6.1121e-02, -4.4690e-01, -4.6260e-01, -2.9221e-01,\n",
      "         -1.2043e+00,  1.1028e+00, -6.8433e-01, -2.7377e-01,  1.0731e+00,\n",
      "         -2.8250e-01, -3.2919e-01,  1.1177e+00, -1.8337e+00,  3.5839e-01,\n",
      "          1.7991e+00,  4.4747e-01,  1.5379e-01, -1.7400e+00, -1.7588e-02,\n",
      "          3.2625e+00,  6.1908e-02,  1.1424e+00, -2.0027e-01,  5.6485e-01,\n",
      "          6.6962e-01,  3.4146e-02, -3.4802e-01,  2.3158e-02,  1.5691e+00,\n",
      "          1.9218e-01, -1.8236e+00,  9.3577e-01,  7.9170e-01,  2.2333e-01,\n",
      "          8.2449e-01,  5.5304e-02, -6.9514e-02, -3.9206e-01, -1.1923e+00,\n",
      "         -2.4561e-01, -1.7571e+00, -7.9230e-01,  1.0441e+00,  7.4458e-01,\n",
      "          6.7222e-02,  7.0555e-01,  1.1197e+00,  9.2816e-01,  8.5773e-01,\n",
      "          3.7991e-01, -2.6289e+00,  5.5074e-02, -1.3252e+00, -3.8815e-01,\n",
      "         -7.6989e-01, -7.9810e-01, -1.7308e+00, -4.2610e-01,  1.9515e+00,\n",
      "          2.2623e-01, -4.1114e-01,  3.1181e-01, -1.7104e-02,  6.5266e-01,\n",
      "          7.6211e-01, -2.7191e-01, -7.2363e-01,  9.3263e-01, -2.3008e-01,\n",
      "          4.3181e-01,  1.0321e+00,  2.8875e-01, -1.6347e+00,  2.2797e+00,\n",
      "         -1.3207e+00, -8.4404e-01,  1.2406e+00, -6.7938e-01,  1.3619e+00,\n",
      "          4.4723e-01,  2.0947e-02,  1.1318e+00, -3.5403e-01,  1.7697e+00,\n",
      "          1.5621e+00,  1.1406e+00,  4.3498e-01],\n",
      "        [-4.4022e-02, -1.5534e+00,  5.1699e-01, -9.9125e-01, -1.9193e+00,\n",
      "         -3.6466e-01, -9.7377e-01, -4.7950e-01, -8.9860e-01,  3.7433e-01,\n",
      "          1.5684e+00,  3.8284e-01,  3.4951e+00,  7.5083e-01, -1.3426e+00,\n",
      "         -1.6940e+00,  2.5119e-02, -4.0623e-01,  8.5144e-02, -7.8194e-01,\n",
      "         -1.4462e+00,  2.0256e+00,  2.0918e+00, -7.2459e-02,  1.4121e-01,\n",
      "          4.4870e-01,  5.0739e-02,  2.0176e-01,  8.6412e-01, -5.5809e-01,\n",
      "          1.0311e+00,  1.7297e+00,  6.0015e-01, -7.6226e-01,  1.0234e+00,\n",
      "          1.3271e+00,  7.0711e-01, -2.1885e+00,  1.2169e+00, -1.7851e+00,\n",
      "         -1.6491e+00, -1.9042e-01, -5.4265e-01,  1.2528e+00,  6.7859e-01,\n",
      "         -5.8991e-01,  1.2779e+00,  5.4526e-01,  1.3124e+00, -8.5060e-01,\n",
      "          2.5125e-01, -5.5715e-01,  1.4971e-01, -1.2220e+00, -1.9241e+00,\n",
      "          9.5657e-02,  1.5971e-01, -2.8171e-01, -2.1017e+00,  1.0335e-01,\n",
      "         -1.7580e+00, -7.1572e-02, -1.5547e+00, -1.4189e+00, -2.6831e+00,\n",
      "          3.8982e-01,  5.5275e-02, -2.9280e-01,  1.5953e+00, -1.6338e+00,\n",
      "         -2.3869e-01,  9.4083e-01,  1.8523e+00,  1.0185e+00, -7.9337e-01,\n",
      "          4.1887e-01, -5.5713e-02, -1.4080e+00,  6.3985e-01,  3.8206e-01,\n",
      "          1.2461e-01, -8.9296e-01,  1.2416e+00,  8.8052e-01, -1.6397e+00,\n",
      "         -1.3517e-02, -2.3435e+00, -4.1813e-01, -5.4713e-01, -9.1886e-01,\n",
      "         -3.5943e-01,  3.0523e-02,  3.6719e-01, -1.0022e+00,  7.0152e-01,\n",
      "         -1.8787e+00, -4.7759e-01,  1.5652e-01, -4.7917e-01, -5.8696e-01,\n",
      "          5.7479e-01, -8.4065e-01, -2.4956e-01,  1.3789e+00, -9.2247e-01,\n",
      "          1.2884e-01, -5.2827e-01, -1.1464e+00,  4.8698e-01, -1.2359e+00,\n",
      "         -1.3556e+00,  1.5891e+00, -2.5291e-01,  4.4999e-01,  9.3590e-01,\n",
      "         -1.2816e+00,  3.9300e-01, -9.5742e-01, -3.4151e-01,  1.1068e+00,\n",
      "          7.2997e-01,  1.4665e-01,  1.1904e+00,  1.0841e+00,  1.7402e+00,\n",
      "         -1.7313e-01, -3.6987e-01,  6.0619e-01],\n",
      "        [-5.4550e-01,  5.7714e-01,  5.9920e-01, -4.5526e-01, -9.1095e-01,\n",
      "         -1.5286e+00, -1.5472e-01, -8.5043e-02,  1.5911e+00, -1.7619e+00,\n",
      "         -8.7858e-01, -2.0636e-01,  5.9463e-01,  1.3832e+00,  3.6496e-01,\n",
      "          6.3127e-01, -1.8188e+00, -1.4755e+00, -9.6315e-01, -1.7832e+00,\n",
      "          1.0149e+00, -1.0742e+00, -5.9184e-01,  5.4860e-01, -3.6116e-01,\n",
      "         -1.1401e+00, -3.1261e-01,  1.0947e+00,  1.6248e+00,  4.5356e-01,\n",
      "         -5.0324e-01,  3.4208e-01, -9.6087e-01,  3.6242e-01, -3.8098e-01,\n",
      "          2.5049e-01, -1.3698e+00, -2.1231e-01, -3.3244e-01, -1.3139e+00,\n",
      "         -3.8978e-01, -6.9013e-01,  2.5003e+00,  1.1364e+00,  2.8799e-01,\n",
      "          4.7876e-01,  1.0477e-01, -1.2526e+00, -3.7843e-01,  1.0001e+00,\n",
      "          7.5511e-01,  1.1662e+00, -1.4607e+00, -9.7451e-01,  9.3385e-02,\n",
      "          1.0096e+00, -1.7213e+00, -7.2884e-01,  3.1753e-01, -1.3481e+00,\n",
      "          2.1238e-01,  2.4498e+00, -1.6666e+00, -6.5270e-01, -4.5108e-01,\n",
      "         -4.5556e-01, -1.8035e+00, -4.3590e-01, -1.1415e+00,  4.0435e-01,\n",
      "         -1.3987e+00,  6.8671e-01, -8.5000e-01,  1.9951e+00,  8.9304e-01,\n",
      "          2.8143e-02, -2.4268e+00,  6.1134e-01, -3.6246e-01, -1.6933e+00,\n",
      "          1.0457e+00,  8.7382e-01, -7.8010e-01,  6.1091e-01,  1.2920e-01,\n",
      "         -7.3756e-01, -4.1403e-01,  2.7043e-01, -1.7711e+00, -2.3459e-01,\n",
      "         -1.1945e+00,  2.3495e-01, -6.4745e-01,  1.8547e+00, -7.2804e-01,\n",
      "          1.0179e-01, -6.2472e-01, -1.2434e+00, -5.5042e-01, -9.9573e-01,\n",
      "          9.5710e-01,  2.9886e-01, -1.1508e+00, -7.8243e-01, -5.7552e-01,\n",
      "         -5.1105e-02,  2.3945e+00,  1.2615e+00,  1.9654e-01,  1.9735e-01,\n",
      "          1.4563e-04, -7.1279e-01,  5.6976e-02, -1.5909e-01,  2.9923e-01,\n",
      "          3.4798e-01,  8.7212e-01,  5.8453e-01,  9.6539e-01, -1.0432e+00,\n",
      "         -1.3439e-01,  2.7764e+00, -1.7937e+00,  8.6734e-01,  2.0886e-01,\n",
      "          9.2234e-01, -1.5538e+00, -1.6168e+00],\n",
      "        [ 2.6801e-01,  1.2596e-01,  8.1882e-03, -1.0525e+00, -8.9817e-01,\n",
      "          4.3085e-01, -1.7599e+00, -6.1883e-02,  4.0166e-01, -2.9981e-01,\n",
      "          6.9647e-01,  9.2753e-01, -2.5075e-01, -3.2230e-01, -1.4944e+00,\n",
      "         -2.7437e-01,  1.5773e+00,  7.6088e-01,  6.2904e-01, -6.4722e-01,\n",
      "          2.3988e-01, -7.4349e-01,  2.4189e-01, -4.7589e-01, -3.7418e-01,\n",
      "          3.3371e-01,  4.3021e-01,  1.3895e-01,  4.9879e-01, -7.8988e-01,\n",
      "          1.5061e+00,  1.6542e+00, -5.9684e-01, -1.8276e-02, -9.7678e-01,\n",
      "          3.2029e-01,  3.9510e-01,  1.7118e-01,  7.4911e-02, -1.1671e+00,\n",
      "         -1.4938e+00,  8.8202e-01, -2.3208e+00,  1.3612e+00,  4.0773e-01,\n",
      "          4.5456e-01, -1.0633e+00, -3.0257e-02, -9.2117e-01,  8.7801e-01,\n",
      "          4.9608e-01,  1.1067e+00, -1.3961e+00, -4.3301e-01, -7.4107e-01,\n",
      "          1.7898e+00,  2.4318e-01, -2.5826e-01, -1.7562e-01, -1.8986e+00,\n",
      "          1.6881e-01, -1.9051e+00,  7.0612e-01,  1.0116e+00,  2.7027e-01,\n",
      "          1.3671e+00, -1.5854e+00,  5.2865e-01,  8.2084e-01,  6.3796e-01,\n",
      "          6.5304e-01,  1.9552e+00, -2.1159e-01,  1.2840e+00, -9.0470e-01,\n",
      "          2.8811e-01,  2.3777e-01,  1.7394e+00,  3.3359e-01, -5.6659e-01,\n",
      "         -5.7141e-01, -1.9597e+00, -1.6244e+00,  6.4866e-02,  5.7195e-02,\n",
      "          2.1949e+00,  1.4644e+00,  7.5794e-01,  5.0767e-01, -4.1830e-01,\n",
      "          1.4922e-01, -1.2528e+00,  7.0284e-01,  1.6145e+00, -1.1513e+00,\n",
      "          5.0112e-01, -2.3297e+00, -8.6088e-01,  1.3679e+00, -1.9557e+00,\n",
      "          3.3164e-01,  2.5743e+00, -1.8586e+00,  7.0623e-01,  9.9937e-01,\n",
      "         -1.3469e+00,  1.1271e+00, -4.6912e-01, -9.2934e-02, -1.0473e+00,\n",
      "          5.4271e-01,  1.5489e+00, -9.7742e-02, -8.3899e-01,  9.3613e-01,\n",
      "         -2.9248e-01,  1.9081e+00, -8.9558e-01, -9.4508e-01,  1.3293e+00,\n",
      "         -5.7956e-01, -1.2094e+00, -2.1822e-02, -8.6827e-01,  6.4437e-01,\n",
      "         -8.7886e-01,  2.6477e-01,  3.1083e-01],\n",
      "        [-1.1443e+00, -1.1308e+00, -1.3673e-01,  3.4710e-01,  1.3384e-01,\n",
      "          2.2875e+00, -1.0398e+00,  7.4463e-01,  3.2325e-01,  5.8823e-01,\n",
      "         -3.0843e-01,  4.2456e-01, -1.2049e+00,  1.9731e+00, -9.8956e-01,\n",
      "         -8.9609e-01,  2.4853e-01,  7.5204e-01, -7.2680e-01,  1.1979e+00,\n",
      "          3.8492e-01,  6.0784e-01,  1.3214e+00,  1.3172e+00,  2.8405e-01,\n",
      "          1.8442e+00,  2.7216e-01, -3.2628e-01,  3.5100e-01,  7.9728e-01,\n",
      "         -5.0342e-01,  1.6049e+00,  2.1117e-01, -2.4416e+00, -1.1715e+00,\n",
      "         -1.8524e+00,  4.8080e-01, -3.2184e-01,  4.6845e-01, -2.6563e-01,\n",
      "         -2.7824e-01, -9.3405e-01, -1.2093e+00, -1.3866e+00, -9.2194e-01,\n",
      "         -1.0977e+00,  6.0418e-01,  2.0373e-01,  3.2282e-01,  1.5404e+00,\n",
      "          7.1582e-02, -1.6475e-01,  2.7681e-01,  1.0072e+00,  5.6252e-01,\n",
      "          3.2509e-01,  1.6594e+00,  2.7156e+00, -1.2533e+00, -2.3873e-01,\n",
      "         -6.5214e-01, -8.6669e-01,  6.0452e-02,  1.2318e-01, -6.1793e-01,\n",
      "          9.8268e-02,  9.1945e-01,  1.1660e+00, -1.0576e+00, -2.0116e+00,\n",
      "          1.8077e-01,  1.6822e-01,  5.8622e-01,  1.1878e+00,  1.4294e+00,\n",
      "         -7.2512e-01,  1.5165e+00,  3.2765e-01,  5.2082e-01,  1.2104e+00,\n",
      "          1.0251e+00,  1.6050e-01,  6.3289e-02,  1.1423e+00, -3.0795e-01,\n",
      "         -1.8805e-01, -1.1104e+00, -9.7910e-01, -9.9860e-01,  1.2749e+00,\n",
      "          1.0681e+00,  7.2335e-01,  1.4992e+00, -6.1045e-02, -9.3977e-03,\n",
      "         -1.1528e+00,  9.6965e-01,  1.6055e+00,  5.3902e-01,  4.2327e-01,\n",
      "          3.5096e-01,  4.9008e-01,  1.0454e+00,  1.4741e-01,  2.8898e-01,\n",
      "          9.2099e-01,  7.4434e-02, -4.4445e-01,  2.3153e+00,  1.3645e+00,\n",
      "         -6.6630e-01,  1.8952e+00, -1.8182e-01, -1.9820e+00,  3.3019e-01,\n",
      "         -8.2900e-01, -8.7692e-01,  1.4804e+00, -2.3116e+00,  8.6237e-02,\n",
      "         -1.4364e+00, -2.1556e-01, -1.9080e+00,  4.0958e-01, -9.3522e-01,\n",
      "          1.0663e+00, -1.0397e+00, -8.8799e-02]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Key Padding Mask Shape: torch.Size([1, 86])\n",
      "Key Padding Mask (Sample): tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False], device='cuda:0')\n",
      "Combined Shape after Transformer Layer 1: torch.Size([1, 86, 128])\n",
      "Combined Shape after Transformer Layer 2: torch.Size([1, 86, 128])\n",
      "Pooled Shape: torch.Size([1, 128])\n",
      "Output Shape: torch.Size([1])\n",
      "Key Padding Mask for First Sample: tensor(False, device='cuda:0')\n",
      "Output for First Sample: [-0.17225224]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train_dimensions_check.py --epochs 1 \\\n",
    "    --train \"../../data/splitted_datasets/allele/beta/train.tsv\" \\\n",
    "        --val \"../../data/splitted_datasets/allele/beta/validation.tsv\" \\\n",
    "    --tcr_train_embeddings  \"../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\" \\\n",
    "        --epitope_train_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\" \\\n",
    "            --tcr_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\" \\\n",
    "                    --batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking dimensions after correction of masking problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch 1/1 [Training]:   0%|                          | 0/280252 [00:00<?, ?it/s]TCR Sequence (Raw, Padded): [[ 28.165472     7.8815627    2.4069612  ...  -0.1388478   -0.0934343\n",
      "    0.19807   ]\n",
      " [ 30.308855    11.450492     5.588977   ...   0.04413191   0.05415228\n",
      "    0.28447914]\n",
      " [  0.5084261   -5.1664767  -24.430494   ...  -0.09976787  -0.03159365\n",
      "   -0.04142576]\n",
      " ...\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]]\n",
      "Epitope Sequence (Raw, Padded): [[-31.500793    -2.0158465   -0.5782006  ...   0.21245402   0.3302949\n",
      "    0.1735704 ]\n",
      " [ -7.6577835    1.4497155   -4.834811   ...   0.07514882   0.14001764\n",
      "    0.24475971]\n",
      " [  4.339406     0.2893368    0.9521746  ...  -0.0702507    0.05455149\n",
      "    0.3681363 ]\n",
      " ...\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]]\n",
      "TCR Input (Raw, Padded): tensor([[ 28.1655,   7.8816,   2.4070,  ...,  -0.1388,  -0.0934,   0.1981],\n",
      "        [ 30.3089,  11.4505,   5.5890,  ...,   0.0441,   0.0542,   0.2845],\n",
      "        [  0.5084,  -5.1665, -24.4305,  ...,  -0.0998,  -0.0316,  -0.0414],\n",
      "        ...,\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "       device='cuda:0')\n",
      "Epitope Input (Raw, Padded): tensor([[-31.5008,  -2.0158,  -0.5782,  ...,   0.2125,   0.3303,   0.1736],\n",
      "        [ -7.6578,   1.4497,  -4.8348,  ...,   0.0751,   0.1400,   0.2448],\n",
      "        [  4.3394,   0.2893,   0.9522,  ...,  -0.0703,   0.0546,   0.3681],\n",
      "        ...,\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "       device='cuda:0')\n",
      "TCR Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Epitope Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Combined Shape: torch.Size([1, 86, 128])\n",
      "Combined Tensor (Sample): tensor([[ 9.2627e-02, -2.3570e-01,  1.2152e-01, -3.4355e-01, -3.6790e-01,\n",
      "          3.6200e-01, -1.0285e+00, -1.4162e+00, -8.9550e-03,  9.3967e-01,\n",
      "          1.5864e+00, -1.9364e+00, -6.9471e-01,  2.2567e-01, -1.3895e-01,\n",
      "          1.4144e+00,  7.5827e-01, -1.3494e+00,  2.7400e-01,  3.3454e-01,\n",
      "         -1.2001e+00,  6.7924e-01, -3.4893e-01, -6.2702e-01,  3.2093e-01,\n",
      "          5.5934e-01,  3.4450e-01, -2.0032e+00, -4.6692e-01,  4.5970e-01,\n",
      "         -5.1949e-02,  2.1608e+00, -3.7656e-01,  5.9992e-01,  7.1406e-01,\n",
      "         -3.9348e-01,  7.4745e-01, -1.5677e+00,  3.6059e-01, -6.7113e-01,\n",
      "          4.0701e-01,  1.4492e+00, -1.2653e+00, -4.8799e-01, -1.5313e+00,\n",
      "         -3.2736e-01, -8.5436e-01,  7.2673e-01, -5.4788e-01, -9.6580e-01,\n",
      "          9.5656e-01,  4.9589e-01, -5.6730e-01,  5.7255e-01,  4.2558e-02,\n",
      "          1.3071e+00,  4.0212e-01, -6.3761e-01,  6.2986e-01,  8.0552e-02,\n",
      "          1.3949e+00, -9.0852e-01,  2.6098e+00,  1.1734e+00, -8.0506e-01,\n",
      "         -3.1584e-01,  9.9888e-01, -8.3128e-01,  9.0891e-01, -1.1874e+00,\n",
      "          1.4634e+00, -6.4273e-01,  1.4068e+00,  7.9055e-03,  7.1088e-01,\n",
      "         -1.6627e-01,  2.7684e-01, -7.1974e-01,  1.6038e-01, -1.6791e+00,\n",
      "          3.3501e-01, -9.0135e-01, -2.4625e-01, -4.6451e-01, -7.9922e-01,\n",
      "         -2.9728e-01,  2.8281e-01, -4.7659e-01,  2.8671e-01,  1.5869e-01,\n",
      "         -1.9184e+00, -1.2352e+00,  1.1881e+00,  5.0363e-02,  1.7060e-01,\n",
      "          7.0043e-01,  1.1040e+00, -1.1911e+00, -1.3897e+00,  9.8256e-01,\n",
      "         -1.1002e-01, -5.2612e-01,  5.9786e-01,  7.9662e-01,  7.9610e-01,\n",
      "          2.5256e-01,  3.8910e-01, -2.7082e-01,  2.1275e+00,  1.3622e+00,\n",
      "         -6.3097e-01,  1.0862e+00,  3.6065e-01,  7.8653e-01,  1.1710e+00,\n",
      "         -6.3039e-01,  3.9803e-01,  1.1635e+00,  4.0677e-01, -3.4288e-01,\n",
      "         -1.0242e+00, -4.9628e-01,  5.6864e-01,  1.2777e+00, -8.3285e-01,\n",
      "          9.7211e-02, -1.0245e-01,  4.9735e-01],\n",
      "        [-2.0234e+00,  1.2568e-01, -1.0724e+00, -3.2990e-02,  4.6956e-02,\n",
      "         -2.9757e-01, -1.1407e+00,  1.0815e+00,  3.0897e-02, -2.2674e+00,\n",
      "         -4.6828e-01, -1.4392e-01,  5.7799e-01,  3.3694e-03, -2.2155e-01,\n",
      "          3.1981e-01,  8.2182e-01, -1.1246e+00, -9.1254e-01, -9.6694e-02,\n",
      "          5.9356e-01, -7.3494e-01,  9.7497e-01, -1.0792e+00,  1.2215e+00,\n",
      "         -8.6802e-01, -8.2796e-02,  4.1970e-01, -4.0452e-01,  2.8964e-02,\n",
      "          2.3256e+00, -1.2736e+00, -2.6332e-01, -6.5455e-01,  9.5449e-01,\n",
      "         -2.1315e+00, -1.3941e+00,  9.7268e-01,  7.4642e-01, -9.9840e-01,\n",
      "         -2.1093e-01,  5.5526e-01,  5.9563e-01, -1.3731e+00,  1.2343e-01,\n",
      "          2.0624e+00,  9.4949e-01, -7.7072e-01,  2.6262e-01, -1.6509e-01,\n",
      "         -9.5321e-01, -1.0282e+00, -6.8367e-01,  8.6661e-01,  4.5794e-01,\n",
      "         -7.6823e-02, -4.7748e-01, -9.7604e-01,  1.4923e-01, -2.1804e-01,\n",
      "         -6.1874e-01, -5.6672e-01,  1.2652e+00,  1.3150e+00, -5.0607e-01,\n",
      "         -2.5635e-01,  6.5040e-01, -8.5652e-01,  9.7231e-01,  4.9373e-01,\n",
      "         -1.0690e+00, -2.0883e-01,  1.6889e-01,  1.8776e+00, -1.3210e+00,\n",
      "          1.3628e+00,  1.5469e+00, -2.1484e-01, -5.9628e-02, -4.1139e-01,\n",
      "         -3.7809e-01,  1.0818e+00,  1.1971e+00, -1.9699e+00, -1.4020e+00,\n",
      "         -8.6497e-02, -5.1749e-01,  1.0415e+00, -7.8251e-01, -1.0467e+00,\n",
      "         -2.2336e-01,  1.0922e-01,  1.0761e+00,  7.9508e-01, -5.3327e-01,\n",
      "         -1.6030e+00, -6.7713e-01, -2.7090e+00,  7.6800e-02, -1.0210e+00,\n",
      "         -1.3369e-01, -7.5641e-01,  4.5884e-01, -6.4233e-01, -5.4498e-01,\n",
      "         -4.8355e-01, -1.3332e+00, -6.0815e-01, -1.6829e+00, -1.4491e+00,\n",
      "          2.2609e-01,  1.1217e-02, -6.5895e-01,  1.3799e+00, -6.5083e-01,\n",
      "         -2.1236e+00,  3.1752e-01,  1.1812e+00,  1.3275e+00, -8.8038e-01,\n",
      "         -9.8627e-01, -2.2404e-01, -1.1940e+00,  4.9333e-01,  7.7523e-01,\n",
      "          3.3789e-01,  5.1130e-01, -1.5547e+00],\n",
      "        [-5.5685e-01, -1.0238e+00,  9.7534e-01,  2.3717e-01, -5.0281e-01,\n",
      "         -1.3174e+00, -6.2959e-01,  9.3636e-02,  6.0156e-01,  7.6529e-01,\n",
      "         -1.3656e+00,  1.8428e+00, -1.7926e+00, -2.4734e-01,  9.8489e-01,\n",
      "         -2.1705e-02, -2.6260e-01,  1.3124e+00, -9.3897e-01, -3.2440e-01,\n",
      "         -1.0665e+00, -1.2082e-01,  9.5295e-01,  7.0327e-01,  4.5948e-01,\n",
      "         -7.2289e-01, -5.1305e-01, -7.3856e-03,  1.3495e+00,  1.1445e+00,\n",
      "         -1.0041e-01,  5.1048e-01,  1.4645e-01,  9.7669e-03,  3.4578e-01,\n",
      "         -2.5167e-01,  3.8356e-01,  1.7782e+00, -7.4114e-01, -6.9958e-01,\n",
      "         -7.8686e-01, -5.1917e-01,  8.1878e-01,  7.1439e-01, -6.4937e-01,\n",
      "          1.3712e+00, -1.6026e+00,  1.8928e+00,  6.7496e-01, -7.6575e-01,\n",
      "          4.0285e-02, -1.1059e-01, -2.7694e-01,  8.0728e-01, -1.1418e+00,\n",
      "         -5.9126e-01, -6.3562e-01,  6.9153e-01,  1.3266e+00, -1.1577e+00,\n",
      "         -5.2921e-01, -1.6597e+00,  2.2200e+00,  5.2900e-01,  1.8143e+00,\n",
      "          9.5007e-02, -2.8447e-01,  1.2082e+00,  4.5214e-01, -8.8921e-01,\n",
      "          1.0660e+00,  1.7739e+00, -1.2490e-01,  7.6176e-01,  6.1739e-01,\n",
      "          2.6336e+00, -1.7029e+00,  6.6720e-01,  5.8914e-01,  2.6145e+00,\n",
      "          6.6556e-01, -1.5949e+00, -1.7697e+00, -1.1604e-01, -1.8012e+00,\n",
      "          1.5342e-01, -2.6103e-01, -4.1719e-02, -1.1796e-01,  1.4853e+00,\n",
      "         -1.0427e+00, -1.1910e+00, -8.9751e-01, -9.9490e-01,  1.7723e+00,\n",
      "         -3.4854e-01,  1.1249e+00,  2.3287e+00, -4.3251e-01,  4.9382e-02,\n",
      "          1.0090e-01,  3.0417e-01, -1.0246e+00, -7.3625e-01, -1.6068e+00,\n",
      "         -1.8888e+00, -6.6805e-01,  8.0795e-01, -1.4816e+00,  9.9377e-01,\n",
      "          1.1543e+00,  1.0131e+00,  1.3901e-01,  6.5387e-01, -4.1337e-01,\n",
      "          6.6025e-01,  3.1277e-01,  1.0701e+00,  1.6773e+00,  8.0689e-01,\n",
      "         -4.3170e-01,  2.8570e+00,  1.3348e+00, -1.1888e+00,  6.3886e-01,\n",
      "          1.1146e+00,  4.8957e-01, -1.1783e+00],\n",
      "        [ 6.6016e-01, -1.0278e+00, -7.8943e-02,  4.5909e-01, -1.0139e+00,\n",
      "         -1.3259e+00, -2.6025e-01,  1.9053e+00, -5.2676e-01, -2.0020e-01,\n",
      "          1.0135e-01, -5.8648e-01, -4.2934e-01, -3.3703e-01,  5.9681e-01,\n",
      "          9.2100e-01,  1.2668e+00, -3.2761e-01,  4.2807e-01,  1.0500e+00,\n",
      "         -1.1178e+00, -2.0268e+00, -5.1378e-01,  1.0395e+00,  1.7125e-01,\n",
      "          2.6500e-01, -5.8632e-01,  5.0113e-01,  2.5368e-01,  1.3417e+00,\n",
      "          5.9872e-01,  5.1781e-01,  2.1171e-01,  2.4634e-01, -5.8459e-01,\n",
      "         -1.7110e+00,  1.3582e+00, -1.4958e-01, -9.5039e-01, -2.6157e-01,\n",
      "         -7.0906e-01, -7.1518e-01, -9.4293e-01,  1.1986e+00, -1.9370e-01,\n",
      "          1.2720e+00, -1.1672e-02, -6.7933e-02,  5.5270e-01,  1.6437e+00,\n",
      "         -3.1716e-01,  1.3325e+00, -5.8791e-01,  8.8432e-01, -3.6739e-01,\n",
      "          9.8410e-01,  1.2091e+00,  8.9837e-01, -7.9515e-01, -7.7882e-02,\n",
      "          2.8070e-01, -4.4731e-01, -1.0946e+00, -5.0629e-04, -1.6120e+00,\n",
      "          1.4785e+00, -7.6389e-01,  4.2638e-01,  2.6996e-01,  6.6588e-02,\n",
      "         -4.6393e-01, -3.8817e-01, -1.1559e+00, -1.3922e+00,  7.5541e-01,\n",
      "          1.1760e-01, -8.7558e-01, -1.2263e+00, -1.8955e+00, -4.8985e-01,\n",
      "          1.3015e+00, -7.0287e-01, -9.8857e-01,  6.2779e-01,  6.4323e-01,\n",
      "          3.5779e-01,  3.9771e-01,  1.4342e+00, -2.5695e-01,  7.4759e-01,\n",
      "          8.2171e-01, -4.5531e-01, -4.4253e-02,  2.0620e+00, -2.1785e-01,\n",
      "         -5.4337e-02,  6.5596e-01,  1.1703e-01, -7.8441e-01, -4.1184e-01,\n",
      "          3.0279e-01, -3.9615e-02,  1.3329e+00, -1.9175e+00,  1.1905e+00,\n",
      "         -1.0672e+00, -4.5989e-01, -2.8189e-01,  1.5041e-01,  5.9311e-01,\n",
      "         -1.0248e+00, -1.1845e-01, -1.2540e-01,  4.0750e-01,  7.5974e-01,\n",
      "          1.3458e+00,  1.3641e+00,  5.5875e-01, -7.5929e-01, -1.0205e+00,\n",
      "         -1.2735e+00, -2.2482e+00, -8.3134e-01, -6.5842e-02, -1.0809e+00,\n",
      "          7.5757e-01, -2.8371e+00, -1.4589e+00],\n",
      "        [ 1.4541e+00, -1.0191e+00, -1.2824e-01,  2.3743e-01,  1.0302e+00,\n",
      "         -7.4143e-01, -6.2336e-01,  4.7673e-01, -6.4371e-01, -9.5227e-01,\n",
      "          1.7256e+00,  2.2508e-01, -8.8378e-01, -2.7870e-01, -1.8402e+00,\n",
      "          1.0140e+00,  2.8959e-01,  1.4819e+00,  1.4246e-01, -1.7548e+00,\n",
      "         -9.9219e-01,  1.1797e+00, -2.2091e-01, -2.1919e+00,  7.0287e-01,\n",
      "         -3.7594e-01, -3.6979e-02,  6.8497e-01,  4.0457e-01,  2.4598e+00,\n",
      "         -7.8250e-02, -2.6556e-01, -1.0900e+00, -2.7078e-01,  2.7631e-01,\n",
      "         -3.2112e-01, -6.0004e-01,  1.2749e+00,  2.7850e-01, -1.9810e+00,\n",
      "          8.6762e-02,  7.5250e-01,  9.3745e-02,  1.0582e-01, -1.0515e+00,\n",
      "         -2.6384e-01,  3.2435e-01,  9.9821e-01, -1.6503e+00,  1.0383e+00,\n",
      "          3.8950e-01,  8.8797e-01,  9.8766e-01, -1.0372e+00, -2.0604e+00,\n",
      "         -4.1395e-01, -1.6485e+00,  1.1420e+00, -2.1932e+00, -1.1017e+00,\n",
      "         -1.0130e+00, -1.5214e-01, -6.8624e-01,  3.8851e-01, -9.8032e-01,\n",
      "          1.6819e-01, -1.6571e+00, -6.4424e-01,  9.1672e-02,  1.6608e+00,\n",
      "          2.5259e+00,  5.7490e-02, -5.0385e-01, -1.3561e+00,  9.0804e-01,\n",
      "          7.6398e-01, -3.1918e-01,  4.1188e-01,  2.4643e+00,  8.5186e-01,\n",
      "         -3.6617e-01, -7.0286e-01,  2.5056e-01, -1.0630e+00, -1.0455e+00,\n",
      "         -3.7488e-01, -5.8787e-01,  2.3171e-01,  1.1008e+00, -1.5996e+00,\n",
      "          1.6824e+00, -1.3856e+00,  1.9164e+00, -2.0427e-02,  1.6883e+00,\n",
      "          5.5463e-01,  5.2939e-01, -9.2960e-02, -4.5581e-01, -1.7098e-01,\n",
      "          9.8027e-02,  6.1115e-02,  1.5533e-01,  4.2117e-01,  1.9504e-01,\n",
      "          3.9924e-01,  1.7858e-01,  1.1290e+00, -2.3582e-01,  8.1401e-01,\n",
      "         -7.3629e-01,  4.0931e-01, -1.3810e+00, -1.7981e+00,  1.2960e+00,\n",
      "         -2.5559e-01, -5.3240e-01, -1.1189e+00,  4.0270e-01, -8.7642e-03,\n",
      "         -6.9539e-01,  5.0628e-01,  2.6871e-01, -3.9741e-01,  7.8500e-01,\n",
      "          1.1651e+00,  3.9280e-01,  9.2557e-02]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Key Padding Mask Shape: torch.Size([1, 86])\n",
      "Key Padding Mask (Sample): tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "Combined Shape after Transformer Layer 1: torch.Size([1, 86, 128])\n",
      "Combined Shape after Transformer Layer 2: torch.Size([1, 86, 128])\n",
      "Pooled Shape: torch.Size([1, 128])\n",
      "Output Shape: torch.Size([1])\n",
      "Key Padding Mask for First Sample: tensor(False, device='cuda:0')\n",
      "Output for First Sample: [0.09024063]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train_dimensions_check.py --epochs 1 \\\n",
    "    --train \"../../data/splitted_datasets/allele/beta/train.tsv\" \\\n",
    "        --val \"../../data/splitted_datasets/allele/beta/validation.tsv\" \\\n",
    "    --tcr_train_embeddings  \"../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\" \\\n",
    "        --epitope_train_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\" \\\n",
    "            --tcr_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\" \\\n",
    "                    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Learning rate: 0.001\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch 1/1 [Training]:   0%|                          | 0/280252 [00:00<?, ?it/s]TCR Sequence (Raw, Padded): [[ 3.10265198e+01  9.99439430e+00  2.25559413e-01 ... -8.73976424e-02\n",
      "  -1.52164459e-01  8.24605227e-02]\n",
      " [ 2.77175541e+01  8.80794239e+00 -1.71738410e+00 ...  4.27829567e-03\n",
      "  -1.36598378e-01  1.18401214e-01]\n",
      " [-1.25605583e+00 -6.35367632e+00 -2.53029232e+01 ... -3.43070575e-03\n",
      "   8.36450979e-02 -1.41142815e-01]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "Epitope Sequence (Raw, Padded): [[-3.1927475e+01 -4.9155040e+00  2.4203629e+00 ...  1.2634981e-01\n",
      "  -3.0370025e-04 -1.5728207e-02]\n",
      " [ 1.0102694e+00  1.5946226e+00  1.3483676e+00 ... -4.2295009e-01\n",
      "  -2.1024846e-01 -1.5614173e-01]\n",
      " [ 9.2440643e+00 -2.4131907e+01 -1.3972182e+00 ...  2.6887900e-01\n",
      "  -5.1234655e-02 -5.8680940e-02]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "TCR Input (Raw, Padded): tensor([[ 3.1027e+01,  9.9944e+00,  2.2556e-01,  ..., -8.7398e-02,\n",
      "         -1.5216e-01,  8.2461e-02],\n",
      "        [ 2.7718e+01,  8.8079e+00, -1.7174e+00,  ...,  4.2783e-03,\n",
      "         -1.3660e-01,  1.1840e-01],\n",
      "        [-1.2561e+00, -6.3537e+00, -2.5303e+01,  ..., -3.4307e-03,\n",
      "          8.3645e-02, -1.4114e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "Epitope Input (Raw, Padded): tensor([[-3.1927e+01, -4.9155e+00,  2.4204e+00,  ...,  1.2635e-01,\n",
      "         -3.0370e-04, -1.5728e-02],\n",
      "        [ 1.0103e+00,  1.5946e+00,  1.3484e+00,  ..., -4.2295e-01,\n",
      "         -2.1025e-01, -1.5614e-01],\n",
      "        [ 9.2441e+00, -2.4132e+01, -1.3972e+00,  ...,  2.6888e-01,\n",
      "         -5.1235e-02, -5.8681e-02],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "TCR Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Epitope Shape after Embedding + Positional Encoding: torch.Size([1, 43, 128])\n",
      "Combined Shape: torch.Size([1, 86, 128])\n",
      "Key Padding Mask Shape: torch.Size([1, 86])\n",
      "Key Padding Mask (Sample): tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "Combined Shape after Transformer Layer 1: torch.Size([1, 86, 128])\n",
      "Combined Shape after Transformer Layer 2: torch.Size([1, 86, 128])\n",
      "Pooled Shape: torch.Size([1, 128])\n",
      "Output Shape: torch.Size([1])\n",
      "Output in Model: tensor([0.1004], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Key Padding Mask for First Sample: tensor(False, device='cuda:0')\n",
      "Output for First Sample: [0.1004149]\n",
      "Label:  tensor([0.], device='cuda:0')\n",
      "Loss: tensor(0.7446, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train_dimensions_check.py --epochs 1 \\\n",
    "    --train \"../../data/splitted_datasets/allele/beta/train.tsv\" \\\n",
    "        --val \"../../data/splitted_datasets/allele/beta/validation.tsv\" \\\n",
    "    --tcr_train_embeddings  \"../../data/embeddings/beta/allele/padded_pca/padded_train_tcr_embeddings_final.h5\" \\\n",
    "        --epitope_train_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_train_epitope_embeddings_final.h5\" \\\n",
    "            --tcr_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_valid_embeddings \"../../data/embeddings/beta/allele/padded_pca/padded_valid_epitope_embeddings_final.h5\" \\\n",
    "                    --batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 1896\n",
      "(9, 1024)\n",
      "Number of keys in the NPZ file: 211529\n",
      "(18, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's check if the dimensions in the version2 were padded correctly for both epitopes and tcrs\n",
    "padd_beta_all_epi_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "padd_beta_all_trb_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "\n",
    "# \"../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\"\n",
    "# \"../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\"\n",
    "# \"../../data/embeddings/beta/allele/padded/padded_tcr_embeddings_final.h5\"\n",
    "# \"../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.h5\"\n",
    "\n",
    "paths = [padd_beta_all_epi_path, padd_beta_all_trb_path]\n",
    "for path in paths:\n",
    "    # Load the NPZ file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Print available keys in the file\n",
    "    print(\"Number of keys in the NPZ file:\", len(data.files))\n",
    "\n",
    "    print(data[data.files[0]].shape)\n",
    "    # Inspect the shape and size of each stored array\n",
    "    # for key in data.files:\n",
    "    #     # array = data[key]\n",
    "    #     print(f\"\\nKey: {key}\")\n",
    "    #     print(len(data[key]))\n",
    "        # print(f\"Shape: {array.shape}\")\n",
    "        # print(f\"Size: {array.size}\")\n",
    "        # print(f\"Data Type: {array.dtype}\")\n",
    "        # print(f\"Sample Data (first 5 elements):\\n{array[:5] if array.ndim == 1 else array[:5, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Datasets (Keys): 838\n",
      "\n",
      "Dataset Name: QASQEVKNW, Shape: (512, 512), Data Type: float32\n",
      "Dataset Name: RAFSPEVIPMF, Shape: (512, 512), Data Type: float32\n",
      "Dataset Name: KAFSPEVIPMF, Shape: (512, 512), Data Type: float32\n",
      "Dataset Name: EAAGIGILTV, Shape: (512, 512), Data Type: float32\n",
      "Dataset Name: KRWIILGLNK, Shape: (512, 512), Data Type: float32\n",
      "\n",
      "First Sample Data (QASQEVKNW): [[ 1.6602614   0.68618757  0.35977575 ... -0.00368146  0.01186421\n",
      "   0.00406068]\n",
      " [ 1.6602614   0.68618757  0.35977575 ... -0.00368146  0.01186421\n",
      "   0.00406068]\n",
      " [ 1.6602614   0.68618757  0.35977575 ... -0.00368146  0.01186421\n",
      "   0.00406068]\n",
      " [ 1.6602614   0.68618757  0.35977575 ... -0.00368146  0.01186421\n",
      "   0.00406068]\n",
      " [ 1.6602614   0.68618757  0.35977575 ... -0.00368146  0.01186421\n",
      "   0.00406068]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to your NPZ file\n",
    "file_path = \"../../data/embeddings/beta/allele/padded/padded_epitope_embeddings_final.npz\"\n",
    "\n",
    "# Load the NPZ file\n",
    "data = np.load(file_path)\n",
    "\n",
    "# List all keys (dataset names)\n",
    "dataset_keys = list(data.keys())\n",
    "print(f\"Total Datasets (Keys): {len(dataset_keys)}\\n\")\n",
    "\n",
    "# Inspect a few datasets\n",
    "num_samples = min(5, len(dataset_keys))  # Show up to 5 keys\n",
    "\n",
    "for i in range(num_samples):\n",
    "    key = dataset_keys[i]\n",
    "    embedding = data[key]  # Retrieve embedding (numpy array)\n",
    "    print(f\"Dataset Name: {key}, Shape: {embedding.shape}, Data Type: {embedding.dtype}\")\n",
    "\n",
    "# Optionally, print part of the first dataset\n",
    "first_key = dataset_keys[0]\n",
    "first_sample = data[first_key][:5]  # Show first 5 elements\n",
    "print(f\"\\nFirst Sample Data ({first_key}): {first_sample}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 1\n",
      "\n",
      "Key: embeddings\n",
      "19260\n"
     ]
    }
   ],
   "source": [
    "path_epi_beta_emb = \"../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\"\n",
    "data_epi = np.load(path)\n",
    "# Print available keys in the file\n",
    "print(\"Number of keys in the NPZ file:\", len(data_epi.files))\n",
    "\n",
    "# Inspect the shape and size of each stored array\n",
    "for key in data.files:\n",
    "    # array = data[key]\n",
    "    print(f\"\\nKey: {key}\")\n",
    "    print(len(data[key]))\n",
    "    # print(f\"Shape: {array.shape}\")\n",
    "    # print(f\"Size: {array.size}\")\n",
    "    # print(f\"Data Type: {array.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the NPZ file: 19260\n",
      "(19260, 512)\n"
     ]
    }
   ],
   "source": [
    "path = \"../../data/embeddings/beta/allele/pca/Epitope_beta_embeddings_reduced_final.npz\"\n",
    "data_epi = np.load(path)\n",
    "# Print available keys in the file\n",
    "print(\"Number of keys in the NPZ file:\", len(data_epi['embeddings']))\n",
    "print(data_epi['embeddings'].shape)\n",
    "# print(data_epi.files)\n",
    "# Inspect the shape and size of each stored array\n",
    "# for key in data.files:\n",
    "#     # array = data[key]\n",
    "#     print(f\"\\nKey: {key}\")\n",
    "#     print(len(data[key]))\n",
    "    # print(f\"Shape: {array.shape}\")\n",
    "    # print(f\"Size: {array.size}\")\n",
    "    # print(f\"Data Type: {array.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try again to train with the original embeddings (1024) without pca or isomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - embeddings 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. create npz batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train data: 749667\n",
      "length tcr_keys_set:  211529\n",
      "length epitope_keys_set:  1896\n",
      "Keys in tcr_data.files: ['CATSDNRQGNEQFF', 'CASSQGGTGEKRDTQYF', 'CASTPYGRAEAFF', 'CASSFDSMGGNSYEQYV', 'CASSQDYRSSGNTIYF']\n",
      "First 5 TCR keys from train_data: ['CASSQEWSDYYNEQFF', 'CSVGGISGTGGSYEQFF', 'CASSLSIGTATLQYF', 'CASSLSPWGTEAFF', 'CASSSEQGANTGELFF']\n",
      "len TCR keys: 749667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Lade den Trainingsdatensatz ===\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "train_data = pd.read_csv(train_path, sep='\\t', low_memory=False)\n",
    "print(\"Length train data:\", len(train_data))\n",
    "\n",
    "\n",
    "epitope_embeddings_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "tcr_embeddings_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "\n",
    "tcr_data = np.load(tcr_embeddings_path, allow_pickle=True)\n",
    "epitope_data = np.load(epitope_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# === Extrahiere die Keys aus dem Trainingsdatensatz ===\n",
    "tcr_keys = train_data['TRB_CDR3'].dropna().tolist()\n",
    "epitope_keys = train_data['Epitope'].dropna().tolist()\n",
    "\n",
    "# === Nur Keys behalten, die in den Embeddings existieren ===\n",
    "'''\n",
    "wurde erstetzt mit === Schnelle Version mit set() ===\n",
    "tcr_keys = [key for key in tcr_keys if key in tcr_data]\n",
    "epitope_keys = [key for key in epitope_keys if key in epitope_data]\n",
    "'''\n",
    "# === Schnelle Version mit set() ===\n",
    "tcr_keys_set = set(tcr_data.files)  # Mache eine schnelle Hash-Map (Set) für Keys\n",
    "print(\"length tcr_keys_set: \", len(tcr_keys_set))\n",
    "epitope_keys_set = set(epitope_data.files)\n",
    "print(\"length epitope_keys_set: \", len(epitope_keys_set))\n",
    "\n",
    "tcr_keys = [key for key in tcr_keys if key in tcr_keys_set]\n",
    "epitope_keys = [key for key in epitope_keys if key in epitope_keys_set]\n",
    "\n",
    "\n",
    "# === Dictionaries mit den Trainings-Embeddings erstellen ===\n",
    "tcr_train_dict = {key: tcr_data[key] for key in tcr_keys}\n",
    "epitope_train_dict = {key: epitope_data[key] for key in epitope_keys}\n",
    "\n",
    "print(f\"Keys in tcr_data.files: {list(tcr_data.files)[:5]}\")  # Check a sample of the keys\n",
    "print(f\"First 5 TCR keys from train_data: {tcr_keys[:5]}\")  # Check the initial TCR keys\n",
    "# print(f\"Filtered TCR keys: {tcr_keys}\")  # Check the final filtered keys\n",
    "print(f\"len TCR keys: {len(tcr_keys)}\")  # Check the final filtered keys\n",
    "\n",
    "# 82m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Max Length: 43 (TCR: 38, Epitope: 43)\n",
      "✅ Saved batch 1/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_0.npz\n",
      "✅ Saved batch 2/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_1.npz\n",
      "✅ Saved batch 3/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_2.npz\n",
      "✅ Saved batch 4/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_3.npz\n",
      "✅ Saved batch 5/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_4.npz\n",
      "✅ Saved batch 6/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_5.npz\n",
      "✅ Saved batch 7/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_6.npz\n",
      "✅ Saved batch 8/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_7.npz\n",
      "✅ Saved batch 9/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_8.npz\n",
      "✅ Saved batch 10/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_9.npz\n",
      "✅ Saved batch 11/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_10.npz\n",
      "✅ Saved batch 12/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_11.npz\n",
      "✅ Saved batch 13/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_12.npz\n",
      "✅ Saved batch 14/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_13.npz\n",
      "✅ Saved batch 15/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_14.npz\n",
      "✅ Saved batch 16/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_15.npz\n",
      "✅ Saved batch 17/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_16.npz\n",
      "✅ Saved batch 18/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_17.npz\n",
      "✅ Saved batch 19/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_18.npz\n",
      "✅ Saved batch 20/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_19.npz\n",
      "✅ Saved batch 21/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_20.npz\n",
      "✅ Saved batch 22/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_21.npz\n",
      "✅ Saved batch 23/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_22.npz\n",
      "✅ Saved batch 24/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_23.npz\n",
      "✅ Saved batch 25/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_24.npz\n",
      "✅ Saved batch 26/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_25.npz\n",
      "✅ Saved batch 27/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_26.npz\n",
      "✅ Saved batch 28/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_27.npz\n",
      "✅ Saved batch 29/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_28.npz\n",
      "✅ Saved batch 30/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_29.npz\n",
      "✅ Saved batch 31/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_30.npz\n",
      "✅ Saved batch 32/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_31.npz\n",
      "✅ Saved batch 33/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_32.npz\n",
      "✅ Saved batch 34/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_33.npz\n",
      "✅ Saved batch 35/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_34.npz\n",
      "✅ Saved batch 36/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_35.npz\n",
      "✅ Saved batch 37/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_36.npz\n",
      "✅ Saved batch 38/38 to ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_37.npz\n",
      "✅ All batches saved successfully!\n",
      "✅ Saved batch 1/1 to ../../data/embeddings/beta/allele/dimension_1024/train_epitope_padded_batches/batch_0.npz\n",
      "✅ All batches saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# === Maximaler Padding-Wert bestimmen ===\n",
    "max_tcr_length = max(embedding.shape[0] for embedding in tcr_train_dict.values())\n",
    "max_epitope_length = max(embedding.shape[0] for embedding in epitope_train_dict.values())\n",
    "\n",
    "max_length = max(max_tcr_length, max_epitope_length)  # Einheitliche Länge für Transformer\n",
    "\n",
    "print(f\"📌 Max Length: {max_length} (TCR: {max_tcr_length}, Epitope: {max_epitope_length})\")\n",
    "\n",
    "# === Padding-Funktion ===\n",
    "def pad_embedding(embedding, max_length):\n",
    "    \"\"\"\n",
    "    Padded ein einzelnes Embedding mit Nullen auf max_length.\n",
    "    \"\"\"\n",
    "    padded = np.zeros((max_length, embedding.shape[1]), dtype=embedding.dtype)\n",
    "    padded[:embedding.shape[0], :] = embedding  # Originalwerte behalten, Rest mit 0 füllen\n",
    "    return padded\n",
    "\n",
    "# === Speicherpfade für Trainingsdaten setzen ===\n",
    "train_tcr_padded_path = '../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches'\n",
    "train_epitope_padded_path = '../../data/embeddings/beta/allele/dimension_1024/train_epitope_padded_batches'\n",
    "\n",
    "os.makedirs(train_tcr_padded_path, exist_ok=True)\n",
    "os.makedirs(train_epitope_padded_path, exist_ok=True)\n",
    "\n",
    "# === Speicher-Funktion mit Batch-Mechanismus ===\n",
    "def save_padded_embeddings_in_batches(embeddings_dict, save_dir, batch_size=5000):\n",
    "    keys = list(embeddings_dict.keys())\n",
    "    num_batches = (len(keys) + batch_size - 1) // batch_size  # Anzahl der Batches berechnen\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_keys = keys[i * batch_size: (i + 1) * batch_size]\n",
    "        padded_batch = {key: pad_embedding(embeddings_dict[key], max_length) for key in batch_keys}\n",
    "        \n",
    "        batch_save_path = os.path.join(save_dir, f\"batch_{i}.npz\")\n",
    "        np.savez_compressed(batch_save_path, **padded_batch)\n",
    "        print(f\"✅ Saved batch {i + 1}/{num_batches} to {batch_save_path}\")\n",
    "\n",
    "    print(\"✅ All batches saved successfully!\")\n",
    "\n",
    "# === Train-Embeddings padden und speichern ===\n",
    "save_padded_embeddings_in_batches(tcr_train_dict, train_tcr_padded_path, batch_size=5000)\n",
    "save_padded_embeddings_in_batches(epitope_train_dict, train_epitope_padded_path, batch_size=5000)\n",
    "\n",
    "# 9m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. convert to hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Batch 1/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_0.npz\n",
      "🔄 Batch 2/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_1.npz\n",
      "🔄 Batch 3/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_2.npz\n",
      "🔄 Batch 4/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_3.npz\n",
      "🔄 Batch 5/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_4.npz\n",
      "🔄 Batch 6/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_5.npz\n",
      "🔄 Batch 7/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_6.npz\n",
      "🔄 Batch 8/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_7.npz\n",
      "🔄 Batch 9/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_8.npz\n",
      "🔄 Batch 10/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_9.npz\n",
      "🔄 Batch 11/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_10.npz\n",
      "🔄 Batch 12/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_11.npz\n",
      "🔄 Batch 13/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_12.npz\n",
      "🔄 Batch 14/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_13.npz\n",
      "🔄 Batch 15/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_14.npz\n",
      "🔄 Batch 16/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_15.npz\n",
      "🔄 Batch 17/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_16.npz\n",
      "🔄 Batch 18/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_17.npz\n",
      "🔄 Batch 19/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_18.npz\n",
      "🔄 Batch 20/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_19.npz\n",
      "🔄 Batch 21/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_20.npz\n",
      "🔄 Batch 22/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_21.npz\n",
      "🔄 Batch 23/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_22.npz\n",
      "🔄 Batch 24/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_23.npz\n",
      "🔄 Batch 25/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_24.npz\n",
      "🔄 Batch 26/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_25.npz\n",
      "🔄 Batch 27/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_26.npz\n",
      "🔄 Batch 28/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_27.npz\n",
      "🔄 Batch 29/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_28.npz\n",
      "🔄 Batch 30/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_29.npz\n",
      "🔄 Batch 31/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_30.npz\n",
      "🔄 Batch 32/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_31.npz\n",
      "🔄 Batch 33/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_32.npz\n",
      "🔄 Batch 34/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_33.npz\n",
      "🔄 Batch 35/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_34.npz\n",
      "🔄 Batch 36/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_35.npz\n",
      "🔄 Batch 37/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_36.npz\n",
      "🔄 Batch 38/38 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_37.npz\n",
      "✅ Finale gepaddete Embeddings gespeichert unter: ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "🔄 Batch 1/1 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/train_epitope_padded_batches/batch_0.npz\n",
      "✅ Finale gepaddete Embeddings gespeichert unter: ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "✅ HDF5-Datei geladen: ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "Anzahl Keys: 187298\n",
      "Beispiel-Keys: ['C*EFVGLAGGCTDTQYF', 'C*GTRPSISVPAA**RETGELFF', 'C*GTRPSISVPAACIYYLDHP*QFF', 'C*GTRPSISVPAACTRGPSYEQYF', 'C*GVGAG*DEQYF']\n",
      "✅ HDF5-Datei geladen: ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "Anzahl Keys: 1165\n",
      "Beispiel-Keys: ['AAFKRSCLK', 'AAGIGILTV', 'AARGPHGGAASGL', 'ACDPHSGHFV', 'AEGSRGGSQA']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === Finale HDF5-Dateien aus gepaddeten Batches erstellen ===\n",
    "def combine_selected_batches_to_hdf5(batch_files, output_path):\n",
    "    \"\"\"\n",
    "    Kombiniert eine spezifische Liste von Batch-Dateien zu einer einzigen HDF5-Datei.\n",
    "    \"\"\"\n",
    "    if not batch_files:\n",
    "        print(f\"❌ Keine Batch-Dateien in der Liste gefunden.\")\n",
    "        return\n",
    "\n",
    "    with h5py.File(output_path, 'w') as hdf5_file:\n",
    "        for i, batch_file in enumerate(batch_files):\n",
    "            batch = np.load(batch_file, allow_pickle=True)\n",
    "\n",
    "            for key in batch.files:\n",
    "                if key not in hdf5_file:\n",
    "                    hdf5_file.create_dataset(key, data=batch[key], compression=\"gzip\")\n",
    "                else:\n",
    "                    print(f\"⚠️ Duplikat-Key übersprungen: {key}\")\n",
    "\n",
    "            print(f\"🔄 Batch {i+1}/{len(batch_files)} verarbeitet: {batch_file}\")\n",
    "\n",
    "    print(f\"✅ Finale gepaddete Embeddings gespeichert unter: {output_path}\")\n",
    "\n",
    "# === TCR & Epitope Batches für Train zusammenführen ===\n",
    "train_tcr_batches = [f\"../../data/embeddings/beta/allele/dimension_1024/train_tcr_padded_batches/batch_{i}.npz\" for i in range(38)]\n",
    "train_epitope_batches = [\"../../data/embeddings/beta/allele/dimension_1024/train_epitope_padded_batches/batch_0.npz\"]\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=train_tcr_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=train_epitope_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "# === Überprüfe die HDF5-Dateien ===\n",
    "def check_hdf5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as hdf5_file:\n",
    "        keys = list(hdf5_file.keys())\n",
    "        print(f\"✅ HDF5-Datei geladen: {file_path}\")\n",
    "        print(f\"Anzahl Keys: {len(keys)}\")\n",
    "        print(f\"Beispiel-Keys: {keys[:5]}\")\n",
    "\n",
    "# Überprüfe Train-TCR\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5')\n",
    "\n",
    "# Überprüfe Train-Epitope\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5')\n",
    "\n",
    "# 11m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create npz batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of valid_tcr_keys:  186951\n",
      "len of valid_epitope_keys:  186951\n",
      "len of valid_tcr_keys after getting emnedding:  186951\n",
      "len of valid_epitope_keys after getting embedding:  186951\n",
      "✅ Saved batch 1/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_0.npz\n",
      "✅ Saved batch 2/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_1.npz\n",
      "✅ Saved batch 3/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_2.npz\n",
      "✅ Saved batch 4/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_3.npz\n",
      "✅ Saved batch 5/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_4.npz\n",
      "✅ Saved batch 6/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_5.npz\n",
      "✅ Saved batch 7/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_6.npz\n",
      "✅ Saved batch 8/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_7.npz\n",
      "✅ Saved batch 9/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_8.npz\n",
      "✅ Saved batch 10/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_9.npz\n",
      "✅ Saved batch 11/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_10.npz\n",
      "✅ Saved batch 12/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_11.npz\n",
      "✅ Saved batch 13/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_12.npz\n",
      "✅ Saved batch 14/14 to ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_13.npz\n",
      "✅ All batches saved successfully!\n",
      "✅ Saved batch 1/1 to ../../data/embeddings/beta/allele/dimension_1024/valid_epitope_padded_batches/batch_0.npz\n",
      "✅ All batches saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# === Lade den Validierungsdatensatz ===\n",
    "validation_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "valid_data = pd.read_csv(validation_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# === Lade die Keys aus dem Validierungsdatensatz ===\n",
    "valid_tcr_keys = valid_data['TRB_CDR3'].dropna().tolist()\n",
    "valid_epitope_keys = valid_data['Epitope'].dropna().tolist()\n",
    "\n",
    "epitope_embeddings_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "tcr_embeddings_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "\n",
    "tcr_data = np.load(tcr_embeddings_path, allow_pickle=True)\n",
    "epitope_data = np.load(epitope_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# === Schnelle Version mit set() ===\n",
    "tcr_keys_set = set(tcr_data.files)  # Mache eine schnelle Hash-Map (Set) für Keys\n",
    "epitope_keys_set = set(epitope_data.files)\n",
    "\n",
    "print(\"len of valid_tcr_keys: \", len(valid_tcr_keys))\n",
    "print(\"len of valid_epitope_keys: \", len(valid_epitope_keys))\n",
    "\n",
    "valid_tcr_keys = [key for key in valid_tcr_keys if key in tcr_keys_set]\n",
    "valid_epitope_keys = [key for key in valid_epitope_keys if key in epitope_keys_set]\n",
    "\n",
    "print(\"len of valid_tcr_keys after getting emnedding: \", len(valid_tcr_keys))\n",
    "print(\"len of valid_epitope_keys after getting embedding: \", len(valid_epitope_keys))\n",
    "\n",
    "# === Dictionaries für Validierungs-Embeddings erstellen ===\n",
    "valid_tcr_embeddings_dict = {key: tcr_data[key] for key in valid_tcr_keys}\n",
    "valid_epitope_embeddings_dict = {key: epitope_data[key] for key in valid_epitope_keys}\n",
    "\n",
    "# === Speicherpfade für Validierungsdaten setzen ===\n",
    "valid_tcr_padded_path = '../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches'\n",
    "valid_epitope_padded_path = '../../data/embeddings/beta/allele/dimension_1024/valid_epitope_padded_batches'\n",
    "\n",
    "os.makedirs(valid_tcr_padded_path, exist_ok=True)\n",
    "os.makedirs(valid_epitope_padded_path, exist_ok=True)\n",
    "\n",
    "# === Validierungsdaten padden und speichern ===\n",
    "save_padded_embeddings_in_batches(valid_tcr_embeddings_dict, valid_tcr_padded_path, batch_size=5000)\n",
    "save_padded_embeddings_in_batches(valid_epitope_embeddings_dict, valid_epitope_padded_path, batch_size=5000)\n",
    "\n",
    "# 22m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create hdf5 files for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Batch 1/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_0.npz\n",
      "🔄 Batch 2/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_1.npz\n",
      "🔄 Batch 3/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_2.npz\n",
      "🔄 Batch 4/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_3.npz\n",
      "🔄 Batch 5/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_4.npz\n",
      "🔄 Batch 6/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_5.npz\n",
      "🔄 Batch 7/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_6.npz\n",
      "🔄 Batch 8/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_7.npz\n",
      "🔄 Batch 9/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_8.npz\n",
      "🔄 Batch 10/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_9.npz\n",
      "🔄 Batch 11/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_10.npz\n",
      "🔄 Batch 12/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_11.npz\n",
      "🔄 Batch 13/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_12.npz\n",
      "🔄 Batch 14/14 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_13.npz\n",
      "✅ Finale gepaddete Embeddings gespeichert unter: ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "🔄 Batch 1/1 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/valid_epitope_padded_batches/batch_0.npz\n",
      "✅ Finale gepaddete Embeddings gespeichert unter: ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n"
     ]
    }
   ],
   "source": [
    "# === Validierungs-Batches zusammenführen ===\n",
    "valid_tcr_batches = [f\"../../data/embeddings/beta/allele/dimension_1024/valid_tcr_padded_batches/batch_{i}.npz\" for i in range(14)]\n",
    "valid_epitope_batches = [\"../../data/embeddings/beta/allele/dimension_1024/valid_epitope_padded_batches/batch_0.npz\"]\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=valid_tcr_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=valid_epitope_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length test data: 56460\n",
      "Test data from:  ../../data/splitted_datasets/allele/beta/test.tsv\n",
      "length tcr_keys_set:  211529\n",
      "length epitope_keys_set:  1896\n",
      "len of test_tcr_keys:  56460\n",
      "len of test_epitope_keys:  56460\n",
      "len of test_tcr_keys after getting emnedding:  56460\n",
      "len of test_epitope_keys after getting embedding:  56460\n",
      "Keys in tcr_data.files: ['CATSDNRQGNEQFF', 'CASSQGGTGEKRDTQYF', 'CASTPYGRAEAFF', 'CASSFDSMGGNSYEQYV', 'CASSQDYRSSGNTIYF']\n",
      "First 5 TCR keys from test_data: ['CAASQNTEAFF', 'CASSLWEKLAKNIQYF', 'CASSAGGGVETQYF', 'CASSQEFVGAVLDTQYF', 'CASRTRGGTLIEQYF']\n",
      "len TCR keys: 56460\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Lade den Trainingsdatensatz ===\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "test_data = pd.read_csv(test_path, sep='\\t', low_memory=False)\n",
    "print(\"Length test data:\", len(test_data))\n",
    "print(\"Test data from: \", test_path)\n",
    "\n",
    "\n",
    "epitope_embeddings_path = '../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz'\n",
    "tcr_embeddings_path = '../../data/embeddings/beta/allele/TRB_beta_embeddings.npz'\n",
    "\n",
    "tcr_data = np.load(tcr_embeddings_path, allow_pickle=True)\n",
    "epitope_data = np.load(epitope_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# === Extrahiere die Keys aus dem Trainingsdatensatz ===\n",
    "tcr_keys = test_data['TRB_CDR3'].dropna().tolist()\n",
    "epitope_keys = test_data['Epitope'].dropna().tolist()\n",
    "\n",
    "# === Nur Keys behalten, die in den Embeddings existieren ===\n",
    "'''\n",
    "wurde erstetzt mit === Schnelle Version mit set() ===\n",
    "tcr_keys = [key for key in tcr_keys if key in tcr_data]\n",
    "epitope_keys = [key for key in epitope_keys if key in epitope_data]\n",
    "'''\n",
    "# === Schnelle Version mit set() ===\n",
    "tcr_keys_set = set(tcr_data.files)  # Mache eine schnelle Hash-Map (Set) für Keys\n",
    "print(\"length tcr_keys_set: \", len(tcr_keys_set))\n",
    "epitope_keys_set = set(epitope_data.files)\n",
    "print(\"length epitope_keys_set: \", len(epitope_keys_set))\n",
    "\n",
    "print(\"len of test_tcr_keys: \", len(tcr_keys))\n",
    "print(\"len of test_epitope_keys: \", len(epitope_keys))\n",
    "\n",
    "tcr_keys = [key for key in tcr_keys if key in tcr_keys_set]\n",
    "epitope_keys = [key for key in epitope_keys if key in epitope_keys_set]\n",
    "\n",
    "print(\"len of test_tcr_keys after getting emnedding: \", len(tcr_keys))\n",
    "print(\"len of test_epitope_keys after getting embedding: \", len(epitope_keys))\n",
    "\n",
    "# === Dictionaries mit den Trainings-Embeddings erstellen ===\n",
    "tcr_test_dict = {key: tcr_data[key] for key in tcr_keys}\n",
    "epitope_test_dict = {key: epitope_data[key] for key in epitope_keys}\n",
    "\n",
    "print(f\"Keys in tcr_data.files: {list(tcr_data.files)[:5]}\")  # Check a sample of the keys\n",
    "print(f\"First 5 TCR keys from test_data: {tcr_keys[:5]}\")  # Check the initial TCR keys\n",
    "# print(f\"Filtered TCR keys: {tcr_keys}\")  # Check the final filtered keys\n",
    "print(f\"len TCR keys: {len(tcr_keys)}\")  # Check the final filtered keys\n",
    "\n",
    "# 5m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 1/3 to ../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches/batch_0.npz\n",
      "✅ Saved batch 2/3 to ../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches/batch_1.npz\n",
      "✅ Saved batch 3/3 to ../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches/batch_2.npz\n",
      "✅ All batches saved successfully!\n",
      "✅ Saved batch 1/1 to ../../data/embeddings/beta/allele/dimension_1024/test_epitope_padded_batches/batch_0.npz\n",
      "✅ All batches saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# === Speicherpfade für Trainingsdaten setzen ===\n",
    "test_tcr_padded_path = '../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches'\n",
    "test_epitope_padded_path = '../../data/embeddings/beta/allele/dimension_1024/test_epitope_padded_batches'\n",
    "\n",
    "os.makedirs(test_tcr_padded_path, exist_ok=True)\n",
    "os.makedirs(test_epitope_padded_path, exist_ok=True)\n",
    "\n",
    "# === Train-Embeddings padden und speichern ===\n",
    "save_padded_embeddings_in_batches(tcr_test_dict, test_tcr_padded_path, batch_size=5000)\n",
    "save_padded_embeddings_in_batches(epitope_test_dict, test_epitope_padded_path, batch_size=5000)\n",
    "\n",
    "# 2m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Batch 1/3 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches/batch_0.npz\n",
      "🔄 Batch 2/3 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches/batch_1.npz\n",
      "🔄 Batch 3/3 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches/batch_2.npz\n",
      "✅ Finale gepaddete Embeddings gespeichert unter: ../../data/embeddings/beta/allele/dimension_1024/padded_test_tcr_embeddings_final.h5\n",
      "🔄 Batch 1/1 verarbeitet: ../../data/embeddings/beta/allele/dimension_1024/test_epitope_padded_batches/batch_0.npz\n",
      "✅ Finale gepaddete Embeddings gespeichert unter: ../../data/embeddings/beta/allele/dimension_1024/padded_test_epitope_embeddings_final.h5\n"
     ]
    }
   ],
   "source": [
    "# === TCR & Epitope Batches für Train zusammenführen ===\n",
    "test_tcr_batches = [f\"../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches/batch_{i}.npz\" for i in range(3)]\n",
    "test_epitope_batches = [\"../../data/embeddings/beta/allele/dimension_1024/test_epitope_padded_batches/batch_0.npz\"]\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=test_tcr_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/dimension_1024/padded_test_tcr_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=test_epitope_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/dimension_1024/padded_test_epitope_embeddings_final.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch = np.load('../../data/embeddings/beta/allele/dimension_1024/test_tcr_padded_batches/batch_2.npz')\n",
    "print(len(batch.files))  # wie viele Samples in einem Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HDF5-Datei geladen: ../../data/embeddings/beta/allele/dimension_1024/padded_test_tcr_embeddings_final.h5\n",
      "Anzahl Keys: 41975\n",
      "Beispiel-Keys: ['C*GTRPSISVPAACIYYLDHP*QFF', 'C*R*GVLEGAGGPTLAGGLNEQFF', 'C*R*GVYRTHHIF', 'C*R*RGPQETQYF', 'C*RGTSGGGFQETQYF']\n",
      "✅ HDF5-Datei geladen: ../../data/embeddings/beta/allele/dimension_1024/padded_test_epitope_embeddings_final.h5\n",
      "Anzahl Keys: 786\n",
      "Beispiel-Keys: ['AAASATLAL', 'AAFKRSCLK', 'AAGIGILTV', 'AARAVFLAL', 'AEGSRGGSQA']\n"
     ]
    }
   ],
   "source": [
    "# === Überprüfe die HDF5-Dateien ===\n",
    "def check_hdf5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as hdf5_file:\n",
    "        keys = list(hdf5_file.keys())\n",
    "        print(f\"✅ HDF5-Datei geladen: {file_path}\")\n",
    "        print(f\"Anzahl Keys: {len(keys)}\")\n",
    "        print(f\"Beispiel-Keys: {keys[:5]}\")\n",
    "\n",
    "# Überprüfe Test-TCR\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_test_tcr_embeddings_final.h5')\n",
    "\n",
    "# Überprüfe Test-Epitope\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_test_epitope_embeddings_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HDF5-Datei geladen: ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "Anzahl Keys: 1222\n",
      "Beispiel-Keys: ['AAFKRSCLK', 'AAGIGILTV', 'AARGPHGGAASGL', 'ACDPHSGHFV', 'AEGSRGGSQA']\n",
      "\n",
      "Erster Key: AAFKRSCLK\n",
      "Inhalt des ersten Keys (AAFKRSCLK):\n",
      "[[ 0.1147386   0.16064653  0.04482584 ... -0.04331092 -0.06523857\n",
      "   0.00185633]\n",
      " [ 0.15384358  0.1894012   0.02227109 ... -0.1130099   0.00741\n",
      "   0.02467221]\n",
      " [ 0.09384871  0.08610557  0.05090743 ... -0.08467659  0.01365718\n",
      "   0.12906814]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "\n",
      "⚠ Große Daten: Typ: float32, Shape: (43, 1024)\n",
      "✅ HDF5-Datei geladen: ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "Anzahl Keys: 186829\n",
      "Beispiel-Keys: ['C*EFVGLAGGCTDTQYF', 'C*GTRPSISVPAA**RETGELFF', 'C*GTRPSISVPAACIYYLDHP*QFF', 'C*GTRPSISVPAACTRGPSYEQYF', 'C*GVGAG*DEQYF']\n",
      "\n",
      "Erster Key: C*EFVGLAGGCTDTQYF\n",
      "Inhalt des ersten Keys (C*EFVGLAGGCTDTQYF):\n",
      "[[-0.00693651  0.1442267  -0.0451696  ... -0.04565899 -0.03926302\n",
      "   0.0213235 ]\n",
      " [-0.02625421  0.04349693 -0.0005188  ... -0.02344458  0.00640687\n",
      "  -0.00356546]\n",
      " [ 0.0090124   0.0441053  -0.02252427 ...  0.05086    -0.07146946\n",
      "   0.07701842]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "\n",
      "⚠ Große Daten: Typ: float32, Shape: (43, 1024)\n"
     ]
    }
   ],
   "source": [
    "# import h5py\n",
    "\n",
    "# def check_hdf5_file(file_path):\n",
    "#     with h5py.File(file_path, 'r') as hdf5_file:\n",
    "#         keys = list(hdf5_file.keys())\n",
    "#         print(f\"✅ HDF5-Datei geladen: {file_path}\")\n",
    "#         print(f\"Anzahl Keys: {len(keys)}\")\n",
    "#         print(f\"Beispiel-Keys: {keys[:5]}\")\n",
    "\n",
    "# # Überprüfe TCR\n",
    "# check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5')\n",
    "\n",
    "# # Überprüfe Epitope\n",
    "# check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5')\n",
    "\n",
    "import h5py\n",
    "\n",
    "def check_hdf5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as hdf5_file:\n",
    "        keys = list(hdf5_file.keys())\n",
    "        print(f\"\\u2705 HDF5-Datei geladen: {file_path}\")\n",
    "        print(f\"Anzahl Keys: {len(keys)}\")\n",
    "        print(f\"Beispiel-Keys: {keys[:5]}\")\n",
    "\n",
    "        if keys:\n",
    "            first_key = keys[0]\n",
    "            print(f\"\\nErster Key: {first_key}\")\n",
    "\n",
    "            # Check if the data is accessible and print its shape or content\n",
    "            try:\n",
    "                value = hdf5_file[first_key][:]  # Read the full content of the key\n",
    "                print(f\"Inhalt des ersten Keys ({first_key}):\")\n",
    "                print(value)\n",
    "\n",
    "                # If the value is large, summarize its shape and datatype instead\n",
    "                if value.size > 20:  # Arbitrary threshold for large data\n",
    "                    print(f\"\\n⚠ Große Daten: Typ: {value.dtype}, Shape: {value.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Lesen des ersten Keys: {e}\")\n",
    "\n",
    "# # Überprüfe TCR\n",
    "# check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5')\n",
    "# # Überprüfe Epitope\n",
    "# check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5')\n",
    "\n",
    "# Überprüfe TCR\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5')\n",
    "\n",
    "# Überprüfe Epitope\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation with 1024 ProtBERT embeddings (no PCA no Isomap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.004518\n",
      "train_path: ../../../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250329_210529-cpx9airr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v1_mha_1024h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cpx9airr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 225.08MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.4\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/train_1024.py\", line 146, in <module>\n",
      "    val_loss = criterion_val(output, label)\n",
      "               ^^^^^^^^^^^^^\n",
      "NameError: name 'criterion_val' is not defined. Did you mean: 'criterion'?\n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v1_mha_1024h\u001b[0m at: \u001b[34mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cpx9airr\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250329_210529-cpx9airr/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! python models_scripts/v1_mha/train_v1_1024.py --configs_path \"./configs/v1_mha_1024_config.yaml\" \n",
    "    \n",
    "# results saved in wandb but not in this output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try v1_1024 with dropout 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.004518\n",
      "train_path: ../../../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250330_230325-ovmm3dz8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v1_mha_1024h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/ovmm3dz8\u001b[0m\n",
      "Dropout:  0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 225.08MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.4\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/20], Loss: 0.5571, Val AUC: 0.6768, Val AP: 0.4419, Val Accuracy: 0.8471, Val F1: 0.4323, TP: 12238, TN: 165810, FP: 9350, FN: 22794\n",
      "Epoch 2/20 [Training]:   0%|     | 51/17516 [00:03<18:30, 15.72it/s, loss=0.483]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17516 that is less than the current step 17517. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [2/20], Loss: 0.4637, Val AUC: 0.6154, Val AP: 0.3140, Val Accuracy: 0.7964, Val F1: 0.3260, TP: 10348, TN: 157058, FP: 18102, FN: 24684\n",
      "No improvement in AUC. Early stop counter: 1/10\n",
      "Epoch 3/20 [Training]:   1%|    | 149/17516 [00:06<11:59, 24.12it/s, loss=0.557]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 35032 that is less than the current step 35033. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [3/20], Loss: 0.4799, Val AUC: 0.7971, Val AP: 0.4793, Val Accuracy: 0.8206, Val F1: 0.4257, TP: 13977, TN: 158501, FP: 16659, FN: 21055\n",
      "Epoch 4/20 [Training]:   1%|    | 118/17516 [00:05<12:16, 23.62it/s, loss=0.455]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 52548 that is less than the current step 52549. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [4/20], Loss: 0.5390, Val AUC: 0.7461, Val AP: 0.4432, Val Accuracy: 0.7989, Val F1: 0.3919, TP: 13617, TN: 154309, FP: 20851, FN: 21415\n",
      "No improvement in AUC. Early stop counter: 1/10\n",
      "Epoch 5/20 [Training]:   0%|      | 4/17516 [00:00<18:07, 16.10it/s, loss=0.821]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70064 that is less than the current step 70065. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [5/20], Loss: 0.6056, Val AUC: 0.5438, Val AP: 0.2683, Val Accuracy: 0.8398, Val F1: 0.0982, TP: 1833, TN: 174682, FP: 478, FN: 33199\n",
      "No improvement in AUC. Early stop counter: 2/10\n",
      "Epoch 6/20 [Training]:   1%|    | 137/17516 [00:06<13:35, 21.31it/s, loss=0.624]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 87580 that is less than the current step 87581. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [6/20], Loss: 0.5909, Val AUC: 0.7800, Val AP: 0.4263, Val Accuracy: 0.8391, Val F1: 0.1974, TP: 4161, TN: 172205, FP: 2955, FN: 30871\n",
      "No improvement in AUC. Early stop counter: 3/10\n",
      "Epoch 7/20 [Training]:   0%|     | 79/17516 [00:05<17:09, 16.94it/s, loss=0.649]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 105096 that is less than the current step 105097. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [7/20], Loss: 0.5887, Val AUC: 0.6360, Val AP: 0.3757, Val Accuracy: 0.8484, Val F1: 0.2081, TP: 4187, TN: 174130, FP: 1030, FN: 30845\n",
      "No improvement in AUC. Early stop counter: 4/10\n",
      "Epoch 8/20 [Training]:   1%|     | 154/17516 [00:09<15:43, 18.40it/s, loss=0.59]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 122612 that is less than the current step 122613. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [8/20], Loss: 0.5405, Val AUC: 0.6323, Val AP: 0.3941, Val Accuracy: 0.8546, Val F1: 0.2935, TP: 6346, TN: 173289, FP: 1871, FN: 28686\n",
      "No improvement in AUC. Early stop counter: 5/10\n",
      "Epoch 9/20 [Training]:   0%|     | 68/17516 [00:05<19:46, 14.71it/s, loss=0.524]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140128 that is less than the current step 140129. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [9/20], Loss: 0.5299, Val AUC: 0.6911, Val AP: 0.4225, Val Accuracy: 0.8155, Val F1: 0.4009, TP: 12974, TN: 158446, FP: 16714, FN: 22058\n",
      "No improvement in AUC. Early stop counter: 6/10\n",
      "Epoch 10/20 [Training]:   1%|   | 128/17516 [00:06<15:30, 18.69it/s, loss=0.531]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 157644 that is less than the current step 157645. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [10/20], Loss: 0.4930, Val AUC: 0.7773, Val AP: 0.4794, Val Accuracy: 0.7501, Val F1: 0.3959, TP: 17214, TN: 140449, FP: 34711, FN: 17818\n",
      "No improvement in AUC. Early stop counter: 7/10\n",
      "Epoch 11/20 [Training]:   0%|    | 66/17516 [00:04<21:46, 13.36it/s, loss=0.452]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 175160 that is less than the current step 175161. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [11/20], Loss: 0.4867, Val AUC: 0.7051, Val AP: 0.4455, Val Accuracy: 0.5554, Val F1: 0.3443, TP: 24538, TN: 92201, FP: 82959, FN: 10494\n",
      "No improvement in AUC. Early stop counter: 8/10\n",
      "Epoch 12/20 [Training]:   0%|    | 35/17516 [00:02<18:51, 15.45it/s, loss=0.497]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 192676 that is less than the current step 192677. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [12/20], Loss: 0.4680, Val AUC: 0.8093, Val AP: 0.5064, Val Accuracy: 0.6565, Val F1: 0.4529, TP: 29889, TN: 108104, FP: 67056, FN: 5143\n",
      "Epoch 13/20 [Training]:   0%|    | 37/17516 [00:02<16:57, 17.18it/s, loss=0.448]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 210192 that is less than the current step 210193. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [13/20], Loss: 0.4472, Val AUC: 0.8240, Val AP: 0.5153, Val Accuracy: 0.7918, Val F1: 0.4583, TP: 18515, TN: 147907, FP: 27253, FN: 16517\n",
      "Epoch 14/20 [Training]:   1%|   | 121/17516 [00:07<16:21, 17.73it/s, loss=0.424]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 227708 that is less than the current step 227709. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [14/20], Loss: 0.4305, Val AUC: 0.8049, Val AP: 0.4838, Val Accuracy: 0.8605, Val F1: 0.3382, TP: 7492, TN: 173385, FP: 1775, FN: 27540\n",
      "No improvement in AUC. Early stop counter: 1/10\n",
      "Epoch 15/20 [Training]:   0%|    | 65/17516 [00:04<21:21, 13.62it/s, loss=0.454]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 245224 that is less than the current step 245225. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [15/20], Loss: 0.4445, Val AUC: 0.7470, Val AP: 0.4438, Val Accuracy: 0.6706, Val F1: 0.3798, TP: 21204, TN: 119748, FP: 55412, FN: 13828\n",
      "No improvement in AUC. Early stop counter: 2/10\n",
      "Epoch 16/20 [Training]:   0%|    | 52/17516 [00:04<21:33, 13.51it/s, loss=0.532]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 262740 that is less than the current step 262741. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [16/20], Loss: 0.4348, Val AUC: 0.8008, Val AP: 0.4816, Val Accuracy: 0.7067, Val F1: 0.4267, TP: 22942, TN: 125598, FP: 49562, FN: 12090\n",
      "No improvement in AUC. Early stop counter: 3/10\n",
      "Epoch 17/20 [Training]:   1%|   | 158/17516 [00:08<16:30, 17.52it/s, loss=0.421]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 280256 that is less than the current step 280257. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [17/20], Loss: 0.4227, Val AUC: 0.8178, Val AP: 0.5187, Val Accuracy: 0.8252, Val F1: 0.4411, TP: 14503, TN: 158943, FP: 16217, FN: 20529\n",
      "No improvement in AUC. Early stop counter: 4/10\n",
      "Epoch 18/20 [Training]:   1%|    | 161/17516 [00:09<14:56, 19.37it/s, loss=0.41]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 297772 that is less than the current step 297773. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [18/20], Loss: 0.4146, Val AUC: 0.8245, Val AP: 0.5317, Val Accuracy: 0.8281, Val F1: 0.4482, TP: 14680, TN: 159371, FP: 15789, FN: 20352\n",
      "Epoch 19/20 [Training]:   0%|    | 68/17516 [00:04<22:41, 12.82it/s, loss=0.436]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 315288 that is less than the current step 315289. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [19/20], Loss: 0.4158, Val AUC: 0.8301, Val AP: 0.5267, Val Accuracy: 0.8217, Val F1: 0.4404, TP: 14743, TN: 157980, FP: 17180, FN: 20289\n",
      "Epoch 20/20 [Training]:   0%|    | 27/17516 [00:01<12:33, 23.20it/s, loss=0.475]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 332804 that is less than the current step 332805. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [20/20], Loss: 0.4251, Val AUC: 0.8235, Val AP: 0.5311, Val Accuracy: 0.8499, Val F1: 0.4465, TP: 12729, TN: 165907, FP: 9253, FN: 22303\n",
      "No improvement in AUC. Early stop counter: 1/10\n",
      "Best model saved with AUC: 0.8301254181346532\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.877 MB of 2.293 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.293 MB of 2.293 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.293 MB of 2.293 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.293 MB of 2.293 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.293 MB of 2.293 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss ▅▃▄▄▃▄▄▅▄▇▄▅▃▄▄▄▄█▅▅▅▃▄▄▃▃▂▂▂▅▃▅▅▄▅▁▄▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch ▆▃▃▆█▇▇▆▅▄▄▃▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy █▇▇▇████▇▅▁▃▆█▄▄▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap ▆▂▇▆▁▅▄▄▅▇▆▇█▇▆▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc ▄▃▇▆▁▇▃▃▅▇▅▇█▇▆▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 ▇▅▇▇▁▃▃▅▇▇▆██▆▆▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn ▅▆▅▅█▇▇▇▅▄▂▁▄▇▃▃▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp ▂▂▂▃▁▁▁▁▂▄█▇▃▁▆▅▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss ▃▃▂▂▂▁▂▂▂▃█▇▂▂▅▄▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision ▅▃▄▃█▅██▄▂▁▂▃█▂▂▄▄▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall ▄▃▄▄▁▂▂▂▄▅▇█▅▂▆▆▄▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn ▇▇▇▆████▇▅▁▂▆█▃▄▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp ▄▃▄▄▁▂▂▂▄▅▇█▅▂▆▆▄▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 0.21063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch 0.42514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy 0.84987\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap 0.53105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc 0.82349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 0.44652\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn 22303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp 9253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss 0.37775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision 0.57906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall 0.36335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn 165907\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp 12729\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v1_mha_1024h\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/ovmm3dz8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 44 artifact file(s) and 40 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250330_230325-ovmm3dz8/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train_v1_1024.py --configs_path \"./configs/v1_mha_1024_config.yaml\" \\\n",
    "    --dropout 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with v12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.004518\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250331_103223-8df69qvg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v12_mha_1024h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/8df69qvg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 225.08MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.4\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/20], Loss: 0.4228, Val AUC: 0.8096, Val AP: 0.5057, Val Accuracy: 0.8343, Val F1: 0.4385, TP: 13604, TN: 161755, FP: 13405, FN: 21428\n",
      "Epoch 2/20 [Training]:   0%|     | 40/17516 [00:05<40:51,  7.13it/s, loss=0.465]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17516 that is less than the current step 17517. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [2/20], Loss: 0.3909, Val AUC: 0.8496, Val AP: 0.5476, Val Accuracy: 0.8328, Val F1: 0.4628, TP: 15135, TN: 159914, FP: 15246, FN: 19897\n",
      "Epoch 3/20 [Training]:   0%|     | 62/17516 [00:09<29:40,  9.80it/s, loss=0.388]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 35032 that is less than the current step 35033. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [3/20], Loss: 0.3829, Val AUC: 0.8285, Val AP: 0.5238, Val Accuracy: 0.8255, Val F1: 0.4548, TP: 15298, TN: 158218, FP: 16942, FN: 19734\n",
      "No improvement in AUC. Early stop counter: 1/5\n",
      "Epoch 4/20 [Training]:   0%|     | 78/17516 [00:08<32:09,  9.04it/s, loss=0.368]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 52548 that is less than the current step 52549. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [4/20], Loss: 0.3792, Val AUC: 0.8392, Val AP: 0.5322, Val Accuracy: 0.8430, Val F1: 0.4642, TP: 14294, TN: 162905, FP: 12255, FN: 20738\n",
      "No improvement in AUC. Early stop counter: 2/5\n",
      "Epoch 5/20 [Training]:   0%|       | 74/17516 [00:07<34:42,  8.37it/s, loss=0.4]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70064 that is less than the current step 70065. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [5/20], Loss: 0.3775, Val AUC: 0.8372, Val AP: 0.5388, Val Accuracy: 0.8284, Val F1: 0.4607, TP: 15410, TN: 158707, FP: 16453, FN: 19622\n",
      "No improvement in AUC. Early stop counter: 3/5\n",
      "Epoch 6/20 [Training]:   0%|     | 55/17516 [00:05<34:39,  8.40it/s, loss=0.401]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 87580 that is less than the current step 87581. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [6/20], Loss: 0.3764, Val AUC: 0.8236, Val AP: 0.5235, Val Accuracy: 0.8484, Val F1: 0.4643, TP: 13805, TN: 164529, FP: 10631, FN: 21227\n",
      "No improvement in AUC. Early stop counter: 4/5\n",
      "Epoch 7/20 [Training]:   0%|     | 67/17516 [00:07<34:20,  8.47it/s, loss=0.339]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 105096 that is less than the current step 105097. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [7/20], Loss: 0.3768, Val AUC: 0.8365, Val AP: 0.5273, Val Accuracy: 0.8576, Val F1: 0.4546, TP: 12478, TN: 167779, FP: 7381, FN: 22554\n",
      "No improvement in AUC. Early stop counter: 5/5\n",
      "Early stopping triggered.\n",
      "Best model saved with AUC: 0.8495856981717065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.355 MB of 2.891 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.386 MB of 2.891 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.891 MB of 2.891 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.912 MB of 2.912 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.912 MB of 2.912 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.912 MB of 2.912 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss ▄▅▃▄▄▃▅▃▆▄▃▃▆▁▃▃▃▆▄▅▃▄▂▆▂▅▂▅█▅▄▃▅▄▄▃▄▆▇▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch █▃▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy ▃▃▁▅▂▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap ▁█▄▅▇▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc ▁█▄▆▆▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 ▁█▅█▇█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn ▅▂▁▄▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp ▅▇█▅█▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss ▃▁▅▂▃██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision ▂▂▁▄▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall ▄▇█▅█▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn ▄▂▁▄▁▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp ▄▇█▅█▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 0.31175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch 0.37684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy 0.85758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap 0.52732\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc 0.8365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 0.45465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn 22554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp 7381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss 0.38934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision 0.62833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall 0.35619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn 167779\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp 12478\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v12_mha_1024h\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/8df69qvg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 14 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250331_103223-8df69qvg/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train_v12_1024.py --configs_path \"./configs/v1_mha_1024_config.yaml\" \\\n",
    "    --model_path \"results/trained_models/v1_mha/v12_mha_1024.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.004518\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250402_105456-hr8eusb0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v12_mha_1024_bigbatchh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/hr8eusb0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 225.08MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.4\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/20], Loss: 0.4156, Val AUC: 0.8543, Val AP: 0.5489, Val Accuracy: 0.8345, Val F1: 0.4586, TP: 14737, TN: 160663, FP: 14497, FN: 20295\n",
      "Epoch 2/20 [Training]:   0%|       | 2/2190 [00:00<13:12,  2.76it/s, loss=0.385]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2190 that is less than the current step 2191. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [2/20], Loss: 0.3835, Val AUC: 0.8442, Val AP: 0.5336, Val Accuracy: 0.8408, Val F1: 0.4452, TP: 13431, TN: 163292, FP: 11868, FN: 21601\n",
      "No improvement in AUC. Early stop counter: 1/5\n",
      "Epoch 3/20 [Training]:   0%|       | 2/2190 [00:00<15:36,  2.34it/s, loss=0.371]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4380 that is less than the current step 4381. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [3/20], Loss: 0.3724, Val AUC: 0.8479, Val AP: 0.5459, Val Accuracy: 0.8553, Val F1: 0.4594, TP: 12922, TN: 166859, FP: 8301, FN: 22110\n",
      "No improvement in AUC. Early stop counter: 2/5\n",
      "Epoch 4/20 [Training]:   1%|      | 27/2190 [00:08<11:49,  3.05it/s, loss=0.358]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6570 that is less than the current step 6571. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [4/20], Loss: 0.3643, Val AUC: 0.8450, Val AP: 0.5454, Val Accuracy: 0.8399, Val F1: 0.4572, TP: 14174, TN: 162365, FP: 12795, FN: 20858\n",
      "No improvement in AUC. Early stop counter: 3/5\n",
      "Epoch 5/20 [Training]:   1%|      | 18/2190 [00:06<12:28,  2.90it/s, loss=0.369]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8760 that is less than the current step 8761. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [5/20], Loss: 0.3566, Val AUC: 0.8587, Val AP: 0.5646, Val Accuracy: 0.8451, Val F1: 0.4631, TP: 14042, TN: 163587, FP: 11573, FN: 20990\n",
      "Epoch 6/20 [Training]:   0%|       | 8/2190 [00:03<13:39,  2.66it/s, loss=0.342]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10950 that is less than the current step 10951. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [6/20], Loss: 0.3487, Val AUC: 0.8538, Val AP: 0.5589, Val Accuracy: 0.8505, Val F1: 0.4610, TP: 13439, TN: 165332, FP: 9828, FN: 21593\n",
      "No improvement in AUC. Early stop counter: 1/5\n",
      "Epoch 7/20 [Training]:   1%|      | 12/2190 [00:03<11:15,  3.22it/s, loss=0.346]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13140 that is less than the current step 13141. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [7/20], Loss: 0.3431, Val AUC: 0.8419, Val AP: 0.5433, Val Accuracy: 0.8447, Val F1: 0.4560, TP: 13677, TN: 163881, FP: 11279, FN: 21355\n",
      "No improvement in AUC. Early stop counter: 2/5\n",
      "Epoch 8/20 [Training]:   1%|      | 28/2190 [00:08<11:08,  3.23it/s, loss=0.341]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15330 that is less than the current step 15331. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [8/20], Loss: 0.3381, Val AUC: 0.8344, Val AP: 0.5408, Val Accuracy: 0.8409, Val F1: 0.4561, TP: 14020, TN: 162736, FP: 12424, FN: 21012\n",
      "No improvement in AUC. Early stop counter: 3/5\n",
      "Epoch 9/20 [Training]:   1%|      | 22/2190 [00:07<12:16,  2.94it/s, loss=0.326]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17520 that is less than the current step 17521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [9/20], Loss: 0.3337, Val AUC: 0.8404, Val AP: 0.5431, Val Accuracy: 0.8409, Val F1: 0.4566, TP: 14055, TN: 162688, FP: 12472, FN: 20977\n",
      "No improvement in AUC. Early stop counter: 4/5\n",
      "Epoch 10/20 [Training]:   0%|      | 4/2190 [00:01<12:40,  2.87it/s, loss=0.299]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 19710 that is less than the current step 19711. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [10/20], Loss: 0.3287, Val AUC: 0.8423, Val AP: 0.5377, Val Accuracy: 0.8214, Val F1: 0.4542, TP: 15617, TN: 157045, FP: 18115, FN: 19415\n",
      "No improvement in AUC. Early stop counter: 5/5\n",
      "Early stopping triggered.\n",
      "Best model saved with AUC: 0.8586797175874068\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.545 MB of 3.003 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.003 MB of 3.003 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.003 MB of 3.003 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.024 MB of 3.024 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.024 MB of 3.024 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.024 MB of 3.024 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▆▆▆▆▆▆▆▆▆▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss ▆▄▆▃▄▂█▄▇▂▄▁▅▄▂▃▂▄▃▂▃▁▅▂▄▆▁▃▅▃▂▁▂▂▃▃▃▅▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch █▅▅▄▃▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy ▄▅█▅▆▇▆▅▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap ▄▁▄▄█▇▃▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc ▇▄▅▄█▇▃▁▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 ▆▁▇▆█▇▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn ▃▇█▅▅▇▆▅▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp ▅▄▁▄▃▂▃▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss ▂▃▁▃▄▂▅█▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision ▃▄█▄▅▆▅▄▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall ▆▂▁▄▄▂▃▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn ▄▅█▅▆▇▆▅▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp ▆▂▁▄▄▂▃▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 0.27721\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch 0.32871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy 0.82145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap 0.53767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc 0.84234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 0.45422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn 19415\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp 18115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss 0.38791\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision 0.46297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall 0.44579\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn 157045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp 15617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v12_mha_1024_bigbatchh\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/hr8eusb0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 24 artifact file(s) and 20 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250402_105456-hr8eusb0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train_v12_1024.py --configs_path \"./configs/v1_mha_1024_config.yaml\" \\\n",
    "    --model_path \"results/trained_models/v1_mha/v12_mha_1024_bigbatch.pth\" \\\n",
    "        --dropout 0.2 \\\n",
    "            --batch_size 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with v3_mah_cross_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.004518\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250331_225906-kts8k6o2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v3_mha_cross_1024h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/kts8k6o2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 225.08MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.4\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/20], Loss: 0.6888, Val AUC: 0.6251, Val AP: 0.2674, Val Accuracy: 0.8382, Val F1: 0.1040, TP: 1973, TN: 174213, FP: 947, FN: 33059\n",
      "Epoch 2/20 [Training]:   1%|    | 195/17516 [00:09<15:00, 19.23it/s, loss=0.668]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17516 that is less than the current step 17517. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [2/20], Loss: 0.6374, Val AUC: 0.4972, Val AP: 0.2439, Val Accuracy: 0.8203, Val F1: 0.1855, TP: 4301, TN: 168109, FP: 7051, FN: 30731\n",
      "No improvement in AUC. Early stop counter: 1/8\n",
      "Epoch 3/20 [Training]:   1%|    | 183/17516 [00:09<14:31, 19.90it/s, loss=0.616]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 35032 that is less than the current step 35033. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [3/20], Loss: 0.5593, Val AUC: 0.5908, Val AP: 0.3291, Val Accuracy: 0.7395, Val F1: 0.3070, TP: 12130, TN: 143309, FP: 31851, FN: 22902\n",
      "No improvement in AUC. Early stop counter: 2/8\n",
      "Epoch 4/20 [Training]:   1%|     | 157/17516 [00:07<14:51, 19.48it/s, loss=0.63]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 52548 that is less than the current step 52549. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [4/20], Loss: 0.5525, Val AUC: 0.7073, Val AP: 0.4329, Val Accuracy: 0.8527, Val F1: 0.2527, TP: 5235, TN: 173987, FP: 1173, FN: 29797\n",
      "Epoch 5/20 [Training]:   1%|    | 125/17516 [00:07<16:21, 17.72it/s, loss=0.507]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70064 that is less than the current step 70065. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [5/20], Loss: 0.5398, Val AUC: 0.5410, Val AP: 0.3424, Val Accuracy: 0.8334, Val F1: 0.0015, TP: 26, TN: 175152, FP: 8, FN: 35006\n",
      "No improvement in AUC. Early stop counter: 1/8\n",
      "Epoch 6/20 [Training]:   1%|     | 92/17516 [00:05<16:07, 18.00it/s, loss=0.573]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 87580 that is less than the current step 87581. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [6/20], Loss: 0.5684, Val AUC: 0.5729, Val AP: 0.2933, Val Accuracy: 0.7910, Val F1: 0.3520, TP: 11934, TN: 154322, FP: 20838, FN: 23098\n",
      "No improvement in AUC. Early stop counter: 2/8\n",
      "Epoch 7/20 [Training]:   0%|     | 16/17516 [00:01<23:56, 12.18it/s, loss=0.614]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 105096 that is less than the current step 105097. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [7/20], Loss: 0.6033, Val AUC: 0.6518, Val AP: 0.2934, Val Accuracy: 0.1662, Val F1: 0.2848, TP: 34888, TN: 37, FP: 175123, FN: 144\n",
      "No improvement in AUC. Early stop counter: 3/8\n",
      "Epoch 8/20 [Training]:   0%|     | 47/17516 [00:03<19:55, 14.61it/s, loss=0.577]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 122612 that is less than the current step 122613. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [8/20], Loss: 0.6289, Val AUC: 0.6310, Val AP: 0.2704, Val Accuracy: 0.1665, Val F1: 0.2855, TP: 34994, TN: 4, FP: 175156, FN: 38\n",
      "No improvement in AUC. Early stop counter: 4/8\n",
      "Epoch 9/20 [Training]:   1%|     | 93/17516 [00:05<16:40, 17.41it/s, loss=0.638]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140128 that is less than the current step 140129. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [9/20], Loss: 0.6415, Val AUC: 0.7242, Val AP: 0.3263, Val Accuracy: 0.6169, Val F1: 0.4134, TP: 28378, TN: 101294, FP: 73866, FN: 6654\n",
      "Epoch 10/20 [Training]:   1%|   | 161/17516 [00:09<18:02, 16.02it/s, loss=0.613]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 157644 that is less than the current step 157645. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [10/20], Loss: 0.6152, Val AUC: 0.5003, Val AP: 0.2770, Val Accuracy: 0.8333, Val F1: 0.0000, TP: 0, TN: 175160, FP: 0, FN: 35032\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "No improvement in AUC. Early stop counter: 1/8\n",
      "Epoch 11/20 [Training]:   0%|     | 85/17516 [00:06<19:13, 15.11it/s, loss=0.68]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 175160 that is less than the current step 175161. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [11/20], Loss: 0.6345, Val AUC: 0.6507, Val AP: 0.3265, Val Accuracy: 0.1919, Val F1: 0.2905, TP: 34773, TN: 5562, FP: 169598, FN: 259\n",
      "No improvement in AUC. Early stop counter: 2/8\n",
      "Epoch 12/20 [Training]:   0%|    | 25/17516 [00:02<26:46, 10.89it/s, loss=0.609]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 192676 that is less than the current step 192677. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [12/20], Loss: 0.6802, Val AUC: 0.4805, Val AP: 0.2600, Val Accuracy: 0.5484, Val F1: 0.2436, TP: 15284, TN: 99992, FP: 75168, FN: 19748\n",
      "No improvement in AUC. Early stop counter: 3/8\n",
      "Epoch 13/20 [Training]:   1%|   | 185/17516 [00:09<14:44, 19.60it/s, loss=0.669]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 210192 that is less than the current step 210193. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [13/20], Loss: 0.7152, Val AUC: 0.4948, Val AP: 0.2520, Val Accuracy: 0.2728, Val F1: 0.2169, TP: 21166, TN: 36178, FP: 138982, FN: 13866\n",
      "No improvement in AUC. Early stop counter: 4/8\n",
      "Epoch 14/20 [Training]:   1%|   | 114/17516 [00:08<24:25, 11.87it/s, loss=0.725]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 227708 that is less than the current step 227709. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [14/20], Loss: 0.7051, Val AUC: 0.5040, Val AP: 0.2613, Val Accuracy: 0.7617, Val F1: 0.2680, TP: 9167, TN: 150939, FP: 24221, FN: 25865\n",
      "No improvement in AUC. Early stop counter: 5/8\n",
      "Epoch 15/20 [Training]:   0%|    | 50/17516 [00:04<25:26, 11.44it/s, loss=0.714]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 245224 that is less than the current step 245225. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [15/20], Loss: 0.7291, Val AUC: 0.4442, Val AP: 0.1907, Val Accuracy: 0.8333, Val F1: 0.0000, TP: 0, TN: 175160, FP: 0, FN: 35032\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "No improvement in AUC. Early stop counter: 6/8\n",
      "Epoch 16/20 [Training]:   0%|    | 10/17516 [00:01<41:15,  7.07it/s, loss=0.719]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 262740 that is less than the current step 262741. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [16/20], Loss: 0.6982, Val AUC: 0.4916, Val AP: 0.3007, Val Accuracy: 0.8346, Val F1: 0.0177, TP: 314, TN: 175111, FP: 49, FN: 34718\n",
      "No improvement in AUC. Early stop counter: 7/8\n",
      "Epoch 17/20 [Training]:   0%|    | 36/17516 [00:03<28:04, 10.38it/s, loss=0.631]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 280256 that is less than the current step 280257. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [17/20], Loss: 0.6456, Val AUC: 0.5095, Val AP: 0.2865, Val Accuracy: 0.8336, Val F1: 0.0042, TP: 74, TN: 175144, FP: 16, FN: 34958\n",
      "No improvement in AUC. Early stop counter: 8/8\n",
      "Early stopping triggered.\n",
      "Best model saved with AUC: 0.7241633355796294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.880 MB of 5.099 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.099 MB of 5.099 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.100 MB of 5.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.138 MB of 5.138 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.138 MB of 5.138 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.138 MB of 5.138 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss ▄▄▄▄▄▃▅▂▅▂▃▇▂▁▂▄▃▅▂▅▄▃▄▅▄▄▃▄▆▄▄▅▅▄▂▆▄█▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch ▇▅▂▁▁▂▃▄▅▄▅▆▇▇█▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy ██▇██▇▁▁▆█▁▅▂▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap ▃▃▅█▅▄▄▃▅▃▅▃▃▃▁▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc ▆▂▅█▃▄▆▆█▂▆▂▂▂▁▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 ▃▄▆▅▁▇▆▆█▁▆▅▅▆▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn █▇▆▇█▆▁▁▂█▁▅▄▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp ▁▁▂▁▁▂██▄▁█▄▇▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss ▂▂▂▁▁▂▆█▂▁▃▂▃▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision ▆▄▃█▇▄▂▂▃▁▂▂▂▃▁██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall ▁▂▃▂▁▃██▇▁█▄▅▃▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn ██▇██▇▁▁▅█▁▅▂▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp ▁▂▃▂▁▃██▇▁█▄▅▃▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 0.64637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch 0.64558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy 0.83361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap 0.28648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc 0.50949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 0.00421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn 34958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss 0.55394\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision 0.82222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall 0.00211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn 175144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp 74\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v3_mha_cross_1024h\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/kts8k6o2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 37 artifact file(s) and 34 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250331_225906-kts8k6o2/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train_v3_mha_cross_1024.py --configs_path \"./configs/v1_mha_1024_config.yaml\" \\\n",
    "    --model_path \"results/trained_models/v3/v3_mha_cross_1024.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v3 after masking corrections in cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Learning rate: 0.004518\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250401_092239-9c2k1ca4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v3_mha_cross_1024h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/9c2k1ca4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 225.08MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.5\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/20], Loss: 0.4204, Val AUC: 0.8449, Val AP: 0.5268, Val Accuracy: 0.8117, Val F1: 0.4551, TP: 16527, TN: 154084, FP: 21076, FN: 18505\n",
      "Epoch 2/20 [Training]:   1%|    | 170/17516 [00:07<13:04, 22.10it/s, loss=0.392]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17516 that is less than the current step 17517. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [2/20], Loss: 0.3943, Val AUC: 0.8366, Val AP: 0.5313, Val Accuracy: 0.8443, Val F1: 0.4575, TP: 13799, TN: 163662, FP: 11498, FN: 21233\n",
      "No improvement in AUC. Early stop counter: 1/8\n",
      "Epoch 3/20 [Training]:   0%|      | 4/17516 [00:00<30:27,  9.58it/s, loss=0.606]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 35032 that is less than the current step 35033. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [3/20], Loss: 0.3873, Val AUC: 0.8219, Val AP: 0.5246, Val Accuracy: 0.8141, Val F1: 0.4497, TP: 15966, TN: 155158, FP: 20002, FN: 19066\n",
      "No improvement in AUC. Early stop counter: 2/8\n",
      "Epoch 4/20 [Training]:   1%|    | 140/17516 [00:06<13:28, 21.48it/s, loss=0.405]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 52548 that is less than the current step 52549. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [4/20], Loss: 0.3852, Val AUC: 0.8441, Val AP: 0.5347, Val Accuracy: 0.8536, Val F1: 0.4437, TP: 12273, TN: 167149, FP: 8011, FN: 22759\n",
      "No improvement in AUC. Early stop counter: 3/8\n",
      "Epoch 5/20 [Training]:   1%|    | 123/17516 [00:06<14:22, 20.16it/s, loss=0.347]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70064 that is less than the current step 70065. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [5/20], Loss: 0.3848, Val AUC: 0.8457, Val AP: 0.5361, Val Accuracy: 0.8334, Val F1: 0.4485, TP: 14241, TN: 160932, FP: 14228, FN: 20791\n",
      "Epoch 6/20 [Training]:   1%|    | 133/17516 [00:07<14:05, 20.57it/s, loss=0.382]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 87580 that is less than the current step 87581. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [6/20], Loss: 0.3843, Val AUC: 0.8389, Val AP: 0.5210, Val Accuracy: 0.8121, Val F1: 0.4419, TP: 15637, TN: 155062, FP: 20098, FN: 19395\n",
      "No improvement in AUC. Early stop counter: 1/8\n",
      "Epoch 7/20 [Training]:   1%|    | 196/17516 [00:09<14:56, 19.32it/s, loss=0.377]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 105096 that is less than the current step 105097. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [7/20], Loss: 0.3849, Val AUC: 0.8423, Val AP: 0.5401, Val Accuracy: 0.8169, Val F1: 0.4523, TP: 15888, TN: 155819, FP: 19341, FN: 19144\n",
      "No improvement in AUC. Early stop counter: 2/8\n",
      "Epoch 8/20 [Training]:   0%|     | 17/17516 [00:00<13:55, 20.94it/s, loss=0.387]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 122612 that is less than the current step 122613. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [8/20], Loss: 0.3854, Val AUC: 0.8483, Val AP: 0.5383, Val Accuracy: 0.8085, Val F1: 0.4525, TP: 16637, TN: 153300, FP: 21860, FN: 18395\n",
      "Epoch 9/20 [Training]:   0%|      | 3/17516 [00:00<36:50,  7.92it/s, loss=0.396]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140128 that is less than the current step 140129. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [9/20], Loss: 0.3857, Val AUC: 0.8407, Val AP: 0.5388, Val Accuracy: 0.8636, Val F1: 0.4266, TP: 10663, TN: 170868, FP: 4292, FN: 24369\n",
      "No improvement in AUC. Early stop counter: 1/8\n",
      "Epoch 10/20 [Training]:   1%|   | 191/17516 [00:09<13:56, 20.72it/s, loss=0.394]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 157644 that is less than the current step 157645. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [10/20], Loss: 0.3832, Val AUC: 0.8477, Val AP: 0.5468, Val Accuracy: 0.8510, Val F1: 0.4655, TP: 13636, TN: 165236, FP: 9924, FN: 21396\n",
      "No improvement in AUC. Early stop counter: 2/8\n",
      "Epoch 11/20 [Training]:   1%|   | 181/17516 [00:09<14:26, 20.01it/s, loss=0.403]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 175160 that is less than the current step 175161. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [11/20], Loss: 0.3852, Val AUC: 0.8170, Val AP: 0.5211, Val Accuracy: 0.8565, Val F1: 0.4674, TP: 13242, TN: 166777, FP: 8383, FN: 21790\n",
      "No improvement in AUC. Early stop counter: 3/8\n",
      "Epoch 12/20 [Training]:   1%|    | 89/17516 [00:04<13:56, 20.82it/s, loss=0.399]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 192676 that is less than the current step 192677. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [12/20], Loss: 0.3860, Val AUC: 0.8638, Val AP: 0.5619, Val Accuracy: 0.8521, Val F1: 0.4643, TP: 13470, TN: 165637, FP: 9523, FN: 21562\n",
      "Epoch 13/20 [Training]:   0%|    | 58/17516 [00:04<19:45, 14.73it/s, loss=0.416]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 210192 that is less than the current step 210193. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [13/20], Loss: 0.3842, Val AUC: 0.8410, Val AP: 0.5295, Val Accuracy: 0.8559, Val F1: 0.4551, TP: 12646, TN: 167260, FP: 7900, FN: 22386\n",
      "No improvement in AUC. Early stop counter: 1/8\n",
      "Epoch 14/20 [Training]:   1%|   | 134/17516 [00:09<18:43, 15.47it/s, loss=0.391]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 227708 that is less than the current step 227709. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [14/20], Loss: 0.3860, Val AUC: 0.8444, Val AP: 0.5311, Val Accuracy: 0.8529, Val F1: 0.4256, TP: 11455, TN: 167813, FP: 7347, FN: 23577\n",
      "No improvement in AUC. Early stop counter: 2/8\n",
      "Epoch 15/20 [Training]:   0%|      | 38/17516 [00:02<18:16, 15.94it/s, loss=0.4]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 245224 that is less than the current step 245225. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [15/20], Loss: 0.3899, Val AUC: 0.8136, Val AP: 0.5125, Val Accuracy: 0.8521, Val F1: 0.4480, TP: 12611, TN: 166502, FP: 8658, FN: 22421\n",
      "No improvement in AUC. Early stop counter: 3/8\n",
      "Epoch 16/20 [Training]:   1%|   | 115/17516 [00:08<18:20, 15.81it/s, loss=0.393]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 262740 that is less than the current step 262741. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [16/20], Loss: 0.3860, Val AUC: 0.8516, Val AP: 0.5389, Val Accuracy: 0.8605, Val F1: 0.4245, TP: 10818, TN: 170045, FP: 5115, FN: 24214\n",
      "No improvement in AUC. Early stop counter: 4/8\n",
      "Epoch 17/20 [Training]:   0%|    | 34/17516 [00:01<16:29, 17.67it/s, loss=0.359]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 280256 that is less than the current step 280257. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [17/20], Loss: 0.3877, Val AUC: 0.8498, Val AP: 0.5447, Val Accuracy: 0.8394, Val F1: 0.4566, TP: 14181, TN: 162252, FP: 12908, FN: 20851\n",
      "No improvement in AUC. Early stop counter: 5/8\n",
      "Epoch 18/20 [Training]:   1%|    | 97/17516 [00:06<17:29, 16.60it/s, loss=0.363]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 297772 that is less than the current step 297773. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [18/20], Loss: 0.3892, Val AUC: 0.8403, Val AP: 0.5364, Val Accuracy: 0.8634, Val F1: 0.4381, TP: 11196, TN: 170276, FP: 4884, FN: 23836\n",
      "No improvement in AUC. Early stop counter: 6/8\n",
      "Epoch 19/20 [Training]:   1%|   | 144/17516 [00:08<17:13, 16.80it/s, loss=0.389]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 315288 that is less than the current step 315289. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [19/20], Loss: 0.3872, Val AUC: 0.8102, Val AP: 0.4981, Val Accuracy: 0.8109, Val F1: 0.4369, TP: 15423, TN: 155020, FP: 20140, FN: 19609\n",
      "No improvement in AUC. Early stop counter: 7/8\n",
      "Epoch 20/20 [Training]:   0%|      | 66/17516 [00:03<17:07, 16.99it/s, loss=0.4]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 332804 that is less than the current step 332805. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [20/20], Loss: 0.3927, Val AUC: 0.8521, Val AP: 0.5460, Val Accuracy: 0.8322, Val F1: 0.4480, TP: 14310, TN: 160618, FP: 14542, FN: 20722\n",
      "No improvement in AUC. Early stop counter: 8/8\n",
      "Early stopping triggered.\n",
      "Best model saved with AUC: 0.8638180182770683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.814 MB of 5.127 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.627 MB of 5.127 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.127 MB of 5.127 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.127 MB of 5.127 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.127 MB of 5.127 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.127 MB of 5.127 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss ▂▄█▂▁▁▂▂▃▃▄▁█▄▂▅▃▂▄▁▂▂▃▁▄▂▄▁▃▃▂▃▁▃▄▄▂▅▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch █▃▂▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy ▁▆▂▇▄▁▂▁█▆▇▇▇▇▇█▅█▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap ▄▅▄▅▅▄▆▅▅▆▄█▄▅▃▅▆▅▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc ▆▄▃▅▆▅▅▆▅▆▂█▅▅▁▆▆▅▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 ▆▆▅▄▅▄▆▆▁██▇▆▁▅▁▆▃▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn ▁▄▂▆▄▂▂▁█▅▅▅▆▇▆█▄▇▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp █▄▇▂▅▇▇█▁▃▃▃▂▂▃▁▄▁▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss ▅▃▅▂▂▅▄▄▅▃▅▁▃▃▆▃▃▄█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision ▁▄▁▅▃▁▁▁█▅▅▅▆▅▅▇▃█▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall █▅▇▃▅▇▇█▁▄▄▄▃▂▃▁▅▂▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn ▁▅▂▇▄▂▂▁█▆▆▆▇▇▆█▅█▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp █▅▇▃▅▇▇█▁▄▄▄▃▂▃▁▅▂▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 0.23513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch 0.3927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy 0.83223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap 0.54598\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc 0.85206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 0.448\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn 20722\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp 14542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss 0.33929\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision 0.49598\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall 0.40848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn 160618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp 14310\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v3_mha_cross_1024h\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/9c2k1ca4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 44 artifact file(s) and 40 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250401_092239-9c2k1ca4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train_v3_mha_cross_1024.py --configs_path \"./configs/v1_mha_1024_config.yaml\" \\\n",
    "    --model_path \"results/trained_models/v3/v3_mha_cross_1024.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1_mah_1024 trained on 31.03. at 7:09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: ../../data/splitted_datasets/allele/beta/test.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250401_224354-3tn3d9sl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v1_mha_1024h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/3tn3d9sl\u001b[0m\n",
      "Loading embeddings...\n",
      "tcr_test  ../../data/embeddings/beta/allele/dimension_1024/padded_test_tcr_embeddings_final.h5\n",
      "epi_test  ../../data/embeddings/beta/allele/dimension_1024/padded_test_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Loading model from: results/trained_models/v1_mha/v1_mha_1024.pth\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v1_1024.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "\n",
      "Starting testing phase...\n",
      "Testing: 100%|██████████████████████████████| 3338/3338 [01:55<00:00, 28.85it/s]\n",
      "Test Results - AUC: 0.8084, Accuracy: 0.8334, AP: 0.4629, F1: 0.4289, TP: 3341, TN: 41163, FP: 3337, FN: 5559\n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v1_mha_1024h\u001b[0m at: \u001b[34mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/3tn3d9sl\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250401_224354-3tn3d9sl/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v1_1024.py --model_path results/trained_models/v1_mha/v1_mha_1024.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v12_mha_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: ../../data/splitted_datasets/allele/beta/test.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250401_225622-reme12d6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v12_mha_1024h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/reme12d6\u001b[0m\n",
      "Loading embeddings...\n",
      "tcr_test  ../../data/embeddings/beta/allele/dimension_1024/padded_test_tcr_embeddings_final.h5\n",
      "epi_test  ../../data/embeddings/beta/allele/dimension_1024/padded_test_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Loading model from: results/trained_models/v1_mha/v12_mha_1024.pth\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v12_1024.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "\n",
      "Starting testing phase...\n",
      "Testing: 100%|██████████████████████████████| 3338/3338 [02:05<00:00, 26.56it/s]\n",
      "Test Results - AUC: 0.8362, Accuracy: 0.8414, AP: 0.4480, F1: 0.4320, TP: 3221, TN: 41708, FP: 2792, FN: 5679\n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v12_mha_1024h\u001b[0m at: \u001b[34mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/reme12d6\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250401_225622-reme12d6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v12_1024.py --model_path results/trained_models/v1_mha/v12_mha_1024.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v13_mha_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: ../../data/splitted_datasets/allele/beta/test.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250401_230237-cygak42g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v13_mha_1024h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cygak42g\u001b[0m\n",
      "Loading embeddings...\n",
      "tcr_test  ../../data/embeddings/beta/allele/dimension_1024/padded_test_tcr_embeddings_final.h5\n",
      "epi_test  ../../data/embeddings/beta/allele/dimension_1024/padded_test_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Loading model from: results/trained_models/v1_mha/v13_mha_1024.pth\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v13_1024.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "\n",
      "Starting testing phase...\n",
      "Testing: 100%|██████████████████████████████| 3338/3338 [01:59<00:00, 27.96it/s]\n",
      "Test Results - AUC: 0.8233, Accuracy: 0.8317, AP: 0.4716, F1: 0.4711, TP: 4003, TN: 40408, FP: 4092, FN: 4897\n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v13_mha_1024h\u001b[0m at: \u001b[34mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/cygak42g\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250401_230237-cygak42g/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v13_1024.py --model_path results/trained_models/v1_mha/v13_mha_1024.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v3_mha_cross_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: ../../data/splitted_datasets/allele/beta/test.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250401_231249-a60srt4r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v3_mha_cross_1024\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/a60srt4r\u001b[0m\n",
      "Loading embeddings...\n",
      "tcr_test  ../../data/embeddings/beta/allele/dimension_1024/padded_test_tcr_embeddings_final.h5\n",
      "epi_test  ../../data/embeddings/beta/allele/dimension_1024/padded_test_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Loading model from: results/trained_models/v3/v3_mha_cross_1024.pth\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v3_mha_cross_1024.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "\n",
      "Starting testing phase...\n",
      "Testing: 100%|██████████████████████████████| 3338/3338 [02:10<00:00, 25.63it/s]\n",
      "Test Results - AUC: 0.8128, Accuracy: 0.8184, AP: 0.4469, F1: 0.4355, TP: 3739, TN: 39966, FP: 4534, FN: 5161\n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v3_mha_cross_1024\u001b[0m at: \u001b[34mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/a60srt4r\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250401_231249-a60srt4r/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v3_mha_cross_1024.py --model_path results/trained_models/v3/v3_mha_cross_1024.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v3_cross  (no mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 256\n",
      "Learning rate: 0.0009\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250403_093633-bmbashnb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v32_cross_1024\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/bmbashnb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 225.08MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.5\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "Epoch [1/20], Loss: 0.4214, Val AUC: 0.8361, Val AP: 0.5349, Val Accuracy: 0.8263, Val F1: 0.4534, TP: 15141, TN: 158547, FP: 16613, FN: 19891\n",
      "Epoch [2/20], Loss: 0.3659, Val AUC: 0.8460, Val AP: 0.5438, Val Accuracy: 0.8415, Val F1: 0.4631, TP: 14367, TN: 162510, FP: 12650, FN: 20665\n",
      "Epoch [3/20], Loss: 0.3539, Val AUC: 0.8330, Val AP: 0.5283, Val Accuracy: 0.8488, Val F1: 0.4617, TP: 13631, TN: 164776, FP: 10384, FN: 21401\n",
      "No improvement in AUC. Early stop counter: 1/4\n",
      "Epoch [4/20], Loss: 0.3461, Val AUC: 0.8566, Val AP: 0.5673, Val Accuracy: 0.8413, Val F1: 0.4792, TP: 15349, TN: 161477, FP: 13683, FN: 19683\n",
      "Epoch [5/20], Loss: 0.3414, Val AUC: 0.8391, Val AP: 0.5367, Val Accuracy: 0.8507, Val F1: 0.4614, TP: 13443, TN: 165369, FP: 9791, FN: 21589\n",
      "No improvement in AUC. Early stop counter: 1/4\n",
      "Epoch [6/20], Loss: 0.3375, Val AUC: 0.8437, Val AP: 0.5509, Val Accuracy: 0.8333, Val F1: 0.4634, TP: 15129, TN: 160028, FP: 15132, FN: 19903\n",
      "No improvement in AUC. Early stop counter: 2/4\n",
      "Epoch [7/20], Loss: 0.3358, Val AUC: 0.8413, Val AP: 0.5544, Val Accuracy: 0.8488, Val F1: 0.4707, TP: 14134, TN: 164268, FP: 10892, FN: 20898\n",
      "No improvement in AUC. Early stop counter: 3/4\n",
      "Epoch [8/20], Loss: 0.3349, Val AUC: 0.8493, Val AP: 0.5582, Val Accuracy: 0.8331, Val F1: 0.4642, TP: 15200, TN: 159907, FP: 15253, FN: 19832\n",
      "No improvement in AUC. Early stop counter: 4/4\n",
      "Early stopping triggered.\n",
      "Best model saved with AUC: 0.8566271592465932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.609 MB of 6.827 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.391 MB of 6.827 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 6.827 MB of 6.827 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 6.827 MB of 6.827 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 6.887 MB of 6.887 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 6.887 MB of 6.887 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 6.887 MB of 6.887 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss █▅▃▄▂▃▃▄▄▃▃▃▂▄▄▃▂▂▃▂▃▃▂▅▂▃▃▃▁▂▃▁▃▂▂▁▂▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch █▄▃▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy ▁▅▇▅█▃▇▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap ▂▄▁█▃▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc ▂▅▁█▃▄▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 ▁▄▃█▃▄▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn ▂▅▇▁█▂▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp █▄▂▅▁▆▂▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss ▆▁▆▁█▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision ▁▅▇▅█▃▇▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall ▇▄▂█▁▇▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn ▁▅▇▄█▃▇▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp ▇▄▂█▁▇▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 0.40655\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch 0.33493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy 0.83308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap 0.55816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc 0.84932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 0.46423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn 19832\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp 15253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss 0.34224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision 0.49913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall 0.43389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn 159907\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp 15200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v32_cross_1024\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/bmbashnb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 16 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250403_093633-bmbashnb/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train_v32_cross_1024.py --configs_path \"./configs/v32_cross_1024_config.yaml\" \\\n",
    "    --model_path results/trained_models/v3/v32_cross_1024.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 1024_bb mit th 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.004518\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250407_101537-yl8xyjza\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v1_th045h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/yl8xyjza\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 314.17MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.5\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/35 [Training]:  18%|▉    | 400/2190 [02:12<09:59,  2.99it/s, loss=0.948]"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train-server2-bb-th045.py --configs_path \"./configs/v1_mha_1024_config-Copy1.yaml\" \\\n",
    "    --model_path results/trained_models/v1_mha/v1_th045.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 1024_bb mit th 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python models_scripts/v1_mha/train-server2-bb-th066.py --configs_path \"./configs/v1_mha_1024_config-Copy1.yaml\" \\\n",
    "    --model_path results/trained_models/v1_mha/v1_th066.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training mit FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.004518\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250408_134256-vrnqve9n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v1_flh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/vrnqve9n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 314.17MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.5\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch [1/35], Train Loss: 0.0911, Val Loss: 0.0966, Val AUC: 0.7782, Val AP: 0.4312, Val Accuracy: 0.7000, Val F1: 0.4094, TP: 21851, TN: 125287, FP: 49873, FN: 13181\n",
      "Epoch 2/35 [Training]:   1%|     | 18/2190 [00:07<11:54,  3.04it/s, loss=0.0986]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2190 that is less than the current step 2192. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2191 that is less than the current step 2192. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [2/35], Train Loss: 0.0996, Val Loss: 0.0792, Val AUC: 0.7672, Val AP: 0.4526, Val Accuracy: 0.8205, Val F1: 0.4237, TP: 13871, TN: 158595, FP: 16565, FN: 21161\n",
      "Epoch 3/35 [Training]:   0%|      | 3/2190 [00:01<12:16,  2.97it/s, loss=0.0914]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4380 that is less than the current step 4382. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4381 that is less than the current step 4382. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [3/35], Train Loss: 0.0892, Val Loss: 0.2593, Val AUC: 0.6344, Val AP: 0.3876, Val Accuracy: 0.8511, Val F1: 0.2592, TP: 5478, TN: 173409, FP: 1751, FN: 29554\n",
      "No improvement in AP. Early stop counter: 1/4\n",
      "Epoch 4/35 [Training]:   0%|       | 7/2190 [00:02<11:46,  3.09it/s, loss=0.102]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6570 that is less than the current step 6572. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6571 that is less than the current step 6572. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [4/35], Train Loss: 0.0856, Val Loss: 0.0799, Val AUC: 0.8089, Val AP: 0.5013, Val Accuracy: 0.8593, Val F1: 0.4349, TP: 11380, TN: 169233, FP: 5927, FN: 23652\n",
      "Epoch 5/35 [Training]:   1%|     | 11/2190 [00:03<11:43,  3.10it/s, loss=0.0817]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8760 that is less than the current step 8762. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8761 that is less than the current step 8762. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [5/35], Train Loss: 0.0825, Val Loss: 0.0930, Val AUC: 0.7843, Val AP: 0.4760, Val Accuracy: 0.8333, Val F1: 0.4309, TP: 13267, TN: 161888, FP: 13272, FN: 21765\n",
      "No improvement in AP. Early stop counter: 1/4\n",
      "Epoch 6/35 [Training]:   0%|      | 7/2190 [00:02<12:01,  3.03it/s, loss=0.0763]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10950 that is less than the current step 10952. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10951 that is less than the current step 10952. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [6/35], Train Loss: 0.0826, Val Loss: 0.0804, Val AUC: 0.7418, Val AP: 0.4589, Val Accuracy: 0.8590, Val F1: 0.4338, TP: 11353, TN: 169208, FP: 5952, FN: 23679\n",
      "No improvement in AP. Early stop counter: 2/4\n",
      "Epoch 7/35 [Training]:   1%|     | 20/2190 [00:06<11:42,  3.09it/s, loss=0.0812]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13140 that is less than the current step 13142. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13141 that is less than the current step 13142. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [7/35], Train Loss: 0.0803, Val Loss: 0.0797, Val AUC: 0.7959, Val AP: 0.4770, Val Accuracy: 0.8462, Val F1: 0.3987, TP: 10718, TN: 167145, FP: 8015, FN: 24314\n",
      "No improvement in AP. Early stop counter: 3/4\n",
      "Epoch 8/35 [Training]:   0%|      | 5/2190 [00:01<12:19,  2.95it/s, loss=0.0749]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15330 that is less than the current step 15332. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15331 that is less than the current step 15332. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Epoch [8/35], Train Loss: 0.0800, Val Loss: 0.0943, Val AUC: 0.7306, Val AP: 0.4510, Val Accuracy: 0.8570, Val F1: 0.4404, TP: 11822, TN: 168323, FP: 6837, FN: 23210\n",
      "No improvement in AP. Early stop counter: 4/4\n",
      "Early stopping triggered.\n",
      "Best model saved with AP: 0.5013314862221576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.459 MB of 3.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.521 MB of 3.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.100 MB of 3.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.100 MB of 3.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.130 MB of 3.130 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.130 MB of 3.130 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.130 MB of 3.130 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    learning_rate ██▄▄▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss ▂█▇▄▃▃▂▅▃▆▅▅▄▂▂▄▃▂▄▅▂▂▁▃▃▃▂▄▄▄▂▂▃▂▅▃▂▂▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch ▅█▄▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy ▁▆██▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap ▄▅▁█▆▅▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc ▇▆▁█▇▅▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 ▇▇▁███▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn ▁▄█▅▅▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp █▃▁▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss ▂▁█▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision ▁▃█▆▄▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall █▅▁▄▄▄▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn ▁▆█▇▆▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp █▅▁▄▄▄▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    learning_rate 0.00056\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 0.0899\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch 0.07997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy 0.85705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_ap 0.45097\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_auc 0.73057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_f1 0.44037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fn 23210\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_fp 6837\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss 0.09431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_precision 0.63358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_recall 0.33746\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tn 168323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_tp 11822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v1_flh\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/vrnqve9n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 21 artifact file(s) and 16 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250408_134256-vrnqve9n/logs\u001b[0m\n",
      "Best Hyperparameters:\n",
      "<wandb.sdk.lib.preinit.PreInitObject object at 0x7a519594fa70>\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train-server1-bb-fl.py --configs_path \"./configs/v1_mha_1024_config-Copy1.yaml\" \\\n",
    "    --model_path results/trained_models/v1_mha/v1_fl.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making val-TPP visible in wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.004518\n",
      "train_path: ./dummy_data/subset_train.tsv\n",
      "val_path: ./dummy_data/subset_validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250417_095749-8iuf67de\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfixing_TPP_in_val\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/8iuf67de\u001b[0m\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/3 [Training]:   0%|                              | 0/27 [00:00<?, ?it/s][DEBUG] Logging 1 to step 0\n",
      "Epoch 1/3 [Training]:   4%|▍          | 1/27 [00:02<00:58,  2.26s/it, loss=2.25][DEBUG] Logging 1 to step 1\n",
      "Epoch 1/3 [Training]:   7%|▊          | 2/27 [00:03<00:37,  1.51s/it, loss=1.98][DEBUG] Logging 1 to step 2\n",
      "Epoch 1/3 [Training]:  11%|█▏         | 3/27 [00:04<00:29,  1.22s/it, loss=1.83][DEBUG] Logging 1 to step 3\n",
      "Epoch 1/3 [Training]:  15%|█▊          | 4/27 [00:04<00:24,  1.08s/it, loss=1.7][DEBUG] Logging 1 to step 4\n",
      "Epoch 1/3 [Training]:  19%|██         | 5/27 [00:05<00:21,  1.04it/s, loss=1.62][DEBUG] Logging 1 to step 5\n",
      "Epoch 1/3 [Training]:  22%|██▍        | 6/27 [00:06<00:18,  1.15it/s, loss=1.57][DEBUG] Logging 1 to step 6\n",
      "Epoch 1/3 [Training]:  26%|██▊        | 7/27 [00:07<00:16,  1.23it/s, loss=1.51][DEBUG] Logging 1 to step 7\n",
      "Epoch 1/3 [Training]:  30%|███▎       | 8/27 [00:07<00:14,  1.29it/s, loss=1.48][DEBUG] Logging 1 to step 8\n",
      "Epoch 1/3 [Training]:  33%|███▋       | 9/27 [00:08<00:13,  1.34it/s, loss=1.45][DEBUG] Logging 1 to step 9\n",
      "Epoch 1/3 [Training]:  37%|███▋      | 10/27 [00:09<00:12,  1.37it/s, loss=1.42][DEBUG] Logging 1 to step 10\n",
      "Epoch 1/3 [Training]:  41%|████      | 11/27 [00:09<00:11,  1.41it/s, loss=1.38][DEBUG] Logging 1 to step 11\n",
      "Epoch 1/3 [Training]:  44%|████▍     | 12/27 [00:10<00:10,  1.43it/s, loss=1.37][DEBUG] Logging 1 to step 12\n",
      "Epoch 1/3 [Training]:  48%|████▊     | 13/27 [00:11<00:09,  1.43it/s, loss=1.35][DEBUG] Logging 1 to step 13\n",
      "Epoch 1/3 [Training]:  52%|█████▏    | 14/27 [00:11<00:08,  1.45it/s, loss=1.34][DEBUG] Logging 1 to step 14\n",
      "Epoch 1/3 [Training]:  56%|█████▌    | 15/27 [00:12<00:08,  1.46it/s, loss=1.33][DEBUG] Logging 1 to step 15\n",
      "Epoch 1/3 [Training]:  59%|█████▉    | 16/27 [00:13<00:07,  1.45it/s, loss=1.32][DEBUG] Logging 1 to step 16\n",
      "Epoch 1/3 [Training]:  63%|██████▉    | 17/27 [00:13<00:06,  1.46it/s, loss=1.3][DEBUG] Logging 1 to step 17\n",
      "Epoch 1/3 [Training]:  67%|██████▋   | 18/27 [00:14<00:06,  1.47it/s, loss=1.29][DEBUG] Logging 1 to step 18\n",
      "Epoch 1/3 [Training]:  70%|███████   | 19/27 [00:15<00:05,  1.39it/s, loss=1.28][DEBUG] Logging 1 to step 19\n",
      "Epoch 1/3 [Training]:  74%|███████▍  | 20/27 [00:16<00:04,  1.43it/s, loss=1.27][DEBUG] Logging 1 to step 20\n",
      "Epoch 1/3 [Training]:  78%|███████▊  | 21/27 [00:16<00:04,  1.43it/s, loss=1.25][DEBUG] Logging 1 to step 21\n",
      "Epoch 1/3 [Training]:  81%|████████▏ | 22/27 [00:17<00:03,  1.44it/s, loss=1.24][DEBUG] Logging 1 to step 22\n",
      "Epoch 1/3 [Training]:  85%|████████▌ | 23/27 [00:18<00:02,  1.43it/s, loss=1.24][DEBUG] Logging 1 to step 23\n",
      "Epoch 1/3 [Training]:  89%|████████▉ | 24/27 [00:18<00:02,  1.44it/s, loss=1.23][DEBUG] Logging 1 to step 24\n",
      "Epoch 1/3 [Training]:  93%|█████████▎| 25/27 [00:19<00:01,  1.44it/s, loss=1.22][DEBUG] Logging 1 to step 25\n",
      "Epoch 1/3 [Training]:  96%|█████████▋| 26/27 [00:20<00:00,  1.43it/s, loss=1.21][DEBUG] Logging 1 to step 26\n",
      "Best threshold (by F1): 0.7032 with F1: 0.3005                                  \n",
      "[DEBUG] Logging 2 to step 27\n",
      "[DEBUG] Logging 3 to step 27\n",
      "Epoch [1/3], Train Loss: 1.2039, Val Loss: 2.6283, Val AUC: 0.4721, Val AP: 0.2961, Val Accuracy: 0.8048, Val F1: 0.2999, TP: 418, TN: 7630, FP: 783, FN: 1169\n",
      "[DEBUG] Logging 4 to step 27\n",
      "[DEBUG] Logging 5 to step 27\n",
      "[DEBUG] Logging 6 to step 27\n",
      "[DEBUG] Logging 7 to step 27\n",
      "[DEBUG] Logging 6 to step 27\n",
      "[DEBUG] Logging 7 to step 27\n",
      "[DEBUG] Logging 6 to step 27\n",
      "[DEBUG] Logging 7 to step 27\n",
      "[DEBUG] Logging 6 to step 27\n",
      "[DEBUG] Logging 7 to step 27\n",
      "Epoch 2/3 [Training]:   0%|                              | 0/27 [00:00<?, ?it/s][DEBUG] Logging 1 to step 27\n",
      "Epoch 2/3 [Training]:   4%|▍          | 1/27 [00:00<00:22,  1.16it/s, loss=1.15][DEBUG] Logging 1 to step 28\n",
      "Epoch 2/3 [Training]:   7%|▉           | 2/27 [00:01<00:18,  1.35it/s, loss=1.1][DEBUG] Logging 1 to step 29\n",
      "Epoch 2/3 [Training]:  11%|█▏         | 3/27 [00:02<00:16,  1.41it/s, loss=1.19][DEBUG] Logging 1 to step 30\n",
      "Epoch 2/3 [Training]:  15%|█▊          | 4/27 [00:02<00:15,  1.47it/s, loss=1.2][DEBUG] Logging 1 to step 31\n",
      "Epoch 2/3 [Training]:  19%|██         | 5/27 [00:03<00:14,  1.50it/s, loss=1.19][DEBUG] Logging 1 to step 32\n",
      "Epoch 2/3 [Training]:  22%|██▍        | 6/27 [00:04<00:14,  1.49it/s, loss=1.15][DEBUG] Logging 1 to step 33\n",
      "Epoch 2/3 [Training]:  26%|██▊        | 7/27 [00:04<00:13,  1.49it/s, loss=1.14][DEBUG] Logging 1 to step 34\n",
      "Epoch 2/3 [Training]:  30%|███▎       | 8/27 [00:05<00:12,  1.50it/s, loss=1.11][DEBUG] Logging 1 to step 35\n",
      "Epoch 2/3 [Training]:  33%|███▋       | 9/27 [00:06<00:11,  1.53it/s, loss=1.08][DEBUG] Logging 1 to step 36\n",
      "Epoch 2/3 [Training]:  37%|███▋      | 10/27 [00:06<00:11,  1.54it/s, loss=1.08][DEBUG] Logging 1 to step 37\n",
      "Epoch 2/3 [Training]:  41%|████      | 11/27 [00:07<00:10,  1.54it/s, loss=1.07][DEBUG] Logging 1 to step 38\n",
      "Epoch 2/3 [Training]:  44%|████▍     | 12/27 [00:08<00:09,  1.55it/s, loss=1.06][DEBUG] Logging 1 to step 39\n",
      "Epoch 2/3 [Training]:  48%|████▊     | 13/27 [00:08<00:09,  1.52it/s, loss=1.04][DEBUG] Logging 1 to step 40\n",
      "Epoch 2/3 [Training]:  52%|█████▏    | 14/27 [00:09<00:08,  1.52it/s, loss=1.03][DEBUG] Logging 1 to step 41\n",
      "Epoch 2/3 [Training]:  56%|█████▌    | 15/27 [00:10<00:07,  1.52it/s, loss=1.04][DEBUG] Logging 1 to step 42\n",
      "Epoch 2/3 [Training]:  59%|█████▉    | 16/27 [00:10<00:07,  1.49it/s, loss=1.03][DEBUG] Logging 1 to step 43\n",
      "Epoch 2/3 [Training]:  63%|██████▎   | 17/27 [00:11<00:06,  1.47it/s, loss=1.01][DEBUG] Logging 1 to step 44\n",
      "Epoch 2/3 [Training]:  67%|██████▋   | 18/27 [00:12<00:06,  1.46it/s, loss=1.02][DEBUG] Logging 1 to step 45\n",
      "Epoch 2/3 [Training]:  70%|███████   | 19/27 [00:12<00:05,  1.46it/s, loss=1.02][DEBUG] Logging 1 to step 46\n",
      "Epoch 2/3 [Training]:  74%|███████▍  | 20/27 [00:13<00:04,  1.45it/s, loss=1.01][DEBUG] Logging 1 to step 47\n",
      "Epoch 2/3 [Training]:  78%|██████████   | 21/27 [00:14<00:04,  1.38it/s, loss=1][DEBUG] Logging 1 to step 48\n",
      "Epoch 2/3 [Training]:  81%|███████▎ | 22/27 [00:15<00:03,  1.38it/s, loss=0.998][DEBUG] Logging 1 to step 49\n",
      "Epoch 2/3 [Training]:  85%|███████▋ | 23/27 [00:15<00:02,  1.39it/s, loss=0.993][DEBUG] Logging 1 to step 50\n",
      "Epoch 2/3 [Training]:  89%|████████ | 24/27 [00:16<00:02,  1.39it/s, loss=0.996][DEBUG] Logging 1 to step 51\n",
      "Epoch 2/3 [Training]:  93%|████████▎| 25/27 [00:17<00:01,  1.41it/s, loss=0.994][DEBUG] Logging 1 to step 52\n",
      "Epoch 2/3 [Training]:  96%|████████▋| 26/27 [00:17<00:00,  1.42it/s, loss=0.991][DEBUG] Logging 1 to step 53\n",
      "Best threshold (by F1): 0.4673 with F1: 0.3095                                  \n",
      "[DEBUG] Logging 2 to step 54\n",
      "[DEBUG] Logging 3 to step 54\n",
      "Epoch [2/3], Train Loss: 0.9916, Val Loss: 1.7433, Val AUC: 0.5178, Val AP: 0.3000, Val Accuracy: 0.7871, Val F1: 0.3090, TP: 476, TN: 7395, FP: 1018, FN: 1111\n",
      "[DEBUG] Logging 4 to step 54\n",
      "[DEBUG] Logging 5 to step 54\n",
      "[DEBUG] Logging 6 to step 54\n",
      "[DEBUG] Logging 7 to step 54\n",
      "[DEBUG] Logging 6 to step 54\n",
      "[DEBUG] Logging 7 to step 54\n",
      "[DEBUG] Logging 6 to step 54\n",
      "[DEBUG] Logging 7 to step 54\n",
      "[DEBUG] Logging 6 to step 54\n",
      "[DEBUG] Logging 7 to step 54\n",
      "Epoch 3/3 [Training]:   0%|                              | 0/27 [00:00<?, ?it/s][DEBUG] Logging 1 to step 54\n",
      "Epoch 3/3 [Training]:   4%|▎         | 1/27 [00:00<00:19,  1.31it/s, loss=0.977][DEBUG] Logging 1 to step 55\n",
      "Epoch 3/3 [Training]:   7%|▊          | 2/27 [00:01<00:19,  1.28it/s, loss=1.01][DEBUG] Logging 1 to step 56\n",
      "Epoch 3/3 [Training]:  11%|█         | 3/27 [00:02<00:18,  1.32it/s, loss=0.985][DEBUG] Logging 1 to step 57\n",
      "Epoch 3/3 [Training]:  15%|█▍        | 4/27 [00:02<00:16,  1.37it/s, loss=0.962][DEBUG] Logging 1 to step 58\n",
      "Epoch 3/3 [Training]:  19%|█▊        | 5/27 [00:03<00:15,  1.38it/s, loss=0.929][DEBUG] Logging 1 to step 59\n",
      "Epoch 3/3 [Training]:  22%|██▏       | 6/27 [00:04<00:14,  1.41it/s, loss=0.937][DEBUG] Logging 1 to step 60\n",
      "Epoch 3/3 [Training]:  26%|██▌       | 7/27 [00:05<00:13,  1.44it/s, loss=0.927][DEBUG] Logging 1 to step 61\n",
      "Epoch 3/3 [Training]:  30%|██▉       | 8/27 [00:05<00:13,  1.44it/s, loss=0.956][DEBUG] Logging 1 to step 62\n",
      "Epoch 3/3 [Training]:  33%|███▎      | 9/27 [00:06<00:12,  1.48it/s, loss=0.946][DEBUG] Logging 1 to step 63\n",
      "Epoch 3/3 [Training]:  37%|███▎     | 10/27 [00:07<00:11,  1.49it/s, loss=0.954][DEBUG] Logging 1 to step 64\n",
      "Epoch 3/3 [Training]:  41%|███▋     | 11/27 [00:07<00:10,  1.49it/s, loss=0.947][DEBUG] Logging 1 to step 65\n",
      "Epoch 3/3 [Training]:  44%|████     | 12/27 [00:08<00:10,  1.49it/s, loss=0.932][DEBUG] Logging 1 to step 66\n",
      "Epoch 3/3 [Training]:  48%|████▎    | 13/27 [00:09<00:09,  1.49it/s, loss=0.953][DEBUG] Logging 1 to step 67\n",
      "Epoch 3/3 [Training]:  52%|████▋    | 14/27 [00:09<00:08,  1.49it/s, loss=0.954][DEBUG] Logging 1 to step 68\n",
      "Epoch 3/3 [Training]:  56%|█████    | 15/27 [00:10<00:08,  1.50it/s, loss=0.951][DEBUG] Logging 1 to step 69\n",
      "Epoch 3/3 [Training]:  59%|█████▎   | 16/27 [00:11<00:07,  1.49it/s, loss=0.941][DEBUG] Logging 1 to step 70\n",
      "Epoch 3/3 [Training]:  63%|█████▋   | 17/27 [00:11<00:06,  1.55it/s, loss=0.941][DEBUG] Logging 1 to step 71\n",
      "Epoch 3/3 [Training]:  67%|██████   | 18/27 [00:11<00:05,  1.80it/s, loss=0.947][DEBUG] Logging 1 to step 72\n",
      "Epoch 3/3 [Training]:  70%|██████▎  | 19/27 [00:12<00:04,  1.97it/s, loss=0.941][DEBUG] Logging 1 to step 73\n",
      "Epoch 3/3 [Training]:  74%|██████▋  | 20/27 [00:12<00:03,  2.10it/s, loss=0.937][DEBUG] Logging 1 to step 74\n",
      "Epoch 3/3 [Training]:  78%|███████  | 21/27 [00:13<00:02,  2.27it/s, loss=0.931][DEBUG] Logging 1 to step 75\n",
      "Epoch 3/3 [Training]:  81%|████████▏ | 22/27 [00:13<00:02,  2.43it/s, loss=0.92][DEBUG] Logging 1 to step 76\n",
      "Epoch 3/3 [Training]:  85%|███████▋ | 23/27 [00:13<00:01,  2.51it/s, loss=0.914][DEBUG] Logging 1 to step 77\n",
      "Epoch 3/3 [Training]:  89%|████████ | 24/27 [00:14<00:01,  2.55it/s, loss=0.911][DEBUG] Logging 1 to step 78\n",
      "Epoch 3/3 [Training]:  93%|████████▎| 25/27 [00:14<00:01,  1.98it/s, loss=0.906][DEBUG] Logging 1 to step 79\n",
      "Epoch 3/3 [Training]:  96%|████████▋| 26/27 [00:15<00:00,  1.86it/s, loss=0.905][DEBUG] Logging 1 to step 80\n",
      "Best threshold (by F1): 0.9957 with F1: 0.3704                                  \n",
      "[DEBUG] Logging 2 to step 81\n",
      "[DEBUG] Logging 3 to step 81\n",
      "Epoch [3/3], Train Loss: 0.9007, Val Loss: 1.8792, Val AUC: 0.6148, Val AP: 0.3573, Val Accuracy: 0.8602, Val F1: 0.3697, TP: 410, TN: 8192, FP: 221, FN: 1177\n",
      "[DEBUG] Logging 4 to step 81\n",
      "[DEBUG] Logging 5 to step 81\n",
      "[DEBUG] Logging 6 to step 81\n",
      "[DEBUG] Logging 7 to step 81\n",
      "[DEBUG] Logging 6 to step 81\n",
      "[DEBUG] Logging 7 to step 81\n",
      "[DEBUG] Logging 6 to step 81\n",
      "[DEBUG] Logging 7 to step 81\n",
      "[DEBUG] Logging 6 to step 81\n",
      "[DEBUG] Logging 7 to step 81\n",
      "Best model saved with AP: 0.3573053934712336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.237 MB of 2.882 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.050 MB of 2.882 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.883 MB of 2.883 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.883 MB of 2.883 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.883 MB of 2.883 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.883 MB of 2.883 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_f1_score_from_curve ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best_threshold ▄▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            learning_rate ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss █▅▅▅▄▄▃▅▄▄▄▄▃▄▃▃▃▂▃▂▂▂▅▁▂▃▂▃▃▃▂▂▃▂▄▂▃▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train_loss_epoch █▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP1_accuracy ▃▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_ap ▃▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP1_auc ▁▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_f1 ▂▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP1_precision ▂▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP1_recall ▄█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP2_accuracy ▁▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_ap ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP2_auc ▁▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_f1 ▁█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP2_precision ▃▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP2_recall ▂█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP3_accuracy ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_ap ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP3_auc ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_f1 ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP3_precision ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP3_recall ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP4_accuracy █▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_ap █▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP4_auc █▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_f1 ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP4_precision ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP4_recall ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy ▃▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_ap ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_auc ▁▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_f1 ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fn ▇▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fp ▆█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss █▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_precision ▂▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_recall ▂█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tn ▃▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tp ▂█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_f1_score_from_curve 0.37044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best_threshold 0.99574\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            learning_rate 0.00452\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.79119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train_loss_epoch 0.90068\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP1_accuracy 0.93059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_ap 0.3911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP1_auc 0.76193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_f1 0.46568\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP1_precision 0.56184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP1_recall 0.39763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP2_accuracy 0.28737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_ap 0.78328\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP2_auc 0.39677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_f1 0.20108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP2_precision 0.97895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP2_recall 0.11205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP3_accuracy 0.60241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_ap 0.99545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP3_auc 0.68293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_f1 0.74809\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP3_precision 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP3_recall 0.59756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP4_accuracy 0.42105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_ap 0.07143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP4_auc 0.27778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_f1 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP4_precision 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP4_recall 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy 0.8602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_ap 0.35731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_auc 0.61483\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_f1 0.3697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fn 1177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fp 221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss 1.87924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_precision 0.64976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_recall 0.25835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tn 8192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tp 410\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfixing_TPP_in_val\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/8iuf67de\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 34 artifact file(s) and 18 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250417_095749-8iuf67de/logs\u001b[0m\n",
      "Best Hyperparameters:\n",
      "<wandb.sdk.lib.preinit.PreInitObject object at 0x7e584210dfa0>\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train-server2-bb-task.py --configs_path \"./configs/v1_mha_1024_config-Copy1.yaml\" \\\n",
    "    --train ./dummy_data/subset_train.tsv \\\n",
    "        --val ./dummy_data/subset_validation.tsv \\\n",
    "            --epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing v1 with flattened instead of pooling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.001411624383330069\n",
      "train_path: ./dummy_data/subset_train.tsv\n",
      "val_path: ./dummy_data/subset_validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250416_151634-jcf8now5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33madjusting_flattend_instead_pooled\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/jcf8now5\u001b[0m\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Best threshold (by F1): 0.4240 with F1: 0.3456                                  \n",
      "Epoch [1/3], Train Loss: 1.0742, Val Loss: 1.0535, Val AUC: 0.6684, Val AP: 0.3176, Val Accuracy: 0.4537, Val F1: 0.3454, TP: 1441, TN: 3096, FP: 5317, FN: 146\n",
      "\n",
      "    TPP1 (8861 Beispiele)\n",
      "AUC:  0.7548226380463491\n",
      "AP:   0.31846426150287965\n",
      "F1:   0.1973\n",
      "Acc:  0.4150\n",
      "Precision: 0.1101\n",
      "Recall:    0.9451\n",
      "\n",
      "    TPP2 (1037 Beispiele)\n",
      "AUC:  0.5297828997148012\n",
      "AP:   0.8049333612407054\n",
      "F1:   0.8508\n",
      "Acc:  0.7541\n",
      "Precision: 0.8271\n",
      "Recall:    0.8759\n",
      "\n",
      "    TPP3 (83 Beispiele)\n",
      "AUC:  0.36585365853658536\n",
      "AP:   0.9881119136921507\n",
      "F1:   0.9560\n",
      "Acc:  0.9157\n",
      "Precision: 0.9870\n",
      "Recall:    0.9268\n",
      "\n",
      "    TPP4 (19 Beispiele)\n",
      "AUC:  0.7222222222222222\n",
      "AP:   0.16666666666666666\n",
      "F1:   0.1053\n",
      "Acc:  0.1053\n",
      "Precision: 0.0556\n",
      "Recall:    1.0000\n",
      "Epoch 2/3 [Training]:  70%|██████▎  | 19/27 [00:05<00:02,  3.34it/s, loss=0.807]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 28 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.9497 with F1: 0.3476                                  \n",
      "Epoch [2/3], Train Loss: 0.8211, Val Loss: 1.4405, Val AUC: 0.6635, Val AP: 0.3824, Val Accuracy: 0.8494, Val F1: 0.3469, TP: 400, TN: 8094, FP: 319, FN: 1187\n",
      "\n",
      "    TPP1 (8861 Beispiele)\n",
      "AUC:  0.781903821611957\n",
      "AP:   0.43553975134970846\n",
      "F1:   0.4295\n",
      "Acc:  0.9206\n",
      "Precision: 0.4732\n",
      "Recall:    0.3932\n",
      "\n",
      "    TPP2 (1037 Beispiele)\n",
      "AUC:  0.4935975787206798\n",
      "AP:   0.8021289328744912\n",
      "F1:   0.1912\n",
      "Acc:  0.2739\n",
      "Precision: 0.8812\n",
      "Recall:    0.1072\n",
      "\n",
      "    TPP3 (83 Beispiele)\n",
      "AUC:  0.3780487804878049\n",
      "AP:   0.9884930112531263\n",
      "F1:   0.7031\n",
      "Acc:  0.5422\n",
      "Precision: 0.9783\n",
      "Recall:    0.5488\n",
      "\n",
      "    TPP4 (19 Beispiele)\n",
      "AUC:  0.6666666666666667\n",
      "AP:   0.14285714285714285\n",
      "F1:   0.1538\n",
      "Acc:  0.4211\n",
      "Precision: 0.0833\n",
      "Recall:    1.0000\n",
      "Epoch 3/3 [Training]:  56%|█████    | 15/27 [00:04<00:03,  3.32it/s, loss=0.749]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 55 that is less than the current step 56. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.8313 with F1: 0.5388                                  \n",
      "Epoch [3/3], Train Loss: 0.7507, Val Loss: 0.9100, Val AUC: 0.8528, Val AP: 0.5904, Val Accuracy: 0.8152, Val F1: 0.5385, TP: 1078, TN: 7074, FP: 1339, FN: 509\n",
      "\n",
      "    TPP1 (8861 Beispiele)\n",
      "AUC:  0.856090788066338\n",
      "AP:   0.5345054690434614\n",
      "F1:   0.3726\n",
      "Acc:  0.8313\n",
      "Precision: 0.2598\n",
      "Recall:    0.6588\n",
      "\n",
      "    TPP2 (1037 Beispiele)\n",
      "AUC:  0.7572783889179907\n",
      "AP:   0.9192491120123705\n",
      "F1:   0.7760\n",
      "Acc:  0.6866\n",
      "Precision: 0.9066\n",
      "Recall:    0.6783\n",
      "\n",
      "    TPP3 (83 Beispiele)\n",
      "AUC:  0.2682926829268293\n",
      "AP:   0.9845393077854473\n",
      "F1:   0.9150\n",
      "Acc:  0.8434\n",
      "Precision: 0.9859\n",
      "Recall:    0.8537\n",
      "\n",
      "    TPP4 (19 Beispiele)\n",
      "AUC:  0.5\n",
      "AP:   0.1\n",
      "F1:   0.1176\n",
      "Acc:  0.2105\n",
      "Precision: 0.0625\n",
      "Recall:    1.0000\n",
      "Best model saved with AP: 0.5904124755482535\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.219 MB of 4.728 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.470 MB of 4.729 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 4.729 MB of 4.729 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 4.729 MB of 4.729 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 4.742 MB of 4.742 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 4.742 MB of 4.742 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 4.742 MB of 4.742 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_f1_score_from_curve ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best_threshold ▁█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch ▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            learning_rate ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss █▇▅▅▅▄▅▅▃▅▄▃▄▆▃▄▃▂▁▃▄▂▂▄▃▄▃▄▃▂▂▃▁▂▃▂▁▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train_loss_epoch █▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy ▁█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_ap ▁▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_auc ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_f1 ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fn ▁█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fp █▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss ▃█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_precision ▁█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_recall █▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tn ▁█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tp █▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_f1_score_from_curve 0.53883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best_threshold 0.83129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            learning_rate 0.00141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.76054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train_loss_epoch 0.75074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy 0.8152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_ap 0.59041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_auc 0.85279\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_f1 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fn 509\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fp 1339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss 0.91003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_precision 0.44601\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_recall 0.67927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tn 7074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tp 1078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33madjusting_flattend_instead_pooled\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/jcf8now5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 34 artifact file(s) and 18 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250416_151634-jcf8now5/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 83. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 83. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 83. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 83. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 83. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 83. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 83. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 83. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best Hyperparameters:\n",
      "<wandb.sdk.lib.preinit.PreInitObject object at 0x77db3a1cbe30>\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train-server2-bb-task_flatten.py --configs_path configs/v1_mha_1024_config-ht.yaml \\\n",
    "    --train ./dummy_data/subset_train.tsv \\\n",
    "        --val ./dummy_data/subset_validation.tsv \\\n",
    "            --epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Train with v1 res flattened instead of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.001411624383330069\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250416_153309-rtcl11wv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v1_mha_1024h_flattened\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/rtcl11wv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 875.52MB. 34 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   34 of 34 files downloaded.  \n",
      "Done. 0:0:0.5\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Best threshold (by F1): 0.6804 with F1: 0.5700                                  \n",
      "Epoch [1/20], Train Loss: 0.6627, Val Loss: 0.7251, Val AUC: 0.8748, Val AP: 0.5824, Val Accuracy: 0.8084, Val F1: 0.5700, TP: 23591, TN: 126590, FP: 29577, FN: 6013\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8928234796115928\n",
      "AP:   0.5465897602798334\n",
      "F1:   0.3806\n",
      "Acc:  0.8121\n",
      "Precision: 0.2562\n",
      "Recall:    0.7404\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.7116722688656003\n",
      "AP:   0.8870090626315097\n",
      "F1:   0.8569\n",
      "Acc:  0.7790\n",
      "Precision: 0.8894\n",
      "Recall:    0.8267\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5812455892731122\n",
      "AP:   0.9947592853501195\n",
      "F1:   0.9722\n",
      "Acc:  0.9459\n",
      "Precision: 0.9922\n",
      "Recall:    0.9530\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.4830181270859237\n",
      "AP:   0.09452062205787859\n",
      "F1:   0.1609\n",
      "Acc:  0.1422\n",
      "Precision: 0.0875\n",
      "Recall:    1.0000\n",
      "Epoch 2/20 [Training]:   1%|      | 10/1976 [00:03<10:32,  3.11it/s, loss=0.601]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1976 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1977 that is less than the current step 1978. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.4741 with F1: 0.4970                                  \n",
      "Epoch [2/20], Train Loss: 0.6231, Val Loss: 0.8274, Val AUC: 0.8483, Val AP: 0.5578, Val Accuracy: 0.7868, Val F1: 0.4970, TP: 19568, TN: 126590, FP: 29577, FN: 10036\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8912244090164645\n",
      "AP:   0.5532738294886121\n",
      "F1:   0.3750\n",
      "Acc:  0.8105\n",
      "Precision: 0.2524\n",
      "Recall:    0.7288\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6494625080257377\n",
      "AP:   0.8692288703310962\n",
      "F1:   0.6827\n",
      "Acc:  0.5776\n",
      "Precision: 0.8563\n",
      "Recall:    0.5676\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5033962597035991\n",
      "AP:   0.9938998713196034\n",
      "F1:   0.9737\n",
      "Acc:  0.9488\n",
      "Precision: 0.9928\n",
      "Recall:    0.9553\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.48393429749361955\n",
      "AP:   0.07695237474406164\n",
      "F1:   0.1605\n",
      "Acc:  0.1400\n",
      "Precision: 0.0873\n",
      "Recall:    1.0000\n",
      "No improvement in AP. Early stop counter: 1/4\n",
      "Epoch 3/20 [Training]:   1%|      | 17/1976 [00:05<11:27,  2.85it/s, loss=0.611]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3952 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3953 that is less than the current step 3954. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.5947 with F1: 0.5615                                  \n",
      "Epoch [3/20], Train Loss: 0.6056, Val Loss: 0.7151, Val AUC: 0.8772, Val AP: 0.5900, Val Accuracy: 0.8021, Val F1: 0.5614, TP: 23535, TN: 125468, FP: 30699, FN: 6069\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.899221813199528\n",
      "AP:   0.5607682959661806\n",
      "F1:   0.3859\n",
      "Acc:  0.8077\n",
      "Precision: 0.2569\n",
      "Recall:    0.7747\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.7205545207761388\n",
      "AP:   0.891602183311249\n",
      "F1:   0.8384\n",
      "Acc:  0.7552\n",
      "Precision: 0.8888\n",
      "Recall:    0.7934\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.49971330275229353\n",
      "AP:   0.9937249511340795\n",
      "F1:   0.9728\n",
      "Acc:  0.9471\n",
      "Precision: 0.9928\n",
      "Recall:    0.9536\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.515803939532753\n",
      "AP:   0.09041717460715062\n",
      "F1:   0.1588\n",
      "Acc:  0.1289\n",
      "Precision: 0.0862\n",
      "Recall:    1.0000\n",
      "Epoch 4/20 [Training]:   0%|       | 5/1976 [00:01<11:19,  2.90it/s, loss=0.618]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5928 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5929 that is less than the current step 5930. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.4693 with F1: 0.5148                                  \n",
      "Epoch [4/20], Train Loss: 0.5987, Val Loss: 0.7770, Val AUC: 0.8599, Val AP: 0.5683, Val Accuracy: 0.7622, Val F1: 0.5148, TP: 23435, TN: 118156, FP: 38011, FN: 6169\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8846811013483817\n",
      "AP:   0.5483567600701282\n",
      "F1:   0.3408\n",
      "Acc:  0.7660\n",
      "Precision: 0.2184\n",
      "Recall:    0.7757\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6602145937241949\n",
      "AP:   0.8711217853576376\n",
      "F1:   0.8208\n",
      "Acc:  0.7257\n",
      "Precision: 0.8599\n",
      "Recall:    0.7851\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5169151376146789\n",
      "AP:   0.9935325766704891\n",
      "F1:   0.9761\n",
      "Acc:  0.9533\n",
      "Precision: 0.9923\n",
      "Recall:    0.9604\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.5314442772069892\n",
      "AP:   0.09883682721701326\n",
      "F1:   0.1574\n",
      "Acc:  0.1200\n",
      "Precision: 0.0855\n",
      "Recall:    1.0000\n",
      "No improvement in AP. Early stop counter: 1/4\n",
      "Epoch 5/20 [Training]:   1%|      | 29/1976 [00:09<10:46,  3.01it/s, loss=0.583]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7904 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7905 that is less than the current step 7906. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.8381 with F1: 0.5678                                  \n",
      "Epoch [5/20], Train Loss: 0.5961, Val Loss: 0.7384, Val AUC: 0.8835, Val AP: 0.6326, Val Accuracy: 0.8531, Val F1: 0.5678, TP: 17926, TN: 140550, FP: 15617, FN: 11678\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8841107290573611\n",
      "AP:   0.566282711674851\n",
      "F1:   0.4478\n",
      "Acc:  0.8821\n",
      "Precision: 0.3528\n",
      "Recall:    0.6131\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.771716921794427\n",
      "AP:   0.9174869821927754\n",
      "F1:   0.6964\n",
      "Acc:  0.6080\n",
      "Precision: 0.9161\n",
      "Recall:    0.5616\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.47918136908962594\n",
      "AP:   0.9918615078792477\n",
      "F1:   0.9538\n",
      "Acc:  0.9118\n",
      "Precision: 0.9926\n",
      "Recall:    0.9180\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.5679274916563053\n",
      "AP:   0.10154682569778131\n",
      "F1:   0.1655\n",
      "Acc:  0.1711\n",
      "Precision: 0.0902\n",
      "Recall:    1.0000\n",
      "Epoch 6/20 [Training]:   1%|        | 20/1976 [00:06<11:19,  2.88it/s, loss=0.6]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9880 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9881 that is less than the current step 9882. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.4783 with F1: 0.5315                                  \n",
      "Epoch [6/20], Train Loss: 0.5894, Val Loss: 0.7648, Val AUC: 0.8691, Val AP: 0.5839, Val Accuracy: 0.7824, Val F1: 0.5315, TP: 22930, TN: 122415, FP: 33752, FN: 6674\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8873408173490657\n",
      "AP:   0.5556278290610013\n",
      "F1:   0.3575\n",
      "Acc:  0.7878\n",
      "Precision: 0.2340\n",
      "Recall:    0.7568\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.7196664163973492\n",
      "AP:   0.8936255081915694\n",
      "F1:   0.8229\n",
      "Acc:  0.7350\n",
      "Precision: 0.8848\n",
      "Recall:    0.7690\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5603828510938602\n",
      "AP:   0.994679858370404\n",
      "F1:   0.9698\n",
      "Acc:  0.9414\n",
      "Precision: 0.9922\n",
      "Recall:    0.9484\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.48471958641450164\n",
      "AP:   0.07764851554909372\n",
      "F1:   0.1569\n",
      "Acc:  0.1400\n",
      "Precision: 0.0853\n",
      "Recall:    0.9730\n",
      "No improvement in AP. Early stop counter: 1/4\n",
      "Epoch 7/20 [Training]:   1%|        | 22/1976 [00:06<10:11,  3.19it/s, loss=0.6]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11856 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11857 that is less than the current step 11858. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.3357 with F1: 0.4863                                  \n",
      "Epoch [7/20], Train Loss: 0.5861, Val Loss: 0.8082, Val AUC: 0.8452, Val AP: 0.5486, Val Accuracy: 0.6969, Val F1: 0.4863, TP: 26652, TN: 102803, FP: 53364, FN: 2952\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8829206053090023\n",
      "AP:   0.5471645248703586\n",
      "F1:   0.3129\n",
      "Acc:  0.6859\n",
      "Precision: 0.1886\n",
      "Recall:    0.9171\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6381882985679487\n",
      "AP:   0.8618030378102397\n",
      "F1:   0.8655\n",
      "Acc:  0.7820\n",
      "Precision: 0.8548\n",
      "Recall:    0.8765\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.47421930134086093\n",
      "AP:   0.9934687923469652\n",
      "F1:   0.9856\n",
      "Acc:  0.9715\n",
      "Precision: 0.9924\n",
      "Recall:    0.9788\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.4944048164387147\n",
      "AP:   0.07654089571679754\n",
      "F1:   0.1548\n",
      "Acc:  0.1022\n",
      "Precision: 0.0839\n",
      "Recall:    1.0000\n",
      "No improvement in AP. Early stop counter: 2/4\n",
      "Epoch 8/20 [Training]:   1%|      | 24/1976 [00:07<10:04,  3.23it/s, loss=0.595]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13832 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13833 that is less than the current step 13834. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.4804 with F1: 0.5253                                  \n",
      "Epoch [8/20], Train Loss: 0.5706, Val Loss: 0.7495, Val AUC: 0.8668, Val AP: 0.5789, Val Accuracy: 0.7707, Val F1: 0.5252, TP: 23568, TN: 119598, FP: 36569, FN: 6036\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8910657079400105\n",
      "AP:   0.5567290439797743\n",
      "F1:   0.3503\n",
      "Acc:  0.7737\n",
      "Precision: 0.2257\n",
      "Recall:    0.7825\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.7048264911262675\n",
      "AP:   0.8885723383173393\n",
      "F1:   0.8305\n",
      "Acc:  0.7424\n",
      "Precision: 0.8774\n",
      "Recall:    0.7883\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5132321806633733\n",
      "AP:   0.993511281140326\n",
      "F1:   0.9752\n",
      "Acc:  0.9516\n",
      "Precision: 0.9923\n",
      "Recall:    0.9587\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.5042209279497415\n",
      "AP:   0.08291016595625333\n",
      "F1:   0.1591\n",
      "Acc:  0.1311\n",
      "Precision: 0.0864\n",
      "Recall:    1.0000\n",
      "No improvement in AP. Early stop counter: 3/4\n",
      "Epoch 9/20 [Training]:   1%|      | 22/1976 [00:06<10:31,  3.10it/s, loss=0.575]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15808 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15809 that is less than the current step 15810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best threshold (by F1): 0.5092 with F1: 0.5314                                  \n",
      "Epoch [9/20], Train Loss: 0.5716, Val Loss: 0.7527, Val AUC: 0.8668, Val AP: 0.5794, Val Accuracy: 0.7830, Val F1: 0.5314, TP: 22860, TN: 122599, FP: 33568, FN: 6744\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8918228856891788\n",
      "AP:   0.5586765570631328\n",
      "F1:   0.3650\n",
      "Acc:  0.7906\n",
      "Precision: 0.2390\n",
      "Recall:    0.7720\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.7002604408151366\n",
      "AP:   0.8860794744505802\n",
      "F1:   0.8092\n",
      "Acc:  0.7166\n",
      "Precision: 0.8776\n",
      "Recall:    0.7507\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5195174664784756\n",
      "AP:   0.993102869825323\n",
      "F1:   0.9728\n",
      "Acc:  0.9471\n",
      "Precision: 0.9928\n",
      "Recall:    0.9536\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.4722204044237943\n",
      "AP:   0.08930385830546814\n",
      "F1:   0.1585\n",
      "Acc:  0.1267\n",
      "Precision: 0.0860\n",
      "Recall:    1.0000\n",
      "No improvement in AP. Early stop counter: 4/4\n",
      "Early stopping triggered.\n",
      "Best model saved with AP: 0.63263641448577\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.512 MB of 5.021 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.700 MB of 5.021 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.021 MB of 5.021 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.021 MB of 5.021 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.021 MB of 5.021 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_f1_score_from_curve █▂▇▃█▅▁▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best_threshold ▆▃▅▃█▃▁▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            learning_rate ██████▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss ▆▄▄▅▄▃▄▄▃▃▃▅▅▅▃▃▄▄▇█▇▃▃▄▁▂▃▅▃▄▆▂▁▄▄▃▆▂▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train_loss_epoch █▅▄▃▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy ▆▅▆▄█▅▁▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_ap ▄▂▄▃█▄▁▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_auc ▆▂▇▄█▅▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_f1 █▂▇▃█▅▁▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fn ▃▇▄▄█▄▁▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fp ▄▄▄▅▁▄█▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss ▂█▁▅▂▄▇▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_precision ▅▃▅▃█▃▁▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_recall ▆▂▅▅▁▅█▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tn ▅▅▅▄█▅▁▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tp ▆▂▅▅▁▅█▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_f1_score_from_curve 0.53145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best_threshold 0.50921\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            learning_rate 0.00035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.52427\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train_loss_epoch 0.57164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy 0.783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_ap 0.57936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_auc 0.86685\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_f1 0.53143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fn 6744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fp 33568\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss 0.75267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_precision 0.40512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_recall 0.77219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tn 122599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tp 22860\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v1_mha_1024h_flattened\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/rtcl11wv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 90 artifact file(s) and 54 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250416_153309-rtcl11wv/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17784 that is less than the current step 17786. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17784 that is less than the current step 17786. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17784 that is less than the current step 17786. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17784 that is less than the current step 17786. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17784 that is less than the current step 17786. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17784 that is less than the current step 17786. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17784 that is less than the current step 17786. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17784 that is less than the current step 17786. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Best Hyperparameters:\n",
      "<wandb.sdk.lib.preinit.PreInitObject object at 0x74177c2f70e0>\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/train-server2-bb-task_flatten.py --configs_path configs/v1_mha_1024_config-ht.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing v1_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250417_153547-4jq73i2n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_Run_v1_mha\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/4jq73i2n\u001b[0m\n",
      "Lade Testdaten von: ../../data/splitted_datasets/allele/beta/test.tsv\n",
      "Lade Embeddings...\n",
      "Lade Modell von results/trained_models/v1_mha/v1_mha_1024_flattened1.pth\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/O__test_v1_1024_model_args.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file))\n",
      "Testing: 100%|████████████████████████████████| 345/345 [01:38<00:00,  3.51it/s]\n",
      "\n",
      "Testergebnisse:\n",
      "AUC:       0.8325\n",
      "AP:        0.5527\n",
      "F1 Score:  0.5524\n",
      "Accuracy:  0.7593\n",
      "Precision: 0.4479\n",
      "Recall:    0.7204\n",
      "TP: 6559, TN: 26971, FP: 8084, FN: 2546\n",
      "\n",
      "TPP1 (36019 Beispiele)\n",
      "AUC:  0.7942603595750295\n",
      "AP:   0.10326886247967913\n",
      "F1:   0.1514\n",
      "Acc:  0.7675\n",
      "Precision: 0.0862\n",
      "Recall:    0.6204\n",
      "\n",
      "TPP2 (7682 Beispiele)\n",
      "AUC:  0.8786935802411348\n",
      "AP:   0.9984495702933897\n",
      "F1:   0.8413\n",
      "Acc:  0.7282\n",
      "Precision: 0.9977\n",
      "Recall:    0.7273\n",
      "\n",
      "TPP3 (294 Beispiele)\n",
      "AUC:  0.5613403272527988\n",
      "AP:   0.852763919109598\n",
      "F1:   0.8781\n",
      "Acc:  0.7857\n",
      "Precision: 0.8225\n",
      "Recall:    0.9419\n",
      "\n",
      "TPP4 (165 Beispiele)\n",
      "AUC:  0.4609563123495012\n",
      "AP:   0.28142588103642535\n",
      "F1:   0.4951\n",
      "Acc:  0.3697\n",
      "Precision: 0.3290\n",
      "Recall:    1.0000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.026 MB of 0.026 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.026 MB of 0.026 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.026 MB of 0.026 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.026 MB of 0.026 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.026 MB of 0.026 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.76748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.10327\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.79426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.15138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.08621\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.62043\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.7282\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.99845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.87869\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.84129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.99766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.7273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.78571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.85276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.56134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.87814\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.82246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.94191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.3697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.28143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.46096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.49515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.32903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.75928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.55267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.83253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.55238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.44793\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.72037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_Run_v1_mha\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/4jq73i2n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 4 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250417_153547-4jq73i2n/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__test_v1_1024_model_args.py --configs_path configs/v1_mha_1024_config-ht.yaml \\\n",
    "    --model_path results/trained_models/v1_mha/v1_mha_1024_flattened1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model with TCRPeg embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250423_230730-u9hedg2u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v0_PEG\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/u9hedg2u\u001b[0m\n",
      "Loading embeddings...\n",
      "Using device: cuda\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/15 [Train]: 100%|███████████████████| 1976/1976 [03:12<00:00, 10.29it/s]\n",
      "[Validation]: 100%|█████████████████████████| 1434/1434 [02:08<00:00, 11.14it/s]\n",
      "Epoch 2/15 [Train]: 100%|███████████████████| 1976/1976 [03:17<00:00, 10.03it/s]\n",
      "[Validation]: 100%|█████████████████████████| 1434/1434 [02:08<00:00, 11.14it/s]\n",
      "Epoch 3/15 [Train]: 100%|███████████████████| 1976/1976 [03:11<00:00, 10.30it/s]\n",
      "[Validation]: 100%|█████████████████████████| 1434/1434 [02:09<00:00, 11.11it/s]\n",
      "Epoch 4/15 [Train]: 100%|███████████████████| 1976/1976 [03:09<00:00, 10.43it/s]\n",
      "[Validation]: 100%|█████████████████████████| 1434/1434 [02:06<00:00, 11.33it/s]\n",
      "Epoch 5/15 [Train]: 100%|███████████████████| 1976/1976 [03:08<00:00, 10.50it/s]\n",
      "[Validation]: 100%|█████████████████████████| 1434/1434 [02:09<00:00, 11.11it/s]\n",
      "Early stopping.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.133 MB of 12.109 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.258 MB of 12.109 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 12.109 MB of 12.109 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 12.109 MB of 12.109 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 12.119 MB of 12.119 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 12.119 MB of 12.119 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 12.119 MB of 12.119 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch ▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate ███▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss ▅▄▅▃▄▆▅▄▄▃▁▃▄█▆▅▄▂▁▄▃▄▃▅▃▃▃▃▃▃▄▃▅▄▄▄▃▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_accuracy █▆▁▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_ap █▄▁▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_auc █▆▁▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_f1 █▅▁▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_fn ▅█▆▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_fp ▁▃█▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss ▁▄█▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_precision █▅▁▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_recall ▄▁▃██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_tn █▆▁▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_tp ▄▁▃██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 0.00025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 0.47374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_accuracy 0.57471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_ap 0.40995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_auc 0.72658\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_f1 0.42597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_fn 641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_fp 77420\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 1.79215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_precision 0.27225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_recall 0.97835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_tn 76522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_tp 28963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v0_PEG\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/u9hedg2u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250423_230730-u9hedg2u/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train_tcrpeg.py --configs_path configs/v1_mha_1024_config-ht.yaml \\\n",
    "    --epochs 15 \\\n",
    "--train \"../../data/splitted_datasets/allele/beta/TCRPeg_data/train.tsv\" \\\n",
    "    --val \"../../data/splitted_datasets/allele/beta/TCRPeg_data/validation.tsv\" \\\n",
    "        --model_path \"results/trained_models/v1_mha/v1_tcrpeg.pth\" \\\n",
    "            --tcr_train_embeddings \"../../data/embeddings/beta/allele/TCRPeg/TCRPeg_train_tcr_embeddings.h5\" \\\n",
    "                --epitope_train_embeddings \"../../data/embeddings/beta/allele/TCRPeg/TCRPeg_train_epitope_embeddings.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../data/embeddings/beta/allele/TCRPeg/TCRPeg_validation_tcr_embeddings.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../data/embeddings/beta/allele/TCRPeg/TCRPeg_validation_epitope_embeddings.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model with subset data to implement SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.004518\n",
      "train_path: ./dummy_data/subset_train.tsv\n",
      "val_path: ./dummy_data/subset_validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250424_105609-g1clacbg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v1_mha_building_SHAPh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/g1clacbg\u001b[0m\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Best threshold (by F1): 0.1597 with F1: 0.2740                                  \n",
      "Epoch [1/3], Train Loss: 1.2044, Val Loss: 1.4478, Val AUC: 0.4645, Val AP: 0.2683, Val Accuracy: 0.1591, Val F1: 0.2739, TP: 1586, TN: 5, FP: 8408, FN: 1\n",
      "\n",
      "    TPP1 (8861 Beispiele)\n",
      "AUC:  0.6618662285399267\n",
      "AP:   0.28482297298774917\n",
      "F1:   0.1413\n",
      "Acc:  0.0765\n",
      "Precision: 0.0760\n",
      "Recall:    0.9985\n",
      "\n",
      "    TPP2 (1037 Beispiele)\n",
      "AUC:  0.278714859437751\n",
      "AP:   0.7351208823014406\n",
      "F1:   0.8891\n",
      "Acc:  0.8004\n",
      "Precision: 0.8004\n",
      "Recall:    1.0000\n",
      "\n",
      "    TPP3 (83 Beispiele)\n",
      "AUC:  0.7195121951219512\n",
      "AP:   0.99607075780819\n",
      "F1:   0.9939\n",
      "Acc:  0.9880\n",
      "Precision: 0.9880\n",
      "Recall:    1.0000\n",
      "\n",
      "    TPP4 (19 Beispiele)\n",
      "AUC:  0.9444444444444444\n",
      "AP:   0.5\n",
      "F1:   0.1000\n",
      "Acc:  0.0526\n",
      "Precision: 0.0526\n",
      "Recall:    1.0000\n",
      "Best threshold (by F1): 0.9101 with F1: 0.2836                                  \n",
      "Epoch [2/3], Train Loss: 0.9975, Val Loss: 2.5274, Val AUC: 0.4873, Val AP: 0.2405, Val Accuracy: 0.2133, Val F1: 0.2835, TP: 1556, TN: 577, FP: 7836, FN: 31\n",
      "\n",
      "    TPP1 (8861 Beispiele)\n",
      "AUC:  0.680189226678033\n",
      "AP:   0.2469090358018872\n",
      "F1:   0.1491\n",
      "Acc:  0.1395\n",
      "Precision: 0.0806\n",
      "Recall:    0.9911\n",
      "\n",
      "    TPP2 (1037 Beispiele)\n",
      "AUC:  0.2973866480414411\n",
      "AP:   0.7310530479325359\n",
      "F1:   0.8790\n",
      "Acc:  0.7859\n",
      "Precision: 0.8028\n",
      "Recall:    0.9711\n",
      "\n",
      "    TPP3 (83 Beispiele)\n",
      "AUC:  0.36585365853658536\n",
      "AP:   0.9881119136921507\n",
      "F1:   0.9878\n",
      "Acc:  0.9759\n",
      "Precision: 0.9878\n",
      "Recall:    0.9878\n",
      "\n",
      "    TPP4 (19 Beispiele)\n",
      "AUC:  0.38888888888888884\n",
      "AP:   0.08333333333333333\n",
      "F1:   0.1000\n",
      "Acc:  0.0526\n",
      "Precision: 0.0526\n",
      "Recall:    1.0000\n",
      "No improvement in AP. Early stop counter: 1/4\n",
      "Best threshold (by F1): 0.0309 with F1: 0.4109                                  \n",
      "Epoch [3/3], Train Loss: 0.8852, Val Loss: 1.6558, Val AUC: 0.7203, Val AP: 0.3063, Val Accuracy: 0.5807, Val F1: 0.4107, TP: 1461, TN: 4346, FP: 4067, FN: 126\n",
      "\n",
      "    TPP1 (8861 Beispiele)\n",
      "AUC:  0.8046745056848104\n",
      "AP:   0.2580602534615153\n",
      "F1:   0.2472\n",
      "Acc:  0.5566\n",
      "Precision: 0.1419\n",
      "Recall:    0.9570\n",
      "\n",
      "    TPP2 (1037 Beispiele)\n",
      "AUC:  0.48092660497060713\n",
      "AP:   0.782613585266873\n",
      "F1:   0.8654\n",
      "Acc:  0.7763\n",
      "Precision: 0.8345\n",
      "Recall:    0.8988\n",
      "\n",
      "    TPP3 (83 Beispiele)\n",
      "AUC:  0.3048780487804878\n",
      "AP:   0.9860042858967668\n",
      "F1:   0.9079\n",
      "Acc:  0.8313\n",
      "Precision: 0.9857\n",
      "Recall:    0.8415\n",
      "\n",
      "    TPP4 (19 Beispiele)\n",
      "AUC:  0.2777777777777778\n",
      "AP:   0.07142857142857142\n",
      "F1:   0.1000\n",
      "Acc:  0.0526\n",
      "Precision: 0.0526\n",
      "Recall:    1.0000\n",
      "Best model saved with AP: 0.30631350139387653\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 1.297 MB of 2.880 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.880 MB of 2.880 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.880 MB of 2.880 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.899 MB of 2.899 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.899 MB of 2.899 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 2.899 MB of 2.899 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_f1_score_from_curve ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best_threshold ▂█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch ▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            learning_rate ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss █▅▄▅▃▄▄▃▃▄▃▃▃▃▂▄▂▃▃▂▂▁▃▄▃▃▂▄▁▂▁▂▁▂▂▃▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train_loss_epoch █▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP1_accuracy ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_ap █▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP1_auc ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_f1 ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP1_precision ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP1_recall █▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP2_accuracy █▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_ap ▂▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP2_auc ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_f1 █▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP2_precision ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP2_recall █▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP3_accuracy █▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_ap █▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP3_auc █▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_f1 ██▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP3_precision ██▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP3_recall █▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP4_accuracy ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_ap █▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP4_auc █▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_f1 ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP4_precision ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP4_recall ▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_ap ▄▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_auc ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_f1 ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fn ▁▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fp █▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss ▁█▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_precision ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_recall █▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tn ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tp █▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_f1_score_from_curve 0.4109\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best_threshold 0.03094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            learning_rate 0.00452\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.8721\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train_loss_epoch 0.88517\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP1_accuracy 0.5566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_ap 0.25806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP1_auc 0.80467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_f1 0.24717\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP1_precision 0.14191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP1_recall 0.95697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP2_accuracy 0.77628\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_ap 0.78261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP2_auc 0.48093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_f1 0.86543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP2_precision 0.83445\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP2_recall 0.8988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP3_accuracy 0.83133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_ap 0.986\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP3_auc 0.30488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_f1 0.90789\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP3_precision 0.98571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP3_recall 0.84146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP4_accuracy 0.05263\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_ap 0.07143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_TPP4_auc 0.27778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_f1 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_TPP4_precision 0.05263\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_TPP4_recall 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy 0.5807\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_ap 0.30631\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_auc 0.72026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_f1 0.41068\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fn 126\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_fp 4067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss 1.65582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_precision 0.26429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_recall 0.9206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tn 4346\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_tp 1461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v1_mha_building_SHAPh\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/g1clacbg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 30 artifact file(s) and 18 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250424_105609-g1clacbg/logs\u001b[0m\n",
      "Best Hyperparameters:\n",
      "<wandb.sdk.lib.preinit.PreInitObject object at 0x7756c9bc6540>\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train-server2-bb-task.py --configs_path \"./configs/v1_mha_1024_config-Copy1.yaml\" \\\n",
    "    --train ./dummy_data/subset_train.tsv \\\n",
    "        --val ./dummy_data/subset_validation.tsv \\\n",
    "            --epochs 3 \\\n",
    "                --model_path \"results/trained_models/v1_mha/v1_mha_building_SHAP.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training a v1 (again) to get the model and parameters and apply SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Learning rate: 0.0005\n",
      "train_path: ../../data/splitted_datasets/allele/beta/train.tsv\n",
      "val_path: ../../data/splitted_datasets/allele/beta/validation.tsv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250424_223523-1v6j2a2y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRun_v1_mha_1024_shap\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/1v6j2a2y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 875.52MB. 34 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   34 of 34 files downloaded.  \n",
      "Done. 0:0:0.9\n",
      "Loading embeddings...\n",
      "tcr_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\n",
      "epi_train  ../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\n",
      "tcr_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\n",
      "epi_valid  ../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\n",
      "Mindestens 5 Epochen nötig, um alle Daten einmal zu verwenden.\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Best threshold (by F1): 0.7130 with F1: 0.5639                                  \n",
      "Epoch [1/20], Train Loss: 1.8371, Val Loss: 0.7901, Val AUC: 0.8656, Val AP: 0.5645, Val Accuracy: 0.8038, Val F1: 0.5639, TP: 23561, TN: 125761, FP: 30406, FN: 6043\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8875024899514016\n",
      "AP:   0.5319941038528333\n",
      "F1:   0.3733\n",
      "Acc:  0.8071\n",
      "Precision: 0.2500\n",
      "Recall:    0.7367\n",
      "  TPP TPP1 — Temperature: 2.1886\n",
      "  Scaled Accuracy: 0.7331, F1: 0.3369\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6907724541092823\n",
      "AP:   0.8768838608028608\n",
      "F1:   0.8586\n",
      "Acc:  0.7804\n",
      "Precision: 0.8859\n",
      "Recall:    0.8329\n",
      "  TPP TPP2 — Temperature: 1.7064\n",
      "  Scaled Accuracy: 0.7919, F1: 0.8709\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.4976623147494707\n",
      "AP:   0.9932451376641691\n",
      "F1:   0.9513\n",
      "Acc:  0.9072\n",
      "Precision: 0.9931\n",
      "Recall:    0.9128\n",
      "  TPP TPP3 — Temperature: 1.9024\n",
      "  Scaled Accuracy: 0.9158, F1: 0.9560\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.43792945487860746\n",
      "AP:   0.06946330193518119\n",
      "F1:   0.1432\n",
      "Acc:  0.1489\n",
      "Precision: 0.0780\n",
      "Recall:    0.8649\n",
      "  TPP TPP4 — Temperature: 9.4177\n",
      "  Scaled Accuracy: 0.1444, F1: 0.1463\n",
      "Best threshold (by F1): 0.7721 with F1: 0.5528                                  \n",
      "Epoch [2/20], Train Loss: 0.6624, Val Loss: 0.8123, Val AUC: 0.8704, Val AP: 0.5796, Val Accuracy: 0.7973, Val F1: 0.5528, TP: 23273, TN: 124841, FP: 31326, FN: 6331\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8921573026038128\n",
      "AP:   0.5537894816918048\n",
      "F1:   0.3716\n",
      "Acc:  0.8025\n",
      "Precision: 0.2471\n",
      "Recall:    0.7489\n",
      "  TPP TPP1 — Temperature: 2.4698\n",
      "  Scaled Accuracy: 0.6359, F1: 0.2929\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.7013917626546955\n",
      "AP:   0.8824955635349884\n",
      "F1:   0.8388\n",
      "Acc:  0.7543\n",
      "Precision: 0.8831\n",
      "Recall:    0.7988\n",
      "  TPP TPP2 — Temperature: 1.2964\n",
      "  Scaled Accuracy: 0.8234, F1: 0.8949\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.4475564573041637\n",
      "AP:   0.9920108012123522\n",
      "F1:   0.9707\n",
      "Acc:  0.9431\n",
      "Precision: 0.9928\n",
      "Recall:    0.9495\n",
      "  TPP TPP3 — Temperature: 1.4165\n",
      "  Scaled Accuracy: 0.9590, F1: 0.9791\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.4436555199267064\n",
      "AP:   0.07002428087280352\n",
      "F1:   0.1469\n",
      "Acc:  0.1222\n",
      "Precision: 0.0798\n",
      "Recall:    0.9189\n",
      "  TPP TPP4 — Temperature: 9.4536\n",
      "  Scaled Accuracy: 0.1156, F1: 0.1532\n",
      "Best threshold (by F1): 0.6701 with F1: 0.5141                                  \n",
      "Epoch [3/20], Train Loss: 0.6404, Val Loss: 0.8195, Val AUC: 0.8541, Val AP: 0.5562, Val Accuracy: 0.7853, Val F1: 0.5141, TP: 21097, TN: 124794, FP: 31373, FN: 8507\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.890839985317725\n",
      "AP:   0.5543321426005379\n",
      "F1:   0.3778\n",
      "Acc:  0.8028\n",
      "Precision: 0.2506\n",
      "Recall:    0.7680\n",
      "  TPP TPP1 — Temperature: 2.3625\n",
      "  Scaled Accuracy: 0.6776, F1: 0.3098\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6636420197655666\n",
      "AP:   0.8677691011054893\n",
      "F1:   0.7352\n",
      "Acc:  0.6326\n",
      "Precision: 0.8689\n",
      "Recall:    0.6371\n",
      "  TPP TPP2 — Temperature: 1.7867\n",
      "  Scaled Accuracy: 0.7893, F1: 0.8703\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.395818630910374\n",
      "AP:   0.9908891174847467\n",
      "F1:   0.9701\n",
      "Acc:  0.9419\n",
      "Precision: 0.9922\n",
      "Recall:    0.9490\n",
      "  TPP TPP3 — Temperature: 1.7122\n",
      "  Scaled Accuracy: 0.9471, F1: 0.9728\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.39941757738367917\n",
      "AP:   0.06481976945530578\n",
      "F1:   0.1432\n",
      "Acc:  0.1222\n",
      "Precision: 0.0778\n",
      "Recall:    0.8919\n",
      "  TPP TPP4 — Temperature: 9.4567\n",
      "  Scaled Accuracy: 0.1200, F1: 0.1429\n",
      "No improvement in AP. Early stop counter: 1/4\n",
      "Best threshold (by F1): 0.4911 with F1: 0.5015                                  \n",
      "Epoch [4/20], Train Loss: 0.6261, Val Loss: 0.7827, Val AUC: 0.8508, Val AP: 0.5501, Val Accuracy: 0.7457, Val F1: 0.5015, TP: 23766, TN: 114757, FP: 41410, FN: 5838\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8870114346813108\n",
      "AP:   0.5493833286592942\n",
      "F1:   0.3349\n",
      "Acc:  0.7473\n",
      "Precision: 0.2107\n",
      "Recall:    0.8160\n",
      "  TPP TPP1 — Temperature: 1.8485\n",
      "  Scaled Accuracy: 0.7538, F1: 0.3373\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6591372078488741\n",
      "AP:   0.8622239926828584\n",
      "F1:   0.8201\n",
      "Acc:  0.7280\n",
      "Precision: 0.8714\n",
      "Recall:    0.7745\n",
      "  TPP TPP2 — Temperature: 2.5545\n",
      "  Scaled Accuracy: 0.7119, F1: 0.8069\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.4624867678193366\n",
      "AP:   0.9910675144063642\n",
      "F1:   0.9692\n",
      "Acc:  0.9402\n",
      "Precision: 0.9928\n",
      "Recall:    0.9467\n",
      "  TPP TPP3 — Temperature: 1.7203\n",
      "  Scaled Accuracy: 0.9391, F1: 0.9686\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.40939729075322306\n",
      "AP:   0.07921899589145653\n",
      "F1:   0.1429\n",
      "Acc:  0.1200\n",
      "Precision: 0.0776\n",
      "Recall:    0.8919\n",
      "  TPP TPP4 — Temperature: 9.4957\n",
      "  Scaled Accuracy: 0.1200, F1: 0.1429\n",
      "No improvement in AP. Early stop counter: 2/4\n",
      "Best threshold (by F1): 0.4543 with F1: 0.5124                                  \n",
      "Epoch [5/20], Train Loss: 0.5982, Val Loss: 0.7789, Val AUC: 0.8561, Val AP: 0.5571, Val Accuracy: 0.7535, Val F1: 0.5124, TP: 24064, TN: 115914, FP: 40253, FN: 5540\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8905404054119641\n",
      "AP:   0.5485515375173391\n",
      "F1:   0.3490\n",
      "Acc:  0.7565\n",
      "Precision: 0.2204\n",
      "Recall:    0.8370\n",
      "  TPP TPP1 — Temperature: 1.7877\n",
      "  Scaled Accuracy: 0.7825, F1: 0.3596\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6586995452859045\n",
      "AP:   0.8636621042703846\n",
      "F1:   0.8165\n",
      "Acc:  0.7223\n",
      "Precision: 0.8663\n",
      "Recall:    0.7722\n",
      "  TPP TPP2 — Temperature: 2.5573\n",
      "  Scaled Accuracy: 0.6671, F1: 0.7686\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.4379410726887791\n",
      "AP:   0.9900563809335893\n",
      "F1:   0.9867\n",
      "Acc:  0.9738\n",
      "Precision: 0.9925\n",
      "Recall:    0.9811\n",
      "  TPP TPP3 — Temperature: 1.4879\n",
      "  Scaled Accuracy: 0.9732, F1: 0.9864\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.3956547346377855\n",
      "AP:   0.06391573712611286\n",
      "F1:   0.1513\n",
      "Acc:  0.1022\n",
      "Precision: 0.0820\n",
      "Recall:    0.9730\n",
      "  TPP TPP4 — Temperature: 9.5120\n",
      "  Scaled Accuracy: 0.1022, F1: 0.1513\n",
      "No improvement in AP. Early stop counter: 3/4\n",
      "Best threshold (by F1): 0.4099 with F1: 0.4932                                  \n",
      "Epoch [6/20], Train Loss: 0.5922, Val Loss: 0.8779, Val AUC: 0.8501, Val AP: 0.5522, Val Accuracy: 0.7541, Val F1: 0.4932, TP: 22226, TN: 117868, FP: 38299, FN: 7378\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8862292632626646\n",
      "AP:   0.5489736900946485\n",
      "F1:   0.3411\n",
      "Acc:  0.7637\n",
      "Precision: 0.2179\n",
      "Recall:    0.7844\n",
      "  TPP TPP1 — Temperature: 2.0210\n",
      "  Scaled Accuracy: 0.8054, F1: 0.3641\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6500824628720198\n",
      "AP:   0.8628162192191057\n",
      "F1:   0.7697\n",
      "Acc:  0.6663\n",
      "Precision: 0.8596\n",
      "Recall:    0.6969\n",
      "  TPP TPP2 — Temperature: 3.8648\n",
      "  Scaled Accuracy: 0.5650, F1: 0.6701\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.48833362738179253\n",
      "AP:   0.9925113225949267\n",
      "F1:   0.9764\n",
      "Acc:  0.9539\n",
      "Precision: 0.9923\n",
      "Recall:    0.9610\n",
      "  TPP TPP3 — Temperature: 1.9332\n",
      "  Scaled Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.4345265362214515\n",
      "AP:   0.07131326748353634\n",
      "F1:   0.1532\n",
      "Acc:  0.1156\n",
      "Precision: 0.0831\n",
      "Recall:    0.9730\n",
      "  TPP TPP4 — Temperature: 9.5981\n",
      "  Scaled Accuracy: 0.1200, F1: 0.1538\n",
      "No improvement in AP. Early stop counter: 4/4\n",
      "Best threshold (by F1): 0.4449 with F1: 0.4976                                  \n",
      "Epoch [7/20], Train Loss: 0.5781, Val Loss: 0.8377, Val AUC: 0.8527, Val AP: 0.5605, Val Accuracy: 0.7706, Val F1: 0.4976, TP: 21098, TN: 122066, FP: 34101, FN: 8506\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.887288292162234\n",
      "AP:   0.5586785822668484\n",
      "F1:   0.3562\n",
      "Acc:  0.7862\n",
      "Precision: 0.2328\n",
      "Recall:    0.7586\n",
      "  TPP TPP1 — Temperature: 1.7468\n",
      "  Scaled Accuracy: 0.8080, F1: 0.3688\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6612137469465594\n",
      "AP:   0.8681222456649261\n",
      "F1:   0.7378\n",
      "Acc:  0.6328\n",
      "Precision: 0.8612\n",
      "Recall:    0.6453\n",
      "  TPP TPP2 — Temperature: 3.4496\n",
      "  Scaled Accuracy: 0.5787, F1: 0.6830\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5056236767819337\n",
      "AP:   0.9919827057460295\n",
      "F1:   0.9698\n",
      "Acc:  0.9414\n",
      "Precision: 0.9922\n",
      "Recall:    0.9484\n",
      "  TPP TPP3 — Temperature: 1.8096\n",
      "  Scaled Accuracy: 0.9385, F1: 0.9683\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.39552385315097177\n",
      "AP:   0.06500536782376884\n",
      "F1:   0.1475\n",
      "Acc:  0.1267\n",
      "Precision: 0.0802\n",
      "Recall:    0.9189\n",
      "  TPP TPP4 — Temperature: 9.4964\n",
      "  Scaled Accuracy: 0.1267, F1: 0.1475\n",
      "No improvement in AP. Early stop counter: 5/4\n",
      "Best threshold (by F1): 0.4790 with F1: 0.5011                                  \n",
      "Epoch [8/20], Train Loss: 0.5765, Val Loss: 0.8322, Val AUC: 0.8556, Val AP: 0.5648, Val Accuracy: 0.7602, Val F1: 0.5011, TP: 22377, TN: 118842, FP: 37325, FN: 7227\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8855325586493549\n",
      "AP:   0.5513210654992514\n",
      "F1:   0.3449\n",
      "Acc:  0.7691\n",
      "Precision: 0.2215\n",
      "Recall:    0.7797\n",
      "  TPP TPP1 — Temperature: 2.0991\n",
      "  Scaled Accuracy: 0.7779, F1: 0.3501\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6685883512886311\n",
      "AP:   0.8731430699588789\n",
      "F1:   0.7794\n",
      "Acc:  0.6783\n",
      "Precision: 0.8639\n",
      "Recall:    0.7099\n",
      "  TPP TPP2 — Temperature: 3.0050\n",
      "  Scaled Accuracy: 0.6595, F1: 0.7623\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5206422018348623\n",
      "AP:   0.9923364590838096\n",
      "F1:   0.9812\n",
      "Acc:  0.9630\n",
      "Precision: 0.9924\n",
      "Recall:    0.9702\n",
      "  TPP TPP3 — Temperature: 1.7496\n",
      "  Scaled Accuracy: 0.9630, F1: 0.9812\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.3757607486421046\n",
      "AP:   0.06251026499966589\n",
      "F1:   0.1529\n",
      "Acc:  0.1133\n",
      "Precision: 0.0829\n",
      "Recall:    0.9730\n",
      "  TPP TPP4 — Temperature: 9.5454\n",
      "  Scaled Accuracy: 0.1133, F1: 0.1529\n",
      "No improvement in AP. Early stop counter: 6/4\n",
      "Best threshold (by F1): 0.4775 with F1: 0.4944                                  \n",
      "Epoch [9/20], Train Loss: 0.5693, Val Loss: 0.8431, Val AUC: 0.8511, Val AP: 0.5563, Val Accuracy: 0.7530, Val F1: 0.4944, TP: 22438, TN: 117442, FP: 38725, FN: 7166\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8896713673930696\n",
      "AP:   0.5566328018840722\n",
      "F1:   0.3474\n",
      "Acc:  0.7631\n",
      "Precision: 0.2212\n",
      "Recall:    0.8086\n",
      "  TPP TPP1 — Temperature: 2.1453\n",
      "  Scaled Accuracy: 0.7733, F1: 0.3527\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6483629553371736\n",
      "AP:   0.8617996469247996\n",
      "F1:   0.7641\n",
      "Acc:  0.6593\n",
      "Precision: 0.8569\n",
      "Recall:    0.6894\n",
      "  TPP TPP2 — Temperature: 3.2195\n",
      "  Scaled Accuracy: 0.6357, F1: 0.7423\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.5100564573041637\n",
      "AP:   0.9921406693285382\n",
      "F1:   0.9803\n",
      "Acc:  0.9613\n",
      "Precision: 0.9924\n",
      "Recall:    0.9685\n",
      "  TPP TPP3 — Temperature: 1.8048\n",
      "  Scaled Accuracy: 0.9607, F1: 0.9800\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.3855114194097245\n",
      "AP:   0.06355357550094341\n",
      "F1:   0.1529\n",
      "Acc:  0.1133\n",
      "Precision: 0.0829\n",
      "Recall:    0.9730\n",
      "  TPP TPP4 — Temperature: 9.5640\n",
      "  Scaled Accuracy: 0.1133, F1: 0.1529\n",
      "No improvement in AP. Early stop counter: 7/4\n",
      "Best threshold (by F1): 0.4676 with F1: 0.5102                                  \n",
      "Epoch [10/20], Train Loss: 0.5651, Val Loss: 0.8483, Val AUC: 0.8596, Val AP: 0.5721, Val Accuracy: 0.7792, Val F1: 0.5102, TP: 21358, TN: 123403, FP: 32764, FN: 8246\n",
      "\n",
      "    TPP1 (164864 Beispiele)\n",
      "AUC:  0.8916947362688342\n",
      "AP:   0.5622863175162269\n",
      "F1:   0.3681\n",
      "Acc:  0.7949\n",
      "Precision: 0.2423\n",
      "Recall:    0.7661\n",
      "  TPP TPP1 — Temperature: 2.0585\n",
      "  Scaled Accuracy: 0.8059, F1: 0.3754\n",
      "\n",
      "    TPP2 (18700 Beispiele)\n",
      "AUC:  0.6711012195398498\n",
      "AP:   0.8735398510246183\n",
      "F1:   0.7442\n",
      "Acc:  0.6401\n",
      "Precision: 0.8632\n",
      "Recall:    0.6540\n",
      "  TPP TPP2 — Temperature: 3.4840\n",
      "  Scaled Accuracy: 0.6111, F1: 0.7160\n",
      "\n",
      "    TPP3 (1757 Beispiele)\n",
      "AUC:  0.4909139026111503\n",
      "AP:   0.9920490462650416\n",
      "F1:   0.9791\n",
      "Acc:  0.9590\n",
      "Precision: 0.9923\n",
      "Recall:    0.9662\n",
      "  TPP TPP3 — Temperature: 1.8734\n",
      "  Scaled Accuracy: 0.9579, F1: 0.9785\n",
      "\n",
      "    TPP4 (450 Beispiele)\n",
      "AUC:  0.40370394607682747\n",
      "AP:   0.06687711676225136\n",
      "F1:   0.1532\n",
      "Acc:  0.1156\n",
      "Precision: 0.0831\n",
      "Recall:    0.9730\n",
      "  TPP TPP4 — Temperature: 9.5526\n",
      "  Scaled Accuracy: 0.1178, F1: 0.1535\n",
      "No improvement in AP. Early stop counter: 8/4\n",
      "Early stopping triggered at epoch 10.\n",
      "Best model saved with AP: 0.5796202037760414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 3.600 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 5.475 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 34.756 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 67.506 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 100.100 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 132.756 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 161.397 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 192.350 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 224.506 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 256.772 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 289.490 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 321.990 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 354.990 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 385.678 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 418.006 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 451.256 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 483.756 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 512.756 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 544.522 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 577.272 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 609.522 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 642.272 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 673.725 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 705.912 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 738.912 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 771.412 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 804.162 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 832.678 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 862.334 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 895.428 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 927.928 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 929.125 MB of 929.125 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 929.149 MB of 929.149 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 929.149 MB of 929.149 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 929.149 MB of 929.149 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  best_f1_score_from_curve █▇▃▂▃▁▁▂▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            best_threshold ▇█▆▃▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             learning_rate ███▄▄▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss ▆█▁▄▅▄▃▃▆▅▄▄▃▄▃▃▃▃▂▃▂▄▂▂▅▃▃▂▃▂▂▁▂▄▅▂▃▄▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss_epoch █▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_TPP1_accuracy █▇▇▁▂▃▆▄▃▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_TPP1_accuracy_scaled ▅▁▃▆▇██▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP1_ap ▁▆▆▅▅▅▇▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_auc ▃█▇▃▆▂▃▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP1_f1 ▇▇█▁▃▂▄▃▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP1_f1_scaled ▅▁▂▅▇▇▇▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP1_precision █▇█▁▃▂▅▃▃▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_TPP1_precision_scaled ▄▁▂▅▆▇█▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_TPP1_recall ▁▂▃▇█▄▃▄▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_TPP1_recall_scaled ▅█▇▃▃▁▁▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_TPP1_temperature ▅█▇▂▁▄▁▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_TPP2_accuracy █▇▁▆▅▃▁▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_TPP2_accuracy_scaled ▇█▇▅▄▁▁▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP2_ap ▆█▃▁▂▁▃▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_auc ▇█▃▂▂▁▃▄▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP2_f1 █▇▁▆▆▃▁▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP2_f1_scaled ▇█▇▅▄▁▁▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP2_precision █▇▄▄▃▂▂▃▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_TPP2_precision_scaled ▆▂▃█▆▁▃▅▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_TPP2_recall █▇▁▆▆▃▁▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_TPP2_recall_scaled ▇█▇▅▄▁▁▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_TPP2_temperature ▂▁▂▄▄█▇▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_TPP3_accuracy ▁▅▅▄█▆▅▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_TPP3_accuracy_scaled ▁▆▅▄█▅▄▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP3_ap █▅▃▃▁▆▅▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_auc ▇▄▁▅▃▆▇█▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP3_f1 ▁▅▅▅█▆▅▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP3_f1_scaled ▁▆▅▄█▅▄▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP3_precision █▆▁▅▃▂▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_TPP3_precision_scaled ▅█▂▇▄█▁▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_TPP3_recall ▁▅▅▄█▆▅▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_TPP3_recall_scaled ▁▆▅▄█▅▄▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_TPP3_temperature █▁▅▅▂█▆▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_TPP4_accuracy █▄▄▄▁▃▅▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_TPP4_accuracy_scaled █▃▄▄▁▄▅▃▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP4_ap ▄▄▂█▂▅▂▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_auc ▇█▃▄▃▇▃▁▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP4_f1 ▁▄▁▁▇█▄███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP4_f1_scaled ▃█▁▁▆█▄▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP4_precision ▂▄▁▁▇█▄███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_TPP4_precision_scaled ▃█▁▁▆█▄▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_TPP4_recall ▁▄▃▃██▄███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_TPP4_recall_scaled ▁█▁▁██▃███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_TPP4_temperature ▁▂▃▄▅█▄▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_accuracy █▇▆▁▂▂▄▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_ap ▄█▂▁▃▁▃▄▂▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_auc ▆█▂▁▃▁▂▃▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_f1 █▇▃▂▃▁▁▂▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_fn ▂▃█▂▁▅█▅▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_fp ▁▂▂█▇▆▃▅▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss ▂▃▄▁▁█▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_precision █▇▅▁▂▁▃▂▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                val_recall ▇▆▁▇█▄▁▄▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_tn █▇▇▁▂▃▆▄▃▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_tp ▇▆▁▇█▄▁▄▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  best_f1_score_from_curve 0.51021\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            best_threshold 0.46763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             learning_rate 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss 0.63805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss_epoch 0.56512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_TPP1_accuracy 0.79492\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_TPP1_accuracy_scaled 0.80587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP1_ap 0.56229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP1_auc 0.89169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP1_f1 0.36813\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP1_f1_scaled 0.37535\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP1_precision 0.24228\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_TPP1_precision_scaled 0.25054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_TPP1_recall 0.7661\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_TPP1_recall_scaled 0.74798\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_TPP1_temperature 2.05847\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_TPP2_accuracy 0.64011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_TPP2_accuracy_scaled 0.61107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP2_ap 0.87354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP2_auc 0.6711\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP2_f1 0.74416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP2_f1_scaled 0.71602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP2_precision 0.86322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_TPP2_precision_scaled 0.86142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_TPP2_recall 0.65397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_TPP2_recall_scaled 0.61261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_TPP2_temperature 3.48403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_TPP3_accuracy 0.95902\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_TPP3_accuracy_scaled 0.95788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP3_ap 0.99205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP3_auc 0.49091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP3_f1 0.97908\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP3_f1_scaled 0.97849\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP3_precision 0.99234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_TPP3_precision_scaled 0.99233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_TPP3_recall 0.96617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_TPP3_recall_scaled 0.96502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_TPP3_temperature 1.87342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_TPP4_accuracy 0.11556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_TPP4_accuracy_scaled 0.11778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP4_ap 0.06688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_TPP4_auc 0.4037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val_TPP4_f1 0.15319\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP4_f1_scaled 0.15352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_TPP4_precision 0.08314\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_TPP4_precision_scaled 0.08333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val_TPP4_recall 0.97297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_TPP4_recall_scaled 0.97297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_TPP4_temperature 9.55261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_accuracy 0.77924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_ap 0.57212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   val_auc 0.85963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_f1 0.51019\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_fn 8246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_fp 32764\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss 0.84829\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_precision 0.39463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                val_recall 0.72146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_tn 123403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_tp 21358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRun_v1_mha_1024_shap\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/1v6j2a2y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 101 artifact file(s) and 140 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250424_223523-1v6j2a2y/logs\u001b[0m\n",
      "Best Hyperparameters:\n",
      "<wandb.sdk.lib.preinit.PreInitObject object at 0x79afa77ab590>\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/O__train-O__onlyresflattenwiBNpre.py --configs_path \"./configs/v1_mha_1024_SHAP_config-ht.yaml\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test v6   (train was in tmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250502_134823-7uhvqlh6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_Run_v6_all_features\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/7uhvqlh6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1001.05MB. 46 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   46 of 46 files downloaded.  \n",
      "Done. 0:0:0.7\n",
      "Lade Testdaten von: ../../data/splitted_datasets/allele/beta/test.tsv\n",
      "162\n",
      "30\n",
      "63\n",
      "\n",
      " TPP-Verteilung im Testset (aus Datei):\n",
      "task\n",
      "TPP2    47912\n",
      "TPP1     7104\n",
      "TPP3     1167\n",
      "TPP4      335\n",
      "Name: count, dtype: int64\n",
      "Lade Embeddings...\n",
      "Lade Modell von wandb...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "✅ Modell geladen: Run_v3_mha_resh_model:v6\n",
      "Testing: 100%|████████████████████████████████| 442/442 [02:40<00:00,  2.75it/s]\n",
      "\n",
      "Testergebnisse:\n",
      "AUC:       0.4244\n",
      "AP:        0.1342\n",
      "F1 Score:  0.2077\n",
      "Accuracy:  0.4988\n",
      "Precision: 0.1391\n",
      "Recall:    0.4096\n",
      "TP: 3713, TN: 24480, FP: 22974, FN: 5351\n",
      "\n",
      "TPP1 (7104 Beispiele)\n",
      "AUC:  0.5160833222793372\n",
      "AP:   0.27683196777342184\n",
      "F1:   0.3061\n",
      "Acc:  0.7473\n",
      "Precision: 0.2920\n",
      "Recall:    0.3217\n",
      "\n",
      "TPP2 (47912 Beispiele)\n",
      "AUC:  0.40694396336777566\n",
      "AP:   0.12634818980093476\n",
      "F1:   0.1971\n",
      "Acc:  0.4654\n",
      "Precision: 0.1289\n",
      "Recall:    0.4186\n",
      "\n",
      "TPP3 (1167 Beispiele)\n",
      "AUC:  0.4414560263032721\n",
      "AP:   0.19820000899288082\n",
      "F1:   0.2762\n",
      "Acc:  0.4250\n",
      "Precision: 0.1936\n",
      "Recall:    0.4812\n",
      "\n",
      "TPP4 (335 Beispiele)\n",
      "AUC:  0.40068538289178857\n",
      "AP:   0.13724378188780437\n",
      "F1:   0.2651\n",
      "Acc:  0.2716\n",
      "Precision: 0.1583\n",
      "Recall:    0.8148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.111 MB of 0.111 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.111 MB of 0.111 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.111 MB of 0.111 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.74733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.27683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.51608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.30615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.29204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.32169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.46537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.12635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.40694\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.19715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.12894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.41861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.42502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.1982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.44146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.27616\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.19365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.4812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.27164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.13724\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.40069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.26506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.15827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 0.81481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.49883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.13417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.42436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.20771\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.13913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.40964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_Run_v6_all_features\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/7uhvqlh6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 8 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250502_134823-7uhvqlh6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py --configs_path configs/v3_mha_1024_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with all data from wandb. Are TPPs again different than expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250502_142740-sjbdsgq6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_Run_v6_all_features\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/sjbdsgq6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1001.05MB. 46 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   46 of 46 files downloaded.  \n",
      "Done. 0:0:0.7\n",
      "Lade Testdaten von: ./WnB_Experiments_Datasets/beta_allele/allele/test.tsv\n",
      "162\n",
      "30\n",
      "63\n",
      "\n",
      " TPP-Verteilung im Testset (aus Datei):\n",
      "task\n",
      "TPP2    47912\n",
      "TPP1     7104\n",
      "TPP3     1167\n",
      "TPP4      335\n",
      "Name: count, dtype: int64\n",
      "Lade Embeddings...\n",
      "Lade Modell von wandb...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "✅ Modell geladen: Run_v3_mha_resh_model:v6\n",
      "Testing: 100%|████████████████████████████████| 442/442 [02:46<00:00,  2.65it/s]\n",
      "\n",
      "Testergebnisse:\n",
      "AUC:       0.4244\n",
      "AP:        0.1342\n",
      "F1 Score:  0.2077\n",
      "Accuracy:  0.4988\n",
      "Precision: 0.1391\n",
      "Recall:    0.4096\n",
      "TP: 3713, TN: 24480, FP: 22974, FN: 5351\n",
      "\n",
      "TPP1 (7104 Beispiele)\n",
      "AUC:  0.5160833222793372\n",
      "AP:   0.27683196777342184\n",
      "F1:   0.3061\n",
      "Acc:  0.7473\n",
      "Precision: 0.2920\n",
      "Recall:    0.3217\n",
      "\n",
      "TPP2 (47912 Beispiele)\n",
      "AUC:  0.40694396336777566\n",
      "AP:   0.12634818980093476\n",
      "F1:   0.1971\n",
      "Acc:  0.4654\n",
      "Precision: 0.1289\n",
      "Recall:    0.4186\n",
      "\n",
      "TPP3 (1167 Beispiele)\n",
      "AUC:  0.4414560263032721\n",
      "AP:   0.19820000899288082\n",
      "F1:   0.2762\n",
      "Acc:  0.4250\n",
      "Precision: 0.1936\n",
      "Recall:    0.4812\n",
      "\n",
      "TPP4 (335 Beispiele)\n",
      "AUC:  0.40068538289178857\n",
      "AP:   0.13724378188780437\n",
      "F1:   0.2651\n",
      "Acc:  0.2716\n",
      "Precision: 0.1583\n",
      "Recall:    0.8148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.74733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.27683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.51608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.30615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.29204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.32169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.46537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.12635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.40694\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.19715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.12894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.41861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.42502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.1982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.44146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.27616\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.19365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.4812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.27164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.13724\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.40069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.26506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.15827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 0.81481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.49883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.13417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.42436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.20771\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.13913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.40964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_Run_v6_all_features\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/sjbdsgq6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 8 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250502_142740-sjbdsgq6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py --configs_path configs/v3_mha_1024_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing v6, trained with lr: 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250504_084735-8gtmaci2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_Run_v6_all_features\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/8gtmaci2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1001.05MB. 46 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   46 of 46 files downloaded.  \n",
      "Done. 0:0:1.1\n",
      "Lade Testdaten von: ./WnB_Experiments_Datasets/beta_allele/allele/test.tsv\n",
      "162\n",
      "30\n",
      "63\n",
      "\n",
      " TPP-Verteilung im Testset (aus Datei):\n",
      "task\n",
      "TPP2    47912\n",
      "TPP1     7104\n",
      "TPP3     1167\n",
      "TPP4      335\n",
      "Name: count, dtype: int64\n",
      "Lade Embeddings...\n",
      "Lade Modell von wandb...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "✅ Modell geladen: Run_v6_all_featuresh_model:v0\n",
      "Testing: 100%|████████████████████████████████| 442/442 [05:25<00:00,  1.36it/s]\n",
      "\n",
      "Testergebnisse:\n",
      "AUC:       0.4271\n",
      "AP:        0.1332\n",
      "F1 Score:  0.2198\n",
      "Accuracy:  0.4519\n",
      "Precision: 0.1424\n",
      "Recall:    0.4816\n",
      "TP: 4365, TN: 21174, FP: 26280, FN: 4699\n",
      "\n",
      "TPP1 (7104 Beispiele)\n",
      "AUC:  0.5957238947375555\n",
      "AP:   0.2935216186824307\n",
      "F1:   0.3610\n",
      "Acc:  0.7613\n",
      "Precision: 0.3366\n",
      "Recall:    0.3891\n",
      "\n",
      "TPP2 (47912 Beispiele)\n",
      "AUC:  0.400747108684971\n",
      "AP:   0.12415477649031179\n",
      "F1:   0.2054\n",
      "Acc:  0.4099\n",
      "Precision: 0.1302\n",
      "Recall:    0.4865\n",
      "\n",
      "TPP3 (1167 Beispiele)\n",
      "AUC:  0.4125491308737994\n",
      "AP:   0.18364324702560364\n",
      "F1:   0.3200\n",
      "Acc:  0.3445\n",
      "Precision: 0.2095\n",
      "Recall:    0.6767\n",
      "\n",
      "TPP4 (335 Beispiele)\n",
      "AUC:  0.3180440226703572\n",
      "AP:   0.11346827174134361\n",
      "F1:   0.2939\n",
      "Acc:  0.2687\n",
      "Precision: 0.1741\n",
      "Recall:    0.9444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.103 MB of 0.103 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.103 MB of 0.103 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.103 MB of 0.103 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.103 MB of 0.103 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.76126\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.29352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.59572\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.36096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.33661\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.38911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.4099\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.12415\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.40075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.20544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.13021\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.48649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.34447\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.18364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.41255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.20955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.67669\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.26866\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.11347\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.31804\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.29395\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.17406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 0.94444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.45187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.13316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.42713\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.21985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.14244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.48158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_Run_v6_all_features\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/8gtmaci2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 8 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250504_084735-8gtmaci2/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py --configs_path configs/v3_mha_1024_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading model v6 from server and not from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250502_152148-gqj1auyc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_Run_v6_all_features\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/gqj1auyc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1001.05MB. 46 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   46 of 46 files downloaded.  \n",
      "Done. 0:0:0.6\n",
      "Lade Testdaten von: ./WnB_Experiments_Datasets/beta_allele/allele/test.tsv\n",
      "162\n",
      "30\n",
      "63\n",
      "\n",
      " TPP-Verteilung im Testset (aus Datei):\n",
      "task\n",
      "TPP2    47912\n",
      "TPP1     7104\n",
      "TPP3     1167\n",
      "TPP4      335\n",
      "Name: count, dtype: int64\n",
      "Lade Embeddings...\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "Testing: 100%|████████████████████████████████| 442/442 [02:46<00:00,  2.66it/s]\n",
      "\n",
      "Testergebnisse:\n",
      "AUC:       0.4244\n",
      "AP:        0.1342\n",
      "F1 Score:  0.2077\n",
      "Accuracy:  0.4988\n",
      "Precision: 0.1391\n",
      "Recall:    0.4096\n",
      "TP: 3713, TN: 24480, FP: 22974, FN: 5351\n",
      "\n",
      "TPP1 (7104 Beispiele)\n",
      "AUC:  0.5160833222793372\n",
      "AP:   0.27683196777342184\n",
      "F1:   0.3061\n",
      "Acc:  0.7473\n",
      "Precision: 0.2920\n",
      "Recall:    0.3217\n",
      "\n",
      "TPP2 (47912 Beispiele)\n",
      "AUC:  0.40694396336777566\n",
      "AP:   0.12634818980093476\n",
      "F1:   0.1971\n",
      "Acc:  0.4654\n",
      "Precision: 0.1289\n",
      "Recall:    0.4186\n",
      "\n",
      "TPP3 (1167 Beispiele)\n",
      "AUC:  0.4414560263032721\n",
      "AP:   0.19820000899288082\n",
      "F1:   0.2762\n",
      "Acc:  0.4250\n",
      "Precision: 0.1936\n",
      "Recall:    0.4812\n",
      "\n",
      "TPP4 (335 Beispiele)\n",
      "AUC:  0.40068538289178857\n",
      "AP:   0.13724378188780437\n",
      "F1:   0.2651\n",
      "Acc:  0.2716\n",
      "Precision: 0.1583\n",
      "Recall:    0.8148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.106 MB of 0.106 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.74733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.27683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.51608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.30615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.29204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.32169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.46537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.12635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.40694\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.19715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.12894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.41861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.42502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.1982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.44146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.27616\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.19365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.4812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.27164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.13724\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.40069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.26506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.15827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 0.81481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.49883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.13417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.42436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.20771\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.13913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.40964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_Run_v6_all_features\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/gqj1auyc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 8 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250502_152148-gqj1auyc/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py --configs_path configs/v3_mha_1024_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train v6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code lines for the terminal in server 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python models_scripts/v1_mha/train-server2-bb-v6-all_features_pe.py --configs_path configs/v3_mha_1024_config.yaml \\\n",
    "--model_path \"results/trained_models/v6_all_features.pth\" \\\n",
    "--learning_rate 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train-server1-bb-v6-all_features_pe.py --configs_path \"../../configs/v3_mha_1024_config-serv1.yaml\" \\\n",
    "--model_path \"results/trained_models/v6_dropout_high_lr_low.pth\" \\\n",
    "--dropout 0.4 \\\n",
    "    --batch_size 512 \\\n",
    "        --epochs 40 \\\n",
    "            --tcr_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\" \\\n",
    "                            --learning_rate 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train-server1-bb-v6-all_features_pe.py --configs_path \"../../configs/v3_mha_1024_config-serv1.yaml\" --model_path \"results/trained_models/v6_all_features_dropout_high.pth\" --dropout 0.5     --batch_size 256         --epochs 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train-server1-bb-v6-all_features_pe.py --configs_path \"../../configs/v3_mha_1024_config-serv1.yaml\" \\\n",
    "    --model_path \"results/trained_models/v6_all_features_MaskChecked\" \\\n",
    "--tcr_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\" \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train-server1-bb-v6-all_features_pe_noCross.py --configs_path \"../../configs/v3_mha_1024_config-serv1.yaml\" \\\n",
    "    --model_path \"results/trained_models/v6_all_features_NoCross_Mask.pth\" \\\n",
    "--tcr_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\" \n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test v6_mask_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250506_145247-5sr1cfw7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_Run_v6_all_features\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/5sr1cfw7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1001.05MB. 46 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   46 of 46 files downloaded.  \n",
      "Done. 0:0:1.5\n",
      "Lade Testdaten von: ./WnB_Experiments_Datasets/beta_allele/allele/test.tsv\n",
      "162\n",
      "30\n",
      "63\n",
      "\n",
      " TPP-Verteilung im Testset (aus Datei):\n",
      "task\n",
      "TPP2    47912\n",
      "TPP1     7104\n",
      "TPP3     1167\n",
      "TPP4      335\n",
      "Name: count, dtype: int64\n",
      "Lade Embeddings...\n",
      "Lade Modell von wandb...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "✅ Modell geladen: Run_v6_all_features_MaskChecked_model:v0\n",
      "Testing: 100%|████████████████████████████████| 442/442 [14:18<00:00,  1.94s/it]\n",
      "\n",
      "Testergebnisse:\n",
      "AUC:       0.4499\n",
      "AP:        0.1425\n",
      "F1 Score:  0.2199\n",
      "Accuracy:  0.4837\n",
      "Precision: 0.1451\n",
      "Recall:    0.4538\n",
      "TP: 4113, TN: 23225, FP: 24229, FN: 4951\n",
      "\n",
      "TPP1 (7104 Beispiele)\n",
      "AUC:  0.6301015690496223\n",
      "AP:   0.3271538996526241\n",
      "F1:   0.3271\n",
      "Acc:  0.7584\n",
      "Precision: 0.3161\n",
      "Recall:    0.3387\n",
      "\n",
      "TPP2 (47912 Beispiele)\n",
      "AUC:  0.4227865590580888\n",
      "AP:   0.1322646997635706\n",
      "F1:   0.2087\n",
      "Acc:  0.4465\n",
      "Precision: 0.1345\n",
      "Recall:    0.4656\n",
      "\n",
      "TPP3 (1167 Beispiele)\n",
      "AUC:  0.465022155833535\n",
      "AP:   0.21537878009270667\n",
      "F1:   0.3078\n",
      "Acc:  0.3950\n",
      "Precision: 0.2082\n",
      "Recall:    0.5902\n",
      "\n",
      "TPP4 (335 Beispiele)\n",
      "AUC:  0.4043100039541321\n",
      "AP:   0.1283518809506018\n",
      "F1:   0.2562\n",
      "Acc:  0.2896\n",
      "Precision: 0.1541\n",
      "Recall:    0.7593\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.099 MB of 0.099 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.100 MB of 0.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.104 MB of 0.104 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.104 MB of 0.104 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.104 MB of 0.104 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.75845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.32715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.6301\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.32706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.31615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.33875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.44649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.13226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.42279\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.20874\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.13452\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.46559\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.39503\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.21538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.46502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.30784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.20822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.59023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.28955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.12835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.40431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.25625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.15414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 0.75926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.4837\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.14254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.44987\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.21991\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.14512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.45377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_Run_v6_all_features\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/5sr1cfw7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 11 artifact file(s) and 8 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250506_145247-5sr1cfw7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py --configs_path configs/v3_mha_1024_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test v6 maskChcked NoCross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250506_211903-n2pub6bp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_Run_v6_NoCross\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/n2pub6bp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1001.05MB. 46 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   46 of 46 files downloaded.  \n",
      "Done. 0:0:1.0\n",
      "Lade Testdaten von: ./WnB_Experiments_Datasets/beta_allele/allele/test.tsv\n",
      "162\n",
      "30\n",
      "63\n",
      "\n",
      " TPP-Verteilung im Testset (aus Datei):\n",
      "task\n",
      "TPP2    47912\n",
      "TPP1     7104\n",
      "TPP3     1167\n",
      "TPP4      335\n",
      "Name: count, dtype: int64\n",
      "Lade Embeddings...\n",
      "Lade Modell von wandb...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "✅ Modell geladen: Run_v6_all_features_NoCross_Mask_model:v0\n",
      "Testing: 100%|████████████████████████████████| 442/442 [08:16<00:00,  1.12s/it]\n",
      "\n",
      "Testergebnisse:\n",
      "AUC:       0.4530\n",
      "AP:        0.1423\n",
      "F1 Score:  0.2287\n",
      "Accuracy:  0.4584\n",
      "Precision: 0.1482\n",
      "Recall:    0.5007\n",
      "TP: 4538, TN: 21370, FP: 26084, FN: 4526\n",
      "\n",
      "TPP1 (7104 Beispiele)\n",
      "AUC:  0.6235366019135331\n",
      "AP:   0.3674969826533384\n",
      "F1:   0.3672\n",
      "Acc:  0.7706\n",
      "Precision: 0.3517\n",
      "Recall:    0.3842\n",
      "\n",
      "TPP2 (47912 Beispiele)\n",
      "AUC:  0.4251933248950991\n",
      "AP:   0.13157382381737465\n",
      "F1:   0.2143\n",
      "Acc:  0.4163\n",
      "Precision: 0.1358\n",
      "Recall:    0.5077\n",
      "\n",
      "TPP3 (1167 Beispiele)\n",
      "AUC:  0.47719326062103096\n",
      "AP:   0.21696699162737335\n",
      "F1:   0.3415\n",
      "Acc:  0.3325\n",
      "Precision: 0.2203\n",
      "Recall:    0.7594\n",
      "\n",
      "TPP4 (335 Beispiele)\n",
      "AUC:  0.5402003426914459\n",
      "AP:   0.2316729298743622\n",
      "F1:   0.2934\n",
      "Acc:  0.2955\n",
      "Precision: 0.1750\n",
      "Recall:    0.9074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.096 MB of 0.096 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.096 MB of 0.096 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.096 MB of 0.096 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.77055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.3675\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.62354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.36724\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.35167\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.38424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.41633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.13157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.42519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.21431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.13583\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.50765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.33248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.21697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.47719\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.3415\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.22028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.7594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.29552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.23167\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.5402\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.29341\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 0.90741\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.4584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.14226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.45305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.2287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.14819\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.50066\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_Run_v6_NoCross\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/n2pub6bp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 8 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250506_211903-n2pub6bp/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py --configs_path configs/v3_mha_1024_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train v6 - small batch size: 32 - dropout: 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python models_scripts/v1_mha/train-server2-bb-v6-all_features_pe.py --configs_path \"configs/v3_mha_1024_config-serv1.yaml\" \\\n",
    "    --batch_size 32 \\\n",
    "        --model_path \"results/trained_models/v6_all_features_smallBatch.pth\" \\\n",
    "            --dropout 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py --configs_path configs/v3_mha_1024_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train v6 double Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's actually 'doubleCross' and not noCross, since the model name was changed in the script\n",
    "\n",
    "python models_scripts/v1_mha/train-server1-bb-v6-all_features_pe_noCross.py --configs_path \"configs/v3_mha_1024_config-serv1.yaml\" \\\n",
    "    --batch_size 32 \\\n",
    "        --model_path \"results/trained_models/v6_all_features_doubleCross.pth\" \\\n",
    "            --dropout 0.2 \\\n",
    "                --tcr_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train v6 with oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's actually '_oversample' and not '_noCross', since the model name was changed in the script and the oversample class is in the train.py \n",
    "\n",
    "python models_scripts/v1_mha/train-server1-bb-v6-all_features_pe_oversample.py --configs_path \"../../configs/v3_mha_1024_config-serv1.yaml\" \\\n",
    "        --model_path \"results/trained_models/v6_all_features_oversample.pth\" \\\n",
    "                --tcr_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test v6_doubleCross batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250509_092134-pm03h852\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_Run_v6_doubleCross\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/pm03h852\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1001.05MB. 46 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   46 of 46 files downloaded.  \n",
      "Done. 0:0:1.0\n",
      "Lade Testdaten von: ./WnB_Experiments_Datasets/beta_allele/allele/test.tsv\n",
      "162\n",
      "30\n",
      "63\n",
      "\n",
      " TPP-Verteilung im Testset (aus Datei):\n",
      "task\n",
      "TPP2    47912\n",
      "TPP1     7104\n",
      "TPP3     1167\n",
      "TPP4      335\n",
      "Name: count, dtype: int64\n",
      "Lade Embeddings...\n",
      "Lade Modell von wandb...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "✅ Modell geladen: Run_v6_all_features_doubleCross_model:v0\n",
      "Testing: 100%|████████████████████████████████| 442/442 [02:35<00:00,  2.84it/s]\n",
      "\n",
      "Testergebnisse:\n",
      "AUC:       0.4564\n",
      "AP:        0.1448\n",
      "F1 Score:  0.2193\n",
      "Accuracy:  0.4835\n",
      "Precision: 0.1447\n",
      "Recall:    0.4522\n",
      "TP: 4099, TN: 23227, FP: 24227, FN: 4965\n",
      "\n",
      "TPP1 (7104 Beispiele)\n",
      "AUC:  0.6697632379268577\n",
      "AP:   0.3538332163147748\n",
      "F1:   0.3329\n",
      "Acc:  0.7620\n",
      "Precision: 0.3236\n",
      "Recall:    0.3428\n",
      "\n",
      "TPP2 (47912 Beispiele)\n",
      "AUC:  0.4254424009893038\n",
      "AP:   0.133799003706038\n",
      "F1:   0.2066\n",
      "Acc:  0.4475\n",
      "Precision: 0.1333\n",
      "Recall:    0.4587\n",
      "\n",
      "TPP3 (1167 Beispiele)\n",
      "AUC:  0.46530797025860987\n",
      "AP:   0.22291718601473876\n",
      "F1:   0.3175\n",
      "Acc:  0.3222\n",
      "Precision: 0.2060\n",
      "Recall:    0.6917\n",
      "\n",
      "TPP4 (335 Beispiele)\n",
      "AUC:  0.5643864505074471\n",
      "AP:   0.25083105196935884\n",
      "F1:   0.2806\n",
      "Acc:  0.2806\n",
      "Precision: 0.1673\n",
      "Recall:    0.8704\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.103 MB of 0.103 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.104 MB of 0.104 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.109 MB of 0.109 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.109 MB of 0.109 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.109 MB of 0.109 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.76197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.35383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.66976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.33294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.32362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.34281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.44755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.1338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.42544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.20659\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.13332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.45867\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.32219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.22292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.46531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.31752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.20605\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.69173\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.2806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.25083\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.56439\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.2806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.16726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 0.87037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.48349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.14477\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.45638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.21926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.14471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.45223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_Run_v6_doubleCross\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/pm03h852\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 9 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250509_092134-pm03h852/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v6_1024-bb-pe-all_features.py --configs_path configs/v3_mha_1024_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train SimplyClassifier in server1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train-server1_v2_simplyClassifier.py --configs_path \"../../configs/v3_mha_1024_config-serv1.yaml\" \\\n",
    "    --epochs 20 \\\n",
    "        --model_path \"results/trained_models/v0_simplyClassifier.pth\" \\\n",
    "                --tcr_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SimpleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marina-frohofer\u001b[0m (\u001b[33mba_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oscar/BA-Cancer-Immunotherapy/wandb/run-20250514_232546-tqzzpjx6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTest_v0_simpleClassifier\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/tqzzpjx6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact beta_allele:latest, 1316.87MB. 93 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   93 of 93 files downloaded.  \n",
      "Done. 0:0:1.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " Lade Testdaten: ./WnB_Experiments_Datasets/beta_allele/allele/test.tsv\n",
      "\n",
      " TPP-Verteilung im Testset (aus Datei):\n",
      "task\n",
      "TPP2    32753\n",
      "TPP1     6272\n",
      "TPP3     5416\n",
      "TPP4      671\n",
      "Name: count, dtype: int64\n",
      "/home/ubuntu/oscar/BA-Cancer-Immunotherapy/models_scripts/v1_mha/test_v0_simple_Classifier.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      " Modell geladen von: Run_v0_simplyClassifierh_model:v0\n",
      "Evaluating: 100%|█████████████████████████████| 353/353 [05:50<00:00,  1.01it/s]\n",
      "\n",
      " Gesamtauswertung:\n",
      "AUC:  0.4635\n",
      "AP:   0.1852\n",
      "F1:   0.2964\n",
      "Acc:  0.4446\n",
      "Precision: 0.1991\n",
      "Recall:    0.5797\n",
      "\n",
      "    TPP1 (6272 Beispiele)\n",
      "AUC:  0.6369928977221353\n",
      "AP:   0.4430681773086374\n",
      "F1:   0.4746\n",
      "Acc:  0.7522\n",
      "Precision: 0.3838\n",
      "Recall:    0.6218\n",
      "\n",
      "    TPP2 (32753 Beispiele)\n",
      "AUC:  0.4257984109356719\n",
      "AP:   0.17553372436919662\n",
      "F1:   0.2832\n",
      "Acc:  0.3912\n",
      "Precision: 0.1874\n",
      "Recall:    0.5790\n",
      "\n",
      "    TPP3 (5416 Beispiele)\n",
      "AUC:  0.4387636650771488\n",
      "AP:   0.1627034081830418\n",
      "F1:   0.2660\n",
      "Acc:  0.4333\n",
      "Precision: 0.1774\n",
      "Recall:    0.5310\n",
      "\n",
      "    TPP4 (671 Beispiele)\n",
      "AUC:  0.4921071792631425\n",
      "AP:   0.22051517074088262\n",
      "F1:   0.2470\n",
      "Acc:  0.2638\n",
      "Precision: 0.1528\n",
      "Recall:    0.6429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.100 MB of 0.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.100 MB of 0.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.100 MB of 0.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.100 MB of 0.100 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.102 MB of 0.102 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.102 MB of 0.102 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.102 MB of 0.102 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.102 MB of 0.102 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.102 MB of 0.102 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.107 MB of 0.107 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.107 MB of 0.107 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 0.107 MB of 0.107 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP1_accuracy 0.75223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_ap 0.44307\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP1_auc 0.63699\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP1_f1 0.47465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP1_precision 0.38382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP1_recall 0.62179\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP2_accuracy 0.39123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_ap 0.17553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP2_auc 0.4258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP2_f1 0.28321\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP2_precision 0.18745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP2_recall 0.57901\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP3_accuracy 0.43335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_ap 0.1627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP3_auc 0.43876\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP3_f1 0.26597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP3_precision 0.17741\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP3_recall 0.53104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  TPP4_accuracy 0.26379\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_ap 0.22052\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       TPP4_auc 0.49211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        TPP4_f1 0.24695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: TPP4_precision 0.15283\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    TPP4_recall 0.64286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.44458\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_ap 0.18522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       test_auc 0.46351\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.29642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.19912\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.57968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTest_v0_simpleClassifier\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele/runs/tqzzpjx6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ba_cancerimmunotherapy/dataset-allele\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 13 artifact file(s) and 9 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250514_232546-tqzzpjx6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python models_scripts/v1_mha/test_v0_simple_Classifier.py --configs_path configs/v0_simpleClassifier.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v6 enhanced!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for server 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train-server2-v6_enhanced.py --configs_path \"../../configs/v3_mha_1024_config-serv1.yaml\" \\\n",
    "    --epochs 10 \\\n",
    "        --model_path \"results/trained_models/v6_enhanced.pth\" \\\n",
    "                --tcr_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_train_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for server 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train-server2-v6_enhanced.py --configs_path \"../../configs/v3_mha_1024_config-Copy1.yaml\" \\\n",
    "    --epochs 10 \\\n",
    "        --model_path \"results/trained_models/v6_enhanced.pth\" \\\n",
    "                --tcr_train_embeddings \"../../../../data/embeddings/beta/allele/dimension_1024/padded_train_tcr_embeddings_final.h5\" \\\n",
    "                --epitope_train_embeddings \"../../../../data/embeddings/beta/allele/dimension_1024/padded_train_epitope_embeddings_final.h5\" \\\n",
    "                    --tcr_valid_embeddings \"../../../../data/embeddings/beta/allele/dimension_1024/padded_valid_tcr_embeddings_final.h5\" \\\n",
    "                        --epitope_valid_embeddings \"../../../../data/embeddings/beta/allele/dimension_1024/padded_valid_epitope_embeddings_final.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddings for trbj-v and mhc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 21:43:27.033632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746308607.058296  137602 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746308607.064602  137602 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746308607.086268  137602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746308607.086287  137602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746308607.086289  137602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746308607.086291  137602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 21:43:27.093495: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Loading: Rostlab/prot_bert\n",
      "Model is on device: cuda:0\n",
      "Sequence: CASSIRSSYEQYF, Embedding shape: (13, 1024)\n",
      "Sequence: TRBV7-2, Embedding shape: (7, 1024)\n",
      "Sequence: HLA-A*02:01, Embedding shape: (11, 1024)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import argparse\n",
    "import re\n",
    "import torch \n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"Using GPU: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using device: CPU\")\n",
    "\n",
    "# Load ProtBERT model and tokenizer\n",
    "transformer_link = \"Rostlab/prot_bert\"\n",
    "print(\"Loading: {}\".format(transformer_link))\n",
    "model = BertModel.from_pretrained(transformer_link)\n",
    "tokenizer = BertTokenizer.from_pretrained(transformer_link, do_lower_case=False, legacy=True)\n",
    "\n",
    "gc.collect()\n",
    "model.to(torch.float32)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "def process_sequences(sequences):\n",
    "    # Preprocess sequences\n",
    "    processed_seqs = [(seq, \" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq)))) for seq in sequences]\n",
    "\n",
    "    # Tokenize\n",
    "    input_texts = [seq[1] for seq in processed_seqs]\n",
    "    ids = tokenizer.batch_encode_plus(input_texts, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    input_ids = ids['input_ids'].to(device)\n",
    "    attention_mask = ids['attention_mask'].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Extract embeddings\n",
    "    embeddings = {}\n",
    "    for i, (original_seq, _) in enumerate(processed_seqs):\n",
    "        seq_len = len(original_seq)\n",
    "        valid_embeddings = last_hidden_states[i, :seq_len]\n",
    "        embeddings[original_seq] = valid_embeddings.cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Example input list: replace with your actual trbj/trbv/mhc strings\n",
    "input_sequences = [\"CASSIRSSYEQYF\", \"TRBV7-2\", \"HLA-A*02:01\"]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = process_sequences(input_sequences)\n",
    "\n",
    "# Optional: Print shape of each embedding\n",
    "for seq, emb in embeddings.items():\n",
    "    print(f\"Sequence: {seq}, Embedding shape: {emb.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful chunks...  just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'correct' padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to embeddings files\n",
    "# paired_all_epi_path = '../../data/embeddings/paired/allele/Epitope_paired_embeddings.npz'\n",
    "# paired_all_tra_path = '../../data/embeddings/paired/allele/TRA_paired_embeddings.npz'\n",
    "# paired_all_trb_path = '../../data/embeddings/paired/allele/TRB_paired_embeddings.npz'\n",
    "\n",
    "beta_all_epi_path = './dummy_data/subset_padded_epitope_embeddings.npz'\n",
    "beta_all_trb_path = './dummy_data/subset_padded_tcr_embeddings.npz'\n",
    "\n",
    "# Load NPZ files into GPU\n",
    "def load_embeddings_to_gpu(path):\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    return {key: torch.tensor(data[key], device=device) for key in data}\n",
    "\n",
    "# epi_embeddings = load_embeddings_to_gpu(paired_all_epi_path)\n",
    "# tra_embeddings = load_embeddings_to_gpu(paired_all_tra_path)\n",
    "# trb_embeddings = load_embeddings_to_gpu(paired_all_trb_path)\n",
    "\n",
    "beta_epi_embeddings = load_embeddings_to_gpu(beta_all_epi_path)\n",
    "beta_trb_embeddings = load_embeddings_to_gpu(beta_all_trb_path)\n",
    "\n",
    "# Find max sequence length\n",
    "# max_len = max(max(e.shape[0] for e in epi_embeddings.values()), \n",
    "#               max(e.shape[0] for e in tra_embeddings.values()), \n",
    "#               max(e.shape[0] for e in trb_embeddings.values()))\n",
    "\n",
    "beta_max_len = max(max(e.shape[0] for e in beta_epi_embeddings.values()), \n",
    "                    max(e.shape[0] for e in beta_trb_embeddings.values()))\n",
    "\n",
    "# Function to pad embeddings on GPU\n",
    "def pad_embeddings(embeddings, max_len):\n",
    "    return pad_sequence(\n",
    "        [torch.nn.functional.pad(e, (0, 0, 0, max_len - e.shape[0])) for e in embeddings.values()],\n",
    "        batch_first=True, padding_value=0.0\n",
    "    )\n",
    "\n",
    "# Pad embeddings (all computations on GPU)\n",
    "# padded_epi = pad_embeddings(epi_embeddings, max_len)\n",
    "# padded_tra = pad_embeddings(tra_embeddings, max_len)\n",
    "# padded_trb = pad_embeddings(trb_embeddings, max_len)\n",
    "\n",
    "padded_beta_epi = pad_embeddings(beta_epi_embeddings, beta_max_len)\n",
    "padded_beta_trb = pad_embeddings(beta_trb_embeddings, beta_max_len)\n",
    "\n",
    "# Move back to CPU before saving\n",
    "# padded_epi = padded_epi.cpu().numpy()\n",
    "# padded_tra = padded_tra.cpu().numpy()\n",
    "# padded_trb = padded_trb.cpu().numpy()\n",
    "\n",
    "padded_beta_epi = padded_beta_epi.cpu().numpy()\n",
    "padded_beta_trb = padded_beta_trb.cpu().numpy()\n",
    "\n",
    "# Save padded embeddings\n",
    "# padd_paired_all_epi_path = '../../data/embeddings/paired/allele/padded_Epitope_paired_embeddings.npz'\n",
    "# padd_paired_all_tra_path = '../../data/embeddings/paired/allele/padded_TRA_paired_embeddings.npz'\n",
    "# padd_paired_all_trb_path = '../../data/embeddings/paired/allele/padded_TRB_paired_embeddings.npz'\n",
    "\n",
    "padd_beta_all_epi_path = './dummy_data/subset_v2_padded_epitope_embeddings.npz'\n",
    "padd_beta_all_trb_path = './dummy_data/subset_v2_padded_tcr_embeddings.npz'\n",
    "\n",
    "# np.savez(padd_paired_all_epi_path, **{key: padded_epi[i] for i, key in enumerate(epi_embeddings)})\n",
    "# np.savez(padd_paired_all_tra_path, **{key: padded_tra[i] for i, key in enumerate(tra_embeddings)})\n",
    "# np.savez(padd_paired_all_trb_path, **{key: padded_trb[i] for i, key in enumerate(trb_embeddings)})\n",
    "\n",
    "np.savez(padd_beta_all_epi_path, **{key: padded_beta_epi[i] for i, key in enumerate(beta_epi_embeddings)})\n",
    "np.savez(padd_beta_all_trb_path, **{key: padded_beta_trb[i] for i, key in enumerate(beta_trb_embeddings)})\n",
    "\n",
    "print(\"Padded embedding saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
