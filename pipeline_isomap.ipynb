{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95b12b4-3293-45af-9438-674000421c33",
   "metadata": {},
   "source": [
    "Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25e0d55-72a2-4190-9877-43a1e955aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cupy as cp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430ec0c-145f-4d39-b8c7-0c23df7a8554",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693816a3-c422-4192-8c01-670d0fc6487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d660a56-7780-4f7e-85f8-2961ca4bdec3",
   "metadata": {},
   "source": [
    "Pfad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7dbdcbb-df74-4181-91db-b78dec71cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade die Dateien\n",
    "beta_all_epi = np.load('../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz')\n",
    "beta_all_trb = np.load('../../data/embeddings/beta/allele/TRB_beta_embeddings.npz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2b33f-e5f3-419f-8de0-93e426bd3ad7",
   "metadata": {},
   "source": [
    "***CHECK VOR DIMENSION REDUCTION***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c79e96-3fe5-4ae7-98d4-3256d17f8d88",
   "metadata": {},
   "source": [
    "Keys, Shape, Dimension von Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd884ce9-7824-4723-ac12-8368560c4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Anzahl der Keys in Epitope_beta_embeddings: 1896\n",
      "üìå Erste 5 Keys: ['YLDELIKNT', 'IQPGQTFSV', 'FLPSDFFPSV', 'YVVPGSPCL', 'LVMPFSIVYI']\n",
      "üìå Letzte 5 Keys: ['FTVLCLTPV', 'GTGPEAGLPY', 'VEQCCTSI', 'KMVAVFYNT', 'YLEPGPVTV']\n",
      "üî¢ Anzahl der Keys in TRB_beta_embeddings: 211529\n",
      "üìå Erste 5 Keys: ['CASSLEGTGVSGANVLTF', 'CASSPDSNTGELFF', 'CASSHSQGADGELFF', 'CASSYPDNYGYTF', 'CAIILSPPWWGYNEQFF']\n",
      "üìå Letzte 5 Keys: ['CASNLKGLAGGPGTQYF', 'CATSDPMDQITGELFF', 'CASSLDKSSYEQYF', 'CASSQGAAETQYF', 'CASSEGDRGQAFF']\n",
      "\n",
      "üîπ Shapes der ersten 5 Epitope Keys:\n",
      "  YLDELIKNT: (9, 1024)\n",
      "  IQPGQTFSV: (9, 1024)\n",
      "  FLPSDFFPSV: (10, 1024)\n",
      "  YVVPGSPCL: (9, 1024)\n",
      "  LVMPFSIVYI: (10, 1024)\n",
      "\n",
      "üîπ Shapes der letzten 5 Epitope Keys:\n",
      "  FTVLCLTPV: (9, 1024)\n",
      "  GTGPEAGLPY: (10, 1024)\n",
      "  VEQCCTSI: (8, 1024)\n",
      "  KMVAVFYNT: (9, 1024)\n",
      "  YLEPGPVTV: (9, 1024)\n",
      "\n",
      "üîπ Shapes der ersten 5 TRB Keys:\n",
      "  CASSLEGTGVSGANVLTF: (18, 1024)\n",
      "  CASSPDSNTGELFF: (14, 1024)\n",
      "  CASSHSQGADGELFF: (15, 1024)\n",
      "  CASSYPDNYGYTF: (13, 1024)\n",
      "  CAIILSPPWWGYNEQFF: (17, 1024)\n",
      "\n",
      "üîπ Shapes der letzten 5 TRB Keys:\n",
      "  CASNLKGLAGGPGTQYF: (17, 1024)\n",
      "  CATSDPMDQITGELFF: (16, 1024)\n",
      "  CASSLDKSSYEQYF: (14, 1024)\n",
      "  CASSQGAAETQYF: (13, 1024)\n",
      "  CASSEGDRGQAFF: (13, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Erhalte die Keys als Liste\n",
    "epi_keys = list(beta_all_epi.keys())\n",
    "trb_keys = list(beta_all_trb.keys())\n",
    "\n",
    "# Drucke die ersten 5 und letzten 5 Keys\n",
    "print(f\"üî¢ Anzahl der Keys in Epitope_beta_embeddings: {len(epi_keys)}\")\n",
    "print(f\"üìå Erste 5 Keys: {epi_keys[:5]}\")\n",
    "print(f\"üìå Letzte 5 Keys: {epi_keys[-5:]}\")\n",
    "\n",
    "print(f\"üî¢ Anzahl der Keys in TRB_beta_embeddings: {len(trb_keys)}\")\n",
    "print(f\"üìå Erste 5 Keys: {trb_keys[:5]}\")\n",
    "print(f\"üìå Letzte 5 Keys: {trb_keys[-5:]}\")\n",
    "\n",
    "# Pr√ºfe die Form der ersten & letzten 5 Keys f√ºr beide Embeddings\n",
    "print(\"\\nüîπ Shapes der ersten 5 Epitope Keys:\")\n",
    "for key in epi_keys[:5]:\n",
    "    print(f\"  {key}: {beta_all_epi[key].shape}\")\n",
    "\n",
    "print(\"\\nüîπ Shapes der letzten 5 Epitope Keys:\")\n",
    "for key in epi_keys[-5:]:\n",
    "    print(f\"  {key}: {beta_all_epi[key].shape}\")\n",
    "\n",
    "print(\"\\nüîπ Shapes der ersten 5 TRB Keys:\")\n",
    "for key in trb_keys[:5]:\n",
    "    print(f\"  {key}: {beta_all_trb[key].shape}\")\n",
    "\n",
    "print(\"\\nüîπ Shapes der letzten 5 TRB Keys:\")\n",
    "for key in trb_keys[-5:]:\n",
    "    print(f\"  {key}: {beta_all_trb[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f33ca9-8f34-4a91-aa84-30c5fbf5f042",
   "metadata": {},
   "source": [
    "***ISOMAP***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7d500-f546-4635-b2a9-fea592695316",
   "metadata": {},
   "source": [
    "sklearn mit GPU-optimierter Berechnung durch CuPy & PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d219976a-262e-4713-991d-c2bf9895e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" # Das erlaubt PyTorch, Speicher besser zu verwalten und Fragmentierung zu vermeiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f63b162-de72-4287-a9ec-d4593b1eb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2b6d55-13c0-4b44-abc9-c9096b2dc22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Lade Datei: ../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\n",
      "üî¢ Gesamtshape vor Isomap: torch.Size([19260, 1024])\n",
      "‚úÖ Gesamtshape nach Isomap: (19260, 512)\n",
      "üíæ Reduzierte Embeddings gespeichert unter: ../../data/embeddings/beta/allele/isomap/Epitope_beta_embeddings_reduced.npz\n",
      "üìÇ Lade Datei: ../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.85 GiB. GPU 0 has a total capacity of 14.56 GiB of which 2.54 GiB is free. Including non-PyTorch memory, this process has 12.02 GiB memory in use. Of the allocated memory 11.85 GiB is allocated by PyTorch, and 58.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m\n\u001b[1;32m     56\u001b[0m apply_isomap_gpu_preprocessed(\n\u001b[1;32m     57\u001b[0m     input_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     58\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/beta/allele/isomap/Epitope_beta_embeddings_reduced.npz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     59\u001b[0m     n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     60\u001b[0m     n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# === Isomap f√ºr TCR-Embeddings ===\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m apply_isomap_gpu_preprocessed(\n\u001b[1;32m     65\u001b[0m     input_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     66\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/beta/allele/isomap/TRB_beta_embeddings_reduced.npz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     67\u001b[0m     n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     68\u001b[0m     n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     69\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mapply_isomap_gpu_preprocessed\u001b[0;34m(input_path, output_path, n_components, n_neighbors)\u001b[0m\n\u001b[1;32m     24\u001b[0m     all_embeddings\u001b[38;5;241m.\u001b[39mappend(tensor)\n\u001b[1;32m     25\u001b[0m     key_list\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m---> 27\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Auf der GPU kombinieren\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müî¢ Gesamtshape vor Isomap: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_embeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Normalisierung auf der GPU\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.85 GiB. GPU 0 has a total capacity of 14.56 GiB of which 2.54 GiB is free. Including non-PyTorch memory, this process has 12.02 GiB memory in use. Of the allocated memory 11.85 GiB is allocated by PyTorch, and 58.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def apply_isomap_gpu_preprocessed(input_path, output_path, n_components=512, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    Nutzt GPU f√ºr Datenvorbereitung (Normalisierung mit Cupy/PyTorch), aber Isomap l√§uft auf CPU.\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Lade Datei: {input_path}\")\n",
    "    data = np.load(input_path, allow_pickle=True)\n",
    "\n",
    "    isomap = Isomap(n_neighbors=n_neighbors, n_components=n_components)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    all_embeddings = []\n",
    "    key_list = []\n",
    "\n",
    "    # 1Ô∏è‚É£ Verarbeitung der Keys auf GPU mit Cupy\n",
    "    for key in data.files:\n",
    "        tensor = torch.tensor(data[key], dtype=torch.float32, device=\"cuda\")  # GPU\n",
    "        all_embeddings.append(tensor)\n",
    "        key_list.append(key)\n",
    "\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)  # Auf der GPU kombinieren\n",
    "    print(f\"üî¢ Gesamtshape vor Isomap: {all_embeddings.shape}\")\n",
    "\n",
    "    # Normalisierung auf der GPU\n",
    "    all_embeddings = (all_embeddings - all_embeddings.mean()) / all_embeddings.std()\n",
    "\n",
    "    # Zur CPU zur√ºck, bevor Isomap aufgerufen wird\n",
    "    all_embeddings_cpu = all_embeddings.cpu().numpy()\n",
    "\n",
    "    # 2Ô∏è‚É£ CPU-basiertes Isomap Fit & Transform\n",
    "    reduced_embeddings = isomap.fit_transform(all_embeddings_cpu)\n",
    "\n",
    "    print(f\"‚úÖ Gesamtshape nach Isomap: {reduced_embeddings.shape}\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Zur√ºck auf Keys aufteilen & speichern\n",
    "    start_idx = 0\n",
    "    reduced_data = {}\n",
    "\n",
    "    for i, key in enumerate(key_list):\n",
    "        num_samples = data[key].shape[0]\n",
    "        reduced_data[key] = reduced_embeddings[start_idx:start_idx+num_samples]\n",
    "        start_idx += num_samples\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    np.savez_compressed(output_path, **reduced_data)\n",
    "    print(f\"üíæ Reduzierte Embeddings gespeichert unter: {output_path}\")\n",
    "\n",
    "# === Isomap f√ºr Epitope-Embeddings ===\n",
    "apply_isomap_gpu_preprocessed(\n",
    "    input_path='../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz',\n",
    "    output_path='../../data/embeddings/beta/allele/isomap/Epitope_beta_embeddings_reduced.npz',\n",
    "    n_components=512,\n",
    "    n_neighbors=10\n",
    ")\n",
    "\n",
    "''' ***TCR MUSS IN BATCHES VERARBEITET WERDEN***\n",
    "# === Isomap f√ºr TCR-Embeddings ===\n",
    "apply_isomap_gpu_preprocessed(\n",
    "    input_path='../../data/embeddings/beta/allele/TRB_beta_embeddings.npz',\n",
    "    output_path='../../data/embeddings/beta/allele/isomap/TRB_beta_embeddings_reduced.npz',\n",
    "    n_components=512,\n",
    "    n_neighbors=10\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d7f69-6155-418a-8821-2c93d3ca2df2",
   "metadata": {},
   "source": [
    "TCRB mit Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d6f2a-895a-460e-b1a1-6f44b3adf7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Lade Datei: ../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.manifold import Isomap\n",
    "import os\n",
    "\n",
    "def apply_isomap_gpu_batched(input_path, output_path, n_components=512, n_neighbors=50, batch_size=2000):\n",
    "    \"\"\"\n",
    "    Nutzt GPU f√ºr Datenvorbereitung (Normalisierung mit PyTorch), aber verarbeitet TRB in Batches, um Speicherprobleme zu vermeiden.\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Lade Datei: {input_path}\")\n",
    "    data_npz = np.load(input_path, allow_pickle=True)\n",
    "\n",
    "    # **Daten in ein ver√§nderbares Dictionary laden & zu float32 konvertieren**\n",
    "    data = {key: data_npz[key].astype(np.float32) for key in data_npz.files}\n",
    "\n",
    "    isomap = Isomap(n_neighbors=n_neighbors, n_components=n_components)\n",
    "    key_list = list(data.keys())\n",
    "\n",
    "    reduced_embeddings = []\n",
    "\n",
    "    for i in range(0, len(key_list), batch_size):\n",
    "        batch_keys = key_list[i:i + batch_size]\n",
    "\n",
    "        # **1Ô∏è‚É£ Lade Daten zuerst in CPU-Speicher**\n",
    "        batch = [torch.tensor(data[key], dtype=torch.float32) for key in batch_keys]  # Erst auf CPU\n",
    "        batch = torch.cat(batch, dim=0).to(\"cuda\")  # Erst dann auf GPU schieben\n",
    "        print(f\"üî¢ Batch {i//batch_size + 1}: {batch.shape}\")\n",
    "\n",
    "        # **2Ô∏è‚É£ Normalisierung auf GPU**\n",
    "        batch = (batch - batch.mean()) / batch.std()\n",
    "\n",
    "        # **3Ô∏è‚É£ Schiebe den Batch zur√ºck auf die CPU f√ºr Isomap**\n",
    "        batch_cpu = batch.cpu().numpy()\n",
    "        del batch  # GPU-Speicher leeren\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # **4Ô∏è‚É£ CPU-basiertes Isomap Fit & Transform**\n",
    "        if i == 0:\n",
    "            reduced_embeddings = isomap.fit_transform(batch_cpu)\n",
    "        else:\n",
    "            reduced_embeddings = np.vstack((reduced_embeddings, isomap.transform(batch_cpu)))\n",
    "\n",
    "        del batch_cpu\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"‚úÖ Gesamtshape nach Isomap: {reduced_embeddings.shape}\")\n",
    "\n",
    "    # **5Ô∏è‚É£ Zur√ºck auf Keys aufteilen & speichern**\n",
    "    start_idx = 0\n",
    "    reduced_data = {}\n",
    "\n",
    "    for key in key_list:\n",
    "        num_samples = data[key].shape[0]\n",
    "        reduced_data[key] = reduced_embeddings[start_idx:start_idx+num_samples]\n",
    "        start_idx += num_samples\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    np.savez_compressed(output_path, **reduced_data)\n",
    "    print(f\"üíæ Reduzierte Embeddings gespeichert unter: {output_path}\")\n",
    "\n",
    "# **Isomap f√ºr TCR-Embeddings (TRB ist gro√ü, daher Batches!)**\n",
    "apply_isomap_gpu_batched(\n",
    "    input_path='../../data/embeddings/beta/allele/TRB_beta_embeddings.npz',\n",
    "    output_path='../../data/embeddings/beta/allele/isomap/TRB_beta_embeddings_reduced.npz',\n",
    "    n_components=512,\n",
    "    n_neighbors=50,\n",
    "    batch_size=2000  # Kleinere Batches f√ºr besseren Swap-Einsatz\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f81d7-ae7f-46cf-b509-50c63489be83",
   "metadata": {},
   "source": [
    "***CHECK NACH DIMENSION REDUCTION***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c2b78-dfd3-4d92-a8a0-1fd3112eb328",
   "metadata": {},
   "source": [
    "Keys, Shape, Dimension von Embeddings (reduced mit ISOMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60378652-cf79-4dde-84e2-8c166d31e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lade die reduzierten Dateien nach Isomap\n",
    "beta_all_epi_reduced = np.load('../../data/embeddings/beta/allele/isomap/Epitope_beta_embeddings_reduced.npz')\n",
    "beta_all_trb_reduced = np.load('../../data/embeddings/beta/allele/isomap/TRB_beta_embeddings_reduced.npz')\n",
    "\n",
    "# Erhalte die Keys als Liste\n",
    "epi_keys_reduced = list(beta_all_epi_reduced.keys())\n",
    "trb_keys_reduced = list(beta_all_trb_reduced.keys())\n",
    "\n",
    "# Drucke die ersten 5 und letzten 5 Keys\n",
    "print(f\"üî¢ Anzahl der Keys in Epitope_beta_embeddings_reduced: {len(epi_keys_reduced)}\")\n",
    "print(f\"üìå Erste 5 Keys: {epi_keys_reduced[:5]}\")\n",
    "print(f\"üìå Letzte 5 Keys: {epi_keys_reduced[-5:]}\")\n",
    "\n",
    "\n",
    "print(f\"üî¢ Anzahl der Keys in TRB_beta_embeddings_reduced: {len(trb_keys_reduced)}\")\n",
    "print(f\"üìå Erste 5 Keys: {trb_keys_reduced[:5]}\")\n",
    "print(f\"üìå Letzte 5 Keys: {trb_keys_reduced[-5:]}\")\n",
    "\n",
    "# Pr√ºfe die Form der ersten & letzten 5 Keys f√ºr beide Embeddings\n",
    "print(\"\\nüîπ Shapes der ersten 5 Epitope Keys nach Isomap:\")\n",
    "for key in epi_keys_reduced[:5]:\n",
    "    print(f\"  {key}: {beta_all_epi_reduced[key].shape}\")\n",
    "\n",
    "print(\"\\nüîπ Shapes der letzten 5 Epitope Keys nach Isomap:\")\n",
    "for key in epi_keys_reduced[-5:]:\n",
    "    print(f\"  {key}: {beta_all_epi_reduced[key].shape}\")\n",
    "\n",
    "print(\"\\nüîπ Shapes der ersten 5 TRB Keys nach Isomap:\")\n",
    "for key in trb_keys_reduced[:5]:\n",
    "    print(f\"  {key}: {beta_all_trb_reduced[key].shape}\")\n",
    "\n",
    "print(\"\\nüîπ Shapes der letzten 5 TRB Keys nach Isomap:\")\n",
    "for key in trb_keys_reduced[-5:]:\n",
    "    print(f\"  {key}: {beta_all_trb_reduced[key].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5fe362-c4ff-4f03-92ee-ecfc923b9a6a",
   "metadata": {},
   "source": [
    "***Padding auf ISOMAP***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c60fa2-9804-4431-9c7f-eefba555cc3a",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff8080-32c6-459c-a6de-6de05d794cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "# === Lade den Trainingsdatensatz ===\n",
    "train_path = '../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "train_data = pd.read_csv(train_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# === Lade die reduzierten Isomap-Embeddings ===\n",
    "tcr_embeddings_path = '../../data/embeddings/beta/allele/isomap/TRB_beta_embeddings_reduced.npz'\n",
    "epitope_embeddings_path = '../../data/embeddings/beta/allele/isomap/Epitope_beta_embeddings_reduced.npz'\n",
    "\n",
    "tcr_data = np.load(tcr_embeddings_path, allow_pickle=True)\n",
    "epitope_data = np.load(epitope_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# === Extrahiere die Keys aus dem Trainingsdatensatz ===\n",
    "tcr_keys = train_data['TRB_CDR3'].dropna().tolist()\n",
    "epitope_keys = train_data['Epitope'].dropna().tolist()\n",
    "\n",
    "# === Nur Keys behalten, die in den Embeddings existieren ===\n",
    "tcr_keys = [key for key in tcr_keys if key in tcr_data]\n",
    "epitope_keys = [key for key in epitope_keys if key in epitope_data]\n",
    "\n",
    "# === Dictionaries mit den Trainings-Embeddings erstellen ===\n",
    "tcr_train_dict = {key: tcr_data[key] for key in tcr_keys}\n",
    "epitope_train_dict = {key: epitope_data[key] for key in epitope_keys}\n",
    "\n",
    "# === Maximaler Padding-Wert bestimmen ===\n",
    "max_tcr_length = max(embedding.shape[0] for embedding in tcr_train_dict.values())\n",
    "max_epitope_length = max(embedding.shape[0] for embedding in epitope_train_dict.values())\n",
    "\n",
    "max_length = max(max_tcr_length, max_epitope_length)  # Einheitliche L√§nge f√ºr Transformer\n",
    "\n",
    "print(f\"üìå Max Length: {max_length} (TCR: {max_tcr_length}, Epitope: {max_epitope_length})\")\n",
    "\n",
    "# === Padding-Funktion ===\n",
    "def pad_embedding(embedding, max_length):\n",
    "    \"\"\"\n",
    "    Padded ein einzelnes Embedding mit Nullen auf max_length.\n",
    "    \"\"\"\n",
    "    padded = np.zeros((max_length, embedding.shape[1]), dtype=embedding.dtype)\n",
    "    padded[:embedding.shape[0], :] = embedding  # Originalwerte behalten, Rest mit 0 f√ºllen\n",
    "    return padded\n",
    "\n",
    "# === Speicherpfade f√ºr Trainingsdaten setzen ===\n",
    "train_tcr_padded_path = '../../data/embeddings/beta/allele/padded_isomap/train_tcr_padded_batches'\n",
    "train_epitope_padded_path = '../../data/embeddings/beta/allele/padded_isomap/train_epitope_padded_batches'\n",
    "\n",
    "os.makedirs(train_tcr_padded_path, exist_ok=True)\n",
    "os.makedirs(train_epitope_padded_path, exist_ok=True)\n",
    "\n",
    "# === Speicher-Funktion mit Batch-Mechanismus ===\n",
    "def save_padded_embeddings_in_batches(embeddings_dict, save_dir, batch_size=5000):\n",
    "    keys = list(embeddings_dict.keys())\n",
    "    num_batches = (len(keys) + batch_size - 1) // batch_size  # Anzahl der Batches berechnen\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_keys = keys[i * batch_size: (i + 1) * batch_size]\n",
    "        padded_batch = {key: pad_embedding(embeddings_dict[key], max_length) for key in batch_keys}\n",
    "        \n",
    "        batch_save_path = os.path.join(save_dir, f\"batch_{i}.npz\")\n",
    "        np.savez_compressed(batch_save_path, **padded_batch)\n",
    "        print(f\"‚úÖ Saved batch {i + 1}/{num_batches} to {batch_save_path}\")\n",
    "\n",
    "    print(\"‚úÖ All batches saved successfully!\")\n",
    "\n",
    "# === Train-Embeddings padden und speichern ===\n",
    "save_padded_embeddings_in_batches(tcr_train_dict, train_tcr_padded_path, batch_size=5000)\n",
    "save_padded_embeddings_in_batches(epitope_train_dict, train_epitope_padded_path, batch_size=5000)\n",
    "\n",
    "# === Finale HDF5-Dateien aus gepaddeten Batches erstellen ===\n",
    "def combine_selected_batches_to_hdf5(batch_files, output_path):\n",
    "    \"\"\"\n",
    "    Kombiniert eine spezifische Liste von Batch-Dateien zu einer einzigen HDF5-Datei.\n",
    "    \"\"\"\n",
    "    if not batch_files:\n",
    "        print(f\"‚ùå Keine Batch-Dateien in der Liste gefunden.\")\n",
    "        return\n",
    "\n",
    "    with h5py.File(output_path, 'w') as hdf5_file:\n",
    "        for i, batch_file in enumerate(batch_files):\n",
    "            batch = np.load(batch_file, allow_pickle=True)\n",
    "\n",
    "            for key in batch.files:\n",
    "                if key not in hdf5_file:\n",
    "                    hdf5_file.create_dataset(key, data=batch[key], compression=\"gzip\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Duplikat-Key √ºbersprungen: {key}\")\n",
    "\n",
    "            print(f\"üîÑ Batch {i+1}/{len(batch_files)} verarbeitet: {batch_file}\")\n",
    "\n",
    "    print(f\"‚úÖ Finale gepaddete Embeddings gespeichert unter: {output_path}\")\n",
    "\n",
    "# === TCR & Epitope Batches f√ºr Train zusammenf√ºhren ===\n",
    "train_tcr_batches = [f\"../../data/embeddings/beta/allele/padded_isomap/train_tcr_padded_batches/batch_{i}.npz\" for i in range(36)]\n",
    "train_epitope_batches = [\"../../data/embeddings/beta/allele/padded_isomap/train_epitope_padded_batches/batch_0.npz\"]\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=train_tcr_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/padded_isomap/padded_train_tcr_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=train_epitope_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/padded_isomap/padded_train_epitope_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "# === √úberpr√ºfe die HDF5-Dateien ===\n",
    "def check_hdf5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as hdf5_file:\n",
    "        keys = list(hdf5_file.keys())\n",
    "        print(f\"‚úÖ HDF5-Datei geladen: {file_path}\")\n",
    "        print(f\"Anzahl Keys: {len(keys)}\")\n",
    "        print(f\"Beispiel-Keys: {keys[:5]}\")\n",
    "\n",
    "# √úberpr√ºfe Train-TCR\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/padded_isomap/padded_train_tcr_embeddings_final.h5')\n",
    "\n",
    "# √úberpr√ºfe Train-Epitope\n",
    "check_hdf5_file('../../data/embeddings/beta/allele/padded_isomap/padded_train_epitope_embeddings_final.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e70473-1ca5-4943-b599-e6ce3d9c35d2",
   "metadata": {},
   "source": [
    "VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de30cf8-8030-4ff8-91d7-74c406db8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Lade den Validierungsdatensatz ===\n",
    "validation_path = '../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "valid_data = pd.read_csv(validation_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# === Lade die Keys aus dem Validierungsdatensatz ===\n",
    "valid_tcr_keys = valid_data['TRB_CDR3'].dropna().tolist()\n",
    "valid_epitope_keys = valid_data['Epitope'].dropna().tolist()\n",
    "\n",
    "# === Nur Keys behalten, die in den Embeddings existieren ===\n",
    "valid_tcr_keys = [key for key in valid_tcr_keys if key in tcr_data]\n",
    "valid_epitope_keys = [key for key in valid_epitope_keys if key in epitope_data]\n",
    "\n",
    "# === Dictionaries f√ºr Validierungs-Embeddings erstellen ===\n",
    "valid_tcr_embeddings_dict = {key: tcr_data[key] for key in valid_tcr_keys}\n",
    "valid_epitope_embeddings_dict = {key: epitope_data[key] for key in valid_epitope_keys}\n",
    "\n",
    "# === Speicherpfade f√ºr Validierungsdaten setzen ===\n",
    "valid_tcr_padded_path = '../../data/embeddings/beta/allele/padded_isomap/valid_tcr_padded_batches'\n",
    "valid_epitope_padded_path = '../../data/embeddings/beta/allele/padded_isomap/valid_epitope_padded_batches'\n",
    "\n",
    "os.makedirs(valid_tcr_padded_path, exist_ok=True)\n",
    "os.makedirs(valid_epitope_padded_path, exist_ok=True)\n",
    "\n",
    "# === Validierungsdaten padden und speichern ===\n",
    "save_padded_embeddings_in_batches(valid_tcr_embeddings_dict, valid_tcr_padded_path, batch_size=5000)\n",
    "save_padded_embeddings_in_batches(valid_epitope_embeddings_dict, valid_epitope_padded_path, batch_size=5000)\n",
    "\n",
    "# === Validierungs-Batches zusammenf√ºhren ===\n",
    "valid_tcr_batches = [f\"../../data/embeddings/beta/allele/padded_isomap/valid_tcr_padded_batches/batch_{i}.npz\" for i in range(5)]\n",
    "valid_epitope_batches = [\"../../data/embeddings/beta/allele/padded_isomap/valid_epitope_padded_batches/batch_0.npz\"]\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=valid_tcr_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/padded_isomap/padded_valid_tcr_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=valid_epitope_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/padded_isomap/padded_valid_epitope_embeddings_final.h5'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26a294-daee-4583-9a2b-613686b8bb74",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ceedad-3458-4f10-9bda-0e9d8366181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Lade den Testdatensatz ===\n",
    "test_path = '../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "test_data = pd.read_csv(test_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# === Lade die Keys aus dem Testdatensatz ===\n",
    "test_tcr_keys = test_data['TRB_CDR3'].dropna().tolist()\n",
    "test_epitope_keys = test_data['Epitope'].dropna().tolist()\n",
    "\n",
    "# === Nur Keys behalten, die in den Embeddings existieren ===\n",
    "test_tcr_keys = [key for key in test_tcr_keys if key in tcr_data]\n",
    "test_epitope_keys = [key for key in test_epitope_keys if key in epitope_data]\n",
    "\n",
    "# === Dictionaries f√ºr Test-Embeddings erstellen ===\n",
    "test_tcr_embeddings_dict = {key: tcr_data[key] for key in test_tcr_keys}\n",
    "test_epitope_embeddings_dict = {key: epitope_data[key] for key in test_epitope_keys}\n",
    "\n",
    "# === Speicherpfade f√ºr Testdaten setzen ===\n",
    "test_tcr_padded_path = '../../data/embeddings/beta/allele/padded_isomap/test_tcr_padded_batches'\n",
    "test_epitope_padded_path = '../../data/embeddings/beta/allele/padded_isomap/test_epitope_padded_batches'\n",
    "\n",
    "os.makedirs(test_tcr_padded_path, exist_ok=True)\n",
    "os.makedirs(test_epitope_padded_path, exist_ok=True)\n",
    "\n",
    "# === Testdaten padden und speichern ===\n",
    "save_padded_embeddings_in_batches(test_tcr_embeddings_dict, test_tcr_padded_path, batch_size=5000)\n",
    "save_padded_embeddings_in_batches(test_epitope_embeddings_dict, test_epitope_padded_path, batch_size=5000)\n",
    "\n",
    "# === Test-Batches zusammenf√ºhren ===\n",
    "test_tcr_batches = [f\"../../data/embeddings/beta/allele/padded_isomap/test_tcr_padded_batches/batch_{i}.npz\" for i in range(5)]\n",
    "test_epitope_batches = [\"../../data/embeddings/beta/allele/padded_isomap/test_epitope_padded_batches/batch_0.npz\"]\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=test_tcr_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/padded_isomap/padded_test_tcr_embeddings_final.h5'\n",
    ")\n",
    "\n",
    "combine_selected_batches_to_hdf5(\n",
    "    batch_files=test_epitope_batches,\n",
    "    output_path='../../data/embeddings/beta/allele/padded_isomap/padded_test_epitope_embeddings_final.h5'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
