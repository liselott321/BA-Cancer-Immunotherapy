{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38a01ba",
   "metadata": {},
   "source": [
    "## LigthtGBM as the Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fb743",
   "metadata": {},
   "source": [
    "### Embeddings Reduction\n",
    "\n",
    "for TCR and Epitope we use the reduced embeddings to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dccc18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TCR EMBEDDING REDUCTION TO 512D ===\n",
      "\n",
      "- Selective: Good compromise, combines mean+max pooling\n",
      "Processing embeddings from ../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\n",
      "Loading embeddings from ../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz...\n",
      "Loaded 211294 sequences\n",
      "\n",
      "Applying selective reduction to all 211294 sequences...\n",
      "Applying selective pooling...\n",
      "Reduction completed in 11.60 seconds\n",
      "Saved reduced embeddings to ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "\n",
      "=== REDUCTION SUMMARY ===\n",
      "Original dimension: 1024 (after mean pooling)\n",
      "Reduced dimension: 512\n",
      "Compression ratio: 2.0x\n",
      "Memory reduction: 50.0%\n",
      "Total sequences processed: 211294\n",
      "\n",
      "Sample statistics for CASSWRDGATGELFF:\n",
      "  Mean: 0.1225\n",
      "  Std: 0.1699\n",
      "  Min: -0.2247\n",
      "  Max: 0.6312\n",
      "\n",
      "============================================================\n",
      "Processing Epitope embeddings...\n",
      "Processing embeddings from ../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\n",
      "Loading embeddings from ../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz...\n",
      "Loaded 1896 sequences\n",
      "\n",
      "Applying selective reduction to all 1896 sequences...\n",
      "Applying selective pooling...\n",
      "Reduction completed in 0.04 seconds\n",
      "Saved reduced embeddings to ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "=== REDUCTION SUMMARY ===\n",
      "Original dimension: 1024 (after mean pooling)\n",
      "Reduced dimension: 512\n",
      "Compression ratio: 2.0x\n",
      "Memory reduction: 50.0%\n",
      "Total sequences processed: 1896\n",
      "\n",
      "Sample statistics for LLAWHFVAV:\n",
      "  Mean: 0.1047\n",
      "  Std: 0.1650\n",
      "  Min: -0.3639\n",
      "  Max: 0.5814\n",
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "You can now use these 512-dimensional embeddings in your LightGBM model!\n",
      "Expected speedup: ~2x faster training\n",
      "Expected memory usage: ~50% less\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import time\n",
    "\n",
    "def load_npz_embeddings(npz_path):\n",
    "    \"\"\"Load embeddings from .npz file\"\"\"\n",
    "    print(f\"Loading embeddings from {npz_path}...\")\n",
    "    data = np.load(npz_path)\n",
    "    embeddings = {}\n",
    "    for key in data.files:\n",
    "        embeddings[key] = data[key]\n",
    "    print(f\"Loaded {len(embeddings)} sequences\")\n",
    "    return embeddings\n",
    "\n",
    "def reduce_embeddings_selective_pooling_512(embeddings_dict):\n",
    "    \"\"\"\n",
    "    Method 4: Selective pooling - combine different pooling methods and reduce\n",
    "    More sophisticated approach that might preserve more information\n",
    "    \"\"\"\n",
    "    print(\"Applying selective pooling...\")\n",
    "    \n",
    "    combined_embeddings = []\n",
    "    seq_ids = []\n",
    "    \n",
    "    for seq_id, embedding in embeddings_dict.items():\n",
    "        # Get different pooling representations\n",
    "        mean_emb = np.mean(embedding, axis=0)      # (1024,)\n",
    "        max_emb = np.max(embedding, axis=0)        # (1024,)\n",
    "        \n",
    "        # Take first 256 dims from each pooling method\n",
    "        combined = np.concatenate([mean_emb[:256], max_emb[:256]])  # (512,)\n",
    "        \n",
    "        combined_embeddings.append(combined)\n",
    "        seq_ids.append(seq_id)\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    result_embeddings = {}\n",
    "    for i, seq_id in enumerate(seq_ids):\n",
    "        result_embeddings[seq_id] = combined_embeddings[i]\n",
    "    \n",
    "    return result_embeddings\n",
    "\n",
    "\n",
    "def process_embeddings_to_512(npz_path, output_path, method='selective', benchmark=False):\n",
    "    \"\"\"\n",
    "    Complete pipeline to reduce embeddings to 512 dimensions\n",
    "    \n",
    "    Args:\n",
    "        npz_path: Path to input .npz file\n",
    "        output_path: Path to save reduced embeddings\n",
    "        method: Reduction method ('pca', 'random', 'truncate', 'selective')\n",
    "        benchmark: Whether to run benchmarking first\n",
    "    \"\"\"\n",
    "    print(f\"Processing embeddings from {npz_path}\")\n",
    "    embeddings = load_npz_embeddings(npz_path)\n",
    "    \n",
    "    if benchmark:\n",
    "        benchmark_results = benchmark_methods(embeddings, sample_size=min(100, len(embeddings)))\n",
    "    \n",
    "    print(f\"\\nApplying {method} reduction to all {len(embeddings)} sequences...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reduced, model = reduce_embeddings_mean_512(embeddings)\n",
    "        # Save the PCA model\n",
    "        model_path = output_path.replace('.pkl', '_pca_model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Saved PCA model to {model_path}\")\n",
    "        \n",
    "    elif method == 'random':\n",
    "        reduced, model = reduce_embeddings_random_projection_512(embeddings)\n",
    "        # Save the random projection model\n",
    "        model_path = output_path.replace('.pkl', '_rp_model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Saved Random Projection model to {model_path}\")\n",
    "        \n",
    "    elif method == 'truncate':\n",
    "        reduced = reduce_embeddings_truncate_512(embeddings)\n",
    "        \n",
    "    elif method == 'selective':\n",
    "        reduced = reduce_embeddings_selective_pooling_512(embeddings)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Reduction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save reduced embeddings\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(reduced, f)\n",
    "    print(f\"Saved reduced embeddings to {output_path}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    sample_key = list(reduced.keys())[0]\n",
    "    sample_embedding = reduced[sample_key]\n",
    "    \n",
    "    print(f\"\\n=== REDUCTION SUMMARY ===\")\n",
    "    print(f\"Original dimension: 1024 (after mean pooling)\")\n",
    "    print(f\"Reduced dimension: {len(sample_embedding)}\")\n",
    "    print(f\"Compression ratio: {1024/len(sample_embedding):.1f}x\")\n",
    "    print(f\"Memory reduction: {(1024-len(sample_embedding))/1024*100:.1f}%\")\n",
    "    print(f\"Total sequences processed: {len(reduced)}\")\n",
    "    \n",
    "    print(f\"\\nSample statistics for {sample_key}:\")\n",
    "    print(f\"  Mean: {np.mean(sample_embedding):.4f}\")\n",
    "    print(f\"  Std: {np.std(sample_embedding):.4f}\")\n",
    "    print(f\"  Min: {np.min(sample_embedding):.4f}\")\n",
    "    print(f\"  Max: {np.max(sample_embedding):.4f}\")\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "# Example usage and recommendations\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=== TCR EMBEDDING REDUCTION TO 512D ===\\n\")\n",
    "    \n",
    "    print(\"- Selective: Good compromise, combines mean+max pooling\")\n",
    "    \n",
    "    # Process TCR embeddings\n",
    "    tcr_reduced = process_embeddings_to_512(\n",
    "        npz_path=\"../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\",\n",
    "        output_path=\"../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\",\n",
    "        method='selective',  # Change to 'random', 'truncate', or 'selective' as needed\n",
    "        benchmark=False  # Set to False to skip benchmarking\n",
    "    )\n",
    "    \n",
    "    # Process Epitope embeddings (keep at 512 if you want consistency)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Processing Epitope embeddings...\")\n",
    "    \n",
    "    epitope_reduced = process_embeddings_to_512(\n",
    "        npz_path=\"../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\",\n",
    "        output_path=\"../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\",\n",
    "        method='selective',\n",
    "        benchmark=False  # Skip benchmarking for epitopes\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(\"You can now use these 512-dimensional embeddings in your LightGBM model!\")\n",
    "    print(\"Expected speedup: ~2x faster training\")\n",
    "    print(\"Expected memory usage: ~50% less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b6ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe12101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48165dbb",
   "metadata": {},
   "source": [
    "## LightGBM - V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8690f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18882/1613012008.py:21: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Combining TCR and Epitope features only...\n",
      "Final feature dimensions: 1024 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - No categorical features used\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model (TCR + Epitope embeddings only)...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttrain's binary_logloss: 0.384779\tval's binary_logloss: 0.430641\n",
      "\n",
      "Training completed. Best iteration: 15\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "VALIDATION METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Log Loss: 0.4306\n",
      "Accuracy: 0.8335\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC: 0.6989\n",
      "F1 Score: 0.0000\n",
      "Macro-F1 Score: 0.4546\n",
      "AP Score: 0.3105\n",
      "\n",
      "========================================\n",
      "PER-TASK VALIDATION METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.8351\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4268\n",
      "  ROC AUC: 0.7383\n",
      "  Average Precision: 0.3358\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4551\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8303\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4308\n",
      "  ROC AUC: 0.6737\n",
      "  Average Precision: 0.3523\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4536\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8199\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4709\n",
      "  ROC AUC: 0.5648\n",
      "  Average Precision: 0.2406\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4505\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.8340\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4462\n",
      "  ROC AUC: 0.6043\n",
      "  Average Precision: 0.3304\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4547\n",
      "\n",
      "Average across tasks (excluding undefined):\n",
      "  Average AUC: 0.6453\n",
      "  Average Accuracy: 0.8298\n",
      "  Average Precision: 0.0000\n",
      "  Average Recall: 0.0000\n",
      "  Average F1: 0.0000\n",
      "  Average Macro-F1: 0.4535\n",
      "\n",
      "============================================================\n",
      "TEST METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Accuracy: 0.8387\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC: 0.6223\n",
      "F1 Score: 0.0000\n",
      "Macro-F1 Score: 0.4561\n",
      "AP Score: 0.2368\n",
      "Log Loss: 0.4337\n",
      "\n",
      "========================================\n",
      "PER-TASK TEST METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8600\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.3752\n",
      "  ROC AUC: 0.8433\n",
      "  Average Precision: 0.4922\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4624\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.8305\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4595\n",
      "  ROC AUC: 0.5535\n",
      "  Average Precision: 0.2314\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4537\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8140\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4818\n",
      "  ROC AUC: 0.5447\n",
      "  Average Precision: 0.2205\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4487\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8278\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4752\n",
      "  ROC AUC: 0.4701\n",
      "  Average Precision: 0.1677\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4529\n",
      "\n",
      "Average across test tasks (excluding undefined):\n",
      "  Average AUC: 0.6029\n",
      "  Average Accuracy: 0.8331\n",
      "  Average Precision: 0.0000\n",
      "  Average Recall: 0.0000\n",
      "  Average F1: 0.0000\n",
      "  Average Macro-F1: 0.4544\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\n",
      "============================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      " epitope_emb_79 109384.979980\n",
      " epitope_emb_67  81719.550781\n",
      "epitope_emb_207  49033.491180\n",
      " epitope_emb_49  29045.609619\n",
      "epitope_emb_253  23694.199829\n",
      "  epitope_emb_6  22059.909790\n",
      "epitope_emb_396  19302.384277\n",
      "epitope_emb_123  18515.747986\n",
      "epitope_emb_440  16376.180176\n",
      "epitope_emb_410  15557.280273\n",
      "epitope_emb_238  13771.799805\n",
      "epitope_emb_350  11999.539917\n",
      "epitope_emb_114  10707.299805\n",
      "epitope_emb_480  10423.500000\n",
      "epitope_emb_132   9115.889648\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 37277.06 (4.5%)\n",
      "Epitope embeddings: 786743.91 (95.5%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 463: 6200.02\n",
      "  Dimension 7: 5395.00\n",
      "  Dimension 15: 4763.95\n",
      "  Dimension 271: 2873.03\n",
      "  Dimension 45: 2152.19\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 79: 109384.98\n",
      "  Dimension 67: 81719.55\n",
      "  Dimension 207: 49033.49\n",
      "  Dimension 49: 29045.61\n",
      "  Dimension 253: 23694.20\n",
      "\n",
      "Importance concentration:\n",
      "Top 5 TCR dims contribute: 57.4% of TCR importance\n",
      "Top 5 Epitope dims contribute: 37.2% of Epitope importance\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY (Embeddings Only)\n",
      "============================================================\n",
      "Best validation AUC: 0.6989\n",
      "Test AUC: 0.6223\n",
      "Training iterations: 15\n",
      "Total features used: 1024\n",
      "TCR embedding contribution: 4.5%\n",
      "Epitope embedding contribution: 95.5%\n",
      "\n",
      "Model uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\n",
      "This shows the predictive power of TCR-Epitope interaction alone\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC ANALYSIS FOR F1 = 0\n",
      "============================================================\n",
      "1. CLASS DISTRIBUTION:\n",
      "Training set:\n",
      "Binding\n",
      "0    0.832902\n",
      "1    0.167098\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Binding\n",
      "0    0.833461\n",
      "1    0.166539\n",
      "Name: proportion, dtype: float64\n",
      "Test set:\n",
      "Binding\n",
      "0    0.838691\n",
      "1    0.161309\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "2. PREDICTION PROBABILITY DISTRIBUTION:\n",
      "Validation predictions - Min: 0.0821, Max: 0.3907, Mean: 0.1395\n",
      "Test predictions - Min: 0.0821, Max: 0.3825, Mean: 0.1540\n",
      "\n",
      "3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\n",
      "Threshold 0.1: Validation=86606/169029 (51.2%), Test=32461/54126 (60.0%)\n",
      "Threshold 0.2: Validation=35914/169029 (21.2%), Test=16803/54126 (31.0%)\n",
      "Threshold 0.3: Validation=3193/169029 (1.9%), Test=2038/54126 (3.8%)\n",
      "Threshold 0.4: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.5: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.6: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.7: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.8: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.9: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "\n",
      "4. CONFUSION MATRIX (threshold=0.5):\n",
      "Validation:\n",
      "TN: 140879, FP: 0\n",
      "FN: 28150, TP: 0\n",
      "Test:\n",
      "TN: 45395, FP: 0\n",
      "FN: 8731, TP: 0\n",
      "\n",
      "5. OPTIMAL THRESHOLD ANALYSIS:\n",
      "Optimal threshold for F1: 0.1023\n",
      "Max F1 score achievable: 0.4395\n",
      "\n",
      "6. PERFORMANCE WITH OPTIMAL THRESHOLD (0.1023):\n",
      "Validation:\n",
      "Precision: 0.2867, Recall: 0.8208\n",
      "F1: 0.4250, Macro-F1: 0.5762\n",
      "Test:\n",
      "Precision: 0.2107, Recall: 0.7655\n",
      "F1: 0.3305, Macro-F1: 0.4655\n",
      "\n",
      "7. POSITIVE CLASS PROBABILITY ANALYSIS:\n",
      "Positive class (binding) predictions:\n",
      "  Count: 28150\n",
      "  Mean probability: 0.1689\n",
      "  Max probability: 0.3907\n",
      "  % above 0.5: 0.0%\n",
      "Negative class (no binding) predictions:\n",
      "  Count: 140879\n",
      "  Mean probability: 0.1336\n",
      "  Max probability: 0.3655\n",
      "  % above 0.5: 0.0%\n",
      "\n",
      "8. RECOMMENDATIONS:\n",
      "- Your model has discriminative power (AUC > 0.6) but conservative threshold\n",
      "- Consider using threshold 0.102 instead of 0.5\n",
      "- The high accuracy with F1=0 indicates severe class imbalance\n",
      "- Macro-F1 > 0 shows the model isn't completely broken\n",
      "- Consider techniques for imbalanced datasets (SMOTE, class weights, etc.)\n",
      "\n",
      "9. CLASS WEIGHT SUGGESTION:\n",
      "Negative samples: 629472\n",
      "Positive samples: 126286\n",
      "Imbalance ratio: 4.98:1\n",
      "Consider adding 'scale_pos_weight': 4.98 to LightGBM params\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.metrics import log_loss, f1_score, precision_score, recall_score\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your 512-dimensional embeddings\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='mean'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        # Show a few examples of missing sequences\n",
    "        if len(missing_sequences) <= 5:\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "        elif len(missing_sequences) > 5:\n",
    "            print(f\"  First few missing: {[seq for _, seq in missing_sequences[:3]]}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Combine ONLY TCR and Epitope features (no categorical features)\n",
    "print(\"\\nCombining TCR and Epitope features only...\")\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\")\n",
    "print(f\"  - No categorical features used\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "for name, features in [(\"train\", train_features), (\"valid\", valid_features), (\"test\", test_features)]:\n",
    "    nan_count = features.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: Found {nan_count} NaN values in {name} features - filling with 0\")\n",
    "        features.fillna(0, inplace=True)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "# Note: No categorical features since we're only using embeddings\n",
    "train_data = lgb.Dataset(train_features, label=train_df[target_col])\n",
    "valid_data = lgb.Dataset(valid_features, label=valid_df[target_col], reference=train_data)\n",
    "\n",
    "# LightGBM parameters - optimized for embedding-only features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,  # Feature subsampling\n",
    "    'bagging_fraction': 0.8,  # Data subsampling  \n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model (TCR + Epitope embeddings only)...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Precision: {precision_score(y_val_true, y_val_pred, zero_division=0):.4f}')\n",
    "print(f'Recall: {recall_score(y_val_true, y_val_pred, zero_division=0):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_val_true, y_val_pred, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    task_results = []\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            precision = precision_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            recall = recall_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = precision = recall = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_tasks = [r for r in task_results if isinstance(r['auc'], float)]\n",
    "    if valid_tasks:\n",
    "        avg_auc = np.mean([r['auc'] for r in valid_tasks])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in valid_tasks])\n",
    "        avg_precision = np.mean([r['precision'] for r in valid_tasks])\n",
    "        avg_recall = np.mean([r['recall'] for r in valid_tasks])\n",
    "        avg_f1 = np.mean([r['f1'] for r in valid_tasks])\n",
    "        avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_tasks])\n",
    "        print(f\"\\nAverage across tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "        print(f\"  Average Recall: {avg_recall:.4f}\")\n",
    "        print(f\"  Average F1: {avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Precision: {precision_score(y_true, y_pred_binary, zero_division=0):.4f}')\n",
    "print(f'Recall: {recall_score(y_true, y_pred_binary, zero_division=0):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_true, y_pred_binary, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    test_task_results = []\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            precision = precision_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            recall = recall_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = precision = recall = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_test_tasks = [r for r in test_task_results if isinstance(r['auc'], float)]\n",
    "    if valid_test_tasks:\n",
    "        test_avg_auc = np.mean([r['auc'] for r in valid_test_tasks])\n",
    "        test_avg_acc = np.mean([r['accuracy'] for r in valid_test_tasks])\n",
    "        test_avg_precision = np.mean([r['precision'] for r in valid_test_tasks])\n",
    "        test_avg_recall = np.mean([r['recall'] for r in valid_test_tasks])\n",
    "        test_avg_f1 = np.mean([r['f1'] for r in valid_test_tasks])\n",
    "        test_avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_test_tasks])\n",
    "        print(f\"\\nAverage across test tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {test_avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {test_avg_acc:.4f}\")\n",
    "        print(f\"  Average Precision: {test_avg_precision:.4f}\")\n",
    "        print(f\"  Average Recall: {test_avg_recall:.4f}\")\n",
    "        print(f\"  Average F1: {test_avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {test_avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# Check if embeddings are well-distributed in importance\n",
    "tcr_top_5_importance = tcr_features['importance'].sum()\n",
    "epitope_top_5_importance = epitope_features['importance'].sum()\n",
    "\n",
    "print(f\"\\nImportance concentration:\")\n",
    "print(f\"Top 5 TCR dims contribute: {tcr_top_5_importance/tcr_emb_importance*100:.1f}% of TCR importance\")\n",
    "print(f\"Top 5 Epitope dims contribute: {epitope_top_5_importance/epitope_emb_importance*100:.1f}% of Epitope importance\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"TCR embedding contribution: {tcr_emb_importance/total_importance*100:.1f}%\")\n",
    "print(f\"Epitope embedding contribution: {epitope_emb_importance/total_importance*100:.1f}%\")\n",
    "\n",
    "# Performance comparison hint\n",
    "print(f\"\\nModel uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\")\n",
    "print(f\"This shows the predictive power of TCR-Epitope interaction alone\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_only_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_only_model.txt'\")\n",
    "\n",
    "# Add this diagnostic code after making predictions to understand the F1=0 issue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC ANALYSIS FOR F1 = 0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check class distribution\n",
    "print(\"1. CLASS DISTRIBUTION:\")\n",
    "print(f\"Training set:\")\n",
    "train_class_dist = train_df[target_col].value_counts(normalize=True)\n",
    "print(train_class_dist)\n",
    "print(f\"Validation set:\")\n",
    "val_class_dist = valid_df[target_col].value_counts(normalize=True)\n",
    "print(val_class_dist)\n",
    "print(f\"Test set:\")\n",
    "test_class_dist = test_df[target_col].value_counts(normalize=True)\n",
    "print(test_class_dist)\n",
    "\n",
    "# 2. Check prediction probabilities distribution\n",
    "print(f\"\\n2. PREDICTION PROBABILITY DISTRIBUTION:\")\n",
    "print(f\"Validation predictions - Min: {y_val_prob.min():.4f}, Max: {y_val_prob.max():.4f}, Mean: {y_val_prob.mean():.4f}\")\n",
    "print(f\"Test predictions - Min: {y_pred.min():.4f}, Max: {y_pred.max():.4f}, Mean: {y_pred.mean():.4f}\")\n",
    "\n",
    "# 3. Check how many predictions are above different thresholds\n",
    "print(f\"\\n3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\")\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for thresh in thresholds:\n",
    "    val_above = (y_val_prob > thresh).sum()\n",
    "    test_above = (y_pred > thresh).sum()\n",
    "    print(f\"Threshold {thresh}: Validation={val_above}/{len(y_val_prob)} ({val_above/len(y_val_prob)*100:.1f}%), Test={test_above}/{len(y_pred)} ({test_above/len(y_pred)*100:.1f}%)\")\n",
    "\n",
    "# 4. Check confusion matrix with current threshold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(f\"\\n4. CONFUSION MATRIX (threshold=0.5):\")\n",
    "print(\"Validation:\")\n",
    "val_cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "print(f\"TN: {val_cm[0,0]}, FP: {val_cm[0,1]}\")\n",
    "print(f\"FN: {val_cm[1,0]}, TP: {val_cm[1,1]}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "test_cm = confusion_matrix(y_true, y_pred_binary)\n",
    "print(f\"TN: {test_cm[0,0]}, FP: {test_cm[0,1]}\")\n",
    "print(f\"FN: {test_cm[1,0]}, TP: {test_cm[1,1]}\")\n",
    "\n",
    "# 5. Find optimal threshold using validation set\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "print(f\"\\n5. OPTIMAL THRESHOLD ANALYSIS:\")\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_val_true, y_val_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "f1_scores = f1_scores[~np.isnan(f1_scores)]  # Remove NaN values\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds_pr[optimal_idx] if optimal_idx < len(thresholds_pr) else 0.5\n",
    "\n",
    "print(f\"Optimal threshold for F1: {optimal_threshold:.4f}\")\n",
    "print(f\"Max F1 score achievable: {f1_scores[optimal_idx]:.4f}\")\n",
    "\n",
    "# 6. Evaluate with optimal threshold\n",
    "y_val_pred_optimal = (y_val_prob > optimal_threshold).astype(int)\n",
    "y_test_pred_optimal = (y_pred > optimal_threshold).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\n6. PERFORMANCE WITH OPTIMAL THRESHOLD ({optimal_threshold:.4f}):\")\n",
    "print(\"Validation:\")\n",
    "val_f1_optimal = f1_score(y_val_true, y_val_pred_optimal)\n",
    "val_macro_f1_optimal = f1_score(y_val_true, y_val_pred_optimal, average='macro')\n",
    "val_precision_optimal = precision_score(y_val_true, y_val_pred_optimal, zero_division=0)\n",
    "val_recall_optimal = recall_score(y_val_true, y_val_pred_optimal, zero_division=0)\n",
    "print(f\"Precision: {val_precision_optimal:.4f}, Recall: {val_recall_optimal:.4f}\")\n",
    "print(f\"F1: {val_f1_optimal:.4f}, Macro-F1: {val_macro_f1_optimal:.4f}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "test_f1_optimal = f1_score(y_true, y_test_pred_optimal)\n",
    "test_macro_f1_optimal = f1_score(y_true, y_test_pred_optimal, average='macro')\n",
    "test_precision_optimal = precision_score(y_true, y_test_pred_optimal, zero_division=0)\n",
    "test_recall_optimal = recall_score(y_true, y_test_pred_optimal, zero_division=0)\n",
    "print(f\"Precision: {test_precision_optimal:.4f}, Recall: {test_recall_optimal:.4f}\")\n",
    "print(f\"F1: {test_f1_optimal:.4f}, Macro-F1: {test_macro_f1_optimal:.4f}\")\n",
    "\n",
    "# 7. Show distribution of positive class probabilities\n",
    "print(f\"\\n7. POSITIVE CLASS PROBABILITY ANALYSIS:\")\n",
    "pos_indices = y_val_true == 1\n",
    "neg_indices = y_val_true == 0\n",
    "\n",
    "if pos_indices.sum() > 0:\n",
    "    pos_probs = y_val_prob[pos_indices]\n",
    "    neg_probs = y_val_prob[neg_indices]\n",
    "    \n",
    "    print(f\"Positive class (binding) predictions:\")\n",
    "    print(f\"  Count: {len(pos_probs)}\")\n",
    "    print(f\"  Mean probability: {pos_probs.mean():.4f}\")\n",
    "    print(f\"  Max probability: {pos_probs.max():.4f}\")\n",
    "    print(f\"  % above 0.5: {(pos_probs > 0.5).mean()*100:.1f}%\")\n",
    "    \n",
    "    print(f\"Negative class (no binding) predictions:\")\n",
    "    print(f\"  Count: {len(neg_probs)}\")\n",
    "    print(f\"  Mean probability: {neg_probs.mean():.4f}\")\n",
    "    print(f\"  Max probability: {neg_probs.max():.4f}\")\n",
    "    print(f\"  % above 0.5: {(neg_probs > 0.5).mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n8. RECOMMENDATIONS:\")\n",
    "print(f\"- Your model has discriminative power (AUC > 0.6) but conservative threshold\")\n",
    "print(f\"- Consider using threshold {optimal_threshold:.3f} instead of 0.5\")\n",
    "print(f\"- The high accuracy with F1=0 indicates severe class imbalance\")\n",
    "print(f\"- Macro-F1 > 0 shows the model isn't completely broken\")\n",
    "print(f\"- Consider techniques for imbalanced datasets (SMOTE, class weights, etc.)\")\n",
    "\n",
    "# 9. Quick retraining suggestion with class weights\n",
    "print(f\"\\n9. CLASS WEIGHT SUGGESTION:\")\n",
    "neg_count = (train_df[target_col] == 0).sum()\n",
    "pos_count = (train_df[target_col] == 1).sum()\n",
    "class_weight_ratio = neg_count / pos_count\n",
    "print(f\"Negative samples: {neg_count}\")\n",
    "print(f\"Positive samples: {pos_count}\")\n",
    "print(f\"Imbalance ratio: {class_weight_ratio:.2f}:1\")\n",
    "print(f\"Consider adding 'scale_pos_weight': {class_weight_ratio:.2f} to LightGBM params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5ce24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5344d694",
   "metadata": {},
   "source": [
    "## LightGBM - V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54502d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18882/1154912800.py:22: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Encoding categorical features...\n",
      "Encoded TRBV: 166 unique values\n",
      "Encoded TRBJ: 31 unique values\n",
      "Encoded MHC: 99 unique values\n",
      "\n",
      "Combining features...\n",
      "Final feature dimensions: 1027 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - Categorical features: 3 features\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.262866\tval's binary_logloss: 0.226142\n",
      "[200]\ttrain's binary_logloss: 0.253374\tval's binary_logloss: 0.221357\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttrain's binary_logloss: 0.251108\tval's binary_logloss: 0.220423\n",
      "\n",
      "Training completed. Best iteration: 224\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "VALIDATION METRICS (Embeddings + Categorical Features)\n",
      "============================================================\n",
      "Log Loss: 0.2204\n",
      "Accuracy: 0.9169\n",
      "Precision: 0.8235\n",
      "Recall: 0.6380\n",
      "AUC: 0.9370\n",
      "F1 Score: 0.7189\n",
      "Macro-F1 Score: 0.8351\n",
      "AP Score: 0.7938\n",
      "\n",
      "========================================\n",
      "PER-TASK VALIDATION METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.9341\n",
      "  Precision: 0.8629\n",
      "  Recall: 0.7139\n",
      "  Log Loss: 0.1796\n",
      "  ROC AUC: 0.9604\n",
      "  Average Precision: 0.8488\n",
      "  F1 Score: 0.7814\n",
      "  Macro-F1 Score: 0.8713\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8533\n",
      "  Precision: 0.5803\n",
      "  Recall: 0.4898\n",
      "  Log Loss: 0.3406\n",
      "  ROC AUC: 0.8676\n",
      "  Average Precision: 0.5689\n",
      "  F1 Score: 0.5312\n",
      "  Macro-F1 Score: 0.7221\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8190\n",
      "  Precision: 0.4867\n",
      "  Recall: 0.0929\n",
      "  Log Loss: 0.4932\n",
      "  ROC AUC: 0.5742\n",
      "  Average Precision: 0.2702\n",
      "  F1 Score: 0.1560\n",
      "  Macro-F1 Score: 0.5273\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.7854\n",
      "  Precision: 0.0714\n",
      "  Recall: 0.0244\n",
      "  Log Loss: 0.4849\n",
      "  ROC AUC: 0.5912\n",
      "  Average Precision: 0.1981\n",
      "  F1 Score: 0.0364\n",
      "  Macro-F1 Score: 0.4578\n",
      "\n",
      "Average across tasks (excluding undefined):\n",
      "  Average AUC: 0.7483\n",
      "  Average Accuracy: 0.8480\n",
      "  Average Precision: 0.5003\n",
      "  Average Recall: 0.3302\n",
      "  Average F1: 0.3762\n",
      "  Average Macro-F1: 0.6446\n",
      "\n",
      "============================================================\n",
      "TEST METRICS (Embeddings + Categorical Features)\n",
      "============================================================\n",
      "Accuracy: 0.8215\n",
      "Precision: 0.2332\n",
      "Recall: 0.0466\n",
      "AUC: 0.6251\n",
      "F1 Score: 0.0777\n",
      "Macro-F1 Score: 0.4894\n",
      "AP Score: 0.2117\n",
      "Log Loss: 0.5219\n",
      "\n",
      "========================================\n",
      "PER-TASK TEST METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8684\n",
      "  Precision: 0.9368\n",
      "  Recall: 0.0641\n",
      "  Log Loss: 0.3585\n",
      "  ROC AUC: 0.9600\n",
      "  Average Precision: 0.7113\n",
      "  F1 Score: 0.1201\n",
      "  Macro-F1 Score: 0.5245\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.7971\n",
      "  Precision: 0.1596\n",
      "  Recall: 0.0461\n",
      "  Log Loss: 0.6057\n",
      "  ROC AUC: 0.4449\n",
      "  Average Precision: 0.1646\n",
      "  F1 Score: 0.0716\n",
      "  Macro-F1 Score: 0.4788\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8015\n",
      "  Precision: 0.0964\n",
      "  Recall: 0.0080\n",
      "  Log Loss: 0.6036\n",
      "  ROC AUC: 0.4362\n",
      "  Average Precision: 0.1566\n",
      "  F1 Score: 0.0148\n",
      "  Macro-F1 Score: 0.4522\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8007\n",
      "  Precision: 0.1071\n",
      "  Recall: 0.0214\n",
      "  Log Loss: 0.5582\n",
      "  ROC AUC: 0.4773\n",
      "  Average Precision: 0.1644\n",
      "  F1 Score: 0.0357\n",
      "  Macro-F1 Score: 0.4623\n",
      "\n",
      "Average across test tasks (excluding undefined):\n",
      "  Average AUC: 0.5796\n",
      "  Average Accuracy: 0.8169\n",
      "  Average Precision: 0.3250\n",
      "  Average Recall: 0.0349\n",
      "  Average F1: 0.0605\n",
      "  Average Macro-F1: 0.4795\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (Embeddings + Categorical)\n",
      "============================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      "   TRBJ_encoded 715965.824492\n",
      "    MHC_encoded 408459.879719\n",
      "   TRBV_encoded 273736.857082\n",
      "epitope_emb_392  80437.656295\n",
      " epitope_emb_79  59646.241247\n",
      " epitope_emb_67  39163.985046\n",
      "epitope_emb_207  30104.105347\n",
      "epitope_emb_228  25835.628696\n",
      " epitope_emb_49  24508.152559\n",
      "epitope_emb_360  21227.130096\n",
      "epitope_emb_233  20496.944202\n",
      "epitope_emb_459  19569.521294\n",
      " epitope_emb_42  15208.068138\n",
      "epitope_emb_384  14727.558907\n",
      "epitope_emb_439  12774.100963\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 70253.62 (2.8%)\n",
      "Epitope embeddings: 1024566.90 (41.1%)\n",
      "Categorical features: 1398162.56 (56.1%)\n",
      "\n",
      "Categorical feature importance breakdown:\n",
      "  TRBV_encoded: 273736.86 (11.0%)\n",
      "  TRBJ_encoded: 715965.82 (28.7%)\n",
      "  MHC_encoded: 408459.88 (16.4%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 271: 5950.44\n",
      "  Dimension 293: 2585.06\n",
      "  Dimension 79: 2120.85\n",
      "  Dimension 15: 1904.23\n",
      "  Dimension 482: 1886.65\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 392: 80437.66\n",
      "  Dimension 79: 59646.24\n",
      "  Dimension 67: 39163.99\n",
      "  Dimension 207: 30104.11\n",
      "  Dimension 228: 25835.63\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY (Embeddings + Categorical)\n",
      "============================================================\n",
      "Best validation AUC: 0.9370\n",
      "Test AUC: 0.6251\n",
      "Training iterations: 224\n",
      "Total features used: 1027\n",
      "Embedding contribution: 43.9%\n",
      "Categorical contribution: 56.1%\n",
      "\n",
      "Model uses sequence embeddings + categorical features (MHC, TRBV, TRBJ)\n",
      "This shows the combined predictive power of sequence and context information\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC ANALYSIS\n",
      "============================================================\n",
      "1. CLASS DISTRIBUTION:\n",
      "Training set:\n",
      "Binding\n",
      "0    0.832902\n",
      "1    0.167098\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Binding\n",
      "0    0.833461\n",
      "1    0.166539\n",
      "Name: proportion, dtype: float64\n",
      "Test set:\n",
      "Binding\n",
      "0    0.838691\n",
      "1    0.161309\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "2. PREDICTION PROBABILITY DISTRIBUTION:\n",
      "Validation predictions - Min: 0.0002, Max: 0.9828, Mean: 0.1801\n",
      "Test predictions - Min: 0.0002, Max: 0.9064, Mean: 0.1044\n",
      "\n",
      "3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\n",
      "Threshold 0.1: Validation=59965/169029 (35.5%), Test=16296/54126 (30.1%)\n",
      "Threshold 0.2: Validation=51465/169029 (30.4%), Test=8533/54126 (15.8%)\n",
      "Threshold 0.3: Validation=32836/169029 (19.4%), Test=5002/54126 (9.2%)\n",
      "Threshold 0.4: Validation=26328/169029 (15.6%), Test=2950/54126 (5.5%)\n",
      "Threshold 0.5: Validation=21809/169029 (12.9%), Test=1745/54126 (3.2%)\n",
      "Threshold 0.6: Validation=18291/169029 (10.8%), Test=1289/54126 (2.4%)\n",
      "Threshold 0.7: Validation=16577/169029 (9.8%), Test=966/54126 (1.8%)\n",
      "Threshold 0.8: Validation=15225/169029 (9.0%), Test=362/54126 (0.7%)\n",
      "Threshold 0.9: Validation=13065/169029 (7.7%), Test=14/54126 (0.0%)\n",
      "\n",
      "4. OPTIMAL THRESHOLD ANALYSIS:\n",
      "Optimal threshold for F1: 0.4018\n",
      "Max F1 score achievable: 0.7282\n",
      "\n",
      "5. PERFORMANCE WITH OPTIMAL THRESHOLD (0.4018):\n",
      "Validation:\n",
      "Precision: 0.7551, Recall: 0.7032\n",
      "F1: 0.7282, Macro-F1: 0.8381\n",
      "Test:\n",
      "Precision: 0.2026, Recall: 0.0679\n",
      "F1: 0.1017, Macro-F1: 0.4967\n",
      "\n",
      "6. CLASS WEIGHT SUGGESTION:\n",
      "Negative samples: 629472\n",
      "Positive samples: 126286\n",
      "Imbalance ratio: 4.98:1\n",
      "Consider adding 'scale_pos_weight': 4.98 to LightGBM params\n",
      "\n",
      "7. RECOMMENDATIONS:\n",
      "- Compare this model with embeddings-only version to see categorical feature impact\n",
      "- Consider using threshold 0.402 instead of 0.5\n",
      "- Categorical features contribute 56.1% of importance\n",
      "- If precision/recall are still low, consider class balancing techniques\n",
      "- The combination of embeddings + categorical may improve generalization\n",
      "\n",
      "8. MODEL COMPARISON INSIGHTS:\n",
      "This model includes both sequence embeddings AND categorical features:\n",
      "  - TCR + Epitope embeddings: 43.9%\n",
      "  - MHC + TRBV + TRBJ features: 56.1%\n",
      "Compare AUC with embeddings-only model to quantify categorical feature value\n",
      "If categorical features show high importance, they're capturing crucial biological context\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss, precision_score, recall_score\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your actual paths\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='zero'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean', 'drop')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "            else:  # 'drop' - will be handled later\n",
    "                embedding_features.append(np.zeros(embedding_dim))  # placeholder\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        if len(missing_sequences) <= 5:  # Show a few examples\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Encode categorical features that don't have embeddings\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "categorical_cols = ['TRBV', 'TRBJ', 'MHC']\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col + '_encoded'] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col + '_encoded'] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col + '_encoded'] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Combine all features\n",
    "print(\"\\nCombining features...\")\n",
    "encoded_categorical_cols = [col + '_encoded' for col in categorical_cols]\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb, \n",
    "    train_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb, \n",
    "    valid_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb, \n",
    "    test_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\") \n",
    "print(f\"  - Categorical features: {len(encoded_categorical_cols)} features\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "if train_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in training features\")\n",
    "    train_features = train_features.fillna(0)\n",
    "if valid_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in validation features\")\n",
    "    valid_features = valid_features.fillna(0)\n",
    "if test_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in test features\")\n",
    "    test_features = test_features.fillna(0)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "train_data = lgb.Dataset(\n",
    "    train_features, \n",
    "    label=train_df[target_col], \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "valid_data = lgb.Dataset(\n",
    "    valid_features, \n",
    "    label=valid_df[target_col], \n",
    "    reference=train_data, \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "\n",
    "# LightGBM parameters - optimized for high-dimensional embedding features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,  # Lower learning rate for stability with embeddings\n",
    "    'feature_fraction': 0.8,  # Feature subsampling to prevent overfitting\n",
    "    'bagging_fraction': 0.8,  # Data subsampling\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS (Embeddings + Categorical Features)\")\n",
    "print(\"=\"*60)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Precision: {precision_score(y_val_true, y_val_pred, zero_division=0):.4f}')\n",
    "print(f'Recall: {recall_score(y_val_true, y_val_pred, zero_division=0):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_val_true, y_val_pred, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    task_results = []\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            precision = precision_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            recall = recall_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = precision = recall = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_tasks = [r for r in task_results if isinstance(r['auc'], float)]\n",
    "    if valid_tasks:\n",
    "        avg_auc = np.mean([r['auc'] for r in valid_tasks])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in valid_tasks])\n",
    "        avg_precision = np.mean([r['precision'] for r in valid_tasks])\n",
    "        avg_recall = np.mean([r['recall'] for r in valid_tasks])\n",
    "        avg_f1 = np.mean([r['f1'] for r in valid_tasks])\n",
    "        avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_tasks])\n",
    "        print(f\"\\nAverage across tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "        print(f\"  Average Recall: {avg_recall:.4f}\")\n",
    "        print(f\"  Average F1: {avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST METRICS (Embeddings + Categorical Features)\")\n",
    "print(\"=\"*60)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Precision: {precision_score(y_true, y_pred_binary, zero_division=0):.4f}')\n",
    "print(f'Recall: {recall_score(y_true, y_pred_binary, zero_division=0):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_true, y_pred_binary, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    test_task_results = []\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            precision = precision_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            recall = recall_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = precision = recall = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_test_tasks = [r for r in test_task_results if isinstance(r['auc'], float)]\n",
    "    if valid_test_tasks:\n",
    "        test_avg_auc = np.mean([r['auc'] for r in valid_test_tasks])\n",
    "        test_avg_acc = np.mean([r['accuracy'] for r in valid_test_tasks])\n",
    "        test_avg_precision = np.mean([r['precision'] for r in valid_test_tasks])\n",
    "        test_avg_recall = np.mean([r['recall'] for r in valid_test_tasks])\n",
    "        test_avg_f1 = np.mean([r['f1'] for r in valid_test_tasks])\n",
    "        test_avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_test_tasks])\n",
    "        print(f\"\\nAverage across test tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {test_avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {test_avg_acc:.4f}\")\n",
    "        print(f\"  Average Precision: {test_avg_precision:.4f}\")\n",
    "        print(f\"  Average Recall: {test_avg_recall:.4f}\")\n",
    "        print(f\"  Average F1: {test_avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {test_avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (Embeddings + Categorical)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "categorical_importance = feature_imp_df[feature_imp_df['feature'].str.endswith('_encoded')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance + categorical_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Categorical features: {categorical_importance:.2f} ({categorical_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show individual categorical feature importance\n",
    "print(f\"\\nCategorical feature importance breakdown:\")\n",
    "for col in encoded_categorical_cols:\n",
    "    col_importance = feature_imp_df[feature_imp_df['feature'] == col]['importance'].sum()\n",
    "    print(f\"  {col}: {col_importance:.2f} ({col_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY (Embeddings + Categorical)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"Embedding contribution: {(tcr_emb_importance + epitope_emb_importance)/total_importance*100:.1f}%\")\n",
    "print(f\"Categorical contribution: {categorical_importance/total_importance*100:.1f}%\")\n",
    "\n",
    "# Performance comparison hint\n",
    "print(f\"\\nModel uses sequence embeddings + categorical features (MHC, TRBV, TRBJ)\")\n",
    "print(f\"This shows the combined predictive power of sequence and context information\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_model.txt'\")\n",
    "\n",
    "# === Additional Diagnostic Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check class distribution\n",
    "print(\"1. CLASS DISTRIBUTION:\")\n",
    "print(f\"Training set:\")\n",
    "train_class_dist = train_df[target_col].value_counts(normalize=True)\n",
    "print(train_class_dist)\n",
    "print(f\"Validation set:\")\n",
    "val_class_dist = valid_df[target_col].value_counts(normalize=True)\n",
    "print(val_class_dist)\n",
    "print(f\"Test set:\")\n",
    "test_class_dist = test_df[target_col].value_counts(normalize=True)\n",
    "print(test_class_dist)\n",
    "\n",
    "# 2. Check prediction probabilities distribution\n",
    "print(f\"\\n2. PREDICTION PROBABILITY DISTRIBUTION:\")\n",
    "print(f\"Validation predictions - Min: {y_val_prob.min():.4f}, Max: {y_val_prob.max():.4f}, Mean: {y_val_prob.mean():.4f}\")\n",
    "print(f\"Test predictions - Min: {y_pred.min():.4f}, Max: {y_pred.max():.4f}, Mean: {y_pred.mean():.4f}\")\n",
    "\n",
    "# 3. Check how many predictions are above different thresholds\n",
    "print(f\"\\n3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\")\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for thresh in thresholds:\n",
    "    val_above = (y_val_prob > thresh).sum()\n",
    "    test_above = (y_pred > thresh).sum()\n",
    "    print(f\"Threshold {thresh}: Validation={val_above}/{len(y_val_prob)} ({val_above/len(y_val_prob)*100:.1f}%), Test={test_above}/{len(y_pred)} ({test_above/len(y_pred)*100:.1f}%)\")\n",
    "\n",
    "# 4. Find optimal threshold using validation set\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "print(f\"\\n4. OPTIMAL THRESHOLD ANALYSIS:\")\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_val_true, y_val_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "f1_scores = f1_scores[~np.isnan(f1_scores)]  # Remove NaN values\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds_pr[optimal_idx] if optimal_idx < len(thresholds_pr) else 0.5\n",
    "\n",
    "print(f\"Optimal threshold for F1: {optimal_threshold:.4f}\")\n",
    "print(f\"Max F1 score achievable: {f1_scores[optimal_idx]:.4f}\")\n",
    "\n",
    "# 5. Evaluate with optimal threshold\n",
    "y_val_pred_optimal = (y_val_prob > optimal_threshold).astype(int)\n",
    "y_test_pred_optimal = (y_pred > optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n5. PERFORMANCE WITH OPTIMAL THRESHOLD ({optimal_threshold:.4f}):\")\n",
    "print(\"Validation:\")\n",
    "val_f1_optimal = f1_score(y_val_true, y_val_pred_optimal)\n",
    "val_macro_f1_optimal = f1_score(y_val_true, y_val_pred_optimal, average='macro')\n",
    "val_precision_optimal = precision_score(y_val_true, y_val_pred_optimal, zero_division=0)\n",
    "val_recall_optimal = recall_score(y_val_true, y_val_pred_optimal, zero_division=0)\n",
    "print(f\"Precision: {val_precision_optimal:.4f}, Recall: {val_recall_optimal:.4f}\")\n",
    "print(f\"F1: {val_f1_optimal:.4f}, Macro-F1: {val_macro_f1_optimal:.4f}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "test_f1_optimal = f1_score(y_true, y_test_pred_optimal)\n",
    "test_macro_f1_optimal = f1_score(y_true, y_test_pred_optimal, average='macro')\n",
    "test_precision_optimal = precision_score(y_true, y_test_pred_optimal, zero_division=0)\n",
    "test_recall_optimal = recall_score(y_true, y_test_pred_optimal, zero_division=0)\n",
    "print(f\"Precision: {test_precision_optimal:.4f}, Recall: {test_recall_optimal:.4f}\")\n",
    "print(f\"F1: {test_f1_optimal:.4f}, Macro-F1: {test_macro_f1_optimal:.4f}\")\n",
    "\n",
    "# 6. Class weight suggestion\n",
    "print(f\"\\n6. CLASS WEIGHT SUGGESTION:\")\n",
    "neg_count = (train_df[target_col] == 0).sum()\n",
    "pos_count = (train_df[target_col] == 1).sum()\n",
    "class_weight_ratio = neg_count / pos_count\n",
    "print(f\"Negative samples: {neg_count}\")\n",
    "print(f\"Positive samples: {pos_count}\")\n",
    "print(f\"Imbalance ratio: {class_weight_ratio:.2f}:1\")\n",
    "print(f\"Consider adding 'scale_pos_weight': {class_weight_ratio:.2f} to LightGBM params\")\n",
    "\n",
    "print(f\"\\n7. RECOMMENDATIONS:\")\n",
    "print(f\"- Compare this model with embeddings-only version to see categorical feature impact\")\n",
    "print(f\"- Consider using threshold {optimal_threshold:.3f} instead of 0.5\")\n",
    "print(f\"- Categorical features contribute {categorical_importance/total_importance*100:.1f}% of importance\")\n",
    "print(f\"- If precision/recall are still low, consider class balancing techniques\")\n",
    "print(f\"- The combination of embeddings + categorical may improve generalization\")\n",
    "\n",
    "# 8. Compare with embeddings-only baseline\n",
    "print(f\"\\n8. MODEL COMPARISON INSIGHTS:\")\n",
    "print(f\"This model includes both sequence embeddings AND categorical features:\")\n",
    "print(f\"  - TCR + Epitope embeddings: {(tcr_emb_importance + epitope_emb_importance)/total_importance*100:.1f}%\")\n",
    "print(f\"  - MHC + TRBV + TRBJ features: {categorical_importance/total_importance*100:.1f}%\")\n",
    "print(f\"Compare AUC with embeddings-only model to quantify categorical feature value\")\n",
    "print(f\"If categorical features show high importance, they're capturing crucial biological context\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
