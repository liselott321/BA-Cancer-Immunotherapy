{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58da0982",
   "metadata": {},
   "source": [
    "### ...using LabelEncoder.  5 Features TRC, Epitope, TRBJ, TRBV, MHC (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b946fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6595288699964577\n",
      "AUC: 0.4899016846583038\n",
      "F1 Score: 0.18300820264354625\n",
      "AP Score: 0.1583093947239425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Define columns\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Label encode all features (basic encoding for simplicity)\n",
    "encoders = {}\n",
    "for col in feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Identify categorical columns (LightGBM handles them natively)\n",
    "categorical_features = ['TRBV', 'TRBJ', 'MHC']\n",
    "\n",
    "# LightGBM datasets\n",
    "train_data = lgb.Dataset(train_df[feature_cols], label=train_df[target_col], categorical_feature=categorical_features)\n",
    "valid_data = lgb.Dataset(valid_df[feature_cols], label=valid_df[target_col], reference=train_data, categorical_feature=categorical_features)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    # early_stopping_rounds=20\n",
    ")\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(test_df[feature_cols])\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_df[target_col]\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred_binary))\n",
    "print('AUC:', roc_auc_score(y_true, y_pred))\n",
    "print('F1 Score:', f1_score(y_true, y_pred_binary))\n",
    "print('AP Score:', average_precision_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade3d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918db1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9054040e",
   "metadata": {},
   "source": [
    "### ...using Separate Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c4a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 126463, number of negative: 623204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 630\n",
      "[LightGBM] [Info] Number of data points in the train set: 749667, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168692 -> initscore=-1.594924\n",
      "[LightGBM] [Info] Start training from score -1.594924\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.391351\n",
      "Accuracy: 0.8401346085724407\n",
      "AUC: 0.5\n",
      "F1 Score: 0.0\n",
      "AP Score: 0.15986539142755934\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "seq_cols = ['TRB_CDR3', 'Epitope']\n",
    "encoders = {}\n",
    "\n",
    "for col in seq_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "for col in ['TRBV', 'TRBJ', 'MHC']:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    valid_df[col] = valid_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "# Now create feature matrices\n",
    "\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "X_train = train_df[feature_cols]\n",
    "X_valid = valid_df[feature_cols]\n",
    "X_test = test_df[feature_cols]\n",
    "y_train = train_df['Binding']\n",
    "y_valid = valid_df['Binding']\n",
    "y_test = test_df['Binding']\n",
    "\n",
    "for col in ['TRBV', 'TRBJ', 'MHC']:\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)],\n",
    "    categorical_feature=['TRBV', 'TRBJ', 'MHC']\n",
    ")\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(test_df[feature_cols])\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_df['Binding']\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred_binary))\n",
    "print('AUC:', roc_auc_score(y_true, y_pred))\n",
    "print('F1 Score:', f1_score(y_true, y_pred_binary))\n",
    "print('AP Score:', average_precision_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f0b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcad2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f4ebce0",
   "metadata": {},
   "source": [
    "### with task-wise evaluation (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b1b8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 126463, number of negative: 623204\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 630\n",
      "[LightGBM] [Info] Number of data points in the train set: 749667, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168692 -> initscore=-1.594924\n",
      "[LightGBM] [Info] Start training from score -1.594924\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.391351\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.8416483463581367\n",
      "Log Loss: 0.39135055242937095\n",
      "ROC AUC: 0.7928649290044056\n",
      "Average Precision: 0.4121502422654325\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8300\n",
      "  Log Loss: 0.3340\n",
      "  ROC AUC: 0.926526261887623\n",
      "  Average Precision: 0.7928157785599795\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8538\n",
      "  Log Loss: 0.4223\n",
      "  ROC AUC: 0.7561215438942985\n",
      "  Average Precision: 0.35026524268266435\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8000\n",
      "  Log Loss: 0.5258\n",
      "  ROC AUC: 0.5593265212050753\n",
      "  Average Precision: 0.22702942478656346\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8398\n",
      "  Log Loss: 0.3725\n",
      "  ROC AUC: 0.8493312900529395\n",
      "  Average Precision: 0.5435338894569318\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.8401346085724407\n",
      "Log Loss: 0.45854332467031544\n",
      "ROC AUC: 0.5377472439553105\n",
      "Average Precision: 0.17166589592299852\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8300\n",
      "  Log Loss: 0.3869\n",
      "  ROC AUC: 0.934366648790363\n",
      "  Average Precision: 0.6325310787857514\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8432\n",
      "  Log Loss: 0.4649\n",
      "  ROC AUC: 0.48053446881993406\n",
      "  Average Precision: 0.15469702331938195\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.7735\n",
      "  Log Loss: 0.6094\n",
      "  ROC AUC: 0.36174757366980786\n",
      "  Average Precision: 0.1810333500666166\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8438\n",
      "  Log Loss: 0.5468\n",
      "  ROC AUC: 0.3972077744319737\n",
      "  Average Precision: 0.13596105956098845\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Ensure categorical columns are properly typed\n",
    "for col in ['TRBV', 'TRBJ', 'MHC']:\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Encode high-cardinality object columns\n",
    "for col in ['TRB_CDR3', 'Epitope']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = le.transform(df[col].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "target_col = 'Binding'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# Train LightGBM model\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)],\n",
    "    categorical_feature=['TRBV', 'TRBJ', 'MHC']\n",
    ")\n",
    "\n",
    "# === Validation Evaluation ===\n",
    "print(\"=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict_proba(X_valid)[:, 1]\n",
    "y_val_label = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_val_label))\n",
    "print(\"Log Loss:\", log_loss(y_valid, y_val_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_valid, y_val_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_valid, y_val_prob))\n",
    "\n",
    "# === Per-task validation metrics ===\n",
    "print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "valid_df_copy = valid_df.copy()\n",
    "valid_df_copy['true'] = y_valid\n",
    "valid_df_copy['pred_prob'] = y_val_prob\n",
    "valid_df_copy['pred_label'] = y_val_label\n",
    "\n",
    "for task_name, group in valid_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n",
    "\n",
    "# === Test Evaluation ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_test_label = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_label))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_test_prob))\n",
    "\n",
    "# === Per-task test metrics ===\n",
    "print(\"\\n=== Per Task Test Metrics ===\")\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['true'] = y_test\n",
    "test_df_copy['pred_prob'] = y_test_prob\n",
    "test_df_copy['pred_label'] = y_test_label\n",
    "\n",
    "for task_name, group in test_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36d368",
   "metadata": {},
   "source": [
    "### using only TCR and EPITOPE (comparison with v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d674379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 126463, number of negative: 623204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 442\n",
      "[LightGBM] [Info] Number of data points in the train set: 749667, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168692 -> initscore=-1.594924\n",
      "[LightGBM] [Info] Start training from score -1.594924\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.432583\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.8416483463581367\n",
      "Log Loss: 0.43258284878226305\n",
      "ROC AUC: 0.6544983565520205\n",
      "Average Precision: 0.22798046896800525\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8300\n",
      "  Log Loss: 0.4231\n",
      "  ROC AUC: 0.7624740791222908\n",
      "  Average Precision: 0.45844087879593465\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8538\n",
      "  Log Loss: 0.4324\n",
      "  ROC AUC: 0.581031947973067\n",
      "  Average Precision: 0.19290592744459037\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8000\n",
      "  Log Loss: 0.5156\n",
      "  ROC AUC: 0.5153348094515402\n",
      "  Average Precision: 0.20399069737293146\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8398\n",
      "  Log Loss: 0.4696\n",
      "  ROC AUC: 0.4769434382836445\n",
      "  Average Precision: 0.1623378515853269\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.8401346085724407\n",
      "Log Loss: 0.4408663802532067\n",
      "ROC AUC: 0.5566301362806036\n",
      "Average Precision: 0.18350244751948588\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8300\n",
      "  Log Loss: 0.4424\n",
      "  ROC AUC: 0.6558798106612124\n",
      "  Average Precision: 0.30830096074282637\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8432\n",
      "  Log Loss: 0.4375\n",
      "  ROC AUC: 0.544664972028467\n",
      "  Average Precision: 0.1744780103837188\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.7735\n",
      "  Log Loss: 0.5680\n",
      "  ROC AUC: 0.4858005789756179\n",
      "  Average Precision: 0.22939019617458037\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8438\n",
      "  Log Loss: 0.4539\n",
      "  ROC AUC: 0.5901656173008486\n",
      "  Average Precision: 0.20558456970333294\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Encode high-cardinality object columns\n",
    "for col in ['TRB_CDR3', 'Epitope']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = le.transform(df[col].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = ['TRB_CDR3', 'Epitope']\n",
    "target_col = 'Binding'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# Train model\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# === Validation Metrics ===\n",
    "print(\"=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict_proba(X_valid)[:, 1]\n",
    "y_val_label = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_val_label))\n",
    "print(\"Log Loss:\", log_loss(y_valid, y_val_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_valid, y_val_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_valid, y_val_prob))\n",
    "\n",
    "# === Per-Task Validation ===\n",
    "print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "valid_df_copy = valid_df.copy()\n",
    "valid_df_copy['true'] = y_valid\n",
    "valid_df_copy['pred_prob'] = y_val_prob\n",
    "valid_df_copy['pred_label'] = y_val_label\n",
    "\n",
    "for task_name, group in valid_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n",
    "\n",
    "# === Test Metrics ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_test_label = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_label))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_test_prob))\n",
    "\n",
    "# === Per-Task Test ===\n",
    "print(\"\\n=== Per Task Test Metrics ===\")\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['true'] = y_test\n",
    "test_df_copy['pred_prob'] = y_test_prob\n",
    "test_df_copy['pred_label'] = y_test_label\n",
    "\n",
    "for task_name, group in test_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be1b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba3589f",
   "metadata": {},
   "source": [
    "### NEW DATASETS (11.05.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e34e91",
   "metadata": {},
   "source": [
    "### 5 Features (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77569622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.7587489902176413\n",
      "AUC: 0.8721124023971237\n",
      "F1 Score: 0.5362276127862162\n",
      "AP Score: 0.6081711524532278\n",
      "Log Loss: 0.7710147292014076\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.9300\n",
      "  Log Loss: 0.20282006183958637\n",
      "  ROC AUC: 0.9628357095681376\n",
      "  Average Precision: 0.8416865374535609\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.6206\n",
      "  Log Loss: 1.3140577712762866\n",
      "  ROC AUC: 0.909488420236908\n",
      "  Average Precision: 0.6961157347542468\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.5230\n",
      "  Log Loss: 1.0703836113051035\n",
      "  ROC AUC: 0.5560485936172912\n",
      "  Average Precision: 0.22375528505626902\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.5600\n",
      "  Log Loss: 1.2563158304713276\n",
      "  ROC AUC: 0.6160714285714286\n",
      "  Average Precision: 0.20596309324115109\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.6231379677247739\n",
      "AUC: 0.47886036925488296\n",
      "F1 Score: 0.19923696481559983\n",
      "AP Score: 0.1870805137103096\n",
      "Log Loss: 1.2484113333004747\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8310\n",
      "  Log Loss: 0.6600801261071554\n",
      "  ROC AUC: 0.7452975115419119\n",
      "  Average Precision: 0.4225541604408659\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.5897\n",
      "  Log Loss: 1.3683715109752352\n",
      "  ROC AUC: 0.436493389377972\n",
      "  Average Precision: 0.17802815856643017\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.6021\n",
      "  Log Loss: 1.2021951188505944\n",
      "  ROC AUC: 0.4320892639664319\n",
      "  Average Precision: 0.1662264135162896\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.4814\n",
      "  Log Loss: 1.2651991866471428\n",
      "  ROC AUC: 0.3788845201689238\n",
      "  Average Precision: 0.1582352154187722\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Define columns\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Label encode all features (basic encoding for simplicity)\n",
    "encoders = {}\n",
    "for col in feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Identify categorical columns (LightGBM handles them natively)\n",
    "categorical_features = ['TRBV', 'TRBJ', 'MHC']\n",
    "\n",
    "# LightGBM datasets\n",
    "train_data = lgb.Dataset(train_df[feature_cols], label=train_df[target_col], categorical_feature=categorical_features)\n",
    "valid_data = lgb.Dataset(valid_df[feature_cols], label=valid_df[target_col], reference=train_data, categorical_feature=categorical_features)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    # early_stopping_rounds=20\n",
    ")\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(test_df[feature_cols])\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict(valid_df[feature_cols])\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(\"Log Loss:\", log_loss(y_val_true, y_val_prob))\n",
    "print('Accuracy:', accuracy_score(y_val_true, y_val_pred))\n",
    "print('AUC:', roc_auc_score(y_val_true, y_val_prob))\n",
    "print('F1 Score:', f1_score(y_val_true, y_val_pred))\n",
    "print('AP Score:', average_precision_score(y_val_true, y_val_prob))\n",
    "\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name}\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred_binary))\n",
    "print('AUC:', roc_auc_score(y_true, y_pred))\n",
    "print('F1 Score:', f1_score(y_true, y_pred_binary))\n",
    "print('AP Score:', average_precision_score(y_true, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_true, y_pred))\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n=== Per Task Test Metrics ===\")\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name}\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee3875",
   "metadata": {},
   "source": [
    "### with only TCR and Epitope (v1)(new datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa664d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.457951\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.8257041942084191\n",
      "Log Loss: 0.4579513818899797\n",
      "ROC AUC: 0.5951871934281516\n",
      "Average Precision: 0.2215396541033768\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8400\n",
      "  Log Loss: 0.4160\n",
      "  ROC AUC: 0.7054980177191659\n",
      "  Average Precision: 0.3608687118824807\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8116\n",
      "  Log Loss: 0.4986\n",
      "  ROC AUC: 0.4768861076839876\n",
      "  Average Precision: 0.20269607444255933\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8203\n",
      "  Log Loss: 0.4771\n",
      "  ROC AUC: 0.5251143783316623\n",
      "  Average Precision: 0.19784586855051475\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8400\n",
      "  Log Loss: 0.4836\n",
      "  ROC AUC: 0.373139880952381\n",
      "  Average Precision: 0.13272558958390446\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.7981690015960277\n",
      "Log Loss: 0.5089030133540323\n",
      "ROC AUC: 0.5416726294922185\n",
      "Average Precision: 0.21877707342979233\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8200\n",
      "  Log Loss: 0.4623\n",
      "  ROC AUC: 0.6271149982080263\n",
      "  Average Precision: 0.26328940293143543\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.7923\n",
      "  Log Loss: 0.5194\n",
      "  ROC AUC: 0.5302786569565677\n",
      "  Average Precision: 0.2208777134318178\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8067\n",
      "  Log Loss: 0.5037\n",
      "  ROC AUC: 0.4992941062793061\n",
      "  Average Precision: 0.18910427941671148\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8122\n",
      "  Log Loss: 0.4755\n",
      "  ROC AUC: 0.5607106451143149\n",
      "  Average Precision: 0.23356506641686017\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Encode high-cardinality object columns\n",
    "for col in ['TRB_CDR3', 'Epitope']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = le.transform(df[col].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = ['TRB_CDR3', 'Epitope']\n",
    "target_col = 'Binding'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# Train model\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# === Validation Metrics ===\n",
    "print(\"=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict_proba(X_valid)[:, 1]\n",
    "y_val_label = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_val_label))\n",
    "print(\"Log Loss:\", log_loss(y_valid, y_val_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_valid, y_val_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_valid, y_val_prob))\n",
    "\n",
    "# === Per-Task Validation ===\n",
    "print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "valid_df_copy = valid_df.copy()\n",
    "valid_df_copy['true'] = y_valid\n",
    "valid_df_copy['pred_prob'] = y_val_prob\n",
    "valid_df_copy['pred_label'] = y_val_label\n",
    "\n",
    "for task_name, group in valid_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n",
    "\n",
    "# === Test Metrics ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_test_label = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_label))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_test_prob))\n",
    "\n",
    "# === Per-Task Test ===\n",
    "print(\"\\n=== Per Task Test Metrics ===\")\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['true'] = y_test\n",
    "test_df_copy['pred_prob'] = y_test_prob\n",
    "test_df_copy['pred_label'] = y_test_label\n",
    "\n",
    "for task_name, group in test_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
