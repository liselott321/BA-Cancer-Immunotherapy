{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58da0982",
   "metadata": {},
   "source": [
    "### ...using LabelEncoder.  5 Features TRC, Epitope, TRBJ, TRBV, MHC (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b946fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6595288699964577\n",
      "AUC: 0.4899016846583038\n",
      "F1 Score: 0.18300820264354625\n",
      "AP Score: 0.1583093947239425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Define columns\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Label encode all features (basic encoding for simplicity)\n",
    "encoders = {}\n",
    "for col in feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Identify categorical columns (LightGBM handles them natively)\n",
    "categorical_features = ['TRBV', 'TRBJ', 'MHC']\n",
    "\n",
    "# LightGBM datasets\n",
    "train_data = lgb.Dataset(train_df[feature_cols], label=train_df[target_col], categorical_feature=categorical_features)\n",
    "valid_data = lgb.Dataset(valid_df[feature_cols], label=valid_df[target_col], reference=train_data, categorical_feature=categorical_features)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    # early_stopping_rounds=20\n",
    ")\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(test_df[feature_cols])\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_df[target_col]\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred_binary))\n",
    "print('AUC:', roc_auc_score(y_true, y_pred))\n",
    "print('F1 Score:', f1_score(y_true, y_pred_binary))\n",
    "print('AP Score:', average_precision_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade3d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918db1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9054040e",
   "metadata": {},
   "source": [
    "### ...using Separate Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c4a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 126463, number of negative: 623204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 630\n",
      "[LightGBM] [Info] Number of data points in the train set: 749667, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168692 -> initscore=-1.594924\n",
      "[LightGBM] [Info] Start training from score -1.594924\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.391351\n",
      "Accuracy: 0.8401346085724407\n",
      "AUC: 0.5\n",
      "F1 Score: 0.0\n",
      "AP Score: 0.15986539142755934\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "seq_cols = ['TRB_CDR3', 'Epitope']\n",
    "encoders = {}\n",
    "\n",
    "for col in seq_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "for col in ['TRBV', 'TRBJ', 'MHC']:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    valid_df[col] = valid_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "# Now create feature matrices\n",
    "\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "X_train = train_df[feature_cols]\n",
    "X_valid = valid_df[feature_cols]\n",
    "X_test = test_df[feature_cols]\n",
    "y_train = train_df['Binding']\n",
    "y_valid = valid_df['Binding']\n",
    "y_test = test_df['Binding']\n",
    "\n",
    "for col in ['TRBV', 'TRBJ', 'MHC']:\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)],\n",
    "    categorical_feature=['TRBV', 'TRBJ', 'MHC']\n",
    ")\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(test_df[feature_cols])\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_df['Binding']\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred_binary))\n",
    "print('AUC:', roc_auc_score(y_true, y_pred))\n",
    "print('F1 Score:', f1_score(y_true, y_pred_binary))\n",
    "print('AP Score:', average_precision_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f0b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcad2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f4ebce0",
   "metadata": {},
   "source": [
    "### with task-wise evaluation (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b1b8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 126463, number of negative: 623204\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 630\n",
      "[LightGBM] [Info] Number of data points in the train set: 749667, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168692 -> initscore=-1.594924\n",
      "[LightGBM] [Info] Start training from score -1.594924\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.391351\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.8416483463581367\n",
      "Log Loss: 0.39135055242937095\n",
      "ROC AUC: 0.7928649290044056\n",
      "Average Precision: 0.4121502422654325\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8300\n",
      "  Log Loss: 0.3340\n",
      "  ROC AUC: 0.926526261887623\n",
      "  Average Precision: 0.7928157785599795\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8538\n",
      "  Log Loss: 0.4223\n",
      "  ROC AUC: 0.7561215438942985\n",
      "  Average Precision: 0.35026524268266435\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8000\n",
      "  Log Loss: 0.5258\n",
      "  ROC AUC: 0.5593265212050753\n",
      "  Average Precision: 0.22702942478656346\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8398\n",
      "  Log Loss: 0.3725\n",
      "  ROC AUC: 0.8493312900529395\n",
      "  Average Precision: 0.5435338894569318\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.8401346085724407\n",
      "Log Loss: 0.45854332467031544\n",
      "ROC AUC: 0.5377472439553105\n",
      "Average Precision: 0.17166589592299852\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8300\n",
      "  Log Loss: 0.3869\n",
      "  ROC AUC: 0.934366648790363\n",
      "  Average Precision: 0.6325310787857514\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8432\n",
      "  Log Loss: 0.4649\n",
      "  ROC AUC: 0.48053446881993406\n",
      "  Average Precision: 0.15469702331938195\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.7735\n",
      "  Log Loss: 0.6094\n",
      "  ROC AUC: 0.36174757366980786\n",
      "  Average Precision: 0.1810333500666166\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8438\n",
      "  Log Loss: 0.5468\n",
      "  ROC AUC: 0.3972077744319737\n",
      "  Average Precision: 0.13596105956098845\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Ensure categorical columns are properly typed\n",
    "for col in ['TRBV', 'TRBJ', 'MHC']:\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Encode high-cardinality object columns\n",
    "for col in ['TRB_CDR3', 'Epitope']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = le.transform(df[col].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "target_col = 'Binding'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# Train LightGBM model\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)],\n",
    "    categorical_feature=['TRBV', 'TRBJ', 'MHC']\n",
    ")\n",
    "\n",
    "# === Validation Evaluation ===\n",
    "print(\"=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict_proba(X_valid)[:, 1]\n",
    "y_val_label = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_val_label))\n",
    "print(\"Log Loss:\", log_loss(y_valid, y_val_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_valid, y_val_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_valid, y_val_prob))\n",
    "\n",
    "# === Per-task validation metrics ===\n",
    "print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "valid_df_copy = valid_df.copy()\n",
    "valid_df_copy['true'] = y_valid\n",
    "valid_df_copy['pred_prob'] = y_val_prob\n",
    "valid_df_copy['pred_label'] = y_val_label\n",
    "\n",
    "for task_name, group in valid_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n",
    "\n",
    "# === Test Evaluation ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_test_label = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_label))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_test_prob))\n",
    "\n",
    "# === Per-task test metrics ===\n",
    "print(\"\\n=== Per Task Test Metrics ===\")\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['true'] = y_test\n",
    "test_df_copy['pred_prob'] = y_test_prob\n",
    "test_df_copy['pred_label'] = y_test_label\n",
    "\n",
    "for task_name, group in test_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36d368",
   "metadata": {},
   "source": [
    "### using only TCR and EPITOPE (comparison with v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d674379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 126463, number of negative: 623204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 442\n",
      "[LightGBM] [Info] Number of data points in the train set: 749667, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168692 -> initscore=-1.594924\n",
      "[LightGBM] [Info] Start training from score -1.594924\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.432583\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.8416483463581367\n",
      "Log Loss: 0.43258284878226305\n",
      "ROC AUC: 0.6544983565520205\n",
      "Average Precision: 0.22798046896800525\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8300\n",
      "  Log Loss: 0.4231\n",
      "  ROC AUC: 0.7624740791222908\n",
      "  Average Precision: 0.45844087879593465\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8538\n",
      "  Log Loss: 0.4324\n",
      "  ROC AUC: 0.581031947973067\n",
      "  Average Precision: 0.19290592744459037\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8000\n",
      "  Log Loss: 0.5156\n",
      "  ROC AUC: 0.5153348094515402\n",
      "  Average Precision: 0.20399069737293146\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8398\n",
      "  Log Loss: 0.4696\n",
      "  ROC AUC: 0.4769434382836445\n",
      "  Average Precision: 0.1623378515853269\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.8401346085724407\n",
      "Log Loss: 0.4408663802532067\n",
      "ROC AUC: 0.5566301362806036\n",
      "Average Precision: 0.18350244751948588\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8300\n",
      "  Log Loss: 0.4424\n",
      "  ROC AUC: 0.6558798106612124\n",
      "  Average Precision: 0.30830096074282637\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8432\n",
      "  Log Loss: 0.4375\n",
      "  ROC AUC: 0.544664972028467\n",
      "  Average Precision: 0.1744780103837188\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.7735\n",
      "  Log Loss: 0.5680\n",
      "  ROC AUC: 0.4858005789756179\n",
      "  Average Precision: 0.22939019617458037\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8438\n",
      "  Log Loss: 0.4539\n",
      "  ROC AUC: 0.5901656173008486\n",
      "  Average Precision: 0.20558456970333294\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Encode high-cardinality object columns\n",
    "for col in ['TRB_CDR3', 'Epitope']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = le.transform(df[col].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = ['TRB_CDR3', 'Epitope']\n",
    "target_col = 'Binding'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# Train model\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# === Validation Metrics ===\n",
    "print(\"=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict_proba(X_valid)[:, 1]\n",
    "y_val_label = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_val_label))\n",
    "print(\"Log Loss:\", log_loss(y_valid, y_val_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_valid, y_val_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_valid, y_val_prob))\n",
    "\n",
    "# === Per-Task Validation ===\n",
    "print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "valid_df_copy = valid_df.copy()\n",
    "valid_df_copy['true'] = y_valid\n",
    "valid_df_copy['pred_prob'] = y_val_prob\n",
    "valid_df_copy['pred_label'] = y_val_label\n",
    "\n",
    "for task_name, group in valid_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n",
    "\n",
    "# === Test Metrics ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_test_label = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_label))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_test_prob))\n",
    "\n",
    "# === Per-Task Test ===\n",
    "print(\"\\n=== Per Task Test Metrics ===\")\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['true'] = y_test\n",
    "test_df_copy['pred_prob'] = y_test_prob\n",
    "test_df_copy['pred_label'] = y_test_label\n",
    "\n",
    "for task_name, group in test_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be1b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba3589f",
   "metadata": {},
   "source": [
    "### NEW DATASETS (11.05.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e34e91",
   "metadata": {},
   "source": [
    "### 5 Features (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77569622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.7587489902176413\n",
      "AUC: 0.8721124023971237\n",
      "F1 Score: 0.5362276127862162\n",
      "AP Score: 0.6081711524532278\n",
      "Log Loss: 0.7710147292014076\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.9300\n",
      "  Log Loss: 0.20282006183958637\n",
      "  ROC AUC: 0.9628357095681376\n",
      "  Average Precision: 0.8416865374535609\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.6206\n",
      "  Log Loss: 1.3140577712762866\n",
      "  ROC AUC: 0.909488420236908\n",
      "  Average Precision: 0.6961157347542468\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.5230\n",
      "  Log Loss: 1.0703836113051035\n",
      "  ROC AUC: 0.5560485936172912\n",
      "  Average Precision: 0.22375528505626902\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.5600\n",
      "  Log Loss: 1.2563158304713276\n",
      "  ROC AUC: 0.6160714285714286\n",
      "  Average Precision: 0.20596309324115109\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.6231379677247739\n",
      "AUC: 0.47886036925488296\n",
      "F1 Score: 0.19923696481559983\n",
      "AP Score: 0.1870805137103096\n",
      "Log Loss: 1.2484113333004747\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8310\n",
      "  Log Loss: 0.6600801261071554\n",
      "  ROC AUC: 0.7452975115419119\n",
      "  Average Precision: 0.4225541604408659\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.5897\n",
      "  Log Loss: 1.3683715109752352\n",
      "  ROC AUC: 0.436493389377972\n",
      "  Average Precision: 0.17802815856643017\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.6021\n",
      "  Log Loss: 1.2021951188505944\n",
      "  ROC AUC: 0.4320892639664319\n",
      "  Average Precision: 0.1662264135162896\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.4814\n",
      "  Log Loss: 1.2651991866471428\n",
      "  ROC AUC: 0.3788845201689238\n",
      "  Average Precision: 0.1582352154187722\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Define columns\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Label encode all features (basic encoding for simplicity)\n",
    "encoders = {}\n",
    "for col in feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Identify categorical columns (LightGBM handles them natively)\n",
    "categorical_features = ['TRBV', 'TRBJ', 'MHC']\n",
    "\n",
    "# LightGBM datasets\n",
    "train_data = lgb.Dataset(train_df[feature_cols], label=train_df[target_col], categorical_feature=categorical_features)\n",
    "valid_data = lgb.Dataset(valid_df[feature_cols], label=valid_df[target_col], reference=train_data, categorical_feature=categorical_features)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    # early_stopping_rounds=20\n",
    ")\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(test_df[feature_cols])\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict(valid_df[feature_cols])\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(\"Log Loss:\", log_loss(y_val_true, y_val_prob))\n",
    "print('Accuracy:', accuracy_score(y_val_true, y_val_pred))\n",
    "print('AUC:', roc_auc_score(y_val_true, y_val_prob))\n",
    "print('F1 Score:', f1_score(y_val_true, y_val_pred))\n",
    "print('AP Score:', average_precision_score(y_val_true, y_val_prob))\n",
    "\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name}\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred_binary))\n",
    "print('AUC:', roc_auc_score(y_true, y_pred))\n",
    "print('F1 Score:', f1_score(y_true, y_pred_binary))\n",
    "print('AP Score:', average_precision_score(y_true, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_true, y_pred))\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n=== Per Task Test Metrics ===\")\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name}\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee3875",
   "metadata": {},
   "source": [
    "### with only TCR and Epitope (v1)(new datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa664d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.457951\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.8257041942084191\n",
      "Log Loss: 0.4579513818899797\n",
      "ROC AUC: 0.5951871934281516\n",
      "Average Precision: 0.2215396541033768\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8400\n",
      "  Log Loss: 0.4160\n",
      "  ROC AUC: 0.7054980177191659\n",
      "  Average Precision: 0.3608687118824807\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8116\n",
      "  Log Loss: 0.4986\n",
      "  ROC AUC: 0.4768861076839876\n",
      "  Average Precision: 0.20269607444255933\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8203\n",
      "  Log Loss: 0.4771\n",
      "  ROC AUC: 0.5251143783316623\n",
      "  Average Precision: 0.19784586855051475\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8400\n",
      "  Log Loss: 0.4836\n",
      "  ROC AUC: 0.373139880952381\n",
      "  Average Precision: 0.13272558958390446\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.7981690015960277\n",
      "Log Loss: 0.5089030133540323\n",
      "ROC AUC: 0.5416726294922185\n",
      "Average Precision: 0.21877707342979233\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8200\n",
      "  Log Loss: 0.4623\n",
      "  ROC AUC: 0.6271149982080263\n",
      "  Average Precision: 0.26328940293143543\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.7923\n",
      "  Log Loss: 0.5194\n",
      "  ROC AUC: 0.5302786569565677\n",
      "  Average Precision: 0.2208777134318178\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8067\n",
      "  Log Loss: 0.5037\n",
      "  ROC AUC: 0.4992941062793061\n",
      "  Average Precision: 0.18910427941671148\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8122\n",
      "  Log Loss: 0.4755\n",
      "  ROC AUC: 0.5607106451143149\n",
      "  Average Precision: 0.23356506641686017\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Encode high-cardinality object columns\n",
    "for col in ['TRB_CDR3', 'Epitope']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = le.transform(df[col].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = ['TRB_CDR3', 'Epitope']\n",
    "target_col = 'Binding'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# Train model\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# === Validation Metrics ===\n",
    "print(\"=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict_proba(X_valid)[:, 1]\n",
    "y_val_label = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_val_label))\n",
    "print(\"Log Loss:\", log_loss(y_valid, y_val_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_valid, y_val_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_valid, y_val_prob))\n",
    "\n",
    "# === Per-Task Validation ===\n",
    "print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "valid_df_copy = valid_df.copy()\n",
    "valid_df_copy['true'] = y_valid\n",
    "valid_df_copy['pred_prob'] = y_val_prob\n",
    "valid_df_copy['pred_label'] = y_val_label\n",
    "\n",
    "for task_name, group in valid_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n",
    "\n",
    "# === Test Metrics ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_test_label = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_label))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_test_prob))\n",
    "\n",
    "# === Per-Task Test ===\n",
    "print(\"\\n=== Per Task Test Metrics ===\")\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['true'] = y_test\n",
    "test_df_copy['pred_prob'] = y_test_prob\n",
    "test_df_copy['pred_label'] = y_test_label\n",
    "\n",
    "for task_name, group in test_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffea33",
   "metadata": {},
   "source": [
    "### same as before, but with new datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6694989",
   "metadata": {},
   "source": [
    "epitope tcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a018af7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84085/553543427.py:12: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 126286, number of negative: 629472\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 465\n",
      "[LightGBM] [Info] Number of data points in the train set: 755758, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.167098 -> initscore=-1.606332\n",
      "[LightGBM] [Info] Start training from score -1.606332\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.425872\n",
      "=== Overall Validation Metrics ===\n",
      "Accuracy: 0.8317625969508191\n",
      "Log Loss: 0.4258722290803819\n",
      "ROC AUC: 0.6806558997752716\n",
      "Average Precision: 0.27078176695998446\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8352\n",
      "  Log Loss: 0.4154\n",
      "  ROC AUC: 0.7160862301372765\n",
      "  Average Precision: 0.3038405539509591\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8135\n",
      "  Log Loss: 0.4549\n",
      "  ROC AUC: 0.643195495899828\n",
      "  Average Precision: 0.2607644780532053\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8186\n",
      "  Log Loss: 0.4976\n",
      "  ROC AUC: 0.5394589926440465\n",
      "  Average Precision: 0.20231014203617037\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8340\n",
      "  Log Loss: 0.5077\n",
      "  ROC AUC: 0.45145631067961167\n",
      "  Average Precision: 0.16391633371735614\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.8386912020101245\n",
      "Log Loss: 0.4598157015914577\n",
      "ROC AUC: 0.5973951689334721\n",
      "Average Precision: 0.2078329056591953\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8600\n",
      "  Log Loss: 0.3669\n",
      "  ROC AUC: 0.7814149567945454\n",
      "  Average Precision: 0.4492573556630517\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8305\n",
      "  Log Loss: 0.4993\n",
      "  ROC AUC: 0.5297666035097325\n",
      "  Average Precision: 0.19803791806708043\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.8140\n",
      "  Log Loss: 0.5445\n",
      "  ROC AUC: 0.4914510857142858\n",
      "  Average Precision: 0.18287235898555643\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.8278\n",
      "  Log Loss: 0.5290\n",
      "  ROC AUC: 0.47100403311398853\n",
      "  Average Precision: 0.16914058547229432\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Encode high-cardinality object columns\n",
    "for col in ['TRB_CDR3', 'Epitope']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], valid_df[col], test_df[col]]).astype(str))\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df[col] = le.transform(df[col].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = ['TRB_CDR3', 'Epitope']\n",
    "target_col = 'Binding'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# Train model\n",
    "model = LGBMClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[early_stopping(20), log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# === Validation Metrics ===\n",
    "print(\"=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict_proba(X_valid)[:, 1]\n",
    "y_val_label = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_val_label))\n",
    "print(\"Log Loss:\", log_loss(y_valid, y_val_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_valid, y_val_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_valid, y_val_prob))\n",
    "\n",
    "# === Per-Task Validation ===\n",
    "print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "valid_df_copy = valid_df.copy()\n",
    "valid_df_copy['true'] = y_valid\n",
    "valid_df_copy['pred_prob'] = y_val_prob\n",
    "valid_df_copy['pred_label'] = y_val_label\n",
    "\n",
    "for task_name, group in valid_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n",
    "\n",
    "# === Test Metrics ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_test_label = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_label))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_test_prob))\n",
    "\n",
    "# === Per-Task Test ===\n",
    "print(\"\\n=== Per Task Test Metrics ===\")\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['true'] = y_test\n",
    "test_df_copy['pred_prob'] = y_test_prob\n",
    "test_df_copy['pred_label'] = y_test_label\n",
    "\n",
    "for task_name, group in test_df_copy.groupby('task'):\n",
    "    acc = accuracy_score(group['true'], group['pred_label'])\n",
    "    loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "    try:\n",
    "        auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "        ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "    except ValueError:\n",
    "        auc = ap = \"Undefined (only one class present)\"\n",
    "    \n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Log Loss: {loss:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc}\")\n",
    "    print(f\"  Average Precision: {ap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22defae7",
   "metadata": {},
   "source": [
    "more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d081f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84085/2101990501.py:15: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Validation Metrics ===\n",
      "Log Loss: 0.26782921524436815\n",
      "Accuracy: 0.8965976252595709\n",
      "AUC: 0.9294099109552929\n",
      "F1 Score: 0.686121686660441\n",
      "AP Score: 0.772210119750735\n",
      "\n",
      "=== Per Task Validation Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.9210\n",
      "  Log Loss: 0.21222189979624845\n",
      "  ROC AUC: 0.9556435909630675\n",
      "  Average Precision: 0.8349085443668809\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.8014\n",
      "  Log Loss: 0.4490073083747943\n",
      "  ROC AUC: 0.8497484874365699\n",
      "  Average Precision: 0.5056873602318208\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.7635\n",
      "  Log Loss: 0.6167057372193301\n",
      "  ROC AUC: 0.5502887897635488\n",
      "  Average Precision: 0.22116191363892912\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.7530\n",
      "  Log Loss: 0.6357053078697638\n",
      "  ROC AUC: 0.590989817665167\n",
      "  Average Precision: 0.19836087089244198\n",
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.7975649410634446\n",
      "AUC: 0.5194982716328727\n",
      "F1 Score: 0.10166434369107158\n",
      "AP Score: 0.17749155485417167\n",
      "Log Loss: 0.8361440783918489\n",
      "\n",
      "=== Per Task Test Metrics ===\n",
      "\n",
      "Task: TPP1\n",
      "  Accuracy: 0.8700\n",
      "  Log Loss: 0.6770567999098005\n",
      "  ROC AUC: 0.6189397084684768\n",
      "  Average Precision: 0.3547318519072744\n",
      "\n",
      "Task: TPP2\n",
      "  Accuracy: 0.7658\n",
      "  Log Loss: 0.8885416650355222\n",
      "  ROC AUC: 0.4568276488090235\n",
      "  Average Precision: 0.1631105751581631\n",
      "\n",
      "Task: TPP3\n",
      "  Accuracy: 0.7317\n",
      "  Log Loss: 1.085644369651103\n",
      "  ROC AUC: 0.41862422857142856\n",
      "  Average Precision: 0.14999378496312615\n",
      "\n",
      "Task: TPP4\n",
      "  Accuracy: 0.7786\n",
      "  Log Loss: 0.8183712960904265\n",
      "  ROC AUC: 0.47141265124177456\n",
      "  Average Precision: 0.17406013885073723\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Load the TSV files\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Define columns\n",
    "feature_cols = ['TRB_CDR3', 'Epitope', 'TRBV', 'TRBJ', 'MHC']\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Label encode all features (basic encoding for simplicity)\n",
    "encoders = {}\n",
    "for col in feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Identify categorical columns (LightGBM handles them natively)\n",
    "categorical_features = ['TRBV', 'TRBJ', 'MHC']\n",
    "\n",
    "# LightGBM datasets\n",
    "train_data = lgb.Dataset(train_df[feature_cols], label=train_df[target_col], categorical_feature=categorical_features)\n",
    "valid_data = lgb.Dataset(valid_df[feature_cols], label=valid_df[target_col], reference=train_data, categorical_feature=categorical_features)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    # early_stopping_rounds=20\n",
    ")\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(test_df[feature_cols])\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n=== Overall Validation Metrics ===\")\n",
    "y_val_prob = model.predict(valid_df[feature_cols])\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(\"Log Loss:\", log_loss(y_val_true, y_val_prob))\n",
    "print('Accuracy:', accuracy_score(y_val_true, y_val_pred))\n",
    "print('AUC:', roc_auc_score(y_val_true, y_val_prob))\n",
    "print('F1 Score:', f1_score(y_val_true, y_val_pred))\n",
    "print('AP Score:', average_precision_score(y_val_true, y_val_prob))\n",
    "\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n=== Per Task Validation Metrics ===\")\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name}\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred_binary))\n",
    "print('AUC:', roc_auc_score(y_true, y_pred))\n",
    "print('F1 Score:', f1_score(y_true, y_pred_binary))\n",
    "print('AP Score:', average_precision_score(y_true, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_true, y_pred))\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n=== Per Task Test Metrics ===\")\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name}\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fb743",
   "metadata": {},
   "source": [
    "## Embeddings Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96b7fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TCR EMBEDDING REDUCTION TO 512D ===\n",
      "\n",
      "Method recommendations:\n",
      "- PCA: Best information preservation, slower\n",
      "- Random Projection: Good balance of speed and quality\n",
      "- Truncation: Fastest, may lose some information\n",
      "- Selective: Good compromise, combines mean+max pooling\n",
      "Processing embeddings from ../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\n",
      "Loading embeddings from ../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz...\n",
      "Loaded 211294 sequences\n",
      "\n",
      "Applying selective reduction to all 211294 sequences...\n",
      "Applying selective pooling...\n",
      "Reduction completed in 5.83 seconds\n",
      "Saved reduced embeddings to ../../../../../data/embeddings/beta/allele/TRB_reduced_512_pca.pkl\n",
      "\n",
      "=== REDUCTION SUMMARY ===\n",
      "Original dimension: 1024 (after mean pooling)\n",
      "Reduced dimension: 512\n",
      "Compression ratio: 2.0x\n",
      "Memory reduction: 50.0%\n",
      "Total sequences processed: 211294\n",
      "\n",
      "Sample statistics for CASSWRDGATGELFF:\n",
      "  Mean: 0.1225\n",
      "  Std: 0.1699\n",
      "  Min: -0.2247\n",
      "  Max: 0.6312\n",
      "\n",
      "============================================================\n",
      "Processing Epitope embeddings...\n",
      "Processing embeddings from ../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\n",
      "Loading embeddings from ../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz...\n",
      "Loaded 1896 sequences\n",
      "\n",
      "Applying pca reduction to all 1896 sequences...\n",
      "Applying mean pooling...\n",
      "Mean pooled embeddings shape: (1896, 1024)\n",
      "Applying PCA to reduce to 512 dimensions...\n",
      "Explained variance ratio (first 10 components): [0.12115332 0.06617555 0.06354962 0.05327855 0.04484442 0.03621911\n",
      " 0.03343031 0.03023493 0.02742529 0.02485104]\n",
      "Total explained variance: 0.9896\n",
      "Saved PCA model to ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_pca_pca_model.pkl\n",
      "Reduction completed in 16.18 seconds\n",
      "Saved reduced embeddings to ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_pca.pkl\n",
      "\n",
      "=== REDUCTION SUMMARY ===\n",
      "Original dimension: 1024 (after mean pooling)\n",
      "Reduced dimension: 512\n",
      "Compression ratio: 2.0x\n",
      "Memory reduction: 50.0%\n",
      "Total sequences processed: 1896\n",
      "\n",
      "Sample statistics for LLAWHFVAV:\n",
      "  Mean: 0.0031\n",
      "  Std: 0.0611\n",
      "  Min: -0.2590\n",
      "  Max: 0.7176\n",
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "You can now use these 512-dimensional embeddings in your LightGBM model!\n",
      "Expected speedup: ~2x faster training\n",
      "Expected memory usage: ~50% less\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import time\n",
    "\n",
    "def load_npz_embeddings(npz_path):\n",
    "    \"\"\"Load embeddings from .npz file\"\"\"\n",
    "    print(f\"Loading embeddings from {npz_path}...\")\n",
    "    data = np.load(npz_path)\n",
    "    embeddings = {}\n",
    "    for key in data.files:\n",
    "        embeddings[key] = data[key]\n",
    "    print(f\"Loaded {len(embeddings)} sequences\")\n",
    "    return embeddings\n",
    "\n",
    "def reduce_embeddings_mean_512(embeddings_dict):\n",
    "    \"\"\"\n",
    "    Method 1: Mean pooling to get 1024 dims, then reduce to 512 using PCA\n",
    "    \"\"\"\n",
    "    print(\"Applying mean pooling...\")\n",
    "    # First do mean pooling to get 1024 dimensions\n",
    "    mean_embeddings = []\n",
    "    seq_ids = []\n",
    "    \n",
    "    for seq_id, embedding in embeddings_dict.items():\n",
    "        mean_embeddings.append(np.mean(embedding, axis=0))  # (1024,)\n",
    "        seq_ids.append(seq_id)\n",
    "    \n",
    "    mean_embeddings = np.array(mean_embeddings)  # (n_sequences, 1024)\n",
    "    print(f\"Mean pooled embeddings shape: {mean_embeddings.shape}\")\n",
    "    \n",
    "    # Use PCA to reduce from 1024 to 512\n",
    "    print(\"Applying PCA to reduce to 512 dimensions...\")\n",
    "    pca = PCA(n_components=512, random_state=42)\n",
    "    reduced_embeddings = pca.fit_transform(mean_embeddings)\n",
    "    \n",
    "    print(f\"Explained variance ratio (first 10 components): {pca.explained_variance_ratio_[:10]}\")\n",
    "    print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    \n",
    "    # Convert back to dictionary\n",
    "    result_embeddings = {}\n",
    "    for i, seq_id in enumerate(seq_ids):\n",
    "        result_embeddings[seq_id] = reduced_embeddings[i]\n",
    "    \n",
    "    return result_embeddings, pca\n",
    "\n",
    "def reduce_embeddings_random_projection_512(embeddings_dict):\n",
    "    \"\"\"\n",
    "    Method 2: Mean pooling + Random Projection (faster than PCA)\n",
    "    Good alternative when you want speed over optimal dimensionality reduction\n",
    "    \"\"\"\n",
    "    print(\"Applying mean pooling...\")\n",
    "    # First do mean pooling to get 1024 dimensions\n",
    "    mean_embeddings = []\n",
    "    seq_ids = []\n",
    "    \n",
    "    for seq_id, embedding in embeddings_dict.items():\n",
    "        mean_embeddings.append(np.mean(embedding, axis=0))\n",
    "        seq_ids.append(seq_id)\n",
    "    \n",
    "    mean_embeddings = np.array(mean_embeddings)\n",
    "    print(f\"Mean pooled embeddings shape: {mean_embeddings.shape}\")\n",
    "    \n",
    "    # Use Random Projection to reduce from 1024 to 512 (much faster than PCA)\n",
    "    print(\"Applying Random Projection to reduce to 512 dimensions...\")\n",
    "    rp = GaussianRandomProjection(n_components=512, random_state=42)\n",
    "    reduced_embeddings = rp.fit_transform(mean_embeddings)\n",
    "    \n",
    "    # Convert back to dictionary\n",
    "    result_embeddings = {}\n",
    "    for i, seq_id in enumerate(seq_ids):\n",
    "        result_embeddings[seq_id] = reduced_embeddings[i]\n",
    "    \n",
    "    return result_embeddings, rp\n",
    "\n",
    "def reduce_embeddings_truncate_512(embeddings_dict):\n",
    "    \"\"\"\n",
    "    Method 3: Mean pooling + simple truncation (fastest)\n",
    "    Just take the first 512 dimensions after mean pooling\n",
    "    \"\"\"\n",
    "    print(\"Applying mean pooling and truncation to 512 dimensions...\")\n",
    "    \n",
    "    result_embeddings = {}\n",
    "    for seq_id, embedding in embeddings_dict.items():\n",
    "        mean_emb = np.mean(embedding, axis=0)  # (1024,)\n",
    "        truncated_emb = mean_emb[:512]  # Take first 512 dimensions\n",
    "        result_embeddings[seq_id] = truncated_emb\n",
    "    \n",
    "    return result_embeddings\n",
    "\n",
    "def reduce_embeddings_selective_pooling_512(embeddings_dict):\n",
    "    \"\"\"\n",
    "    Method 4: Selective pooling - combine different pooling methods and reduce\n",
    "    More sophisticated approach that might preserve more information\n",
    "    \"\"\"\n",
    "    print(\"Applying selective pooling...\")\n",
    "    \n",
    "    combined_embeddings = []\n",
    "    seq_ids = []\n",
    "    \n",
    "    for seq_id, embedding in embeddings_dict.items():\n",
    "        # Get different pooling representations\n",
    "        mean_emb = np.mean(embedding, axis=0)      # (1024,)\n",
    "        max_emb = np.max(embedding, axis=0)        # (1024,)\n",
    "        \n",
    "        # Take first 256 dims from each pooling method\n",
    "        combined = np.concatenate([mean_emb[:256], max_emb[:256]])  # (512,)\n",
    "        \n",
    "        combined_embeddings.append(combined)\n",
    "        seq_ids.append(seq_id)\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    result_embeddings = {}\n",
    "    for i, seq_id in enumerate(seq_ids):\n",
    "        result_embeddings[seq_id] = combined_embeddings[i]\n",
    "    \n",
    "    return result_embeddings\n",
    "\n",
    "def benchmark_methods(embeddings_dict, sample_size=100):\n",
    "    \"\"\"\n",
    "    Benchmark different reduction methods on speed and memory\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== BENCHMARKING METHODS (sample size: {sample_size}) ===\")\n",
    "    \n",
    "    # Use a subset for benchmarking\n",
    "    sample_keys = list(embeddings_dict.keys())[:sample_size]\n",
    "    sample_embeddings = {k: embeddings_dict[k] for k in sample_keys}\n",
    "    \n",
    "    methods = [\n",
    "        (\"PCA\", reduce_embeddings_mean_512),\n",
    "        (\"Random Projection\", reduce_embeddings_random_projection_512),\n",
    "        (\"Truncation\", reduce_embeddings_truncate_512),\n",
    "        (\"Selective Pooling\", reduce_embeddings_selective_pooling_512)\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method_name, method_func in methods:\n",
    "        print(f\"\\nTesting {method_name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if method_name in [\"PCA\", \"Random Projection\"]:\n",
    "                reduced, model = method_func(sample_embeddings)\n",
    "            else:\n",
    "                reduced = method_func(sample_embeddings)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Check output\n",
    "            sample_key = list(reduced.keys())[0]\n",
    "            output_shape = reduced[sample_key].shape\n",
    "            \n",
    "            results[method_name] = {\n",
    "                'time': end_time - start_time,\n",
    "                'output_shape': output_shape,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "            print(f\"  Time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"  Output shape: {output_shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[method_name] = {\n",
    "                'time': float('inf'),\n",
    "                'output_shape': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== BENCHMARK SUMMARY ===\")\n",
    "    successful_methods = {k: v for k, v in results.items() if v['success']}\n",
    "    if successful_methods:\n",
    "        fastest = min(successful_methods.items(), key=lambda x: x[1]['time'])\n",
    "        print(f\"Fastest method: {fastest[0]} ({fastest[1]['time']:.2f}s)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_embeddings_to_512(npz_path, output_path, method='pca', benchmark=False):\n",
    "    \"\"\"\n",
    "    Complete pipeline to reduce embeddings to 512 dimensions\n",
    "    \n",
    "    Args:\n",
    "        npz_path: Path to input .npz file\n",
    "        output_path: Path to save reduced embeddings\n",
    "        method: Reduction method ('pca', 'random', 'truncate', 'selective')\n",
    "        benchmark: Whether to run benchmarking first\n",
    "    \"\"\"\n",
    "    print(f\"Processing embeddings from {npz_path}\")\n",
    "    embeddings = load_npz_embeddings(npz_path)\n",
    "    \n",
    "    if benchmark:\n",
    "        benchmark_results = benchmark_methods(embeddings, sample_size=min(100, len(embeddings)))\n",
    "    \n",
    "    print(f\"\\nApplying {method} reduction to all {len(embeddings)} sequences...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reduced, model = reduce_embeddings_mean_512(embeddings)\n",
    "        # Save the PCA model\n",
    "        model_path = output_path.replace('.pkl', '_pca_model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Saved PCA model to {model_path}\")\n",
    "        \n",
    "    elif method == 'random':\n",
    "        reduced, model = reduce_embeddings_random_projection_512(embeddings)\n",
    "        # Save the random projection model\n",
    "        model_path = output_path.replace('.pkl', '_rp_model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Saved Random Projection model to {model_path}\")\n",
    "        \n",
    "    elif method == 'truncate':\n",
    "        reduced = reduce_embeddings_truncate_512(embeddings)\n",
    "        \n",
    "    elif method == 'selective':\n",
    "        reduced = reduce_embeddings_selective_pooling_512(embeddings)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Reduction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save reduced embeddings\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(reduced, f)\n",
    "    print(f\"Saved reduced embeddings to {output_path}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    sample_key = list(reduced.keys())[0]\n",
    "    sample_embedding = reduced[sample_key]\n",
    "    \n",
    "    print(f\"\\n=== REDUCTION SUMMARY ===\")\n",
    "    print(f\"Original dimension: 1024 (after mean pooling)\")\n",
    "    print(f\"Reduced dimension: {len(sample_embedding)}\")\n",
    "    print(f\"Compression ratio: {1024/len(sample_embedding):.1f}x\")\n",
    "    print(f\"Memory reduction: {(1024-len(sample_embedding))/1024*100:.1f}%\")\n",
    "    print(f\"Total sequences processed: {len(reduced)}\")\n",
    "    \n",
    "    print(f\"\\nSample statistics for {sample_key}:\")\n",
    "    print(f\"  Mean: {np.mean(sample_embedding):.4f}\")\n",
    "    print(f\"  Std: {np.std(sample_embedding):.4f}\")\n",
    "    print(f\"  Min: {np.min(sample_embedding):.4f}\")\n",
    "    print(f\"  Max: {np.max(sample_embedding):.4f}\")\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "# Example usage and recommendations\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=== TCR EMBEDDING REDUCTION TO 512D ===\\n\")\n",
    "    \n",
    "    # Method recommendations based on your priorities:\n",
    "    print(\"Method recommendations:\")\n",
    "    print(\"- PCA: Best information preservation, slower\")\n",
    "    print(\"- Random Projection: Good balance of speed and quality\")\n",
    "    print(\"- Truncation: Fastest, may lose some information\")\n",
    "    print(\"- Selective: Good compromise, combines mean+max pooling\")\n",
    "    \n",
    "    # Process TCR embeddings\n",
    "    tcr_reduced = process_embeddings_to_512(\n",
    "        npz_path=\"../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\",\n",
    "        output_path=\"../../../../../data/embeddings/beta/allele/TRB_reduced_512_pca.pkl\",\n",
    "        method='selective',  # Change to 'random', 'truncate', or 'selective' as needed\n",
    "        benchmark=False  # Set to False to skip benchmarking\n",
    "    )\n",
    "    \n",
    "    # Process Epitope embeddings (keep at 512 if you want consistency)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Processing Epitope embeddings...\")\n",
    "    \n",
    "    epitope_reduced = process_embeddings_to_512(\n",
    "        npz_path=\"../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\",\n",
    "        output_path=\"../../../../../data/embeddings/beta/allele/Epitope_reduced_512_pca.pkl\",\n",
    "        method='pca',\n",
    "        benchmark=False  # Skip benchmarking for epitopes\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(\"You can now use these 512-dimensional embeddings in your LightGBM model!\")\n",
    "    print(\"Expected speedup: ~2x faster training\")\n",
    "    print(\"Expected memory usage: ~50% less\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc5eac",
   "metadata": {},
   "source": [
    "### Train and Test LightGBM\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ca7950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3761/171908633.py:22: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_pca.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_pca.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Encoding categorical features...\n",
      "Encoded TRBV: 166 unique values\n",
      "Encoded TRBJ: 31 unique values\n",
      "Encoded MHC: 99 unique values\n",
      "\n",
      "Combining features...\n",
      "Final feature dimensions: 1027 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - Categorical features: 3 features\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.263405\tval's binary_logloss: 0.236823\n",
      "[200]\ttrain's binary_logloss: 0.252306\tval's binary_logloss: 0.227989\n",
      "Early stopping, best iteration is:\n",
      "[226]\ttrain's binary_logloss: 0.250291\tval's binary_logloss: 0.227178\n",
      "\n",
      "Training completed. Best iteration: 226\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "==================================================\n",
      "VALIDATION METRICS\n",
      "==================================================\n",
      "Log Loss: 0.2272\n",
      "Accuracy: 0.9121\n",
      "AUC: 0.9326\n",
      "F1 Score: 0.7069\n",
      "AP Score: 0.7809\n",
      "\n",
      "==============================\n",
      "PER-TASK VALIDATION METRICS\n",
      "==============================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.9304\n",
      "  Log Loss: 0.1861554834655542\n",
      "  ROC AUC: 0.956704621657149\n",
      "  Average Precision: 0.8396180557746133\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8389\n",
      "  Log Loss: 0.35219232639504144\n",
      "  ROC AUC: 0.8590298998425739\n",
      "  Average Precision: 0.5252885722248617\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8154\n",
      "  Log Loss: 0.4959488889992186\n",
      "  ROC AUC: 0.5618378562786225\n",
      "  Average Precision: 0.2487114020056142\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.7935\n",
      "  Log Loss: 0.47863137821133456\n",
      "  ROC AUC: 0.5921146104664929\n",
      "  Average Precision: 0.21831743316140928\n",
      "\n",
      "==================================================\n",
      "TEST METRICS\n",
      "==================================================\n",
      "Accuracy: 0.8193\n",
      "AUC: 0.6220\n",
      "F1 Score: 0.0746\n",
      "AP Score: 0.2058\n",
      "Log Loss: 0.5302\n",
      "\n",
      "==============================\n",
      "PER-TASK TEST METRICS\n",
      "==============================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8679\n",
      "  Log Loss: 0.3596379195688806\n",
      "  ROC AUC: 0.9605212171738475\n",
      "  Average Precision: 0.7017168958003508\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.7974\n",
      "  Log Loss: 0.6099207225635573\n",
      "  ROC AUC: 0.439617228155025\n",
      "  Average Precision: 0.16052147473902964\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.7788\n",
      "  Log Loss: 0.6548745317837761\n",
      "  ROC AUC: 0.4378756571428571\n",
      "  Average Precision: 0.15499606669798352\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8020\n",
      "  Log Loss: 0.594339192742581\n",
      "  ROC AUC: 0.47234132880492463\n",
      "  Average Precision: 0.1591185394676653\n",
      "\n",
      "==================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "==================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      "   TRBJ_encoded 731207.409882\n",
      "    MHC_encoded 429298.861797\n",
      "   TRBV_encoded 277027.985093\n",
      "  epitope_emb_1  71929.644191\n",
      "epitope_emb_142  40876.582670\n",
      " epitope_emb_86  40852.136814\n",
      "epitope_emb_214  29373.979383\n",
      "epitope_emb_118  22190.067989\n",
      "epitope_emb_343  19842.020674\n",
      "epitope_emb_410  19095.418268\n",
      "epitope_emb_386  12807.758030\n",
      "epitope_emb_502  12787.124798\n",
      "epitope_emb_151  12478.754219\n",
      "epitope_emb_455  12344.816528\n",
      "epitope_emb_169  12232.555153\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 64821.49 (2.6%)\n",
      "Epitope embeddings: 1000478.34 (40.0%)\n",
      "Categorical features: 1437534.26 (57.4%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 271: 6096.06\n",
      "  Dimension 377: 2496.26\n",
      "  Dimension 15: 2359.30\n",
      "  Dimension 463: 2098.20\n",
      "  Dimension 79: 1733.72\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 1: 71929.64\n",
      "  Dimension 142: 40876.58\n",
      "  Dimension 86: 40852.14\n",
      "  Dimension 214: 29373.98\n",
      "  Dimension 118: 22190.07\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "Best validation AUC: 0.9326\n",
      "Test AUC: 0.6220\n",
      "Training iterations: 226\n",
      "Total features used: 1027\n",
      "Embedding contribution: 42.6%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your actual paths\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_pca.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_pca.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='zero'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean', 'drop')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "            else:  # 'drop' - will be handled later\n",
    "                embedding_features.append(np.zeros(embedding_dim))  # placeholder\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        if len(missing_sequences) <= 5:  # Show a few examples\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Encode categorical features that don't have embeddings\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "categorical_cols = ['TRBV', 'TRBJ', 'MHC']\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col + '_encoded'] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col + '_encoded'] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col + '_encoded'] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Combine all features\n",
    "print(\"\\nCombining features...\")\n",
    "encoded_categorical_cols = [col + '_encoded' for col in categorical_cols]\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb, \n",
    "    train_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb, \n",
    "    valid_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb, \n",
    "    test_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\") \n",
    "print(f\"  - Categorical features: {len(encoded_categorical_cols)} features\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "if train_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in training features\")\n",
    "    train_features = train_features.fillna(0)\n",
    "if valid_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in validation features\")\n",
    "    valid_features = valid_features.fillna(0)\n",
    "if test_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in test features\")\n",
    "    test_features = test_features.fillna(0)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "train_data = lgb.Dataset(\n",
    "    train_features, \n",
    "    label=train_df[target_col], \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "valid_data = lgb.Dataset(\n",
    "    valid_features, \n",
    "    label=valid_df[target_col], \n",
    "    reference=train_data, \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "\n",
    "# LightGBM parameters - optimized for high-dimensional embedding features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,  # Lower learning rate for stability with embeddings\n",
    "    'feature_fraction': 0.8,  # Feature subsampling to prevent overfitting\n",
    "    'bagging_fraction': 0.8,  # Data subsampling\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*30)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*30)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "categorical_importance = feature_imp_df[feature_imp_df['feature'].str.endswith('_encoded')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance + categorical_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Categorical features: {categorical_importance:.2f} ({categorical_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"Embedding contribution: {(tcr_emb_importance + epitope_emb_importance)/total_importance*100:.1f}%\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_model.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00001c9a",
   "metadata": {},
   "source": [
    "### Train and Test using only TCR and Epitope (v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64913d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3761/3519228093.py:21: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_pca.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_pca.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Combining TCR and Epitope features only...\n",
      "Final feature dimensions: 1024 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - No categorical features used\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model (TCR + Epitope embeddings only)...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttrain's binary_logloss: 0.360111\tval's binary_logloss: 0.437987\n",
      "\n",
      "Training completed. Best iteration: 34\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "VALIDATION METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Log Loss: 0.4380\n",
      "Accuracy: 0.8335\n",
      "AUC: 0.7033\n",
      "F1 Score: 0.0004\n",
      "AP Score: 0.2954\n",
      "\n",
      "========================================\n",
      "PER-TASK VALIDATION METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.8352\n",
      "  Log Loss: 0.4358586036797498\n",
      "  ROC AUC: 0.7429843885895561\n",
      "  Average Precision: 0.307973164782711\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8303\n",
      "  Log Loss: 0.431738280130594\n",
      "  ROC AUC: 0.6851443946641311\n",
      "  Average Precision: 0.36232260690724527\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8199\n",
      "  Log Loss: 0.46822575236684383\n",
      "  ROC AUC: 0.5894830530769981\n",
      "  Average Precision: 0.25004982162232264\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.8340\n",
      "  Log Loss: 0.4482227523117875\n",
      "  ROC AUC: 0.6048425290078144\n",
      "  Average Precision: 0.23076865968301769\n",
      "\n",
      "Average across tasks (excluding undefined):\n",
      "  Average AUC: 0.6556\n",
      "  Average Accuracy: 0.8298\n",
      "\n",
      "============================================================\n",
      "TEST METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Accuracy: 0.8387\n",
      "AUC: 0.6307\n",
      "F1 Score: 0.0000\n",
      "AP Score: 0.2384\n",
      "Log Loss: 0.4420\n",
      "\n",
      "========================================\n",
      "PER-TASK TEST METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8600\n",
      "  Log Loss: 0.3784291629121858\n",
      "  ROC AUC: 0.8753339586600117\n",
      "  Average Precision: 0.48101519159333805\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.8304\n",
      "  Log Loss: 0.4720346168677994\n",
      "  ROC AUC: 0.5634805994545611\n",
      "  Average Precision: 0.23353588755739366\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8140\n",
      "  Log Loss: 0.48548555121346254\n",
      "  ROC AUC: 0.5522566857142857\n",
      "  Average Precision: 0.22872170144855597\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8278\n",
      "  Log Loss: 0.47551165971524767\n",
      "  ROC AUC: 0.49646571853109744\n",
      "  Average Precision: 0.1828769770328649\n",
      "\n",
      "Average across test tasks (excluding undefined):\n",
      "  Average AUC: 0.6219\n",
      "  Average Accuracy: 0.8330\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\n",
      "============================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      "epitope_emb_142 116139.212463\n",
      "epitope_emb_470  46663.543091\n",
      "  epitope_emb_1  45438.152817\n",
      "epitope_emb_386  28946.510010\n",
      "epitope_emb_410  24386.897858\n",
      "epitope_emb_427  23844.919067\n",
      "epitope_emb_375  23332.578094\n",
      " epitope_emb_78  21147.636017\n",
      "epitope_emb_289  19394.166199\n",
      " epitope_emb_77  19153.595978\n",
      "epitope_emb_227  16663.892609\n",
      "epitope_emb_171  16152.119049\n",
      "epitope_emb_347  15974.770782\n",
      "epitope_emb_348  14204.744751\n",
      " epitope_emb_35  14051.099792\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 63422.52 (5.6%)\n",
      "Epitope embeddings: 1067518.70 (94.4%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 463: 9398.25\n",
      "  Dimension 15: 6931.62\n",
      "  Dimension 45: 5061.27\n",
      "  Dimension 271: 4440.50\n",
      "  Dimension 147: 3072.51\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 142: 116139.21\n",
      "  Dimension 470: 46663.54\n",
      "  Dimension 1: 45438.15\n",
      "  Dimension 386: 28946.51\n",
      "  Dimension 410: 24386.90\n",
      "\n",
      "Importance concentration:\n",
      "Top 5 TCR dims contribute: 45.6% of TCR importance\n",
      "Top 5 Epitope dims contribute: 24.5% of Epitope importance\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY (Embeddings Only)\n",
      "============================================================\n",
      "Best validation AUC: 0.7033\n",
      "Test AUC: 0.6307\n",
      "Training iterations: 34\n",
      "Total features used: 1024\n",
      "TCR embedding contribution: 5.6%\n",
      "Epitope embedding contribution: 94.4%\n",
      "\n",
      "Model uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\n",
      "This shows the predictive power of TCR-Epitope interaction alone\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your 512-dimensional embeddings\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_pca.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_pca.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='mean'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        # Show a few examples of missing sequences\n",
    "        if len(missing_sequences) <= 5:\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "        elif len(missing_sequences) > 5:\n",
    "            print(f\"  First few missing: {[seq for _, seq in missing_sequences[:3]]}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Combine ONLY TCR and Epitope features (no categorical features)\n",
    "print(\"\\nCombining TCR and Epitope features only...\")\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\")\n",
    "print(f\"  - No categorical features used\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "for name, features in [(\"train\", train_features), (\"valid\", valid_features), (\"test\", test_features)]:\n",
    "    nan_count = features.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: Found {nan_count} NaN values in {name} features - filling with 0\")\n",
    "        features.fillna(0, inplace=True)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "# Note: No categorical features since we're only using embeddings\n",
    "train_data = lgb.Dataset(train_features, label=train_df[target_col])\n",
    "valid_data = lgb.Dataset(valid_features, label=valid_df[target_col], reference=train_data)\n",
    "\n",
    "# LightGBM parameters - optimized for embedding-only features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,  # Feature subsampling\n",
    "    'bagging_fraction': 0.8,  # Data subsampling  \n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model (TCR + Epitope embeddings only)...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    task_results = []\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            \n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_tasks = [r for r in task_results if isinstance(r['auc'], float)]\n",
    "    if valid_tasks:\n",
    "        avg_auc = np.mean([r['auc'] for r in valid_tasks])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in valid_tasks])\n",
    "        print(f\"\\nAverage across tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    test_task_results = []\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            \n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_test_tasks = [r for r in test_task_results if isinstance(r['auc'], float)]\n",
    "    if valid_test_tasks:\n",
    "        test_avg_auc = np.mean([r['auc'] for r in valid_test_tasks])\n",
    "        test_avg_acc = np.mean([r['accuracy'] for r in valid_test_tasks])\n",
    "        print(f\"\\nAverage across test tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {test_avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {test_avg_acc:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# Check if embeddings are well-distributed in importance\n",
    "tcr_top_5_importance = tcr_features['importance'].sum()\n",
    "epitope_top_5_importance = epitope_features['importance'].sum()\n",
    "\n",
    "print(f\"\\nImportance concentration:\")\n",
    "print(f\"Top 5 TCR dims contribute: {tcr_top_5_importance/tcr_emb_importance*100:.1f}% of TCR importance\")\n",
    "print(f\"Top 5 Epitope dims contribute: {epitope_top_5_importance/epitope_emb_importance*100:.1f}% of Epitope importance\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"TCR embedding contribution: {tcr_emb_importance/total_importance*100:.1f}%\")\n",
    "print(f\"Epitope embedding contribution: {epitope_emb_importance/total_importance*100:.1f}%\")\n",
    "\n",
    "# Performance comparison hint\n",
    "print(f\"\\nModel uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\")\n",
    "print(f\"This shows the predictive power of TCR-Epitope interaction alone\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_only_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_only_model.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b14d2",
   "metadata": {},
   "source": [
    "I just noticed the embeddings reduction was made differently for tcr than epitope.\n",
    "That's not necessarely a mistake, but for the record, we generate both reductions the same way and train and test the models LightGBM-v1 and LightGBM-v2 again. We keep the results above for comparisson. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b001ca",
   "metadata": {},
   "source": [
    "## embeddings reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dccc18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TCR EMBEDDING REDUCTION TO 512D ===\n",
      "\n",
      "- Selective: Good compromise, combines mean+max pooling\n",
      "Processing embeddings from ../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\n",
      "Loading embeddings from ../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz...\n",
      "Loaded 211294 sequences\n",
      "\n",
      "Applying selective reduction to all 211294 sequences...\n",
      "Applying selective pooling...\n",
      "Reduction completed in 11.60 seconds\n",
      "Saved reduced embeddings to ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "\n",
      "=== REDUCTION SUMMARY ===\n",
      "Original dimension: 1024 (after mean pooling)\n",
      "Reduced dimension: 512\n",
      "Compression ratio: 2.0x\n",
      "Memory reduction: 50.0%\n",
      "Total sequences processed: 211294\n",
      "\n",
      "Sample statistics for CASSWRDGATGELFF:\n",
      "  Mean: 0.1225\n",
      "  Std: 0.1699\n",
      "  Min: -0.2247\n",
      "  Max: 0.6312\n",
      "\n",
      "============================================================\n",
      "Processing Epitope embeddings...\n",
      "Processing embeddings from ../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\n",
      "Loading embeddings from ../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz...\n",
      "Loaded 1896 sequences\n",
      "\n",
      "Applying selective reduction to all 1896 sequences...\n",
      "Applying selective pooling...\n",
      "Reduction completed in 0.04 seconds\n",
      "Saved reduced embeddings to ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "=== REDUCTION SUMMARY ===\n",
      "Original dimension: 1024 (after mean pooling)\n",
      "Reduced dimension: 512\n",
      "Compression ratio: 2.0x\n",
      "Memory reduction: 50.0%\n",
      "Total sequences processed: 1896\n",
      "\n",
      "Sample statistics for LLAWHFVAV:\n",
      "  Mean: 0.1047\n",
      "  Std: 0.1650\n",
      "  Min: -0.3639\n",
      "  Max: 0.5814\n",
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "You can now use these 512-dimensional embeddings in your LightGBM model!\n",
      "Expected speedup: ~2x faster training\n",
      "Expected memory usage: ~50% less\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import time\n",
    "\n",
    "def load_npz_embeddings(npz_path):\n",
    "    \"\"\"Load embeddings from .npz file\"\"\"\n",
    "    print(f\"Loading embeddings from {npz_path}...\")\n",
    "    data = np.load(npz_path)\n",
    "    embeddings = {}\n",
    "    for key in data.files:\n",
    "        embeddings[key] = data[key]\n",
    "    print(f\"Loaded {len(embeddings)} sequences\")\n",
    "    return embeddings\n",
    "\n",
    "def reduce_embeddings_selective_pooling_512(embeddings_dict):\n",
    "    \"\"\"\n",
    "    Method 4: Selective pooling - combine different pooling methods and reduce\n",
    "    More sophisticated approach that might preserve more information\n",
    "    \"\"\"\n",
    "    print(\"Applying selective pooling...\")\n",
    "    \n",
    "    combined_embeddings = []\n",
    "    seq_ids = []\n",
    "    \n",
    "    for seq_id, embedding in embeddings_dict.items():\n",
    "        # Get different pooling representations\n",
    "        mean_emb = np.mean(embedding, axis=0)      # (1024,)\n",
    "        max_emb = np.max(embedding, axis=0)        # (1024,)\n",
    "        \n",
    "        # Take first 256 dims from each pooling method\n",
    "        combined = np.concatenate([mean_emb[:256], max_emb[:256]])  # (512,)\n",
    "        \n",
    "        combined_embeddings.append(combined)\n",
    "        seq_ids.append(seq_id)\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    result_embeddings = {}\n",
    "    for i, seq_id in enumerate(seq_ids):\n",
    "        result_embeddings[seq_id] = combined_embeddings[i]\n",
    "    \n",
    "    return result_embeddings\n",
    "\n",
    "\n",
    "def process_embeddings_to_512(npz_path, output_path, method='selective', benchmark=False):\n",
    "    \"\"\"\n",
    "    Complete pipeline to reduce embeddings to 512 dimensions\n",
    "    \n",
    "    Args:\n",
    "        npz_path: Path to input .npz file\n",
    "        output_path: Path to save reduced embeddings\n",
    "        method: Reduction method ('pca', 'random', 'truncate', 'selective')\n",
    "        benchmark: Whether to run benchmarking first\n",
    "    \"\"\"\n",
    "    print(f\"Processing embeddings from {npz_path}\")\n",
    "    embeddings = load_npz_embeddings(npz_path)\n",
    "    \n",
    "    if benchmark:\n",
    "        benchmark_results = benchmark_methods(embeddings, sample_size=min(100, len(embeddings)))\n",
    "    \n",
    "    print(f\"\\nApplying {method} reduction to all {len(embeddings)} sequences...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reduced, model = reduce_embeddings_mean_512(embeddings)\n",
    "        # Save the PCA model\n",
    "        model_path = output_path.replace('.pkl', '_pca_model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Saved PCA model to {model_path}\")\n",
    "        \n",
    "    elif method == 'random':\n",
    "        reduced, model = reduce_embeddings_random_projection_512(embeddings)\n",
    "        # Save the random projection model\n",
    "        model_path = output_path.replace('.pkl', '_rp_model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Saved Random Projection model to {model_path}\")\n",
    "        \n",
    "    elif method == 'truncate':\n",
    "        reduced = reduce_embeddings_truncate_512(embeddings)\n",
    "        \n",
    "    elif method == 'selective':\n",
    "        reduced = reduce_embeddings_selective_pooling_512(embeddings)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Reduction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save reduced embeddings\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(reduced, f)\n",
    "    print(f\"Saved reduced embeddings to {output_path}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    sample_key = list(reduced.keys())[0]\n",
    "    sample_embedding = reduced[sample_key]\n",
    "    \n",
    "    print(f\"\\n=== REDUCTION SUMMARY ===\")\n",
    "    print(f\"Original dimension: 1024 (after mean pooling)\")\n",
    "    print(f\"Reduced dimension: {len(sample_embedding)}\")\n",
    "    print(f\"Compression ratio: {1024/len(sample_embedding):.1f}x\")\n",
    "    print(f\"Memory reduction: {(1024-len(sample_embedding))/1024*100:.1f}%\")\n",
    "    print(f\"Total sequences processed: {len(reduced)}\")\n",
    "    \n",
    "    print(f\"\\nSample statistics for {sample_key}:\")\n",
    "    print(f\"  Mean: {np.mean(sample_embedding):.4f}\")\n",
    "    print(f\"  Std: {np.std(sample_embedding):.4f}\")\n",
    "    print(f\"  Min: {np.min(sample_embedding):.4f}\")\n",
    "    print(f\"  Max: {np.max(sample_embedding):.4f}\")\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "# Example usage and recommendations\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=== TCR EMBEDDING REDUCTION TO 512D ===\\n\")\n",
    "    \n",
    "    print(\"- Selective: Good compromise, combines mean+max pooling\")\n",
    "    \n",
    "    # Process TCR embeddings\n",
    "    tcr_reduced = process_embeddings_to_512(\n",
    "        npz_path=\"../../../../../data/embeddings/beta/allele/TRB_beta_embeddings.npz\",\n",
    "        output_path=\"../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\",\n",
    "        method='selective',  # Change to 'random', 'truncate', or 'selective' as needed\n",
    "        benchmark=False  # Set to False to skip benchmarking\n",
    "    )\n",
    "    \n",
    "    # Process Epitope embeddings (keep at 512 if you want consistency)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Processing Epitope embeddings...\")\n",
    "    \n",
    "    epitope_reduced = process_embeddings_to_512(\n",
    "        npz_path=\"../../../../../data/embeddings/beta/allele/Epitope_beta_embeddings.npz\",\n",
    "        output_path=\"../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\",\n",
    "        method='selective',\n",
    "        benchmark=False  # Skip benchmarking for epitopes\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(\"You can now use these 512-dimensional embeddings in your LightGBM model!\")\n",
    "    print(\"Expected speedup: ~2x faster training\")\n",
    "    print(\"Expected memory usage: ~50% less\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d498b",
   "metadata": {},
   "source": [
    "## v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f330bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3761/2151094315.py:21: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Combining TCR and Epitope features only...\n",
      "Final feature dimensions: 1024 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - No categorical features used\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model (TCR + Epitope embeddings only)...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttrain's binary_logloss: 0.384779\tval's binary_logloss: 0.430641\n",
      "\n",
      "Training completed. Best iteration: 15\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "VALIDATION METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Log Loss: 0.4306\n",
      "Accuracy: 0.8335\n",
      "AUC: 0.6989\n",
      "F1 Score: 0.0000\n",
      "AP Score: 0.3105\n",
      "\n",
      "========================================\n",
      "PER-TASK VALIDATION METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.8351\n",
      "  Log Loss: 0.4267883704271435\n",
      "  ROC AUC: 0.7383469162657069\n",
      "  Average Precision: 0.3358150634327923\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8303\n",
      "  Log Loss: 0.4307512689304343\n",
      "  ROC AUC: 0.6736629656042027\n",
      "  Average Precision: 0.35225699171804087\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8199\n",
      "  Log Loss: 0.47087182761983537\n",
      "  ROC AUC: 0.5648303347176611\n",
      "  Average Precision: 0.24063467811176348\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.8340\n",
      "  Log Loss: 0.4461543560720591\n",
      "  ROC AUC: 0.6042505327965901\n",
      "  Average Precision: 0.33039444055765543\n",
      "\n",
      "Average across tasks (excluding undefined):\n",
      "  Average AUC: 0.6453\n",
      "  Average Accuracy: 0.8298\n",
      "\n",
      "============================================================\n",
      "TEST METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Accuracy: 0.8387\n",
      "AUC: 0.6223\n",
      "F1 Score: 0.0000\n",
      "AP Score: 0.2368\n",
      "Log Loss: 0.4337\n",
      "\n",
      "========================================\n",
      "PER-TASK TEST METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8600\n",
      "  Log Loss: 0.37518494555668586\n",
      "  ROC AUC: 0.843266338260485\n",
      "  Average Precision: 0.4921743898282786\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.8305\n",
      "  Log Loss: 0.4595132721149802\n",
      "  ROC AUC: 0.553462733006262\n",
      "  Average Precision: 0.2314047696055549\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8140\n",
      "  Log Loss: 0.48179733430047306\n",
      "  ROC AUC: 0.5446888\n",
      "  Average Precision: 0.22048952140061928\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8278\n",
      "  Log Loss: 0.47523210880231403\n",
      "  ROC AUC: 0.4701071959244321\n",
      "  Average Precision: 0.16767195900411414\n",
      "\n",
      "Average across test tasks (excluding undefined):\n",
      "  Average AUC: 0.6029\n",
      "  Average Accuracy: 0.8331\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\n",
      "============================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      " epitope_emb_79 109384.979980\n",
      " epitope_emb_67  81719.550781\n",
      "epitope_emb_207  49033.491180\n",
      " epitope_emb_49  29045.609619\n",
      "epitope_emb_253  23694.199829\n",
      "  epitope_emb_6  22059.909790\n",
      "epitope_emb_396  19302.384277\n",
      "epitope_emb_123  18515.747986\n",
      "epitope_emb_440  16376.180176\n",
      "epitope_emb_410  15557.280273\n",
      "epitope_emb_238  13771.799805\n",
      "epitope_emb_350  11999.539917\n",
      "epitope_emb_114  10707.299805\n",
      "epitope_emb_480  10423.500000\n",
      "epitope_emb_132   9115.889648\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 37277.06 (4.5%)\n",
      "Epitope embeddings: 786743.91 (95.5%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 463: 6200.02\n",
      "  Dimension 7: 5395.00\n",
      "  Dimension 15: 4763.95\n",
      "  Dimension 271: 2873.03\n",
      "  Dimension 45: 2152.19\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 79: 109384.98\n",
      "  Dimension 67: 81719.55\n",
      "  Dimension 207: 49033.49\n",
      "  Dimension 49: 29045.61\n",
      "  Dimension 253: 23694.20\n",
      "\n",
      "Importance concentration:\n",
      "Top 5 TCR dims contribute: 57.4% of TCR importance\n",
      "Top 5 Epitope dims contribute: 37.2% of Epitope importance\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY (Embeddings Only)\n",
      "============================================================\n",
      "Best validation AUC: 0.6989\n",
      "Test AUC: 0.6223\n",
      "Training iterations: 15\n",
      "Total features used: 1024\n",
      "TCR embedding contribution: 4.5%\n",
      "Epitope embedding contribution: 95.5%\n",
      "\n",
      "Model uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\n",
      "This shows the predictive power of TCR-Epitope interaction alone\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your 512-dimensional embeddings\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='mean'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        # Show a few examples of missing sequences\n",
    "        if len(missing_sequences) <= 5:\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "        elif len(missing_sequences) > 5:\n",
    "            print(f\"  First few missing: {[seq for _, seq in missing_sequences[:3]]}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Combine ONLY TCR and Epitope features (no categorical features)\n",
    "print(\"\\nCombining TCR and Epitope features only...\")\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\")\n",
    "print(f\"  - No categorical features used\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "for name, features in [(\"train\", train_features), (\"valid\", valid_features), (\"test\", test_features)]:\n",
    "    nan_count = features.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: Found {nan_count} NaN values in {name} features - filling with 0\")\n",
    "        features.fillna(0, inplace=True)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "# Note: No categorical features since we're only using embeddings\n",
    "train_data = lgb.Dataset(train_features, label=train_df[target_col])\n",
    "valid_data = lgb.Dataset(valid_features, label=valid_df[target_col], reference=train_data)\n",
    "\n",
    "# LightGBM parameters - optimized for embedding-only features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,  # Feature subsampling\n",
    "    'bagging_fraction': 0.8,  # Data subsampling  \n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model (TCR + Epitope embeddings only)...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    task_results = []\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            \n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_tasks = [r for r in task_results if isinstance(r['auc'], float)]\n",
    "    if valid_tasks:\n",
    "        avg_auc = np.mean([r['auc'] for r in valid_tasks])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in valid_tasks])\n",
    "        print(f\"\\nAverage across tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    test_task_results = []\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            \n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_test_tasks = [r for r in test_task_results if isinstance(r['auc'], float)]\n",
    "    if valid_test_tasks:\n",
    "        test_avg_auc = np.mean([r['auc'] for r in valid_test_tasks])\n",
    "        test_avg_acc = np.mean([r['accuracy'] for r in valid_test_tasks])\n",
    "        print(f\"\\nAverage across test tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {test_avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {test_avg_acc:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# Check if embeddings are well-distributed in importance\n",
    "tcr_top_5_importance = tcr_features['importance'].sum()\n",
    "epitope_top_5_importance = epitope_features['importance'].sum()\n",
    "\n",
    "print(f\"\\nImportance concentration:\")\n",
    "print(f\"Top 5 TCR dims contribute: {tcr_top_5_importance/tcr_emb_importance*100:.1f}% of TCR importance\")\n",
    "print(f\"Top 5 Epitope dims contribute: {epitope_top_5_importance/epitope_emb_importance*100:.1f}% of Epitope importance\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"TCR embedding contribution: {tcr_emb_importance/total_importance*100:.1f}%\")\n",
    "print(f\"Epitope embedding contribution: {epitope_emb_importance/total_importance*100:.1f}%\")\n",
    "\n",
    "# Performance comparison hint\n",
    "print(f\"\\nModel uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\")\n",
    "print(f\"This shows the predictive power of TCR-Epitope interaction alone\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_only_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_only_model.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d4b75",
   "metadata": {},
   "source": [
    "## v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ab0cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3761/1457597670.py:22: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Encoding categorical features...\n",
      "Encoded TRBV: 166 unique values\n",
      "Encoded TRBJ: 31 unique values\n",
      "Encoded MHC: 99 unique values\n",
      "\n",
      "Combining features...\n",
      "Final feature dimensions: 1027 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - Categorical features: 3 features\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.262866\tval's binary_logloss: 0.226142\n",
      "[200]\ttrain's binary_logloss: 0.253374\tval's binary_logloss: 0.221357\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttrain's binary_logloss: 0.251108\tval's binary_logloss: 0.220423\n",
      "\n",
      "Training completed. Best iteration: 224\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "==================================================\n",
      "VALIDATION METRICS\n",
      "==================================================\n",
      "Log Loss: 0.2204\n",
      "Accuracy: 0.9169\n",
      "AUC: 0.9370\n",
      "F1 Score: 0.7189\n",
      "AP Score: 0.7938\n",
      "\n",
      "==============================\n",
      "PER-TASK VALIDATION METRICS\n",
      "==============================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.9341\n",
      "  Log Loss: 0.17958200660944423\n",
      "  ROC AUC: 0.9604263538400092\n",
      "  Average Precision: 0.8488030676229732\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8533\n",
      "  Log Loss: 0.3405710179551045\n",
      "  ROC AUC: 0.8675831567130236\n",
      "  Average Precision: 0.5688933135918065\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8190\n",
      "  Log Loss: 0.49324161557097945\n",
      "  ROC AUC: 0.5741629338787526\n",
      "  Average Precision: 0.2702015167035089\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.7854\n",
      "  Log Loss: 0.48485099171965446\n",
      "  ROC AUC: 0.5911674165285342\n",
      "  Average Precision: 0.1980967457656985\n",
      "\n",
      "==================================================\n",
      "TEST METRICS\n",
      "==================================================\n",
      "Accuracy: 0.8215\n",
      "AUC: 0.6251\n",
      "F1 Score: 0.0777\n",
      "AP Score: 0.2117\n",
      "Log Loss: 0.5219\n",
      "\n",
      "==============================\n",
      "PER-TASK TEST METRICS\n",
      "==============================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8684\n",
      "  Log Loss: 0.358520523090956\n",
      "  ROC AUC: 0.9599942454414524\n",
      "  Average Precision: 0.7112547366650877\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.7971\n",
      "  Log Loss: 0.6056602863617626\n",
      "  ROC AUC: 0.4448910883084427\n",
      "  Average Precision: 0.16459580651482408\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8015\n",
      "  Log Loss: 0.6035744637227987\n",
      "  ROC AUC: 0.43615657142857145\n",
      "  Average Precision: 0.15664155148881542\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8007\n",
      "  Log Loss: 0.5582253997839184\n",
      "  ROC AUC: 0.4772659732540862\n",
      "  Average Precision: 0.16436714996049456\n",
      "\n",
      "==================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "==================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      "   TRBJ_encoded 715965.824492\n",
      "    MHC_encoded 408459.879719\n",
      "   TRBV_encoded 273736.857082\n",
      "epitope_emb_392  80437.656295\n",
      " epitope_emb_79  59646.241247\n",
      " epitope_emb_67  39163.985046\n",
      "epitope_emb_207  30104.105347\n",
      "epitope_emb_228  25835.628696\n",
      " epitope_emb_49  24508.152559\n",
      "epitope_emb_360  21227.130096\n",
      "epitope_emb_233  20496.944202\n",
      "epitope_emb_459  19569.521294\n",
      " epitope_emb_42  15208.068138\n",
      "epitope_emb_384  14727.558907\n",
      "epitope_emb_439  12774.100963\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 70253.62 (2.8%)\n",
      "Epitope embeddings: 1024566.90 (41.1%)\n",
      "Categorical features: 1398162.56 (56.1%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 271: 5950.44\n",
      "  Dimension 293: 2585.06\n",
      "  Dimension 79: 2120.85\n",
      "  Dimension 15: 1904.23\n",
      "  Dimension 482: 1886.65\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 392: 80437.66\n",
      "  Dimension 79: 59646.24\n",
      "  Dimension 67: 39163.99\n",
      "  Dimension 207: 30104.11\n",
      "  Dimension 228: 25835.63\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "Best validation AUC: 0.9370\n",
      "Test AUC: 0.6251\n",
      "Training iterations: 224\n",
      "Total features used: 1027\n",
      "Embedding contribution: 43.9%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your actual paths\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='zero'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean', 'drop')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "            else:  # 'drop' - will be handled later\n",
    "                embedding_features.append(np.zeros(embedding_dim))  # placeholder\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        if len(missing_sequences) <= 5:  # Show a few examples\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Encode categorical features that don't have embeddings\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "categorical_cols = ['TRBV', 'TRBJ', 'MHC']\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col + '_encoded'] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col + '_encoded'] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col + '_encoded'] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Combine all features\n",
    "print(\"\\nCombining features...\")\n",
    "encoded_categorical_cols = [col + '_encoded' for col in categorical_cols]\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb, \n",
    "    train_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb, \n",
    "    valid_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb, \n",
    "    test_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\") \n",
    "print(f\"  - Categorical features: {len(encoded_categorical_cols)} features\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "if train_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in training features\")\n",
    "    train_features = train_features.fillna(0)\n",
    "if valid_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in validation features\")\n",
    "    valid_features = valid_features.fillna(0)\n",
    "if test_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in test features\")\n",
    "    test_features = test_features.fillna(0)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "train_data = lgb.Dataset(\n",
    "    train_features, \n",
    "    label=train_df[target_col], \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "valid_data = lgb.Dataset(\n",
    "    valid_features, \n",
    "    label=valid_df[target_col], \n",
    "    reference=train_data, \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "\n",
    "# LightGBM parameters - optimized for high-dimensional embedding features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,  # Lower learning rate for stability with embeddings\n",
    "    'feature_fraction': 0.8,  # Feature subsampling to prevent overfitting\n",
    "    'bagging_fraction': 0.8,  # Data subsampling\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*30)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*30)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "        except ValueError:\n",
    "            loss = auc = ap = \"Undefined (only one class present)\"\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss}\")\n",
    "        print(f\"  ROC AUC: {auc}\")\n",
    "        print(f\"  Average Precision: {ap}\")\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "categorical_importance = feature_imp_df[feature_imp_df['feature'].str.endswith('_encoded')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance + categorical_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Categorical features: {categorical_importance:.2f} ({categorical_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"Embedding contribution: {(tcr_emb_importance + epitope_emb_importance)/total_importance*100:.1f}%\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_model.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e2e1a3",
   "metadata": {},
   "source": [
    "### Again v1 - now with macro-f1 and rounded metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2d2a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15062/4235081259.py:21: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Combining TCR and Epitope features only...\n",
      "Final feature dimensions: 1024 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - No categorical features used\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model (TCR + Epitope embeddings only)...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttrain's binary_logloss: 0.384779\tval's binary_logloss: 0.430641\n",
      "\n",
      "Training completed. Best iteration: 15\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "VALIDATION METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Log Loss: 0.4306\n",
      "Accuracy: 0.8335\n",
      "AUC: 0.6989\n",
      "F1 Score: 0.0000\n",
      "Macro-F1 Score: 0.4546\n",
      "AP Score: 0.3105\n",
      "\n",
      "========================================\n",
      "PER-TASK VALIDATION METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.8351\n",
      "  Log Loss: 0.4268\n",
      "  ROC AUC: 0.7383\n",
      "  Average Precision: 0.3358\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4551\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8303\n",
      "  Log Loss: 0.4308\n",
      "  ROC AUC: 0.6737\n",
      "  Average Precision: 0.3523\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4536\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8199\n",
      "  Log Loss: 0.4709\n",
      "  ROC AUC: 0.5648\n",
      "  Average Precision: 0.2406\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4505\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.8340\n",
      "  Log Loss: 0.4462\n",
      "  ROC AUC: 0.6043\n",
      "  Average Precision: 0.3304\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4547\n",
      "\n",
      "Average across tasks (excluding undefined):\n",
      "  Average AUC: 0.6453\n",
      "  Average Accuracy: 0.8298\n",
      "  Average F1: 0.0000\n",
      "  Average Macro-F1: 0.4535\n",
      "\n",
      "============================================================\n",
      "TEST METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Accuracy: 0.8387\n",
      "AUC: 0.6223\n",
      "F1 Score: 0.0000\n",
      "Macro-F1 Score: 0.4561\n",
      "AP Score: 0.2368\n",
      "Log Loss: 0.4337\n",
      "\n",
      "========================================\n",
      "PER-TASK TEST METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8600\n",
      "  Log Loss: 0.3752\n",
      "  ROC AUC: 0.8433\n",
      "  Average Precision: 0.4922\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4624\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.8305\n",
      "  Log Loss: 0.4595\n",
      "  ROC AUC: 0.5535\n",
      "  Average Precision: 0.2314\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4537\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8140\n",
      "  Log Loss: 0.4818\n",
      "  ROC AUC: 0.5447\n",
      "  Average Precision: 0.2205\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4487\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8278\n",
      "  Log Loss: 0.4752\n",
      "  ROC AUC: 0.4701\n",
      "  Average Precision: 0.1677\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4529\n",
      "\n",
      "Average across test tasks (excluding undefined):\n",
      "  Average AUC: 0.6029\n",
      "  Average Accuracy: 0.8331\n",
      "  Average F1: 0.0000\n",
      "  Average Macro-F1: 0.4544\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\n",
      "============================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      " epitope_emb_79 109384.979980\n",
      " epitope_emb_67  81719.550781\n",
      "epitope_emb_207  49033.491180\n",
      " epitope_emb_49  29045.609619\n",
      "epitope_emb_253  23694.199829\n",
      "  epitope_emb_6  22059.909790\n",
      "epitope_emb_396  19302.384277\n",
      "epitope_emb_123  18515.747986\n",
      "epitope_emb_440  16376.180176\n",
      "epitope_emb_410  15557.280273\n",
      "epitope_emb_238  13771.799805\n",
      "epitope_emb_350  11999.539917\n",
      "epitope_emb_114  10707.299805\n",
      "epitope_emb_480  10423.500000\n",
      "epitope_emb_132   9115.889648\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 37277.06 (4.5%)\n",
      "Epitope embeddings: 786743.91 (95.5%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 463: 6200.02\n",
      "  Dimension 7: 5395.00\n",
      "  Dimension 15: 4763.95\n",
      "  Dimension 271: 2873.03\n",
      "  Dimension 45: 2152.19\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 79: 109384.98\n",
      "  Dimension 67: 81719.55\n",
      "  Dimension 207: 49033.49\n",
      "  Dimension 49: 29045.61\n",
      "  Dimension 253: 23694.20\n",
      "\n",
      "Importance concentration:\n",
      "Top 5 TCR dims contribute: 57.4% of TCR importance\n",
      "Top 5 Epitope dims contribute: 37.2% of Epitope importance\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY (Embeddings Only)\n",
      "============================================================\n",
      "Best validation AUC: 0.6989\n",
      "Test AUC: 0.6223\n",
      "Training iterations: 15\n",
      "Total features used: 1024\n",
      "TCR embedding contribution: 4.5%\n",
      "Epitope embedding contribution: 95.5%\n",
      "\n",
      "Model uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\n",
      "This shows the predictive power of TCR-Epitope interaction alone\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC ANALYSIS FOR F1 = 0\n",
      "============================================================\n",
      "1. CLASS DISTRIBUTION:\n",
      "Training set:\n",
      "Binding\n",
      "0    0.832902\n",
      "1    0.167098\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Binding\n",
      "0    0.833461\n",
      "1    0.166539\n",
      "Name: proportion, dtype: float64\n",
      "Test set:\n",
      "Binding\n",
      "0    0.838691\n",
      "1    0.161309\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "2. PREDICTION PROBABILITY DISTRIBUTION:\n",
      "Validation predictions - Min: 0.0821, Max: 0.3907, Mean: 0.1395\n",
      "Test predictions - Min: 0.0821, Max: 0.3825, Mean: 0.1540\n",
      "\n",
      "3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\n",
      "Threshold 0.1: Validation=86606/169029 (51.2%), Test=32461/54126 (60.0%)\n",
      "Threshold 0.2: Validation=35914/169029 (21.2%), Test=16803/54126 (31.0%)\n",
      "Threshold 0.3: Validation=3193/169029 (1.9%), Test=2038/54126 (3.8%)\n",
      "Threshold 0.4: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.5: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.6: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.7: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.8: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.9: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "\n",
      "4. CONFUSION MATRIX (threshold=0.5):\n",
      "Validation:\n",
      "TN: 140879, FP: 0\n",
      "FN: 28150, TP: 0\n",
      "Test:\n",
      "TN: 45395, FP: 0\n",
      "FN: 8731, TP: 0\n",
      "\n",
      "5. OPTIMAL THRESHOLD ANALYSIS:\n",
      "Optimal threshold for F1: 0.1023\n",
      "Max F1 score achievable: 0.4395\n",
      "\n",
      "6. PERFORMANCE WITH OPTIMAL THRESHOLD (0.1023):\n",
      "Validation:\n",
      "F1: 0.4250, Macro-F1: 0.5762\n",
      "Test:\n",
      "F1: 0.3305, Macro-F1: 0.4655\n",
      "\n",
      "7. POSITIVE CLASS PROBABILITY ANALYSIS:\n",
      "Positive class (binding) predictions:\n",
      "  Count: 28150\n",
      "  Mean probability: 0.1689\n",
      "  Max probability: 0.3907\n",
      "  % above 0.5: 0.0%\n",
      "Negative class (no binding) predictions:\n",
      "  Count: 140879\n",
      "  Mean probability: 0.1336\n",
      "  Max probability: 0.3655\n",
      "  % above 0.5: 0.0%\n",
      "\n",
      "8. RECOMMENDATIONS:\n",
      "- Your model has discriminative power (AUC > 0.6) but conservative threshold\n",
      "- Consider using threshold 0.102 instead of 0.5\n",
      "- The high accuracy with F1=0 indicates severe class imbalance\n",
      "- Macro-F1 > 0 shows the model isn't completely broken\n",
      "- Consider techniques for imbalanced datasets (SMOTE, class weights, etc.)\n",
      "\n",
      "9. CLASS WEIGHT SUGGESTION:\n",
      "Negative samples: 629472\n",
      "Positive samples: 126286\n",
      "Imbalance ratio: 4.98:1\n",
      "Consider adding 'scale_pos_weight': 4.98 to LightGBM params\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your 512-dimensional embeddings\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='mean'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        # Show a few examples of missing sequences\n",
    "        if len(missing_sequences) <= 5:\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "        elif len(missing_sequences) > 5:\n",
    "            print(f\"  First few missing: {[seq for _, seq in missing_sequences[:3]]}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Combine ONLY TCR and Epitope features (no categorical features)\n",
    "print(\"\\nCombining TCR and Epitope features only...\")\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\")\n",
    "print(f\"  - No categorical features used\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "for name, features in [(\"train\", train_features), (\"valid\", valid_features), (\"test\", test_features)]:\n",
    "    nan_count = features.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: Found {nan_count} NaN values in {name} features - filling with 0\")\n",
    "        features.fillna(0, inplace=True)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "# Note: No categorical features since we're only using embeddings\n",
    "train_data = lgb.Dataset(train_features, label=train_df[target_col])\n",
    "valid_data = lgb.Dataset(valid_features, label=valid_df[target_col], reference=train_data)\n",
    "\n",
    "# LightGBM parameters - optimized for embedding-only features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,  # Feature subsampling\n",
    "    'bagging_fraction': 0.8,  # Data subsampling  \n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model (TCR + Epitope embeddings only)...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_val_true, y_val_pred, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    task_results = []\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_tasks = [r for r in task_results if isinstance(r['auc'], float)]\n",
    "    if valid_tasks:\n",
    "        avg_auc = np.mean([r['auc'] for r in valid_tasks])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in valid_tasks])\n",
    "        avg_f1 = np.mean([r['f1'] for r in valid_tasks])\n",
    "        avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_tasks])\n",
    "        print(f\"\\nAverage across tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"  Average F1: {avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_true, y_pred_binary, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    test_task_results = []\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_test_tasks = [r for r in test_task_results if isinstance(r['auc'], float)]\n",
    "    if valid_test_tasks:\n",
    "        test_avg_auc = np.mean([r['auc'] for r in valid_test_tasks])\n",
    "        test_avg_acc = np.mean([r['accuracy'] for r in valid_test_tasks])\n",
    "        test_avg_f1 = np.mean([r['f1'] for r in valid_test_tasks])\n",
    "        test_avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_test_tasks])\n",
    "        print(f\"\\nAverage across test tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {test_avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {test_avg_acc:.4f}\")\n",
    "        print(f\"  Average F1: {test_avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {test_avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# Check if embeddings are well-distributed in importance\n",
    "tcr_top_5_importance = tcr_features['importance'].sum()\n",
    "epitope_top_5_importance = epitope_features['importance'].sum()\n",
    "\n",
    "print(f\"\\nImportance concentration:\")\n",
    "print(f\"Top 5 TCR dims contribute: {tcr_top_5_importance/tcr_emb_importance*100:.1f}% of TCR importance\")\n",
    "print(f\"Top 5 Epitope dims contribute: {epitope_top_5_importance/epitope_emb_importance*100:.1f}% of Epitope importance\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"TCR embedding contribution: {tcr_emb_importance/total_importance*100:.1f}%\")\n",
    "print(f\"Epitope embedding contribution: {epitope_emb_importance/total_importance*100:.1f}%\")\n",
    "\n",
    "# Performance comparison hint\n",
    "print(f\"\\nModel uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\")\n",
    "print(f\"This shows the predictive power of TCR-Epitope interaction alone\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_only_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_only_model.txt'\")\n",
    "\n",
    "# Add this diagnostic code after making predictions to understand the F1=0 issue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC ANALYSIS FOR F1 = 0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check class distribution\n",
    "print(\"1. CLASS DISTRIBUTION:\")\n",
    "print(f\"Training set:\")\n",
    "train_class_dist = train_df[target_col].value_counts(normalize=True)\n",
    "print(train_class_dist)\n",
    "print(f\"Validation set:\")\n",
    "val_class_dist = valid_df[target_col].value_counts(normalize=True)\n",
    "print(val_class_dist)\n",
    "print(f\"Test set:\")\n",
    "test_class_dist = test_df[target_col].value_counts(normalize=True)\n",
    "print(test_class_dist)\n",
    "\n",
    "# 2. Check prediction probabilities distribution\n",
    "print(f\"\\n2. PREDICTION PROBABILITY DISTRIBUTION:\")\n",
    "print(f\"Validation predictions - Min: {y_val_prob.min():.4f}, Max: {y_val_prob.max():.4f}, Mean: {y_val_prob.mean():.4f}\")\n",
    "print(f\"Test predictions - Min: {y_pred.min():.4f}, Max: {y_pred.max():.4f}, Mean: {y_pred.mean():.4f}\")\n",
    "\n",
    "# 3. Check how many predictions are above different thresholds\n",
    "print(f\"\\n3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\")\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for thresh in thresholds:\n",
    "    val_above = (y_val_prob > thresh).sum()\n",
    "    test_above = (y_pred > thresh).sum()\n",
    "    print(f\"Threshold {thresh}: Validation={val_above}/{len(y_val_prob)} ({val_above/len(y_val_prob)*100:.1f}%), Test={test_above}/{len(y_pred)} ({test_above/len(y_pred)*100:.1f}%)\")\n",
    "\n",
    "# 4. Check confusion matrix with current threshold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(f\"\\n4. CONFUSION MATRIX (threshold=0.5):\")\n",
    "print(\"Validation:\")\n",
    "val_cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "print(f\"TN: {val_cm[0,0]}, FP: {val_cm[0,1]}\")\n",
    "print(f\"FN: {val_cm[1,0]}, TP: {val_cm[1,1]}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "test_cm = confusion_matrix(y_true, y_pred_binary)\n",
    "print(f\"TN: {test_cm[0,0]}, FP: {test_cm[0,1]}\")\n",
    "print(f\"FN: {test_cm[1,0]}, TP: {test_cm[1,1]}\")\n",
    "\n",
    "# 5. Find optimal threshold using validation set\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "print(f\"\\n5. OPTIMAL THRESHOLD ANALYSIS:\")\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_val_true, y_val_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "f1_scores = f1_scores[~np.isnan(f1_scores)]  # Remove NaN values\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds_pr[optimal_idx] if optimal_idx < len(thresholds_pr) else 0.5\n",
    "\n",
    "print(f\"Optimal threshold for F1: {optimal_threshold:.4f}\")\n",
    "print(f\"Max F1 score achievable: {f1_scores[optimal_idx]:.4f}\")\n",
    "\n",
    "# 6. Evaluate with optimal threshold\n",
    "y_val_pred_optimal = (y_val_prob > optimal_threshold).astype(int)\n",
    "y_test_pred_optimal = (y_pred > optimal_threshold).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\n6. PERFORMANCE WITH OPTIMAL THRESHOLD ({optimal_threshold:.4f}):\")\n",
    "print(\"Validation:\")\n",
    "val_f1_optimal = f1_score(y_val_true, y_val_pred_optimal)\n",
    "val_macro_f1_optimal = f1_score(y_val_true, y_val_pred_optimal, average='macro')\n",
    "print(f\"F1: {val_f1_optimal:.4f}, Macro-F1: {val_macro_f1_optimal:.4f}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "test_f1_optimal = f1_score(y_true, y_test_pred_optimal)\n",
    "test_macro_f1_optimal = f1_score(y_true, y_test_pred_optimal, average='macro')\n",
    "print(f\"F1: {test_f1_optimal:.4f}, Macro-F1: {test_macro_f1_optimal:.4f}\")\n",
    "\n",
    "# 7. Show distribution of positive class probabilities\n",
    "print(f\"\\n7. POSITIVE CLASS PROBABILITY ANALYSIS:\")\n",
    "pos_indices = y_val_true == 1\n",
    "neg_indices = y_val_true == 0\n",
    "\n",
    "if pos_indices.sum() > 0:\n",
    "    pos_probs = y_val_prob[pos_indices]\n",
    "    neg_probs = y_val_prob[neg_indices]\n",
    "    \n",
    "    print(f\"Positive class (binding) predictions:\")\n",
    "    print(f\"  Count: {len(pos_probs)}\")\n",
    "    print(f\"  Mean probability: {pos_probs.mean():.4f}\")\n",
    "    print(f\"  Max probability: {pos_probs.max():.4f}\")\n",
    "    print(f\"  % above 0.5: {(pos_probs > 0.5).mean()*100:.1f}%\")\n",
    "    \n",
    "    print(f\"Negative class (no binding) predictions:\")\n",
    "    print(f\"  Count: {len(neg_probs)}\")\n",
    "    print(f\"  Mean probability: {neg_probs.mean():.4f}\")\n",
    "    print(f\"  Max probability: {neg_probs.max():.4f}\")\n",
    "    print(f\"  % above 0.5: {(neg_probs > 0.5).mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n8. RECOMMENDATIONS:\")\n",
    "print(f\"- Your model has discriminative power (AUC > 0.6) but conservative threshold\")\n",
    "print(f\"- Consider using threshold {optimal_threshold:.3f} instead of 0.5\")\n",
    "print(f\"- The high accuracy with F1=0 indicates severe class imbalance\")\n",
    "print(f\"- Macro-F1 > 0 shows the model isn't completely broken\")\n",
    "print(f\"- Consider techniques for imbalanced datasets (SMOTE, class weights, etc.)\")\n",
    "\n",
    "# 9. Quick retraining suggestion with class weights\n",
    "print(f\"\\n9. CLASS WEIGHT SUGGESTION:\")\n",
    "neg_count = (train_df[target_col] == 0).sum()\n",
    "pos_count = (train_df[target_col] == 1).sum()\n",
    "class_weight_ratio = neg_count / pos_count\n",
    "print(f\"Negative samples: {neg_count}\")\n",
    "print(f\"Positive samples: {pos_count}\")\n",
    "print(f\"Imbalance ratio: {class_weight_ratio:.2f}:1\")\n",
    "print(f\"Consider adding 'scale_pos_weight': {class_weight_ratio:.2f} to LightGBM params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae9783",
   "metadata": {},
   "source": [
    "### v2 with macro-f1 and rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8e2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15062/1494801776.py:22: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Encoding categorical features...\n",
      "Encoded TRBV: 166 unique values\n",
      "Encoded TRBJ: 31 unique values\n",
      "Encoded MHC: 99 unique values\n",
      "\n",
      "Combining features...\n",
      "Final feature dimensions: 1027 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - Categorical features: 3 features\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.262866\tval's binary_logloss: 0.226142\n",
      "[200]\ttrain's binary_logloss: 0.253374\tval's binary_logloss: 0.221357\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttrain's binary_logloss: 0.251108\tval's binary_logloss: 0.220423\n",
      "\n",
      "Training completed. Best iteration: 224\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "==================================================\n",
      "VALIDATION METRICS\n",
      "==================================================\n",
      "Log Loss: 0.2204\n",
      "Accuracy: 0.9169\n",
      "AUC: 0.9370\n",
      "F1 Score: 0.7189\n",
      "Macro-F1 Score: 0.8351\n",
      "AP Score: 0.7938\n",
      "\n",
      "==============================\n",
      "PER-TASK VALIDATION METRICS\n",
      "==============================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.9341\n",
      "  Log Loss: 0.1796\n",
      "  ROC AUC: 0.9604\n",
      "  Average Precision: 0.8488\n",
      "  F1 Score: 0.7814\n",
      "  Macro-F1 Score: 0.8713\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8533\n",
      "  Log Loss: 0.3406\n",
      "  ROC AUC: 0.8676\n",
      "  Average Precision: 0.5689\n",
      "  F1 Score: 0.5312\n",
      "  Macro-F1 Score: 0.7221\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8190\n",
      "  Log Loss: 0.4932\n",
      "  ROC AUC: 0.5742\n",
      "  Average Precision: 0.2702\n",
      "  F1 Score: 0.1560\n",
      "  Macro-F1 Score: 0.5273\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.7854\n",
      "  Log Loss: 0.4849\n",
      "  ROC AUC: 0.5912\n",
      "  Average Precision: 0.1981\n",
      "  F1 Score: 0.0364\n",
      "  Macro-F1 Score: 0.4578\n",
      "\n",
      "Average across tasks (excluding undefined):\n",
      "  Average AUC: 0.7483\n",
      "  Average Accuracy: 0.8480\n",
      "  Average F1: 0.3762\n",
      "  Average Macro-F1: 0.6446\n",
      "\n",
      "==================================================\n",
      "TEST METRICS\n",
      "==================================================\n",
      "Accuracy: 0.8215\n",
      "AUC: 0.6251\n",
      "F1 Score: 0.0777\n",
      "Macro-F1 Score: 0.4894\n",
      "AP Score: 0.2117\n",
      "Log Loss: 0.5219\n",
      "\n",
      "==============================\n",
      "PER-TASK TEST METRICS\n",
      "==============================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8684\n",
      "  Log Loss: 0.3585\n",
      "  ROC AUC: 0.9600\n",
      "  Average Precision: 0.7113\n",
      "  F1 Score: 0.1201\n",
      "  Macro-F1 Score: 0.5245\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.7971\n",
      "  Log Loss: 0.6057\n",
      "  ROC AUC: 0.4449\n",
      "  Average Precision: 0.1646\n",
      "  F1 Score: 0.0716\n",
      "  Macro-F1 Score: 0.4788\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8015\n",
      "  Log Loss: 0.6036\n",
      "  ROC AUC: 0.4362\n",
      "  Average Precision: 0.1566\n",
      "  F1 Score: 0.0148\n",
      "  Macro-F1 Score: 0.4522\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8007\n",
      "  Log Loss: 0.5582\n",
      "  ROC AUC: 0.4773\n",
      "  Average Precision: 0.1644\n",
      "  F1 Score: 0.0357\n",
      "  Macro-F1 Score: 0.4623\n",
      "\n",
      "Average across test tasks (excluding undefined):\n",
      "  Average AUC: 0.5796\n",
      "  Average Accuracy: 0.8169\n",
      "  Average F1: 0.0605\n",
      "  Average Macro-F1: 0.4795\n",
      "\n",
      "==================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "==================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      "   TRBJ_encoded 715965.824492\n",
      "    MHC_encoded 408459.879719\n",
      "   TRBV_encoded 273736.857082\n",
      "epitope_emb_392  80437.656295\n",
      " epitope_emb_79  59646.241247\n",
      " epitope_emb_67  39163.985046\n",
      "epitope_emb_207  30104.105347\n",
      "epitope_emb_228  25835.628696\n",
      " epitope_emb_49  24508.152559\n",
      "epitope_emb_360  21227.130096\n",
      "epitope_emb_233  20496.944202\n",
      "epitope_emb_459  19569.521294\n",
      " epitope_emb_42  15208.068138\n",
      "epitope_emb_384  14727.558907\n",
      "epitope_emb_439  12774.100963\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 70253.62 (2.8%)\n",
      "Epitope embeddings: 1024566.90 (41.1%)\n",
      "Categorical features: 1398162.56 (56.1%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 271: 5950.44\n",
      "  Dimension 293: 2585.06\n",
      "  Dimension 79: 2120.85\n",
      "  Dimension 15: 1904.23\n",
      "  Dimension 482: 1886.65\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 392: 80437.66\n",
      "  Dimension 79: 59646.24\n",
      "  Dimension 67: 39163.99\n",
      "  Dimension 207: 30104.11\n",
      "  Dimension 228: 25835.63\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "Best validation AUC: 0.9370\n",
      "Test AUC: 0.6251\n",
      "Training iterations: 224\n",
      "Total features used: 1027\n",
      "Embedding contribution: 43.9%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your actual paths\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='zero'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean', 'drop')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "            else:  # 'drop' - will be handled later\n",
    "                embedding_features.append(np.zeros(embedding_dim))  # placeholder\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        if len(missing_sequences) <= 5:  # Show a few examples\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Encode categorical features that don't have embeddings\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "categorical_cols = ['TRBV', 'TRBJ', 'MHC']\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col + '_encoded'] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col + '_encoded'] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col + '_encoded'] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Combine all features\n",
    "print(\"\\nCombining features...\")\n",
    "encoded_categorical_cols = [col + '_encoded' for col in categorical_cols]\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb, \n",
    "    train_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb, \n",
    "    valid_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb, \n",
    "    test_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\") \n",
    "print(f\"  - Categorical features: {len(encoded_categorical_cols)} features\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "if train_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in training features\")\n",
    "    train_features = train_features.fillna(0)\n",
    "if valid_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in validation features\")\n",
    "    valid_features = valid_features.fillna(0)\n",
    "if test_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in test features\")\n",
    "    test_features = test_features.fillna(0)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "train_data = lgb.Dataset(\n",
    "    train_features, \n",
    "    label=train_df[target_col], \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "valid_data = lgb.Dataset(\n",
    "    valid_features, \n",
    "    label=valid_df[target_col], \n",
    "    reference=train_data, \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "\n",
    "# LightGBM parameters - optimized for high-dimensional embedding features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,  # Lower learning rate for stability with embeddings\n",
    "    'feature_fraction': 0.8,  # Feature subsampling to prevent overfitting\n",
    "    'bagging_fraction': 0.8,  # Data subsampling\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_val_true, y_val_pred, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*30)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    task_results = []\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_tasks = [r for r in task_results if isinstance(r['auc'], float)]\n",
    "    if valid_tasks:\n",
    "        avg_auc = np.mean([r['auc'] for r in valid_tasks])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in valid_tasks])\n",
    "        avg_f1 = np.mean([r['f1'] for r in valid_tasks])\n",
    "        avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_tasks])\n",
    "        print(f\"\\nAverage across tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"  Average F1: {avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_true, y_pred_binary, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*30)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    test_task_results = []\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_test_tasks = [r for r in test_task_results if isinstance(r['auc'], float)]\n",
    "    if valid_test_tasks:\n",
    "        test_avg_auc = np.mean([r['auc'] for r in valid_test_tasks])\n",
    "        test_avg_acc = np.mean([r['accuracy'] for r in valid_test_tasks])\n",
    "        test_avg_f1 = np.mean([r['f1'] for r in valid_test_tasks])\n",
    "        test_avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_test_tasks])\n",
    "        print(f\"\\nAverage across test tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {test_avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {test_avg_acc:.4f}\")\n",
    "        print(f\"  Average F1: {test_avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {test_avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "categorical_importance = feature_imp_df[feature_imp_df['feature'].str.endswith('_encoded')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance + categorical_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Categorical features: {categorical_importance:.2f} ({categorical_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"Embedding contribution: {(tcr_emb_importance + epitope_emb_importance)/total_importance*100:.1f}%\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_model.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48165dbb",
   "metadata": {},
   "source": [
    "## LightGBM - V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8690f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18882/1613012008.py:21: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Combining TCR and Epitope features only...\n",
      "Final feature dimensions: 1024 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - No categorical features used\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model (TCR + Epitope embeddings only)...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttrain's binary_logloss: 0.384779\tval's binary_logloss: 0.430641\n",
      "\n",
      "Training completed. Best iteration: 15\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "VALIDATION METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Log Loss: 0.4306\n",
      "Accuracy: 0.8335\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC: 0.6989\n",
      "F1 Score: 0.0000\n",
      "Macro-F1 Score: 0.4546\n",
      "AP Score: 0.3105\n",
      "\n",
      "========================================\n",
      "PER-TASK VALIDATION METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.8351\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4268\n",
      "  ROC AUC: 0.7383\n",
      "  Average Precision: 0.3358\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4551\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8303\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4308\n",
      "  ROC AUC: 0.6737\n",
      "  Average Precision: 0.3523\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4536\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8199\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4709\n",
      "  ROC AUC: 0.5648\n",
      "  Average Precision: 0.2406\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4505\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.8340\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4462\n",
      "  ROC AUC: 0.6043\n",
      "  Average Precision: 0.3304\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4547\n",
      "\n",
      "Average across tasks (excluding undefined):\n",
      "  Average AUC: 0.6453\n",
      "  Average Accuracy: 0.8298\n",
      "  Average Precision: 0.0000\n",
      "  Average Recall: 0.0000\n",
      "  Average F1: 0.0000\n",
      "  Average Macro-F1: 0.4535\n",
      "\n",
      "============================================================\n",
      "TEST METRICS (TCR + Epitope Embeddings Only)\n",
      "============================================================\n",
      "Accuracy: 0.8387\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC: 0.6223\n",
      "F1 Score: 0.0000\n",
      "Macro-F1 Score: 0.4561\n",
      "AP Score: 0.2368\n",
      "Log Loss: 0.4337\n",
      "\n",
      "========================================\n",
      "PER-TASK TEST METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8600\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.3752\n",
      "  ROC AUC: 0.8433\n",
      "  Average Precision: 0.4922\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4624\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.8305\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4595\n",
      "  ROC AUC: 0.5535\n",
      "  Average Precision: 0.2314\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4537\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8140\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4818\n",
      "  ROC AUC: 0.5447\n",
      "  Average Precision: 0.2205\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4487\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8278\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  Log Loss: 0.4752\n",
      "  ROC AUC: 0.4701\n",
      "  Average Precision: 0.1677\n",
      "  F1 Score: 0.0000\n",
      "  Macro-F1 Score: 0.4529\n",
      "\n",
      "Average across test tasks (excluding undefined):\n",
      "  Average AUC: 0.6029\n",
      "  Average Accuracy: 0.8331\n",
      "  Average Precision: 0.0000\n",
      "  Average Recall: 0.0000\n",
      "  Average F1: 0.0000\n",
      "  Average Macro-F1: 0.4544\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\n",
      "============================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      " epitope_emb_79 109384.979980\n",
      " epitope_emb_67  81719.550781\n",
      "epitope_emb_207  49033.491180\n",
      " epitope_emb_49  29045.609619\n",
      "epitope_emb_253  23694.199829\n",
      "  epitope_emb_6  22059.909790\n",
      "epitope_emb_396  19302.384277\n",
      "epitope_emb_123  18515.747986\n",
      "epitope_emb_440  16376.180176\n",
      "epitope_emb_410  15557.280273\n",
      "epitope_emb_238  13771.799805\n",
      "epitope_emb_350  11999.539917\n",
      "epitope_emb_114  10707.299805\n",
      "epitope_emb_480  10423.500000\n",
      "epitope_emb_132   9115.889648\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 37277.06 (4.5%)\n",
      "Epitope embeddings: 786743.91 (95.5%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 463: 6200.02\n",
      "  Dimension 7: 5395.00\n",
      "  Dimension 15: 4763.95\n",
      "  Dimension 271: 2873.03\n",
      "  Dimension 45: 2152.19\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 79: 109384.98\n",
      "  Dimension 67: 81719.55\n",
      "  Dimension 207: 49033.49\n",
      "  Dimension 49: 29045.61\n",
      "  Dimension 253: 23694.20\n",
      "\n",
      "Importance concentration:\n",
      "Top 5 TCR dims contribute: 57.4% of TCR importance\n",
      "Top 5 Epitope dims contribute: 37.2% of Epitope importance\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY (Embeddings Only)\n",
      "============================================================\n",
      "Best validation AUC: 0.6989\n",
      "Test AUC: 0.6223\n",
      "Training iterations: 15\n",
      "Total features used: 1024\n",
      "TCR embedding contribution: 4.5%\n",
      "Epitope embedding contribution: 95.5%\n",
      "\n",
      "Model uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\n",
      "This shows the predictive power of TCR-Epitope interaction alone\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC ANALYSIS FOR F1 = 0\n",
      "============================================================\n",
      "1. CLASS DISTRIBUTION:\n",
      "Training set:\n",
      "Binding\n",
      "0    0.832902\n",
      "1    0.167098\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Binding\n",
      "0    0.833461\n",
      "1    0.166539\n",
      "Name: proportion, dtype: float64\n",
      "Test set:\n",
      "Binding\n",
      "0    0.838691\n",
      "1    0.161309\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "2. PREDICTION PROBABILITY DISTRIBUTION:\n",
      "Validation predictions - Min: 0.0821, Max: 0.3907, Mean: 0.1395\n",
      "Test predictions - Min: 0.0821, Max: 0.3825, Mean: 0.1540\n",
      "\n",
      "3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\n",
      "Threshold 0.1: Validation=86606/169029 (51.2%), Test=32461/54126 (60.0%)\n",
      "Threshold 0.2: Validation=35914/169029 (21.2%), Test=16803/54126 (31.0%)\n",
      "Threshold 0.3: Validation=3193/169029 (1.9%), Test=2038/54126 (3.8%)\n",
      "Threshold 0.4: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.5: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.6: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.7: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.8: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "Threshold 0.9: Validation=0/169029 (0.0%), Test=0/54126 (0.0%)\n",
      "\n",
      "4. CONFUSION MATRIX (threshold=0.5):\n",
      "Validation:\n",
      "TN: 140879, FP: 0\n",
      "FN: 28150, TP: 0\n",
      "Test:\n",
      "TN: 45395, FP: 0\n",
      "FN: 8731, TP: 0\n",
      "\n",
      "5. OPTIMAL THRESHOLD ANALYSIS:\n",
      "Optimal threshold for F1: 0.1023\n",
      "Max F1 score achievable: 0.4395\n",
      "\n",
      "6. PERFORMANCE WITH OPTIMAL THRESHOLD (0.1023):\n",
      "Validation:\n",
      "Precision: 0.2867, Recall: 0.8208\n",
      "F1: 0.4250, Macro-F1: 0.5762\n",
      "Test:\n",
      "Precision: 0.2107, Recall: 0.7655\n",
      "F1: 0.3305, Macro-F1: 0.4655\n",
      "\n",
      "7. POSITIVE CLASS PROBABILITY ANALYSIS:\n",
      "Positive class (binding) predictions:\n",
      "  Count: 28150\n",
      "  Mean probability: 0.1689\n",
      "  Max probability: 0.3907\n",
      "  % above 0.5: 0.0%\n",
      "Negative class (no binding) predictions:\n",
      "  Count: 140879\n",
      "  Mean probability: 0.1336\n",
      "  Max probability: 0.3655\n",
      "  % above 0.5: 0.0%\n",
      "\n",
      "8. RECOMMENDATIONS:\n",
      "- Your model has discriminative power (AUC > 0.6) but conservative threshold\n",
      "- Consider using threshold 0.102 instead of 0.5\n",
      "- The high accuracy with F1=0 indicates severe class imbalance\n",
      "- Macro-F1 > 0 shows the model isn't completely broken\n",
      "- Consider techniques for imbalanced datasets (SMOTE, class weights, etc.)\n",
      "\n",
      "9. CLASS WEIGHT SUGGESTION:\n",
      "Negative samples: 629472\n",
      "Positive samples: 126286\n",
      "Imbalance ratio: 4.98:1\n",
      "Consider adding 'scale_pos_weight': 4.98 to LightGBM params\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.metrics import log_loss, f1_score, precision_score, recall_score\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your 512-dimensional embeddings\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='mean'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        # Show a few examples of missing sequences\n",
    "        if len(missing_sequences) <= 5:\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "        elif len(missing_sequences) > 5:\n",
    "            print(f\"  First few missing: {[seq for _, seq in missing_sequences[:3]]}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Combine ONLY TCR and Epitope features (no categorical features)\n",
    "print(\"\\nCombining TCR and Epitope features only...\")\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\")\n",
    "print(f\"  - No categorical features used\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "for name, features in [(\"train\", train_features), (\"valid\", valid_features), (\"test\", test_features)]:\n",
    "    nan_count = features.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: Found {nan_count} NaN values in {name} features - filling with 0\")\n",
    "        features.fillna(0, inplace=True)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "# Note: No categorical features since we're only using embeddings\n",
    "train_data = lgb.Dataset(train_features, label=train_df[target_col])\n",
    "valid_data = lgb.Dataset(valid_features, label=valid_df[target_col], reference=train_data)\n",
    "\n",
    "# LightGBM parameters - optimized for embedding-only features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,  # Feature subsampling\n",
    "    'bagging_fraction': 0.8,  # Data subsampling  \n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model (TCR + Epitope embeddings only)...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Precision: {precision_score(y_val_true, y_val_pred, zero_division=0):.4f}')\n",
    "print(f'Recall: {recall_score(y_val_true, y_val_pred, zero_division=0):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_val_true, y_val_pred, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    task_results = []\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            precision = precision_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            recall = recall_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = precision = recall = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_tasks = [r for r in task_results if isinstance(r['auc'], float)]\n",
    "    if valid_tasks:\n",
    "        avg_auc = np.mean([r['auc'] for r in valid_tasks])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in valid_tasks])\n",
    "        avg_precision = np.mean([r['precision'] for r in valid_tasks])\n",
    "        avg_recall = np.mean([r['recall'] for r in valid_tasks])\n",
    "        avg_f1 = np.mean([r['f1'] for r in valid_tasks])\n",
    "        avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_tasks])\n",
    "        print(f\"\\nAverage across tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "        print(f\"  Average Recall: {avg_recall:.4f}\")\n",
    "        print(f\"  Average F1: {avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST METRICS (TCR + Epitope Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Precision: {precision_score(y_true, y_pred_binary, zero_division=0):.4f}')\n",
    "print(f'Recall: {recall_score(y_true, y_pred_binary, zero_division=0):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_true, y_pred_binary, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    test_task_results = []\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            precision = precision_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            recall = recall_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = precision = recall = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_test_tasks = [r for r in test_task_results if isinstance(r['auc'], float)]\n",
    "    if valid_test_tasks:\n",
    "        test_avg_auc = np.mean([r['auc'] for r in valid_test_tasks])\n",
    "        test_avg_acc = np.mean([r['accuracy'] for r in valid_test_tasks])\n",
    "        test_avg_precision = np.mean([r['precision'] for r in valid_test_tasks])\n",
    "        test_avg_recall = np.mean([r['recall'] for r in valid_test_tasks])\n",
    "        test_avg_f1 = np.mean([r['f1'] for r in valid_test_tasks])\n",
    "        test_avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_test_tasks])\n",
    "        print(f\"\\nAverage across test tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {test_avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {test_avg_acc:.4f}\")\n",
    "        print(f\"  Average Precision: {test_avg_precision:.4f}\")\n",
    "        print(f\"  Average Recall: {test_avg_recall:.4f}\")\n",
    "        print(f\"  Average F1: {test_avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {test_avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# Check if embeddings are well-distributed in importance\n",
    "tcr_top_5_importance = tcr_features['importance'].sum()\n",
    "epitope_top_5_importance = epitope_features['importance'].sum()\n",
    "\n",
    "print(f\"\\nImportance concentration:\")\n",
    "print(f\"Top 5 TCR dims contribute: {tcr_top_5_importance/tcr_emb_importance*100:.1f}% of TCR importance\")\n",
    "print(f\"Top 5 Epitope dims contribute: {epitope_top_5_importance/epitope_emb_importance*100:.1f}% of Epitope importance\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY (Embeddings Only)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"TCR embedding contribution: {tcr_emb_importance/total_importance*100:.1f}%\")\n",
    "print(f\"Epitope embedding contribution: {epitope_emb_importance/total_importance*100:.1f}%\")\n",
    "\n",
    "# Performance comparison hint\n",
    "print(f\"\\nModel uses ONLY sequence embeddings (no MHC, TRBV, TRBJ)\")\n",
    "print(f\"This shows the predictive power of TCR-Epitope interaction alone\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_only_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_only_model.txt'\")\n",
    "\n",
    "# Add this diagnostic code after making predictions to understand the F1=0 issue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC ANALYSIS FOR F1 = 0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check class distribution\n",
    "print(\"1. CLASS DISTRIBUTION:\")\n",
    "print(f\"Training set:\")\n",
    "train_class_dist = train_df[target_col].value_counts(normalize=True)\n",
    "print(train_class_dist)\n",
    "print(f\"Validation set:\")\n",
    "val_class_dist = valid_df[target_col].value_counts(normalize=True)\n",
    "print(val_class_dist)\n",
    "print(f\"Test set:\")\n",
    "test_class_dist = test_df[target_col].value_counts(normalize=True)\n",
    "print(test_class_dist)\n",
    "\n",
    "# 2. Check prediction probabilities distribution\n",
    "print(f\"\\n2. PREDICTION PROBABILITY DISTRIBUTION:\")\n",
    "print(f\"Validation predictions - Min: {y_val_prob.min():.4f}, Max: {y_val_prob.max():.4f}, Mean: {y_val_prob.mean():.4f}\")\n",
    "print(f\"Test predictions - Min: {y_pred.min():.4f}, Max: {y_pred.max():.4f}, Mean: {y_pred.mean():.4f}\")\n",
    "\n",
    "# 3. Check how many predictions are above different thresholds\n",
    "print(f\"\\n3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\")\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for thresh in thresholds:\n",
    "    val_above = (y_val_prob > thresh).sum()\n",
    "    test_above = (y_pred > thresh).sum()\n",
    "    print(f\"Threshold {thresh}: Validation={val_above}/{len(y_val_prob)} ({val_above/len(y_val_prob)*100:.1f}%), Test={test_above}/{len(y_pred)} ({test_above/len(y_pred)*100:.1f}%)\")\n",
    "\n",
    "# 4. Check confusion matrix with current threshold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(f\"\\n4. CONFUSION MATRIX (threshold=0.5):\")\n",
    "print(\"Validation:\")\n",
    "val_cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "print(f\"TN: {val_cm[0,0]}, FP: {val_cm[0,1]}\")\n",
    "print(f\"FN: {val_cm[1,0]}, TP: {val_cm[1,1]}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "test_cm = confusion_matrix(y_true, y_pred_binary)\n",
    "print(f\"TN: {test_cm[0,0]}, FP: {test_cm[0,1]}\")\n",
    "print(f\"FN: {test_cm[1,0]}, TP: {test_cm[1,1]}\")\n",
    "\n",
    "# 5. Find optimal threshold using validation set\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "print(f\"\\n5. OPTIMAL THRESHOLD ANALYSIS:\")\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_val_true, y_val_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "f1_scores = f1_scores[~np.isnan(f1_scores)]  # Remove NaN values\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds_pr[optimal_idx] if optimal_idx < len(thresholds_pr) else 0.5\n",
    "\n",
    "print(f\"Optimal threshold for F1: {optimal_threshold:.4f}\")\n",
    "print(f\"Max F1 score achievable: {f1_scores[optimal_idx]:.4f}\")\n",
    "\n",
    "# 6. Evaluate with optimal threshold\n",
    "y_val_pred_optimal = (y_val_prob > optimal_threshold).astype(int)\n",
    "y_test_pred_optimal = (y_pred > optimal_threshold).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\n6. PERFORMANCE WITH OPTIMAL THRESHOLD ({optimal_threshold:.4f}):\")\n",
    "print(\"Validation:\")\n",
    "val_f1_optimal = f1_score(y_val_true, y_val_pred_optimal)\n",
    "val_macro_f1_optimal = f1_score(y_val_true, y_val_pred_optimal, average='macro')\n",
    "val_precision_optimal = precision_score(y_val_true, y_val_pred_optimal, zero_division=0)\n",
    "val_recall_optimal = recall_score(y_val_true, y_val_pred_optimal, zero_division=0)\n",
    "print(f\"Precision: {val_precision_optimal:.4f}, Recall: {val_recall_optimal:.4f}\")\n",
    "print(f\"F1: {val_f1_optimal:.4f}, Macro-F1: {val_macro_f1_optimal:.4f}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "test_f1_optimal = f1_score(y_true, y_test_pred_optimal)\n",
    "test_macro_f1_optimal = f1_score(y_true, y_test_pred_optimal, average='macro')\n",
    "test_precision_optimal = precision_score(y_true, y_test_pred_optimal, zero_division=0)\n",
    "test_recall_optimal = recall_score(y_true, y_test_pred_optimal, zero_division=0)\n",
    "print(f\"Precision: {test_precision_optimal:.4f}, Recall: {test_recall_optimal:.4f}\")\n",
    "print(f\"F1: {test_f1_optimal:.4f}, Macro-F1: {test_macro_f1_optimal:.4f}\")\n",
    "\n",
    "# 7. Show distribution of positive class probabilities\n",
    "print(f\"\\n7. POSITIVE CLASS PROBABILITY ANALYSIS:\")\n",
    "pos_indices = y_val_true == 1\n",
    "neg_indices = y_val_true == 0\n",
    "\n",
    "if pos_indices.sum() > 0:\n",
    "    pos_probs = y_val_prob[pos_indices]\n",
    "    neg_probs = y_val_prob[neg_indices]\n",
    "    \n",
    "    print(f\"Positive class (binding) predictions:\")\n",
    "    print(f\"  Count: {len(pos_probs)}\")\n",
    "    print(f\"  Mean probability: {pos_probs.mean():.4f}\")\n",
    "    print(f\"  Max probability: {pos_probs.max():.4f}\")\n",
    "    print(f\"  % above 0.5: {(pos_probs > 0.5).mean()*100:.1f}%\")\n",
    "    \n",
    "    print(f\"Negative class (no binding) predictions:\")\n",
    "    print(f\"  Count: {len(neg_probs)}\")\n",
    "    print(f\"  Mean probability: {neg_probs.mean():.4f}\")\n",
    "    print(f\"  Max probability: {neg_probs.max():.4f}\")\n",
    "    print(f\"  % above 0.5: {(neg_probs > 0.5).mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n8. RECOMMENDATIONS:\")\n",
    "print(f\"- Your model has discriminative power (AUC > 0.6) but conservative threshold\")\n",
    "print(f\"- Consider using threshold {optimal_threshold:.3f} instead of 0.5\")\n",
    "print(f\"- The high accuracy with F1=0 indicates severe class imbalance\")\n",
    "print(f\"- Macro-F1 > 0 shows the model isn't completely broken\")\n",
    "print(f\"- Consider techniques for imbalanced datasets (SMOTE, class weights, etc.)\")\n",
    "\n",
    "# 9. Quick retraining suggestion with class weights\n",
    "print(f\"\\n9. CLASS WEIGHT SUGGESTION:\")\n",
    "neg_count = (train_df[target_col] == 0).sum()\n",
    "pos_count = (train_df[target_col] == 1).sum()\n",
    "class_weight_ratio = neg_count / pos_count\n",
    "print(f\"Negative samples: {neg_count}\")\n",
    "print(f\"Positive samples: {pos_count}\")\n",
    "print(f\"Imbalance ratio: {class_weight_ratio:.2f}:1\")\n",
    "print(f\"Consider adding 'scale_pos_weight': {class_weight_ratio:.2f} to LightGBM params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5ce24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5344d694",
   "metadata": {},
   "source": [
    "## LightGBM - V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54502d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18882/1154912800.py:22: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 755758 samples\n",
      "Validation set: 169029 samples\n",
      "Test set: 54126 samples\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded 211294 embeddings from ../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl\n",
      "Loaded 1896 embeddings from ../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for tcr: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "Embedding dimension for epitope: 512\n",
      "\n",
      "Encoding categorical features...\n",
      "Encoded TRBV: 166 unique values\n",
      "Encoded TRBJ: 31 unique values\n",
      "Encoded MHC: 99 unique values\n",
      "\n",
      "Combining features...\n",
      "Final feature dimensions: 1027 features\n",
      "  - TCR embeddings: 512 features\n",
      "  - Epitope embeddings: 512 features\n",
      "  - Categorical features: 3 features\n",
      "\n",
      "Creating LightGBM datasets...\n",
      "\n",
      "Training LightGBM model...\n",
      "Parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'verbosity': -1, 'seed': 42, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_data_in_leaf': 20, 'lambda_l1': 0.1, 'lambda_l2': 0.1}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.262866\tval's binary_logloss: 0.226142\n",
      "[200]\ttrain's binary_logloss: 0.253374\tval's binary_logloss: 0.221357\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttrain's binary_logloss: 0.251108\tval's binary_logloss: 0.220423\n",
      "\n",
      "Training completed. Best iteration: 224\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "VALIDATION METRICS (Embeddings + Categorical Features)\n",
      "============================================================\n",
      "Log Loss: 0.2204\n",
      "Accuracy: 0.9169\n",
      "Precision: 0.8235\n",
      "Recall: 0.6380\n",
      "AUC: 0.9370\n",
      "F1 Score: 0.7189\n",
      "Macro-F1 Score: 0.8351\n",
      "AP Score: 0.7938\n",
      "\n",
      "========================================\n",
      "PER-TASK VALIDATION METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=138846)\n",
      "  Accuracy: 0.9341\n",
      "  Precision: 0.8629\n",
      "  Recall: 0.7139\n",
      "  Log Loss: 0.1796\n",
      "  ROC AUC: 0.9604\n",
      "  Average Precision: 0.8488\n",
      "  F1 Score: 0.7814\n",
      "  Macro-F1 Score: 0.8713\n",
      "\n",
      "Task: TPP2 (n=16780)\n",
      "  Accuracy: 0.8533\n",
      "  Precision: 0.5803\n",
      "  Recall: 0.4898\n",
      "  Log Loss: 0.3406\n",
      "  ROC AUC: 0.8676\n",
      "  Average Precision: 0.5689\n",
      "  F1 Score: 0.5312\n",
      "  Macro-F1 Score: 0.7221\n",
      "\n",
      "Task: TPP3 (n=13156)\n",
      "  Accuracy: 0.8190\n",
      "  Precision: 0.4867\n",
      "  Recall: 0.0929\n",
      "  Log Loss: 0.4932\n",
      "  ROC AUC: 0.5742\n",
      "  Average Precision: 0.2702\n",
      "  F1 Score: 0.1560\n",
      "  Macro-F1 Score: 0.5273\n",
      "\n",
      "Task: TPP4 (n=247)\n",
      "  Accuracy: 0.7854\n",
      "  Precision: 0.0714\n",
      "  Recall: 0.0244\n",
      "  Log Loss: 0.4849\n",
      "  ROC AUC: 0.5912\n",
      "  Average Precision: 0.1981\n",
      "  F1 Score: 0.0364\n",
      "  Macro-F1 Score: 0.4578\n",
      "\n",
      "Average across tasks (excluding undefined):\n",
      "  Average AUC: 0.7483\n",
      "  Average Accuracy: 0.8480\n",
      "  Average Precision: 0.5003\n",
      "  Average Recall: 0.3302\n",
      "  Average F1: 0.3762\n",
      "  Average Macro-F1: 0.6446\n",
      "\n",
      "============================================================\n",
      "TEST METRICS (Embeddings + Categorical Features)\n",
      "============================================================\n",
      "Accuracy: 0.8215\n",
      "Precision: 0.2332\n",
      "Recall: 0.0466\n",
      "AUC: 0.6251\n",
      "F1 Score: 0.0777\n",
      "Macro-F1 Score: 0.4894\n",
      "AP Score: 0.2117\n",
      "Log Loss: 0.5219\n",
      "\n",
      "========================================\n",
      "PER-TASK TEST METRICS\n",
      "========================================\n",
      "\n",
      "Task: TPP1 (n=18150)\n",
      "  Accuracy: 0.8684\n",
      "  Precision: 0.9368\n",
      "  Recall: 0.0641\n",
      "  Log Loss: 0.3585\n",
      "  ROC AUC: 0.9600\n",
      "  Average Precision: 0.7113\n",
      "  F1 Score: 0.1201\n",
      "  Macro-F1 Score: 0.5245\n",
      "\n",
      "Task: TPP2 (n=29788)\n",
      "  Accuracy: 0.7971\n",
      "  Precision: 0.1596\n",
      "  Recall: 0.0461\n",
      "  Log Loss: 0.6057\n",
      "  ROC AUC: 0.4449\n",
      "  Average Precision: 0.1646\n",
      "  F1 Score: 0.0716\n",
      "  Macro-F1 Score: 0.4788\n",
      "\n",
      "Task: TPP3 (n=5375)\n",
      "  Accuracy: 0.8015\n",
      "  Precision: 0.0964\n",
      "  Recall: 0.0080\n",
      "  Log Loss: 0.6036\n",
      "  ROC AUC: 0.4362\n",
      "  Average Precision: 0.1566\n",
      "  F1 Score: 0.0148\n",
      "  Macro-F1 Score: 0.4522\n",
      "\n",
      "Task: TPP4 (n=813)\n",
      "  Accuracy: 0.8007\n",
      "  Precision: 0.1071\n",
      "  Recall: 0.0214\n",
      "  Log Loss: 0.5582\n",
      "  ROC AUC: 0.4773\n",
      "  Average Precision: 0.1644\n",
      "  F1 Score: 0.0357\n",
      "  Macro-F1 Score: 0.4623\n",
      "\n",
      "Average across test tasks (excluding undefined):\n",
      "  Average AUC: 0.5796\n",
      "  Average Accuracy: 0.8169\n",
      "  Average Precision: 0.3250\n",
      "  Average Recall: 0.0349\n",
      "  Average F1: 0.0605\n",
      "  Average Macro-F1: 0.4795\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (Embeddings + Categorical)\n",
      "============================================================\n",
      "Top 15 most important features:\n",
      "        feature    importance\n",
      "   TRBJ_encoded 715965.824492\n",
      "    MHC_encoded 408459.879719\n",
      "   TRBV_encoded 273736.857082\n",
      "epitope_emb_392  80437.656295\n",
      " epitope_emb_79  59646.241247\n",
      " epitope_emb_67  39163.985046\n",
      "epitope_emb_207  30104.105347\n",
      "epitope_emb_228  25835.628696\n",
      " epitope_emb_49  24508.152559\n",
      "epitope_emb_360  21227.130096\n",
      "epitope_emb_233  20496.944202\n",
      "epitope_emb_459  19569.521294\n",
      " epitope_emb_42  15208.068138\n",
      "epitope_emb_384  14727.558907\n",
      "epitope_emb_439  12774.100963\n",
      "\n",
      "Feature group importance:\n",
      "TCR embeddings: 70253.62 (2.8%)\n",
      "Epitope embeddings: 1024566.90 (41.1%)\n",
      "Categorical features: 1398162.56 (56.1%)\n",
      "\n",
      "Categorical feature importance breakdown:\n",
      "  TRBV_encoded: 273736.86 (11.0%)\n",
      "  TRBJ_encoded: 715965.82 (28.7%)\n",
      "  MHC_encoded: 408459.88 (16.4%)\n",
      "\n",
      "Top 5 most important TCR embedding dimensions:\n",
      "  Dimension 271: 5950.44\n",
      "  Dimension 293: 2585.06\n",
      "  Dimension 79: 2120.85\n",
      "  Dimension 15: 1904.23\n",
      "  Dimension 482: 1886.65\n",
      "\n",
      "Top 5 most important Epitope embedding dimensions:\n",
      "  Dimension 392: 80437.66\n",
      "  Dimension 79: 59646.24\n",
      "  Dimension 67: 39163.99\n",
      "  Dimension 207: 30104.11\n",
      "  Dimension 228: 25835.63\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY (Embeddings + Categorical)\n",
      "============================================================\n",
      "Best validation AUC: 0.9370\n",
      "Test AUC: 0.6251\n",
      "Training iterations: 224\n",
      "Total features used: 1027\n",
      "Embedding contribution: 43.9%\n",
      "Categorical contribution: 56.1%\n",
      "\n",
      "Model uses sequence embeddings + categorical features (MHC, TRBV, TRBJ)\n",
      "This shows the combined predictive power of sequence and context information\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC ANALYSIS\n",
      "============================================================\n",
      "1. CLASS DISTRIBUTION:\n",
      "Training set:\n",
      "Binding\n",
      "0    0.832902\n",
      "1    0.167098\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Binding\n",
      "0    0.833461\n",
      "1    0.166539\n",
      "Name: proportion, dtype: float64\n",
      "Test set:\n",
      "Binding\n",
      "0    0.838691\n",
      "1    0.161309\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "2. PREDICTION PROBABILITY DISTRIBUTION:\n",
      "Validation predictions - Min: 0.0002, Max: 0.9828, Mean: 0.1801\n",
      "Test predictions - Min: 0.0002, Max: 0.9064, Mean: 0.1044\n",
      "\n",
      "3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\n",
      "Threshold 0.1: Validation=59965/169029 (35.5%), Test=16296/54126 (30.1%)\n",
      "Threshold 0.2: Validation=51465/169029 (30.4%), Test=8533/54126 (15.8%)\n",
      "Threshold 0.3: Validation=32836/169029 (19.4%), Test=5002/54126 (9.2%)\n",
      "Threshold 0.4: Validation=26328/169029 (15.6%), Test=2950/54126 (5.5%)\n",
      "Threshold 0.5: Validation=21809/169029 (12.9%), Test=1745/54126 (3.2%)\n",
      "Threshold 0.6: Validation=18291/169029 (10.8%), Test=1289/54126 (2.4%)\n",
      "Threshold 0.7: Validation=16577/169029 (9.8%), Test=966/54126 (1.8%)\n",
      "Threshold 0.8: Validation=15225/169029 (9.0%), Test=362/54126 (0.7%)\n",
      "Threshold 0.9: Validation=13065/169029 (7.7%), Test=14/54126 (0.0%)\n",
      "\n",
      "4. OPTIMAL THRESHOLD ANALYSIS:\n",
      "Optimal threshold for F1: 0.4018\n",
      "Max F1 score achievable: 0.7282\n",
      "\n",
      "5. PERFORMANCE WITH OPTIMAL THRESHOLD (0.4018):\n",
      "Validation:\n",
      "Precision: 0.7551, Recall: 0.7032\n",
      "F1: 0.7282, Macro-F1: 0.8381\n",
      "Test:\n",
      "Precision: 0.2026, Recall: 0.0679\n",
      "F1: 0.1017, Macro-F1: 0.4967\n",
      "\n",
      "6. CLASS WEIGHT SUGGESTION:\n",
      "Negative samples: 629472\n",
      "Positive samples: 126286\n",
      "Imbalance ratio: 4.98:1\n",
      "Consider adding 'scale_pos_weight': 4.98 to LightGBM params\n",
      "\n",
      "7. RECOMMENDATIONS:\n",
      "- Compare this model with embeddings-only version to see categorical feature impact\n",
      "- Consider using threshold 0.402 instead of 0.5\n",
      "- Categorical features contribute 56.1% of importance\n",
      "- If precision/recall are still low, consider class balancing techniques\n",
      "- The combination of embeddings + categorical may improve generalization\n",
      "\n",
      "8. MODEL COMPARISON INSIGHTS:\n",
      "This model includes both sequence embeddings AND categorical features:\n",
      "  - TCR + Epitope embeddings: 43.9%\n",
      "  - MHC + TRBV + TRBJ features: 56.1%\n",
      "Compare AUC with embeddings-only model to quantify categorical feature value\n",
      "If categorical features show high importance, they're capturing crucial biological context\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss, precision_score, recall_score\n",
    "\n",
    "# File paths\n",
    "test_path = '../../../../../data/splitted_datasets/allele/beta/test.tsv'\n",
    "train_path = '../../../../../data/splitted_datasets/allele/beta/train.tsv'\n",
    "valid_path = '../../../../../data/splitted_datasets/allele/beta/validation.tsv'\n",
    "\n",
    "# Embedding file paths - update these to your actual paths\n",
    "tcr_embedding_path = '../../../../../data/embeddings/beta/allele/TRB_reduced_512_select.pkl'\n",
    "epitope_embedding_path = '../../../../../data/embeddings/beta/allele/Epitope_reduced_512_select.pkl'\n",
    "\n",
    "# Load the TSV files\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', low_memory=False)\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(valid_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "def load_reduced_embeddings(embedding_path):\n",
    "    \"\"\"Load pre-computed reduced embeddings from pickle file\"\"\"\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_path}\")\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding_features(df, sequence_col, embeddings_dict, prefix, missing_strategy='zero'):\n",
    "    \"\"\"\n",
    "    Convert sequences to embedding features using pre-computed embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sequences\n",
    "        sequence_col: Column name containing sequences\n",
    "        embeddings_dict: Dictionary mapping sequences to embeddings\n",
    "        prefix: Prefix for feature column names\n",
    "        missing_strategy: How to handle missing sequences ('zero', 'mean', 'drop')\n",
    "    \"\"\"\n",
    "    embedding_features = []\n",
    "    missing_sequences = []\n",
    "    \n",
    "    # Get embedding dimension from first embedding\n",
    "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
    "    print(f\"Embedding dimension for {prefix}: {embedding_dim}\")\n",
    "    \n",
    "    # Compute mean embedding for missing sequences if needed\n",
    "    if missing_strategy == 'mean':\n",
    "        all_embeddings = np.array(list(embeddings_dict.values()))\n",
    "        mean_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    for idx, seq in enumerate(df[sequence_col]):\n",
    "        if seq in embeddings_dict:\n",
    "            embedding_features.append(embeddings_dict[seq])\n",
    "        else:\n",
    "            missing_sequences.append((idx, seq))\n",
    "            if missing_strategy == 'zero':\n",
    "                embedding_features.append(np.zeros(embedding_dim))\n",
    "            elif missing_strategy == 'mean':\n",
    "                embedding_features.append(mean_embedding)\n",
    "            else:  # 'drop' - will be handled later\n",
    "                embedding_features.append(np.zeros(embedding_dim))  # placeholder\n",
    "    \n",
    "    if missing_sequences:\n",
    "        print(f\"Warning: {len(missing_sequences)} sequences not found in {prefix} embeddings\")\n",
    "        print(f\"Using {missing_strategy} strategy for missing sequences\")\n",
    "        if len(missing_sequences) <= 5:  # Show a few examples\n",
    "            for idx, seq in missing_sequences[:5]:\n",
    "                print(f\"  Missing: {seq}\")\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embedding_features, \n",
    "        columns=[f'{prefix}_emb_{i}' for i in range(embedding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return embedding_df, missing_sequences\n",
    "\n",
    "# Load pre-computed embeddings\n",
    "print(\"\\nLoading embeddings...\")\n",
    "tcr_embeddings = load_reduced_embeddings(tcr_embedding_path)\n",
    "epitope_embeddings = load_reduced_embeddings(epitope_embedding_path)\n",
    "\n",
    "# Convert sequences to embeddings for all datasets\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "\n",
    "# TCR embeddings\n",
    "train_tcr_emb, train_tcr_missing = get_embedding_features(\n",
    "    train_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "valid_tcr_emb, valid_tcr_missing = get_embedding_features(\n",
    "    valid_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "test_tcr_emb, test_tcr_missing = get_embedding_features(\n",
    "    test_df, 'TRB_CDR3', tcr_embeddings, 'tcr', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Epitope embeddings\n",
    "train_epitope_emb, train_epitope_missing = get_embedding_features(\n",
    "    train_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "valid_epitope_emb, valid_epitope_missing = get_embedding_features(\n",
    "    valid_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "test_epitope_emb, test_epitope_missing = get_embedding_features(\n",
    "    test_df, 'Epitope', epitope_embeddings, 'epitope', missing_strategy='mean'\n",
    ")\n",
    "\n",
    "# Encode categorical features that don't have embeddings\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "categorical_cols = ['TRBV', 'TRBJ', 'MHC']\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data = pd.concat([train_df[col], valid_df[col], test_df[col]], axis=0)\n",
    "    le.fit(all_data.astype(str))\n",
    "    train_df[col + '_encoded'] = le.transform(train_df[col].astype(str))\n",
    "    valid_df[col + '_encoded'] = le.transform(valid_df[col].astype(str))\n",
    "    test_df[col + '_encoded'] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Combine all features\n",
    "print(\"\\nCombining features...\")\n",
    "encoded_categorical_cols = [col + '_encoded' for col in categorical_cols]\n",
    "\n",
    "train_features = pd.concat([\n",
    "    train_tcr_emb, \n",
    "    train_epitope_emb, \n",
    "    train_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "valid_features = pd.concat([\n",
    "    valid_tcr_emb, \n",
    "    valid_epitope_emb, \n",
    "    valid_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "test_features = pd.concat([\n",
    "    test_tcr_emb, \n",
    "    test_epitope_emb, \n",
    "    test_df[encoded_categorical_cols].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final feature dimensions: {train_features.shape[1]} features\")\n",
    "print(f\"  - TCR embeddings: {train_tcr_emb.shape[1]} features\")\n",
    "print(f\"  - Epitope embeddings: {train_epitope_emb.shape[1]} features\") \n",
    "print(f\"  - Categorical features: {len(encoded_categorical_cols)} features\")\n",
    "\n",
    "target_col = 'Binding'\n",
    "\n",
    "# Check for any NaN values\n",
    "if train_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in training features\")\n",
    "    train_features = train_features.fillna(0)\n",
    "if valid_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in validation features\")\n",
    "    valid_features = valid_features.fillna(0)\n",
    "if test_features.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Found NaN values in test features\")\n",
    "    test_features = test_features.fillna(0)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\nCreating LightGBM datasets...\")\n",
    "train_data = lgb.Dataset(\n",
    "    train_features, \n",
    "    label=train_df[target_col], \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "valid_data = lgb.Dataset(\n",
    "    valid_features, \n",
    "    label=valid_df[target_col], \n",
    "    reference=train_data, \n",
    "    categorical_feature=encoded_categorical_cols\n",
    ")\n",
    "\n",
    "# LightGBM parameters - optimized for high-dimensional embedding features\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,  # Lower learning rate for stability with embeddings\n",
    "    'feature_fraction': 0.8,  # Feature subsampling to prevent overfitting\n",
    "    'bagging_fraction': 0.8,  # Data subsampling\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,  # L1 regularization\n",
    "    'lambda_l2': 0.1,  # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_true = test_df[target_col]\n",
    "\n",
    "# === Overall Validation Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS (Embeddings + Categorical Features)\")\n",
    "print(\"=\"*60)\n",
    "y_val_prob = model.predict(valid_features, num_iteration=model.best_iteration)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)\n",
    "y_val_true = valid_df[target_col]\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_val_true, y_val_prob):.4f}\")\n",
    "print(f'Accuracy: {accuracy_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Precision: {precision_score(y_val_true, y_val_pred, zero_division=0):.4f}')\n",
    "print(f'Recall: {recall_score(y_val_true, y_val_pred, zero_division=0):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_val_true, y_val_pred):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_val_true, y_val_pred, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_val_true, y_val_prob):.4f}')\n",
    "\n",
    "# === Per-task Validation Evaluation ===\n",
    "if 'task' in valid_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK VALIDATION METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    valid_df_copy = valid_df.copy()\n",
    "    valid_df_copy['true'] = y_val_true\n",
    "    valid_df_copy['pred_prob'] = y_val_prob\n",
    "    valid_df_copy['pred_label'] = y_val_pred\n",
    "\n",
    "    task_results = []\n",
    "    for task_name, group in valid_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            precision = precision_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            recall = recall_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = precision = recall = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_tasks = [r for r in task_results if isinstance(r['auc'], float)]\n",
    "    if valid_tasks:\n",
    "        avg_auc = np.mean([r['auc'] for r in valid_tasks])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in valid_tasks])\n",
    "        avg_precision = np.mean([r['precision'] for r in valid_tasks])\n",
    "        avg_recall = np.mean([r['recall'] for r in valid_tasks])\n",
    "        avg_f1 = np.mean([r['f1'] for r in valid_tasks])\n",
    "        avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_tasks])\n",
    "        print(f\"\\nAverage across tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "        print(f\"  Average Recall: {avg_recall:.4f}\")\n",
    "        print(f\"  Average F1: {avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in validation set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Overall Test Evaluation ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST METRICS (Embeddings + Categorical Features)\")\n",
    "print(\"=\"*60)\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Precision: {precision_score(y_true, y_pred_binary, zero_division=0):.4f}')\n",
    "print(f'Recall: {recall_score(y_true, y_pred_binary, zero_division=0):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_true, y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred_binary):.4f}')\n",
    "print(f'Macro-F1 Score: {f1_score(y_true, y_pred_binary, average=\"macro\"):.4f}')\n",
    "print(f'AP Score: {average_precision_score(y_true, y_pred):.4f}')\n",
    "print(f\"Log Loss: {log_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "# === Per-task Test Evaluation ===\n",
    "if 'task' in test_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-TASK TEST METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['true'] = y_true\n",
    "    test_df_copy['pred_prob'] = y_pred\n",
    "    test_df_copy['pred_label'] = y_pred_binary\n",
    "\n",
    "    test_task_results = []\n",
    "    for task_name, group in test_df_copy.groupby('task'):\n",
    "        acc = accuracy_score(group['true'], group['pred_label'])\n",
    "        try:\n",
    "            loss = log_loss(group['true'], group['pred_prob'], labels=[0, 1])\n",
    "            auc = roc_auc_score(group['true'], group['pred_prob'])\n",
    "            ap = average_precision_score(group['true'], group['pred_prob'])\n",
    "            precision = precision_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            recall = recall_score(group['true'], group['pred_label'], zero_division=0)\n",
    "            f1 = f1_score(group['true'], group['pred_label'])\n",
    "            macro_f1 = f1_score(group['true'], group['pred_label'], average='macro')\n",
    "            \n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "        except ValueError:\n",
    "            loss = auc = ap = precision = recall = f1 = macro_f1 = \"Undefined (only one class present)\"\n",
    "            test_task_results.append({\n",
    "                'task': task_name,\n",
    "                'n_samples': len(group),\n",
    "                'accuracy': acc,\n",
    "                'log_loss': loss,\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "        print(f\"\\nTask: {task_name} (n={len(group)})\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  Log Loss: {loss:.4f}\")\n",
    "        print(f\"  ROC AUC: {auc:.4f}\")\n",
    "        print(f\"  Average Precision: {ap:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Summary of per-task performance\n",
    "    valid_test_tasks = [r for r in test_task_results if isinstance(r['auc'], float)]\n",
    "    if valid_test_tasks:\n",
    "        test_avg_auc = np.mean([r['auc'] for r in valid_test_tasks])\n",
    "        test_avg_acc = np.mean([r['accuracy'] for r in valid_test_tasks])\n",
    "        test_avg_precision = np.mean([r['precision'] for r in valid_test_tasks])\n",
    "        test_avg_recall = np.mean([r['recall'] for r in valid_test_tasks])\n",
    "        test_avg_f1 = np.mean([r['f1'] for r in valid_test_tasks])\n",
    "        test_avg_macro_f1 = np.mean([r['macro_f1'] for r in valid_test_tasks])\n",
    "        print(f\"\\nAverage across test tasks (excluding undefined):\")\n",
    "        print(f\"  Average AUC: {test_avg_auc:.4f}\")\n",
    "        print(f\"  Average Accuracy: {test_avg_acc:.4f}\")\n",
    "        print(f\"  Average Precision: {test_avg_precision:.4f}\")\n",
    "        print(f\"  Average Recall: {test_avg_recall:.4f}\")\n",
    "        print(f\"  Average F1: {test_avg_f1:.4f}\")\n",
    "        print(f\"  Average Macro-F1: {test_avg_macro_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNote: 'task' column not found in test set; skipping per-task evaluation.\")\n",
    "\n",
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (Embeddings + Categorical)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = train_features.columns\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_imp_df.head(15).to_string(index=False))\n",
    "\n",
    "# Analyze feature group importance\n",
    "tcr_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')]['importance'].sum()\n",
    "epitope_emb_importance = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')]['importance'].sum()\n",
    "categorical_importance = feature_imp_df[feature_imp_df['feature'].str.endswith('_encoded')]['importance'].sum()\n",
    "total_importance = tcr_emb_importance + epitope_emb_importance + categorical_importance\n",
    "\n",
    "print(f\"\\nFeature group importance:\")\n",
    "print(f\"TCR embeddings: {tcr_emb_importance:.2f} ({tcr_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Epitope embeddings: {epitope_emb_importance:.2f} ({epitope_emb_importance/total_importance*100:.1f}%)\")\n",
    "print(f\"Categorical features: {categorical_importance:.2f} ({categorical_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show individual categorical feature importance\n",
    "print(f\"\\nCategorical feature importance breakdown:\")\n",
    "for col in encoded_categorical_cols:\n",
    "    col_importance = feature_imp_df[feature_imp_df['feature'] == col]['importance'].sum()\n",
    "    print(f\"  {col}: {col_importance:.2f} ({col_importance/total_importance*100:.1f}%)\")\n",
    "\n",
    "# Show most important embedding dimensions\n",
    "print(f\"\\nTop 5 most important TCR embedding dimensions:\")\n",
    "tcr_features = feature_imp_df[feature_imp_df['feature'].str.startswith('tcr_emb')].head(5)\n",
    "for _, row in tcr_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most important Epitope embedding dimensions:\")\n",
    "epitope_features = feature_imp_df[feature_imp_df['feature'].str.startswith('epitope_emb')].head(5)\n",
    "for _, row in epitope_features.iterrows():\n",
    "    dim = row['feature'].split('_')[-1]\n",
    "    print(f\"  Dimension {dim}: {row['importance']:.2f}\")\n",
    "\n",
    "# === Model Performance Summary ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY (Embeddings + Categorical)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation AUC: {roc_auc_score(y_val_true, y_val_prob):.4f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Training iterations: {model.best_iteration}\")\n",
    "print(f\"Total features used: {train_features.shape[1]}\")\n",
    "print(f\"Embedding contribution: {(tcr_emb_importance + epitope_emb_importance)/total_importance*100:.1f}%\")\n",
    "print(f\"Categorical contribution: {categorical_importance/total_importance*100:.1f}%\")\n",
    "\n",
    "# Performance comparison hint\n",
    "print(f\"\\nModel uses sequence embeddings + categorical features (MHC, TRBV, TRBJ)\")\n",
    "print(f\"This shows the combined predictive power of sequence and context information\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_model('lightgbm_tcr_epitope_model.txt')\n",
    "# print(\"\\nModel saved to 'lightgbm_tcr_epitope_model.txt'\")\n",
    "\n",
    "# === Additional Diagnostic Analysis ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check class distribution\n",
    "print(\"1. CLASS DISTRIBUTION:\")\n",
    "print(f\"Training set:\")\n",
    "train_class_dist = train_df[target_col].value_counts(normalize=True)\n",
    "print(train_class_dist)\n",
    "print(f\"Validation set:\")\n",
    "val_class_dist = valid_df[target_col].value_counts(normalize=True)\n",
    "print(val_class_dist)\n",
    "print(f\"Test set:\")\n",
    "test_class_dist = test_df[target_col].value_counts(normalize=True)\n",
    "print(test_class_dist)\n",
    "\n",
    "# 2. Check prediction probabilities distribution\n",
    "print(f\"\\n2. PREDICTION PROBABILITY DISTRIBUTION:\")\n",
    "print(f\"Validation predictions - Min: {y_val_prob.min():.4f}, Max: {y_val_prob.max():.4f}, Mean: {y_val_prob.mean():.4f}\")\n",
    "print(f\"Test predictions - Min: {y_pred.min():.4f}, Max: {y_pred.max():.4f}, Mean: {y_pred.mean():.4f}\")\n",
    "\n",
    "# 3. Check how many predictions are above different thresholds\n",
    "print(f\"\\n3. PREDICTIONS ABOVE DIFFERENT THRESHOLDS:\")\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for thresh in thresholds:\n",
    "    val_above = (y_val_prob > thresh).sum()\n",
    "    test_above = (y_pred > thresh).sum()\n",
    "    print(f\"Threshold {thresh}: Validation={val_above}/{len(y_val_prob)} ({val_above/len(y_val_prob)*100:.1f}%), Test={test_above}/{len(y_pred)} ({test_above/len(y_pred)*100:.1f}%)\")\n",
    "\n",
    "# 4. Find optimal threshold using validation set\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "print(f\"\\n4. OPTIMAL THRESHOLD ANALYSIS:\")\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_val_true, y_val_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "f1_scores = f1_scores[~np.isnan(f1_scores)]  # Remove NaN values\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds_pr[optimal_idx] if optimal_idx < len(thresholds_pr) else 0.5\n",
    "\n",
    "print(f\"Optimal threshold for F1: {optimal_threshold:.4f}\")\n",
    "print(f\"Max F1 score achievable: {f1_scores[optimal_idx]:.4f}\")\n",
    "\n",
    "# 5. Evaluate with optimal threshold\n",
    "y_val_pred_optimal = (y_val_prob > optimal_threshold).astype(int)\n",
    "y_test_pred_optimal = (y_pred > optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n5. PERFORMANCE WITH OPTIMAL THRESHOLD ({optimal_threshold:.4f}):\")\n",
    "print(\"Validation:\")\n",
    "val_f1_optimal = f1_score(y_val_true, y_val_pred_optimal)\n",
    "val_macro_f1_optimal = f1_score(y_val_true, y_val_pred_optimal, average='macro')\n",
    "val_precision_optimal = precision_score(y_val_true, y_val_pred_optimal, zero_division=0)\n",
    "val_recall_optimal = recall_score(y_val_true, y_val_pred_optimal, zero_division=0)\n",
    "print(f\"Precision: {val_precision_optimal:.4f}, Recall: {val_recall_optimal:.4f}\")\n",
    "print(f\"F1: {val_f1_optimal:.4f}, Macro-F1: {val_macro_f1_optimal:.4f}\")\n",
    "\n",
    "print(\"Test:\")\n",
    "test_f1_optimal = f1_score(y_true, y_test_pred_optimal)\n",
    "test_macro_f1_optimal = f1_score(y_true, y_test_pred_optimal, average='macro')\n",
    "test_precision_optimal = precision_score(y_true, y_test_pred_optimal, zero_division=0)\n",
    "test_recall_optimal = recall_score(y_true, y_test_pred_optimal, zero_division=0)\n",
    "print(f\"Precision: {test_precision_optimal:.4f}, Recall: {test_recall_optimal:.4f}\")\n",
    "print(f\"F1: {test_f1_optimal:.4f}, Macro-F1: {test_macro_f1_optimal:.4f}\")\n",
    "\n",
    "# 6. Class weight suggestion\n",
    "print(f\"\\n6. CLASS WEIGHT SUGGESTION:\")\n",
    "neg_count = (train_df[target_col] == 0).sum()\n",
    "pos_count = (train_df[target_col] == 1).sum()\n",
    "class_weight_ratio = neg_count / pos_count\n",
    "print(f\"Negative samples: {neg_count}\")\n",
    "print(f\"Positive samples: {pos_count}\")\n",
    "print(f\"Imbalance ratio: {class_weight_ratio:.2f}:1\")\n",
    "print(f\"Consider adding 'scale_pos_weight': {class_weight_ratio:.2f} to LightGBM params\")\n",
    "\n",
    "print(f\"\\n7. RECOMMENDATIONS:\")\n",
    "print(f\"- Compare this model with embeddings-only version to see categorical feature impact\")\n",
    "print(f\"- Consider using threshold {optimal_threshold:.3f} instead of 0.5\")\n",
    "print(f\"- Categorical features contribute {categorical_importance/total_importance*100:.1f}% of importance\")\n",
    "print(f\"- If precision/recall are still low, consider class balancing techniques\")\n",
    "print(f\"- The combination of embeddings + categorical may improve generalization\")\n",
    "\n",
    "# 8. Compare with embeddings-only baseline\n",
    "print(f\"\\n8. MODEL COMPARISON INSIGHTS:\")\n",
    "print(f\"This model includes both sequence embeddings AND categorical features:\")\n",
    "print(f\"  - TCR + Epitope embeddings: {(tcr_emb_importance + epitope_emb_importance)/total_importance*100:.1f}%\")\n",
    "print(f\"  - MHC + TRBV + TRBJ features: {categorical_importance/total_importance*100:.1f}%\")\n",
    "print(f\"Compare AUC with embeddings-only model to quantify categorical feature value\")\n",
    "print(f\"If categorical features show high importance, they're capturing crucial biological context\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
